Planetary-scale computation takes different forms at different scales: energy grids and mineral sourcing; cloud infrastructure; urban software and public service privatization; massive universal addressing systems; interfaces drawn by the augmentation of the hand, of the eye, or dissolved into objects; users both overdetermined by self-quantification and exploded by the arrival of legions of nonhuman users (sensors, cars, robots). Instead of seeing the various species of contemporary computational technologies as so many different genres of machines, spinning out on their own, we should instead see them as forming the body of an accidental megastructure. Perhaps these parts align, layer by layer, into something not unlike a vast (if also incomplete), pervasive (if also irregular) software and hardware Stack. This model is of a Stack that both does and does not exist as such: it is a machine that serves as a schema, as much as it is a schema of machines.  As such, perhaps the image of a totality that this conception provides would—as theories of totality have before—make the composition of new governmentalities and new sovereignties both more legible and more effective.
My interest in the geopolitics of planetary-scale computation focuses less on issues of personal privacy and state surveillance than on how it distorts and deforms traditional Westphalian modes of political geography, jurisdiction, and sovereignty, and produces new territories in its image. It draws from (and against) Carl Schmitt’s later work on The Nomos of the Earth, and from his (albeit) flawed history of the geometries of geopolitical architectures.2“Nomos” refers to the dominant and essential logic to the political subdivisions of the earth (of land, seas, and/or air, and now also of the domain that the US military simply calls “cyber”) and to the geopolitical order that stabilizes these subdivisions accordingly. Today, as thenomos that was defined by the horizontal loop geometry of the modern state system creaks and groans, and as “Seeing like a State” takes leave of that initial territorial nest—both with and against the demands of planetary-scale computation3—we wrestle with the irregular abstractions of information, time, and territory, and the chaotic de-lamination of (practical) sovereignty from the occupation of place. For this, a nomos of the Cloud would, for example, draw jurisdiction not only according to the horizontal subdivision of physical sites by and for states, but also according to the vertical stacking of interdependent layers on top of one another: two geometries sometimes in cahoots, sometimes completely diagonal and unrecognizable to one another.4

The Stack, in short, is that new nomos rendered now as vertically thickened political geography. In my analysis, there are six layers to this Stack: Earth, Cloud, City, Address, Interface, and User. Rather than demonstrating each layer of the Stack as a whole, I’ll focus specifically on the Cloud and the User layers, and articulate some alternative designs for these layers and for the totality (or even better, for the next totality, the nomos to come). The Black Stack, then, is to the Stack what the shadow of the future is to the form of the present. The Black Stack is less the anarchist stack, or the death-metal stack, or the utterly opaque stack, than the computational totality-to-come, defined at this moment by what it is not, by the empty content fields of its framework, and by its dire inevitability. It is not the platform we have, but the platform that might be. That platform would be defined by the productivity of its accidents, and by the strategy for which whatever may appear at first as the worst option (even evil) may ultimately be where to look for the best way out. It is less a “possible future” than an escape from the present.

The platforms of the Cloud layer of the Stack are structured by dense, plural, and noncontiguous geographies, a hybrid of US super-jurisdiction and Charter Cities, which have carved new partially privatized polities from the whole cloth of de-sovereigned lands. But perhaps there is more there.

The immediate geographical drama of the Cloud layer is seen most directly in the ongoing Sino-Google conflicts of 2008 to the present: China hacking Google, Google pulling out of China, the NSA hacking China, the NSA hacking Google, Google ghostwriting books for the State Department, and Google wordlessly circumventing the last instances of state oversight altogether, not by transgressing them but by absorbing them into its service offering. Meanwhile, Chinese router firmware bides its time.

The geographies at work are often weird. For example, Google filed a series of patents on offshore data centers, to be built in international waters on towers using tidal currents and available water to keep the servers cool. The complexities of jurisdiction suggested by a global Cloud piped in from non-state space are fantastic, but they are now less exceptional than exemplary of a new normal. Between the “hackers” of the People’s Liberation Army and Google there exists more than a standoff between the proxies of two state apparatuses. There is rather a fundamental conflict over the geometry of political geography itself, with one side bound by the territorial integrity of the state, and the other by the gossamer threads of the world’s information demanding to be “organized and made useful.” This is a clash between two logics of governance, two geometries of territory: one a subdivision of the horizontal, the other a stacking of vertical layers; one a state, the other a para-state; one superimposed on top of the other at any point on the map, and never resolving into some consensual cosmopolitanism, but rather continuing to grind against the grain of one another’s planes. This characterizes the geopolitics of our moment (this, plus the gravity of generalized succession, but the two are interrelated).

From here we see that contemporary Cloud platforms are displacing, if not also replacing, traditional core functions of states, and demonstrating, for both good and ill, new spatial and temporal models of politics and publics. Archaic states drew their authority from the regular provision of food. Over the course of modernization, more was added to the intricate bargains of Leviathan: energy, infrastructure, legal identity and standing, objective and comprehensive maps, credible currencies, and flag-brand loyalties. Bit by bit, each of these and more are now provided by Cloud platforms, not necessarily as formal replacements for the state versions but, like Google ID, simply more useful and effective for daily life. For these platforms, the terms of participation are not mandatory, and because of this, their social contracts are more extractive than constitutional. The Cloud Polis draws revenue from the cognitive capital of its Users, who trade attention and microeconomic compliance in exchange for global infrastructural services, and in turn, it provides each of them with an active discrete online identity and the license to use this infrastructure.

Looking toward the Black Stack, we observe that new forms of governmentality arise through new capacities to tax flows (at ports, at gates, on property, on income, on attention, on clicks, on movement, on electrons, on carbon, and so forth). It is not at all clear whether, in the long run, Cloud platforms will overwhelm state control on such flows, or whether states will continue to evolve into Cloud platforms, absorbing the displaced functions back into themselves, or whether both will split or rotate diagonally to one another, or how deeply what we may now recognize as the surveillance state (US, China, and so forth) will become a universal solvent of compulsory transparency and/or a cosmically opaque megastructure of absolute paranoia, or all of the above, or none of the above.

Between the state, the market, and the platform, which is better designed to tax the interfaces of everyday life and draw sovereignty thereby? It is a false choice to be sure, but one that raises the question of where to locate the proper site of governance as such. What would we mean by “the public” if not that which is constituted by such interfaces, and where else should “governance”—meant here as the necessary, deliberate, and enforceable composition of durable political subjects and their mediations—live if not there? Not in some obtuse chain of parliamentary representation, nor in some delusional monadic individual unit, nor in some sad little community consensus powered by moral hectoring, but instead in the immanent, immediate, and exactly present interfaces that cleave and bind us. Where should sovereignty reside if not in what is in-between us—derived not from each of us individually but from what draws the world through us?

For this, it’s critical to underscore that Cloud platforms (including sometimes state apparatuses) are exactly that: platforms. It is important as well to recognize that “platforms” are not only a technical architecture; they are also an institutional form. They centralize (like states), scaffolding the terms of participation according to rigid but universal protocols, even as they decentralize (like markets), coordinating economies not through the superimposition of fixed plans but through interoperable and emergent interaction. Next to states and markets, platforms are a third form, coordinating through fixed protocols while scattering free-range Users watched over in loving, if also disconcertingly omniscient, grace. In the platform-as-totality, drawing the interfaces of everyday life into one another, the maximal state and the minimal state, Red Plenty and Google Gosplan, start to look weirdly similar.

Our own subjective enrollment in this is less as citizens of a polis or as homo economicuswithin a market, but rather as Users of a platform. As I see it, the work of geopolitical theory is to develop a proper history, typology, and program for such platforms. These would not be a shorthand for Cloud Feudalism (nor for the network politics of the “multitude”) but models for the organization of durable alter-totalities which command the force of law, if not necessarily its forms and formality. Our understanding of the political economy of platforms demands its own Hobbes, Marx, Hayek, and Keynes.

One of the useful paradoxes of the User’s position as a political subject is the contradictory impulse directed simultaneously toward his artificial over-individuation and his ultimatepluralization, with both participating differently in the geopolitics of transparency. For example, the Quantified Self movement (a true medical theology in California) is haunted by this contradiction. At first, the intensity and granularity of a new informational mirror image convinces the User of his individuated coherency and stability as a subject. He is flattered by the singular beauty of his reflection, and this is why QSelf is so popular with those inspired by an X-Men reading of Atlas Shrugged. But as more data is added to the diagram that quantifies the outside world’s impact on his person—the health of the microbial biome in his gut, immediate and long-term environmental conditions, his various epidemiological contexts, and so on—the quality of everything that is “not him” comes to overcode and overwhelm any notion of himself as a withdrawn and self-contained agent. Like Theseus’s Paradox—where after every component of a thing has been replaced, nothing original remains but a metaphysical husk—the User is confronted with the existential lesson that at any point he is only the intersection of many streams. At first, the subject position of the User overproduces individual identity, but in the continuance of the same mechanisms, it then succeeds in exploding it.

The geopolitics of the User we have now is inadequate, including its oppositional modes. The Oedipal discourse of privacy and transparency in relation to the Evil Eye of the uninvited stepfather is a necessary process toward an alterglobalism, but it has real limits worth spelling out. A geopolitics of computation predicated at its core upon the biopolitics of privacy, of self-immunization from any compulsory appearance in front of publics, of platforms, of states, of Others, can sometimes also serve a psychological internalization of a now-ascendant general economy of succession, castration anxiety—whatever. The result is the pre-paranoia of withdrawal into an atomic and anomic dream of self-mastery that elsewhere we call the “neoliberal subject.”

The space in which the discursive formation of the subject meets the technical constitution of the User enjoys a much larger horizon than the one defined by these kinds of individuation. Consider, for example, proxy users. uProxy, a project supported by Google Ideas, is a browser modification that lets users easily pair up across distances to allow someone in one location (trapped in the Bad Internets) to send information unencumbered through the virtual position of another User in another location (enjoying the Good Internets). Recalling the proxy servers set up during the Arab Spring, one can see how Google Ideas (Jared Cohen’s group) might take special interest in baking this into Chrome. For Sino-Google geopolitics, the platform could theoretically be available at a billion-user scale to those who live in China, even if Google is not technically “in China,” because those Users, acting through and as foreign proxies, are themselves, as far as internet geography is concerned, both in and not in China. Developers of uProxy believe that it would take two simultaneous and synchronized man-in-the-middle attacks to hack the link, and at a population scale that would prove difficult even for the best state actors, for now. More disconcerting perhaps is that such a framework could just as easily be used to withdraw data from a paired site—a paired “user”—which for good reasons should be left alone.

Some plural User subject that is conjoined by a proxy link or other means could be composed of different types of addressable subjects: two humans in different countries, or a human and a sensor, a sensor and a bot, a human and a robot and a sensor, a whatever and a whatever. In principle, any one of these subcomponents could not only be part of multiple conjoined positions, but might not even know or need to know which meta-User they contribute to, any more than the microbial biome in your gut needs to know your name. Spoofing with honeypot identities, between humans and nonhumans, is measured against the theoretical address space of IPv6 (roughly 1023 addresses per person) or some other massive universal addressing scheme. The abyssal quantity and range of “things” that could, in principle, participate in these vast pluralities includes real and fictional addressable persons, objects, and locations, and even addressable mass-less relations between things, any of which could be a sub-User in this Internet of Haeccities.

So while the Stack (and the Black Stack) stage the death of the User in one sense—the eclipse of a certain resolute humanism—they do so because they also bring the multiplication and proliferation of other kinds of nonhuman Users (including sensors, financial algorithms, and robots from nanometric to landscape scale), any combination of which one might enter into a relationship with as part of a composite User. This is where the recent shift by major Cloud platforms into robotics may prove especially vital, because—like Darwin’s tortoises finding their way to different Galapagos islands—the Cambrian explosion in robotics sees speciation occur in the wild, not just in the lab, and with “us” on “their” inside, not on the outside. As robotics and Cloud hardware of all scales blend into a common category of machine, it will be unclear in general human-robotic interaction whether one is encountering a fully autonomous, partially autonomous, or completely human-piloted synthetic intelligence. Everyday interactions replay the Turing Test over and over. Is there a person behind this machine, and if so, how much? In time, the answer will matter less, and the postulation of human (or even carbon-based life) as the threshold measure of intelligence and as the qualifying gauge of a political ethics may seem like tasteless vestigial racism, replaced by less anthropocentric frames of reference.

The position of the User then maps only very incompletely onto any one individual body. From the perspective of the platform, what looks like one is really many, and what looks like many may only be one. Elaborate schizophrenias already take hold in our early negotiation of these composite User positions. The neoliberal subject position makes absurd demands on people as Users, as Quantified Selves, as SysAdmins of their own psyche, and from this, paranoia and narcissism are two symptoms of the same disposition, two functions of the same mask. For one, the mask works to pluralize identity according to the subjective demands of the User position as composite alloy; and for another, it defends against those same demands on behalf of the illusory integrity of a self-identity fracturing around its existential core. Ask yourself: Is that User “Anonymous” because he is dissolved into a vital machinic plurality, or because public identification threatens individual self-mastery, sense of autonomy, social unaccountability, and so forth? The former and the latter are two very different politics, yet they use the same masks and the same software suite. Given the schizophrenic economy of the User—first over-individuated and then multiplied and de-differentiated—this really isn’t an unexpected or neurotic reaction at all. It is, however, fragile and inadequate.

In the construction of the User as an aggregate profile that both is and is not specific to any one entity, there is no identity to deduce other than the pattern of interaction between partial actors. We may find, perhaps ironically, that the User position of the Stack actually has far less in common with the neoliberal form of the subject than some of today’s oppositionalist formats for political subjectivity that hope (quite rightly) to challenge, reform, and resist the State Stack as it is currently configuring itself. However, something like a Digital Bill of Rights for Users, despite its cosmopolitan optimism, becomes a much more complicated, fragile, and limited solution when the discrete identification of a User is both so heterogeneous and so fluid. Are all proxy composite users one User? Is anything with an IP address a User? If not, why not? If this throne is reserved for one species—humans—when is any one animal of that species being a User, and when is it not? Is it a User anytime that it is generating information? If so, that policy would in practice crisscross and trespass some of our most basic concepts of the political, and for that reason alone it may be a good place to start.

In addition to the fortification of the User as a geopolitical subject, we also require a redefinition of the political subject in relation to the real operations of the User, one that is based not on homo economicus, nor on parliamentary liberalism, nor on post-structuralist linguistic reduction, nor on the will to secede into the moral safety of individual privacy and withdraw from coercion. Instead, this definition should focus on composing and elevating sites of governance from the immediate, suturing, interfacial material between subjects, in the stitches and the traces and the folds of interaction between bodies and things at a distance, congealing into different networks demanding very different kinds of platform sovereignty.

I will conclude with some thoughts on the Stack-we-have and on the Black Stack, the generic figure for its alternative totalities: the Stack-to-come. The Stack-we-have is defined not only by its form, its layers, its platforms, and their interrelations, but also by its content. As leak after leak has made painfully clear, its content is also the content of our daily communications, now weaponized against us. If the panopticon effect is when you don’t know if you are being watched or not, and so you behave as if you are, then the inverse panopticon effect is when you know you are being watched but act as if you aren’t. This is today’s surveillance culture: exhibitionism in bad faith. The emergence of Stack platforms doesn’t promise any solution, or even any distinctions between friend and enemy within this optical geopolitics. At some dark day in the future, when considered versus the Google Caliphate, the NSA may even come to be seen by some as the “public option.” “At least it is accountable in principle to some parliamentary limits,” they will say, “rather than merely stockholder avarice and flimsy user agreements.”

If we take 9/11 and the rollout of the Patriot Act as Year Zero for the USA’s massive data gathering, encapsulation, and digestion campaign (one that we are only now beginning to comprehend, even as parallel projects from China, Russia, and Europe are sure to come to light in time), then we can imagine the entirety of network communication for the last decade—the Big Haul—as a single, deep-and-wide digital simulation of the world (or a significant section of it). It is an archive, a library of the real. Its existence as the purloined property of a state, just as a physical fact, is almost occult. Almost.

The geophilosophical profile of the Big Haul, from the energy necessary to preserve it to its governing instrumentality understood as both a text (a very large text) and as a machine with various utilities, overflows the traditional politics of software. Its story is much more Borges than Lawrence Lessig. As is its fate. Can it be destroyed? Is it possible to delete this simulation, and is it desirable to do so? Is there a trash can big enough for the Big Delete? Even if the plug could be pulled on all future data hauls, surely there must be a backup somewhere, the identical double of the simulation, such that if we delete one, the other will forever haunt history until it is rediscovered by future AI archaeologists interested in their own Paleolithic origins. Would we bury it, even if we could? Would we need signs around it like those designed for the Yucca Mountain nuclear waste disposal site that warn off unknowable future excavations? Those of us “lucky” enough to be alive during this fifteen-year span would enjoy a certain illegible immortality, curiosities to whatever meta-cognitive entity pieces us back together using our online activities, both public and private, proud and furtive, each of us rising again centuries from now, each of us a little Ozymandias of cat videos and Pornhub.

In light of this, the Black Stack could come to mean very different things. On the one hand, it would imply that this simulation is opaque and unmappable—not disappeared, but ultimately redacted entirely. It could imply that, from the ruined fragments of this history, another coherent totality can be carved against the grain, even from the deep recombinancy at and below the Earth layer of the Stack. Its blackness is the surface of a world that can no longer be composed by addition because it is so absolutely full, overwritten, and overdetermined, that to add more is just so much ink in the ocean. Instead of tabula rasa, this tabula plenus allows for creativity and figuration only by subtraction, like scratching paint from a canvas—only by carving away, by death, by replacement.

The structural logic of any Stack system allows for the replacement of whatever occupies one layer with something else, and for the rest of the architecture to continue to function without pause. For example, the content of any one layer—Earth, Cloud, City, Address, Interface, User—could be replaced (including the masochistic hysterical fiction of the individual User, both neoliberal and neo-other-things), while the rest of the layers remain a viable armature for global infrastructure. The Stack is designed to be remade. That is its technical form, but unlike replacing copper wire with fiber optics in the transmission layer of TCP/IP, replacing one kind of User with another is more difficult. Today, we are doing it by adding more and different kinds of things into the User position, as described above. We should, however, also allow for more comprehensive displacements, not just by elevating things to the status of political subjects or technical agents, but by making way for genuinely posthuman and ahuman positions.

In time, perhaps at the eclipse of the Anthropocene, the historical phase of Google Gosplan will give way to stateless platforms for multiple strata of synthetic intelligence and biocommunication to settle into new continents of cyborg symbiosis. Or perhaps instead, if nothing else, the carbon and energy appetite osf this ambitious embryonic ecology will starve its host.

For some dramas, but hopefully not for the fabrication of the Stack-to-come (Black or otherwise), a certain humanism and companion figure of humanity still presumes its traditional place in the center of the frame. We must let go of the demand that any Artificial Intelligence arriving at sentience or sapience must care deeply about humanity—us specifically—as the subject and object of its knowing and its desire. The real nightmare, worse than the one in which the big machine wants to kill you, is the one in which it sees you as irrelevant, or as not even a discrete thing to know. Worse than being seen as an enemy is not being seen at all. As Eliezer Yudkowsky puts it, “The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.”6

One of the integral accidents of the Stack may be an anthrocidal trauma that shifts us from a design career as the authors of the Anthropocene, to the role of supporting actors in the arrival of the Post-Anthropocene. The Black Stack may also be black because we cannot see our own reflection in it. In the last instance, its accelerationist geopolitics is less eschatological than chemical, because its grounding of time is based less on the promise of historical dialectics than on the rot of isotope decay. It is drawn, I believe, by an inhuman and inhumanist molecular form-finding: pre-Cambrian flora changed into peat oil changed into children’s toys, dinosaurs changed into birds changed into ceremonial headdresses, computation itself converted into whatever meta-machine comes next, and Stack into Black Stack.

Through the chapters that follow, we demonstrate that such questioning is not the end of anthropology but, to the contrary, a fruitful endeavor leading to the discovery of new ends and purposes for our enduring commitment to engage and interpret other lifeways. A critical examination of the “human” sheds light on how the anthropocentric presumptions of much anthropology ignore not just the “unhuman” but also the “animal” and the “not-quitehuman” (transgendered, disabled, or psychologically impaired persons), inevitably leading to a challenge, and perhaps an outright rejection, of the whole category of the human, at least as a core concept for anthropological theory. Anthropology is only one academic discipline currently engaged with the posthuman (Wolfe 2010), but arguably anthropology has the most to contribute to such debates through ethnographic engagement with cultural worlds in which Western Enlightenment definitions and exclusions (Latour 1993) are not so prominent. In this regard, the essays here demonstrate that new forms of ethnographic engagement with “unhuman” populations (as in the chapters by Heckenberger, Whitehead, and Wisniewski) can inform and be informed by studies of online phenomena that also challenge and subvert traditional notions of the human. The chapters here illustrate emergent cultural contexts in which embodied, “rational” individuals are but one of the forms of agency present in virtual and socially occluded worlds. As Matt Bernius demonstrates, software programs create chatbots, spambots, searchbots, and even ballot-stuffing bots, some of which are fully equipped to interact with “real humans” socially, sexually, and financially. Such bots appear alongside and engage with simulacra of our offline selves.

The theme of these chapters—human no more?—resonates as a question throughout. Although Whitehead answers firmly in the affirmative and eagerly embraces the posthuman as a potential liberation from the late capitalist disciplines of the corporeal and the mental, others more cautiously question whether we are in fact posthuman at all and whether we should radically rethink anthropology. This is partly the reaction of Anne Allison (whose afterword was originally a response to the American Anthropological Association (AAA) panel at which these chapters were first presented), but her highly constructive and open-ended engagement with the question of “humanity” contrasts with other commentators (Boellstorff 2008) who find the prospect of the posthuman threatening, both subjectively and with regard to the preservation of the iconic founding figures of the discipline. Just as the idea of the “postmodern” provoked surprisingly conservative reactions among otherwise innovative thinkers such as Johannes Fabian, Marshall Sahlins, and Eric Wolf (Whitehead 2004), it is important to distinguish the critique of anthropology’s intellectual frameworks, which the asking of questions about the “human” permits, from the assumption that “posthumanism” is a disguised political gambit aimed at taking over the profession. Attempting to avoid such pitfalls, and in a similar vein to Allison, Tufekci notes that the first symbols ever created allowed for a form of “disembodiment” by separating the thought from the human, arguing that “the essence of humanity is that we have always been both symbolic and embodied.” “We were always human,” she suggests. “Or maybe, said alternatively, we were always posthuman.” Tufekci and others in this volume discover that even in these virtual worlds that create always expanding possibilities for disembodied sociality, embodiment remains crucial. It is the body that “centers and unites us,” even as we play with different social roles and personas. “Typing on a keyboard,” Tufekci suggests, does not “create an ontological split within the body.” But, as Tufekci rightly points out, this still begs the question of what we are to make of emerging technologies that now promise to immerse us in fully realistic simulated worlds—skin suits, goggles, and other devices that will bring digital inputs seamlessly into our increasingly augmented intelligence—or those futuristic Kurzwellian visions of nanobots swarming through our bodies, giving us access to all the information on the web from inside our own skin while repairing and rebuilding our cells and transporting us into virtual worlds whenever we desire (Kurzwell 2005).

Nonetheless, the bodily disciplines imposed by subjective engagements through such technological devices do represent a historical and cultural rupture and disjuncture. Thus, it is not our potential forms of disembodiment that make us posthuman but rather the way in which this historical movement away from prior cultural forms of embodiment are understood. As a result, Tufekci necessarily oscillates between the formulation “we were always human” and the formulation “we were always posthuman,” precisely because the notion of the “human” is always a contingent category and different regimes of “humanity” have been deployed throughout history to produce the exclusions and inclusions so necessary for the construction of power through difference. The whole realm of ecstatic experience through ritual and shamanism, for example, is a perfect example of the persistent presence of an instability in the human/non-human boundary and in ideas about embodiment. Consequently, the ethnographic literature on shamanism, particularly from Amazonia (see Whitehead, this volume), has been inspirational and reinvigorating for this kind of discussion, especially for some of our authors (see also Alemán, Hoesterey, Wisniewski). Bringing not just cross-cultural but historical sensibilities to our analyses thus allows us to see both the contingency of current posthuman forms and also how there have been perhaps many “posthumanisms.”

Matthew Bernius provides an example that is especially “good to think” in this regard by examining how the “human” is manufactured in the creation of artificial intelligence (AI). As he demonstrates, AI-bots are becoming increasingly common in chatrooms, creating a social space in which not all “subjects” (or apparent subjects) are human. In a second example, Bernius looks at the Virtual Peers project at Northwestern University, where animated AI characters on screen are paired with digital objects throughout the physical environment, so that “the virtual peer is a complex system that extends itself into much of the environment in which she and her human interlocutors interact.” As tracking systems, two-dimensional barcodes, sensors, and other digital objects become increasingly part of our everyday lived spaces of sociality, such an example becomes especially important to examine. Jenny Ryan moves us into the realm of the truly “posthuman,” exploring how the dead live on through their digital traces and in virtual spaces where grievers gather to share and post their memories. Perhaps most interesting in this regard is the way in which most posters address their comments directly to the deceased. Here Ryan makes the point that “embodiment” may need to be reconfigured as the highly immersive nature of online interaction can create the sense that the deceased is really “there,” even after death. Perhaps nothing could say this more forcefully than the protest movement against Facebook’s policy of removing personal information from profiles after somebody dies, aptly named “Facebook Memoralization Is Misguided: Dead Friends are Still People.” For decades now scholars have speculated about and documented the possibilities of identity play online in virtual worlds (see, e.g., Rheingold 1993; Turkle 1995), a notion captured in the public imagination by the now classic New Yorker cartoon “On the Internet, nobody knows you are a dog.” But more recent forms of online sociality, such as Facebook, allow for what Zeynep Tufekci calls “grassroots surveillance,” in which ubiquitous peer surveillance of the most mundane activities monitored through status updates, photos, and videos make identity play much harder. In a play on the New Yorker title at a panel discussion Tufekci noted, “On the Internet, everybody knows you are a dog."
Wesch examines one of these new forms in the phenomenon of Anonymous, an “ongoing collective happening” that challenges our traditional notion of identity and group as all interlocutors on the site remain anonymous, communicating through text, links, and imagery. Their peculiar form of sociality, along with the values they express, presents a scathing critique of our cultural obsession with individualism and identity and the cult of celebrity that emerges from this obsession. Moreover, following the furor around Wikileaks and the support Anonymous gave to the figure of Julian Assange, as well as ongoing AnonOps against various governments and their agencies, there is an important way in which Anonymous’s challenges to individualism and identity represent a potentially new form of political engagement and resistance. Traditional notions of human identity become increasingly irrelevant to life as it is lived even as they begin to seem the most pertinent to the procedures of power and governance. As Wesch forcefully illustrates, we must recognize the ways in which “identity” is used as a way of rendering people legible to those who exercise power; and this remains a powerful structural element in current academic practice, no less than in the panopticon of the security state. In this way, questions of identity emerge as less important for scientific fact than for the way in which such notions validate the truth of the cultural quest for ever-expanding discovery and knowledge of others.

Radhika Gajjala has been rethinking “participant observation” during her two decades of research on South Asian diasporas, developing an especially acute sense of how different media “mediate” the communities we study. In this volume, she teams up with her colleague Sue Ellen McComas to explore how Indian diasporas have been mediated across generations, from pre-digital mediated narratives to a study of how members of a diaspora interact as avatars among the scripted objects of Second Life. There she has the opportunity to re-encounter those recurring critical questions that she has raised in earlier work: How do ethnographic practices and the ethnographer evolve in the online context? How are they revolutionized? What constitutes the field? For Gray Graffam, the “field” is the World of Warcraft, a virtual reality and massive social space that to date has more than 10 million players. In adapting ethnography to suit this field, Graffam found that interviews with players outside the game lacked the proper context that provides the performative cues for people to “be themselves” and answer his questions effectively. Fieldwork experiences like this remind us that observation of any kind (online and offline) is by definition utterly dependent on the forms of participation that the ethnographer may choose or have available. Such issues affect ethnography even as it is most traditionally conceived, especially as there is a growing worldwide cultural investment in online life, even among the most remote and marginal populations. In this volume, Stephanie Alemán reports on her experiences in a remote Guyanese village where the Waiwai first encounter the Internet and start to present themselves online. In one telling example, a young Waiwai man presents himself on Facebook “in a Taekwondo uniform and pose, and another, in a gangsta pose with a knit hat, with dark glasses and headphones and making hand signs.” Meanwhile, as the anthropologist, Alemán has collected images of him in more traditional “native activities”: adorned in full black body paint, shooting a bow and arrow, and dancing in the communal house. Alemán begins to see a clear dilemma for many anthropologists today: how do we represent those who can and will represent themselves? How do we address “their multiple and complex entailments with the regional and global networks to which they not only now have access but actively seek to engage”?

In this context of increasing anthropological anxiety, there is a growing need for the ethnographer to explicitly theorize participation no less adequately than we have painstakingly theorized observation and representation. As Wisniewski notes in this volume about his study of the invisible caboclos of Brazil in which he teamed up with two hippie “vagabond ethnographers,” “theorizing participation will give us a clearer understanding of how ethnographic knowledge is produced, revealing it as a shared product, an intersubjective product, not just of and about humans but of and about human interaction with all categories in a way that does not privilege or overvalue the role of the anthropologist in its production.” The need to theorize participation extends beyond the emerging contexts of online life to all spaces of virtuality where traditional notions of the human limit us or fail altogether. As Michael Heckenberger points out in this volume, not all “virtual realities” are digital or online, so the import of engaging digital subjectivities for wider theory lies precisely in how it may offer the possibility of eluding the Foucaldian/Marcusian nexus of knowledge-powermedia. In São Paulo, public discourses about public health and security create the unhuman nóias, a drug addict who has taken center stage as the persona of the irrational and subnormal in public discourse. Also referred to as zombies, inhabiting a space between life and death, such unhumans invoke different forms and moments of marginalization and oppression than those of the South Asian digital online residents, but nonetheless they experience displacement and disorientation similar to that produced in social encounters within a geographical diaspora. Like the zombies of the cityscapes, the diasporic are also engaged in an attempt to reframe their cultural identities to stave off the threat of cultural—if not physical—genocide through the effects of a rampant globalization.

In fact, such categories have revealed a vast field of social and cultural continuities among the human, animal, and technological. Current notions of biopower, the deployment of artificial intelligence and robotic systems in warfare and law enforcement, and the cultural logic of cinematic and televisual representations are all indications of the urgency with which anthropology should engage its new subjects. Regardless of where one stands on the question of the posthuman, it is clear that a jailbreak from late modernity does not go unchallenged. Stalwart symbols and institutions, such as hungry profiteers and militarized governance, seek to delimit the “human terrain” in both online and offline contexts, creating yet one more piece of the complex contexts and new spaces of cultural and social significance that have proliferated in the last decade. Here the “native populations”—the freaks, geeks, weirdos, techies, and net-addicts— like the savages at the margins of an earlier colonial order, defy simple inclusion into the frameworks of the state and its ethnographies. Living with the Mek, or the caboclos or the Waiwai, no less than the character-subjects of Anonymous or the online worlds of Second Life or My(Death)Space, must now take account of the endless interplay between offline and online subjectivity, while also expanding our notions and understandings of the vast potential of human diversity and social interaction. The stakes are high. Quoting Judith Butler, Heckenberger nicely observes how our cultural frames for thinking the human set limits on certain lives that are not considered lives at all: “[V]iolence against those who are already not quite living, living in a state of suspension between life and death, leaves a mark that is no mark.” These are the “killable bodies” discussed by Giorgio Agamben (1995) in his characterization of contemporary power and governance. As Wisniewski notes in this volume, it is time to expand and refine our approach so that we are equipped to grapple with the relationship between humans and technology, while also recognizing that humans are part of much larger systems that include relationships with animals, insects, microorganisms, spirits, and people who are not always considered human by others. And as humans become more digitally connected, we must also recognize that the sociality that emerges from such connections might not always be immediately analogous to traditional social formations and may involve unhuman actors and agencies (which may or may not be conceptualized or treated as human).

This is a book about the things people say about images. It is not primarily concerned with specific pictures and the things people say about them, but rather with the way we talk about the idea of imagery, and all its related notions of picturing, imagining, perceiving, likening, and imitating. It is a book about images, therefore, that has no illustrations except for a few schematic diagrams, a book about vision written as if by a blind author for a blind reader. If it contains any insight into real, material pictures, it is the sort that might come to a blind listener, overhearing the conversation of sighted speakers talking about images. M y hypothesis is that such a listener might see patterns in these conversations that would be invisible to the sighted participant. The book reflects on answers to two questions that come up regularly in these conversations: What is an image? What is the difference between images and words? It attempts to understand the traditional answers to these questions in relation to the human interests that give them urgency in particular situations. Why does it matter what an image is? What is at stake in marking off or erasing the differences between images and words? What are the systems of power and canons of value—that is, the ideologies—that inform the answers to these questions and make them matters of polemical dispute rather than purely theoretical interest?
If all this sounds impossibly comprehensive, it may help to note that this study has very definite limits, both in terms of the questions it raises and the body of texts it considers. Except for the first chapter this is primarily a series of close readings of a few important texts in the theory of imagery, and these readings revolve around two historical centers, one in the late eighteenth century (roughly, the era of the French Revolution and the rise of Romanticism), the other in the era of modern criticism. The aim of these readings is to show how the notion of imagery serves as a kind of relay connecting theories of art, language, and the mind with conceptions of social, cultural, and political value.

My only apology for these strange conjunctions of topics and texts is that they seemed to surface as I pursued the theoretical questions that inspired the study in the first place. Every theoretical answer to the questions, What is an image? Ho w are images different from words? seemed inevitably to fall back into prior questions of value and interest that could only be answered in historical terms. The simplest way of stating this is to admit that a book which began with the intention of producing a valid theory of images became a book about the fear of images. "Iconology" turned out to be, not just the science of icons, but the political psychology of icons, the study of iconophobia, iconophilia, and the struggle between iconoclasm and idolatry. The movement of this book is thus from modern attempts to establish a true theory of imagery (Gombrich, Goodman, the early Wittgenstein) to the "classic" accounts of imagery these theories sought to replace. In the process, my theoretical ambitions have inevitably been chastened by my narrow limits as an intellectual historian. My hope is that this critical fall into the space between theory and history will open up a region for other scholars to explore, and that it will suggest something about the necessary limits of any attempt to provide a theoretical account of symbolic practices.

Two things must immediately strike the notice of anyone who tries to take a general view of the phenomena called by the name of imagery. The first is simply the wide variety of things that go by this name. We speak of pictures, statues, optical illusions, maps, diagrams, dreams, hallucinations, spectacles, projections, poems, patterns, memories, and even ideas as images, and the sheer diversity of this list would seem to make any systematic, unified understanding impossible. The second thing that may strike us is that the calling of all these things by the name of "image" does not necessarily mean that they all have something in common. It might be better to begin by thinking of images as a far-flung family which has migrated in time and space and undergone profound mutations in the process. If images are a family, however, it may be possible to construct some sense of their genealogy. If we begin by looking, not for some universal definition of the term, but at those places where images have differentiated themselves from one another on the basis of boundaries between different institutional discourses, we come up with a family tree something like the following.

The mental and verbal images on the right side of our diagram, for instance, would seem to be images only in some doubtful, metaphork sense. People may report experiencing images in their heads while reading or dreaming, but we have only their word for this; there is no way (so the argument goes) to check up on this objectively. An d even if we trust the reports of mental imagery, it seems clear that they must be different from real, material pictures. Mental images don't seem to be stable and permanent the way real images are, and they vary from one person to the next: if I say "green," some listeners may see green in their mind's eye, but some may see a word, or nothing at all. An d mental images don't seem to be exclusively visual the way real pictures are; they involve all the senses. Verbal imagery, moreover, can involve all the senses, or it may involve no sensory component at all, sometimes suggesting nothing more than a recurrent abstract idea like justice or grace or evil. It is no wonder that literary scholars get very nervous when people start taking the notion of verbal imagery too literally." An d it is hardly surprising that one of the main thrusts of modern psychology and philosophy has been to discredit the notions of both mental and verbal imagery.
Wittgenstein's way of attacking mental imagery is not, however, the direct strategy of denying the existence of such images. He freely concedes that we may have mental images associated with thought or speech, insisting only that these images should not be thought of as private, metaphysical, immaterial entities any more than real images are. Wittgenstein's tactic is to demystify the mental image by bringing it right out in the open where we can see it: "Mental images of colours, shapes, sounds, etc., etc., which play a role in communication by means of language we put in the same category with patches of color actually seen, sounds heard."  It is a bit hard, however, to see how we can put mental and physical images "in the same category." We certainly can't do it by cutting open someone's head to compare mental pictures with the ones on our walls. A better strategy, and more in the Wittgensteinian spirit, would be to examine the ways we put those images "into our heads" in the first place by trying to picture the sort of world in which this move would make sense. I offer the figure on the next page as just such a picture.

But if the key to the recognition of real, material images in the world is our curious ability to say "there" and "not there" at the same time, we must then ask why mental images should be seen as any more—or less—mysterious than "real" images. The problem philosophers and ordinary people have always had with the notion of mental images is that they seem to have a universal basis in real, shared experience (we all dream, visualize, and are capable, in varying degrees, of re-presenting concrete sensations to ourselves), but we cannot point to them and say "There—that is a mental image." Exacdy the same sort of problem occurs, however, if I try to point to a real image and explain what it is to someone who doesn't already know what an image is. I point at Xeuxis's painting and say "There, that is an image." And the reply is, "D o you mean that colored surface?" Or "Do you mean those grapes?"

This is not to be taken as a claim that the mind really is a blank slate or a mirror—only that these are ways the mind is capable of picturing itself. It might picture itself in other ways: as a building, a statue, as an invisible gas or fluid, as a text, a narrative, or a melody, or as nothing in particular. It might decline to have a picture of itself, and refuse all selfrepresentation, just as we can look at a picture, a statue, or a mirror and not see it as a representational object. We might look at mirrors as shiny vertical objects, paintings as masses of colors on flat surfaces. There is no rule that the mind has to picture itself, or see pictures in itself, any more than there is a rule that we must go into a picture gallery, or that once inside we must look at the pictures. If we eliminate the notion that there is something necessary, natural, or automatic about the formation of both mental and material images, then we can do as Wittgenstein suggests, and put them "in the same category" as functional symbols, or, as in our model, in the same logical space. 17 This does not eliminate all differences between mental and physical images, but it may help to demystify the metaphysical or occult quality of this difference, and to allay our suspicion that mental images are somehow improper or illegitimately modeled on the "real thing." The path of derivation from original model to illegitimate analogy could as easily be traced in the opposite direction.

A good way to clarify the relation of mental and physical images is to reflect on the way we have just used a diagram to illustrate the matrix of analogies that connects theories of representation to theories of mind. We might be tempted to say that a mental version of this diagram was in our heads all along, before it appeared on the page, and that it was governing the way we discussed the boundary between mental and physical images. Well, perhaps it was; or perhaps it only occurred to us at a certain point in the discussion, when we began to use words like "boundary line" and "realm." Or perhaps it never occurred to us at all while thinking about these things or writing them down, and it was only later, after many revisions, that it came to mind. Does that mean that the mental diagram was there all along as a kind of unconscious deep structure determining our usage of the word "image"? Or is it a posterior construction, a graphic projection of the logical space implied in our propositions about imagery? In either case we certainly cannot regard the diagram as something mental in the sense of "private" or "subjective"; it is rather something that surfaced in language, and not just my language, but a way of speaking that we inherit from a long tradition of talking about minds and pictures. Our diagram might just as well be called a "verbal image" as a mental one, which brings us to that other notoriously illegitimate branch in the family tree of imagery, the notion of imagery in language.

This view of poetry, and of language in general, as a process of pictorial production and reproduction was accompanied in seventeenthand eighteenth-century English literary theory by a decline in the prestige of rhetorical figures and tropes. The notion of "image" replaced that of "figure," which began to be regarded as a feature of old-fashioned "ornamented" language. The literary style of verbal imagery is "plain" and "perspicuous," a style that reaches right out to objects, representing them (as Addison claims) even more vividly than the objects can represent themselves. This in contrast to the "deceptive ornament" of rhetoric, which is now seen as nothing but a matter of relations among signs. When the rhetorical figures are mentioned, they are either dismissed as the artificial excesses of a prerational, prescientific age, or they are redefined in ways that accommodate them to the hegemony of the verbal image. Metaphors are redefined as "short descriptions"; "allusions and similes are descriptions placed in an opposite point of view . . . and hyperbole is often nothing more than a description carried beyond the bounds of probability." Even abstractions are treated as pictorial, visual objects, projected in the verbal imagery of personification. In Romantic and modern poetics the verbal image retained its hold over the understanding of literary language, and the confused application of the term to both literal and figurative expression continued to encourage a lumping of notions such as description, concrete nouns, tropes, "sensory" terms, and even recurrent semantic, syntactic, or phonemic motifs under the rubric of "imagery." In order to do all this work, however, the notion of imagery had to be sublimated and mystified. Romantic writers typically assimilate mental, verbal, and even pictorial imagery into the mysterious process of "imagination," which is typically defined in contrast to the "mere" recall of mental pictures, the "mere" description of external scenes, and (in painting) the "mere" depiction of external visibilia, as opposed to the spirit, feeling, or "poetry" of a scene.

This progressive sublimation of the image reaches its logical culmination when the entire poem or text is regarded as an image or 'Verbal icon," and this image is defined, not as a pictorial likeness or impression, but as a synchronic structure in some metaphorical space—"that which" (in Pound's words) "presents an intellectual and emotional complex in an instant of time." The Imagists's emphasis on concrete, particular descriptions in their poetry is, by itself, a residue of the eighteenthcentury notion we have seen in Addison that poetry strives to outdo in vividness and immediacy the "images which flow from objects themselves" (Williams's "no ideas but in things" would seem to be another version of this idea). But the distinctive modernist emphasis is on the image as a sort of crystalline structure, a dynamic pattern of the intellectual and emotional energy bodied forth by a poem. Formalist criticism is both a poetics and a hermeneutics for this kind of verbal image, showing us how poems contain their energies in matrices of architectonic tension, and demonstrating the congruence of these matrices with the propositional content of the poem.

If the figure of the pictogram or hieroglyph demands a viewer who knows what to say, it also has a way of shaping the things that can be said. Consider further the ambiguous emblem/signature/ideogram of the petroglyph "eagle." If the warrior is an eagle, or "like" an eagle, or (more likely) if "Eagle himself" goes to war, and returns to tell about it, we can expect the picture to be extended. Eagle will no doubt see his enemies from afar and swoop down on them without warning. The "verbal image" of Eagle is a complex of speech, depiction, and writing that not only describes what he does, but predicts and shapes what he can and will do. It is his "character," a signature that is both verbal and pictorial, both a narrative of his actions and a summation of what he is.

But what exactly is this "spiritual" likeness which is not to be confused with any material image? We should note first that it seems to include a presumption of difference. To say that one tree, or one member of a species of tree, is like another, is not to argue that they are identical but that they are similar in some respects and not in others. Normally, however, we don't say that every likeness is an image. One tree is like another, but we don't call one the image of the other. The word "image" only comes up in relation to this sort of likeness when we try to construct a theory about the way we perceive the likeness between one tree and another. This explanation will typically resort to some intermediate or transcendental object—an idea, form, or mental image—that provides a mechanism for explaining how our categories arise. The "origin of species" is not just a matter of biological evolution then, but of the mechanisms of consciousness as they are described in representational models of the mind.

Is man created in the image of God, then, in that he looks like God, or in that we can say similar things about man and God? Milton wants to have it both ways, a desire we can trace to his rather unorthodox materialism, or perhaps more fundamentally, to a historic transformation in the concept of imagery which tended to identify the notion of spiritual likeness—particularly the "rational soul" that makes man an image of God—with a certain kind of material image. Milton's poetry is the scene of a struggle between iconoclastic distrust of the outward image and iconophilic fascination with its power, a struggle which manifests itself in his practice of proliferating visual images in order to prevent readers from focusing on any particular picture or scene. In order to see how the stage was set for this struggle we need to look more closely at the revolution which identified pictures or "artificial forms" with images as "likenesses" (Maimonides' "specific forms").

The revolution I am thinking of here was, of course, the invention of artificial perspective, first systematized by Alberti in 1435. The effect of this invention was nothing less than to convince an entire civilization that it possessed an infallible method of representation, a system for the automatic and mechanical production of truths about the material and the mental worlds. The best index to the hegemony of artificial perspective is the way it denies its own artificiality and lays claims to being a "natural" representation of "the way things look," "the way we see," or (in a phrase that turns Maimonides on his head) "the way things really are." Aided by the political and economic ascendance of Western Europe, artificial perspective conquered the world o f representation under the banner of reason, science, and objectivity. N o amount of counterdemonstration from artists that there are other ways of picturing what "we really see" has been able to shake the conviction that these pictures have a kind of identity with natural human vision and objective external space. An d the invention of a machine (the camera) built to produce this sort of image has, ironically, only reinforced the conviction that this is the natural mode of representation. What is natural is, evidendy, what we can build a machine to do for us.

If we discount the obvious hostility from Twain and Lessing's comments on the poverty of pictorial expression, we find a rather perspicuous account of what is meant by the notion of painting the invisible. What expression amounts to is the artful planting of certain clues in a picture that allow us to form an act of ventriloquism, an act which endows the picture with eloquence, and particularly with a nonvisual and verbal eloquence. A picture may articulate abstract ideas by means of allegorical imagery, a practice which, as Lessing notes, approaches the notational procedures of writing systems. The image of an eagle may depict a feathered predator, but it expresses the idea of wisdom, and thus works as a hieroglyph. Or we may understand expression in dramatic, oratorical terms, as did the Renaissance humanists who formulated a rhetoric of history painting complete with a language of facial expression and gesture, a language precise enough to let us verbalize what depicted figures are thinking, feeling, or saying. And expression need not be limited to predicates we can attach to pictured objects: the setting, compositional arrangement, and color scheme may all carry expressive charge, so that we can speak of moods and emotional atmospheres whose appropriate verbal counterparts may be something on the order of a lyric poem.

If I seem to be taking Twain's ironic attitude toward the claims of pictorial expression, it is not because I think that expression is impossible or illusory, but because our understanding of it is so often clouded by the same mystique of "natural representation" that obstructs our understanding of mimetic representation. Twain says that the label is worth more, for information, than "a ton of significant expression." But we might ask Twain how much the label would be worth, for information or for anything else, without this picture by Guido Reni, or the entire tradition of representing in pictorial, dramatic, or literary images the story of the Cenci. The painting is a confluence of pictorial and verbal traditions, neither of which is apparent to the innocent eyes of Twain, and so he can scarcely see what it is, much less respond to it.

What are we to make of this contest between the interests of verbal and pictorial representation? I propose that we historicize it, and treat it, not as a matter for peaceful settlement under the terms of some allembracing theory of signs, but as a struggle that carries the fundamental contradictions of our culture into the heart of theoretical discourse itself. The point, then, is not to heal the split between words and images, but to see what interests and powers it serves. This view can only be had, of course, from a standpoint which begins with skepticism about the adequacy of any particular theory of the relation of words and images, but which also preserves an intuitive conviction that there is some difference that is fundamental. It seems to me that Lessing, for instance, is absolutely right insofar as he regards poetry and painting as radically different modes or representation, but that his "mistake" (which theory still participates in) is the reification of this difference in terms of analogous oppositions like nature and culture, space and time.

Emerson once noted that the most fruitful conversations are always between two persons, not three. This principle may help to explain why the dialogue between poetry and painting has tended to dominate general discussions of the arts, and why music has seemed something of an outsider to the conversation. Al l the arts may aspire to the condition of music, but when they set out to argue, poetry and painting hold the stage. One reason for this is that they both lay claim to the same territory (reference, representation, denotation, meaning), a territory that music has tended to renounce. Another reason is that the differences between words and images seem so fundamental. They are not merely different kinds of creatures, but opposite kinds. They attract to their contest all the contrarieties and oppositions that riddle the discourse of criticism, the very discourse that takes as one of its projects a unified theory of the arts, an "aesthetics" which aspires to a synoptic view of artistic signs, a "semiotics" which hopes to comprehend all signs whatsoever.

This gap has two important functions in discussions of the arts and their symbol systems: it lends an air of tough-minded common sense to assertions of difference between the arts, and it gives an air of paradoxical daring and ingenuity to assertions of sameness or transference. The topic of the text-image difference provides an occasion for the exercise of the two great rhetorical skills, wit and judgment, "wit," as Edmund Burke noted, being "chiefly conversant in tracing resemblances," and judgment concerned mainly with "finding differences."1 Since aesthetics and semiotics dream of a theory that will satisfy both the need to discriminate artistic signs and to identify the principles that unite them, both these approaches to the topic have established themselves as traditional alternatives within the discourse of criticism. The mode of wit, the "tracing of resemblances," is the foundation of the ut pictura poesis and "sister arts" tradition in criticism, the construction of analogies or critical conceits that identify points of transference and resemblance between texts and images. Although these conceits are almost always accompanied by acknowledgments of differences between the arts, they are generally perceived as violations of good judgment that criticism ought to correct. Lessing opens the Laocoon by observing that "the first who compared painting with poetry was a man of fine feeling," not a critic or philosopher. 2 H e was, as Lessing goes on to explain, Simonides of Ceos, the legendary founder of the ut pictura poesis tradition. Lessing characterizes Simonides as a man of feeling and wit, "the Greek Voltaire," whose "dazzling antithesis that painting is dumb poetry and poetry speaking painting, stood in no textbook. It was one of those conceits, occurring frequendy in Simonides, the inexactness and falsity of which we feel constrained to overlook for the sake of the truth they contain.

In suggesting that these judicious discriminations are figurative I do not mean to assert that they are simply false, illusory, or without efficacy. O n the contrary, I want to suggest that they are powerful distinctions that effect the way the arts arc practiced and understood. I do mean to imply, however, that they are literally false, or (more generously) figuratively true. M y argument here will be twofold: (i) there is no essential difference between poetry and painting, no difference, that is, that is given for all time by the inherent natures of the media, the objects they represent, or the laws of the human mind; (2) there arc always a number of differences in effect in a culture which allow it to sort out the distinctive qualities o f its ensemble of signs and symbols. These differences, as I have suggested, are riddled with all the antithetical values the culture wants to embrace or repudiate: the parqgone or debate of poetry and painting is never just a contest between two kinds of signs, but a struggle between body and soul, world and mind, nature and culture.

Modern discussions of the relation between texts and images have tended to reduce this question to a problem of grammar. The traditional distinctions expressed in notions like time and space, nature and convention, have, in the work of modern theorists, been replaced by distinctions between different kinds of sign functions and communicative systems. We now speak of the difference between images and texts in terms such as the analogical and the digital, the iconic and the symbolic, the singleand the double-articulated. 1 These terms, drawn from fields such as systems analysis, semiotics, and linguistics, seem to promise a new, more scientific understanding of the boundaries between painting and poetry. They hold out the hope for a more rigorous definition of the difference and, especially in the work of the structuralists, the hope for a systematic way of comparing the arts. Modern theory has, in short, promised something to both sides of the traditional quarrel between the witty comparatists and the judicious differentiators of the arts: to the former it promises a higher level of generality and the prospect of large structural homologies between the arts; to the latter it offers a rigorous taxonomy that allows precise differentiation of sign-types and aesthetic modes.

For an arch-conventionalist like Goodman, the phrase that would probably stand out in this passage is the one that equates "artificial divisions" between the arts with "false" divisions, thus implying that man-made, conventional distinctions are, by virtue of their artificiality, automatically false. The contrast implied between these "false, artificial divisions" and "essential" features founded on "empirical and important facts" sets off a warning bell in the mind of a conventionalist; it also ought to sound an alarm for anyone who dislikes red herrings in arguments, and send them off in search of counterexamples to those "empirical facts" that lead us to Langer's categorical imperative: "there can be no hybrid works." An empirical survey of works that attempt graftings of verbal and pictorial signs (illustrated books, narrative paintings, films and dramas) does not immediately lead us to the conclusion that such hybrids are impossible. Even Langer's own logic of "essential" differences between the arts, aside from what the empirical facts tell us, leads to no such conclusion. One could as easily argue that such differences are a necessary condition for hybridization; the "crossing" of disparate forms to form new, composite unities makes no sense without an established set of differences, artificial or natural, to be overcome. Langer's notorious claim that "there are no happy marriages in art—only successful rape" illustrates perfecdy the sense of violence and violation she associates with the conjunction of artistic media, and hints (rather vividly) at its ideological basis in categories of gender.

Or , more accurately, one might say that the same old distinctions whose inadequacy motivated the search for a "general science of signs" tend to crop up in spite of the best efforts to weed them out. The disparities within the field of iconic signs that lead Eco to regard it as an incoherent category are precisely those sorts of oppositions that have traditionally figured the difference between texts and images. Some icons are "ruled by convention but are at the same time motivated." The word "motivated" in this context stands in the place occupied by terms like "nature" in traditional accounts of the text-image difference: "motivated" signs have a natural, necessary connection with what they signify; "unmotivated" signs are arbitrary and conventional. Eco's observation that icons sometimes seem "to be more firmly linked to the basic mechanisms of perception than to explicit cultural habits" is, similarly, a semiotic redaction of the notion that (some) images are "natural signs," and amounts to a contradiction in terms for a system that begins with the notion of the sign based in language.

There would be nothing wrong with this sort of redescription if it were not advertised as a liberation from metaphysics into a new science. The translation of Hume's laws of association into sign-types or modes of figuration has considerable interest. Among other things, it helps us to see just how riddled with notions of indirect, symbolic mediation are the supposedly "direct" perceptual mechanisms of the empirical tradition. The most striking example of this sort of mediation is, as we have seen, the notion of the mental or perceptual image ("ideas" and "sense-data"), which, on the one hand, seem to guarantee veridical access to the world, on the other hand to indefinitely and irretrievably distance the world through a system of intermediate signs. This double bind may be seen most clearly in the attempt of semioticians to come up with an account of the "photographic sign."

These doubly natural signs, iconic and indexical, then serve as the foundation for all further intellection and discourse. Among other things, they stand as the referents for words, which unlike the ideaimpression-mental image, signify (as Locke puts it) "not by any natural connexion . . . but by a voluntary imposition, whereby such a word is made arbitrarily the mark of such an idea." Ideas, by contrast, are naturally imprinted by experience and reflection: they are natural signs that (ideally) stand behind the arbitrary signs of language. The relation of words and ideas, discourse and thought, turns on the very same hinge that, in semiotics, connects the symbol with the indexical icon, the arbitrary code with the "natural" code.  Small wonder, then, that Roland Bardies finds himself saying the following sorts of things about photographs.

When Wilson catalogues what he shares with his Furby, there are things of the body (the burping) and there are things of the mind. Like many children, he thinks that because Furbies have language, they are more “peoplelike” than a “regular” pet. They arrive speaking Furbish, a language with its own dictionary, which many children try to commit to memory because they would like to meet their Furbies more than half way. The Furby manual instructs children, “I can learn to speak English by listening to you talk. The more you play with me, the more I will use your language.” Actually, Furby English emerges over time, whether or not a child talks to the robot. (Furbies have no hearing or language-learning ability.5) But until age eight, children are convinced by the illusion and believe they are teaching their Furbies to speak. The Furbies are alive enough to need them. Children enjoy the teaching task. From the first encounter, it gives them something in common with their Furbies and it implies that the Furbies can grow to better understand them. “I once didn’t know English,” says one six-year-old. “And now I do. So I know what my Furby is going through.” In the classroom with Furbies, children shout to each other in competitive delight: “My Furby speaks more English than yours! My Furby speaks English.” I have done several studies in which I send Furbies home with schoolchildren, often with the request that they (and their parents) keep a “Furby diary.” In my first study of kindergarten to third graders, I loan the Furbies out for two weeks at a time. It is not a good decision. I do not count on how great will be children’s sense of loss when I ask them to return the Furbies. I extend the length of the loans, often encouraged by parental requests. Their children have grown too attached to give up the robots. Nor are they mollified by parents’ offers to buy them new Furbies. Even more so than with Tamagotchis, children attach to a particular Furby, the one they have taught English, the one they have raised. For three decades, in describing people’s relationships with computers, I have often used the metaphor of the Rorschach, the inkblot test that psychologists use as a screen onto which people can project their feelings and styles of thought. But as children interact with sociable robots like Furbies, they move beyond a psychology of projection to a new psychology of engagement. They try to deal with the robot as they would deal with a pet or a person. Nineyear-old Leah, in an after-school playgroup, admits, “It’s hard to turn it [the Furby] off when it is talking to me.” Children quickly understand that to get the most out of your Furby, you have to pay attention to what it is telling you. When you are with a Furby, you can’t play a simple game of projective make-believe. You have to continually assess your Furby’s “emotional” and “physical” state. And children fervently believe that the child who loves his or her Furby best will be most loved in return.
This mutuality is at the heart of what makes the Furby, a primitive exemplar of sociable robotics, different from traditional dolls. As we’ve seen, such relational artifacts do not wait for children to “animate” them in the spirit of a Raggedy Ann doll or a teddy bear. They present themselves as already animated and ready for relationship. They promise reciprocity because, unlike traditional dolls, they are not passive. They make demands. They present as having their own needs and inner lives. They teach us the rituals of love that will make them thrive. For decades computers have asked us to think with them; these days, computers and robots, deemed sociable, affective, and relational, ask us to feel for and with them. Children see traditional dolls as they want them or need them to be. For example, an eight-year-old girl who feels guilty about breaking her mother’s best crystal pitcher might punish a row of Barbie dolls. She might take them away from their tea party and put them in detention, doing unto the dolls what she imagines should be done unto her. In contrast, since relational artifacts present themselves as having minds and intentions of their own, they cannot be so easily punished for one’s own misdeeds. Two eight-year-old girls comment on how their “regular dolls” differ from the robotic Furbies. The first says, “A regular doll, like my Madeleine doll . . . you can make it go to sleep, but its eyes are painted open, so, um, you cannot get them to close their eyes.... Like a Madeleine doll cannot go, ‘Hello, good morning.’” But this is precisely the sort of thing a Furby can do. The second offers, “The Furby tells you what it wants.” Indeed, Furbies come with manuals that provide detailed marching orders. They want language practice, food, rest, and protestations of love. So, for example, the manual instructs, “Make sure you say ‘HEY FURBY! I love you!’ frequently so that I feel happy and know I’m loved.” There is general agreement among children that a penchant for giving instructions distinguishes Furbies from traditional dolls. A seven-year-old girl puts it this way: “Dolls let you tell them what they want. The Furbies have their own ideas.” A nine-year-old boy sums up the difference between Furbies and his action figures: “You don’t play with the Furby, you sort of hang out with it. You do try to get power over it, but it has power over you too.” Children say that traditional dolls can be “hard work” because you have to do all the work of giving them ideas; Furbies are hard work for the opposite reason. They have plenty of ideas, but you have to give them what they want and when they want it. When children attach to a doll through the psychology of projection, they attribute to the doll what is most on their mind. But they need to accommodate a Furby. This give-and-take prepares children for the expectation of relationship with machines that is at the heart of the robotic moment.

Daisy, six, with a Furby at home, believes that each Furby’s owner must help his or her Furby fulfill its mission to learn about people. “You have to teach it; when you buy it, that is your job.” Daisy tells me that she taught her Furby about Brownie Girl Scouts, kindergarten, and whales. “It’s alive; I teach it about whales; it loves me.” Padma, eight, says that she likes meeting what she calls “Furby requests” and thinks that her Furby is “kind of like a person” because “it talks.” She goes on: “It’s kind of like me because I’m a chatterbox.” After two weeks, it is time for Padma to return her Furby, and afterward she feels regret: “I miss how it talked, and now it’s so quiet at my house.... I didn’t get a chance to make him a bed.” After a month with her Furby, Bianca, seven, speaks with growing confidence about their mutual affection: “I love my Furby because it loves me. . . . It was like he really knew me.”6 She knows her Furby well enough to believe that “it doesn’t want to miss fun . . . at a party.” In order to make sure that her social butterfly Furby gets some rest when her parents entertain late into the evening, Bianca clips its ears back with clothespins to fool the robot into thinking that “nothing is going on . . . so he can fall asleep.” This move is ineffective, and all of this activity is exhausting, but Bianca calmly sums up her commitment: “It takes lots of work to take care of these.” When Wilson, who so enjoys burping in synchrony with his Furby, faces up to the hard work of getting his Furby to sleep, he knows that if he forces sleep by removing his Furby’s batteries, the robot will “forget” whatever has passed between them—this is unacceptable. So Furby sleep has to come naturally. Wilson tries to exhaust his Furby by keeping it up late at night watching television. He experiments with Furby “sleep houses” made of blankets piled high over towers of blocks. When Wilson considers Furby sleep, his thoughts turn to Furby dreams. He is sure his Furby dreams “when his eyes are closed.” What do Furbies dream of? Second and third graders think they dream “of life on their flying saucers.”7 And they dream about learning languages and playing with the children they love.
In the 1980s, the computer toy Merlin made happy and sad noises depending on whether it was winning or losing the sound-and-light game it played with children. Children saw Merlin as “sort of alive” because of how well it played memory games, but they did not fully believe in Merlin’s shows of emotion. When a Merlin broke down, children were sorry to lose a playmate. When a Furby doesn’t work, however, children see a creature that might be in pain. Lily, ten, worries that her broken Furby is hurting. But she doesn’t want to turn it off, because “that means you aren’t taking care of it.” She fears that if she shuts off a Furby in pain, she might make things worse. Two eight-year-olds fret about how much their Furbies sneeze. The first worries that his sneezing Furby is allergic to him. The other fears his Furby got its cold because “I didn’t do a good enough job taking care of him.” Several children become tense when Furbies make unfamiliar sounds that might be signals of distress. I observe children with their other toys: dolls, toy soldiers, action figures. If these toys make strange sounds, they are usually put aside; broken toys lead easily to boredom. But when a Furby is in trouble, children ask, “Is it tired?” “Is it sad?” “Have I hurt it?” “Is it sick?” “What shall I do?” Taking care of a robot is a high-stakes game. Things can—and do—go wrong. In one kindergarten, when a Furby breaks down, the children decide they want to heal it. Ten children volunteer, seeing themselves as doctors in an emergency room. They decide they’ll begin by taking it apart. The proceedings begin in a state of relative calm. When talking about their sick Furby, the children insist that this breakdown does not mean the end: people get sick and get better. But as soon as scissors and pliers appear, they become anxious. At this point, Alicia screams, “The Furby is going to die!” Sven, to his classmates’ horror, pinpoints the moment when Furbies die: it happens when a Furby’s skin is ripped off. Sven considers the Furby as an animal. You can shave an animal’s fur, and it will live. But you cannot take its skin off. As the operation continues, Sven reconsiders. Perhaps the Furby can live without its skin, “but it will be cold.” He doesn’t back completely away from the biological (the Furby is sensitive to the cold) but reconstructs it. For Sven, the biological now includes creatures such as Furbies, whose “insides” stay “all in the same place” when their skin is removed. This accommodation calms him down. If a Furby is simultaneously biological and mechanical, the operation in process, which is certainly removing the Furby’s skin, is not necessarily destructive. Children make theories when they are confused or anxious. A good theory can reduce anxiety.
But some children become more anxious as the operation continues. One suggests that if the Furby dies, it might haunt them. It is alive enough to turn into a ghost. Indeed, a group of children start to call the empty Furby skin “the ghost of Furby” and the Furby’s naked body “the goblin.” They are not happy that this operation might leave a Furby goblin and ghost at large. One girl comes up with the idea that the ghost of the Furby will be less fearful if distrib- uted. She asks if it would be okay “if every child took home a piece of Furby skin.” She is told this would be fine, but, unappeased, she asks the same question two more times. In the end, most children leave with a bit of Furby fur.8 Some talk about burying it when they get home. They leave room for a private ritual to placate the goblin and say good-bye. Inside the classroom, most of the children feel they are doing the best they can with a sick pet. But from outside the classroom, the Furby surgery looks alarming. Children passing by call out, “You killed him.” “How dare you kill Furby?” “You’ll go to Furby jail.” Denise, eight, watches some of the goings-on from the safety of the hall. She has a Furby at home and says that she does not like to talk about its problems as diseases because “Furbies are not animals.” She uses the word “fake” to mean nonbiological and says, “Furbies are fake, and they don’t get diseases.” But later, she reconsiders her position when her own Furby’s batteries run out and the robot, so chatty only moments before, becomes inert. Denise panics: “It’s dead. It’s dead right now.... Its eyes are closed.” She then declares her Furby “both fake and dead.” Denise concludes that worn-out batteries and water can kill a Furby. It is a mechanism, but alive enough to die. Linda, six, is one of the children whose family has volunteered to keep a Furby for a twoweek home study. She looked forward to speaking to her Furby, sure that unlike her other dolls, this robot would be worth talking to. But on its very first night at her home, her Furby stops working: “Yeah, I got used to it, and then it broke that night—the night that I got it. I felt like I was broken or something.... I cried a lot. . . . I was really sad that it broke, ’cause Furbies talk, they’re like real, they’re like real people.” Linda is so upset about not protecting her Furby that when it breaks she feels herself broken. Things get more complicated when I give Linda a new Furby. Unlike children like Zach who have invested time and love in a “first Furby” and want no replacements, Linda had her original Furby in working condition for only a few hours. She likes having Furby #2: “It plays hide-and-seek with me. I play red light, green light, just like in the manual.” Linda feeds it and makes sure it gets enough rest, and she reports that her new Furby is grateful and affectionate. She makes this compatible with her assessment of a Furby as “just a toy” because she has come to see gratitude, conversation, and affection as something that toys can manage. But now she will not name her Furby or say it is alive. There would be risk in that: Linda might feel guilty if the new Furby were alive enough to die and she had a replay of her painful first experience.
When a mechanism breaks, we may feel regretful, inconvenienced, or angry. We debate whether it is worth getting it fixed. When a doll cries, children know that they are themselves creating the tears. But a robot with a body can get “hurt,” as we saw in the improvised Furby surgical theater. Sociable robotics exploits the idea of a robotic body to move people to relate to machines as subjects, as creatures in pain rather than broken objects. That even the most primitive Tamagotchi can inspire these feelings demonstrates that objects cross that line not because of their sophistication but because of the feelings of attachment they evoke. The Furby, even more than the Tamagotchi, is alive enough to suggest a body in pain as well as a troubled mind. Furbies whine and moan, leaving it to their users to discover what might help. And what to make of the moment when an upside down Furby says, “Me scared!”? Freedom Baird takes this question very seriously.9 A recent graduate of the MIT Media Lab, she finds herself engaged with her Furby as a creature and a machine. But how seriously does she take the idea of the Furby as a creature? To determine this, she proposes an exercise in the spirit of the Turing test. In the original Turing test, published in 1950, mathematician Alan Turing, inventor of the first general-purpose computer, asked under what conditions people would consider a computer intelligent. In the end, he settled on a test in which the computer would be declared intelligent if it could convince people it was not a machine. Turing was working with computers made up of vacuum tubes and Teletype terminals. He suggested that if participants couldn’t tell, as they worked at their Teletypes, if they were talking to a person or a computer, that computer would be deemed “intelligent.” 10 A half century later, Baird asks under what conditions a creature is deemed alive enough for people to experience an ethical dilemma if it is distressed. She designs a Turing test not for the head but for the heart and calls it the “upside-down test.” A person is asked to invert three creatures: a Barbie doll, a Furby, and a biological gerbil. Baird’s question is simple: “How long can you hold the object upside down before your emotions make you turn it back?” Baird’s experiment assumes that a sociable robot makes new ethical demands. Why? The robot performs a psychology; many experience this as evidence of an inner life, no matter how primitive. Even those who do not think a Furby has a mind—and this, on a conscious level, includes most people—find themselves in a new place with an upside-down Furby that is whining and telling them it is scared. They feel themselves, often despite themselves, in a situation that calls for an ethical response. This usually happens at the moment when they identify with the “creature” before them, all the while knowing that it is “only a machine.” This simultaneity of vision gives Baird the predictable results of the upside-down test. As Baird puts it, “People are willing to be carrying the Barbie around by the feet, slinging it by the hair . . . no problem.... People are not going to mess around with their gerbil.” But in the case of the Furby, people will “hold the Furby upside down for thirty seconds or so, but when it starts crying and saying it’s scared, most people feel guilty and turn it over.” The work of neuroscientist Antonio Damasio offers insight into the origins of this guilt. Damasio describes two levels of experiencing pain. The first is a physical response to a painful stimulus. The second, a far more complex reaction, is an emotion associated with pain. This is an internal representation of the physical. 11 When the Furby says, “Me scared,” it signals that it has crossed the line between a physical response and an emotion, the internal representation. When people hold a Furby upside down, they do something that would be painful if done to an animal. The Furby cries out—as if it were an animal. But then it says, “Me scared”—as if it were a person.
People are surprised by how upset they get in this theater of distress. And then they get upset that they are upset. They often try to reassure themselves, saying things like, “Chill, chill, it’s only a toy!” They are experiencing something new: you can feel bad about yourself for how you behave with a computer program. Adults come to the upside-down test knowing two things: the Furby is a machine and they are not torturers. By the end, with a whimpering Furby in tow, they are on new ethical terrain.12 We are at the point of seeing digital objects as both creatures and machines. A series of fractured surfaces—pet, voice, machine, friend—come together to create an experience in which knowing that a Furby is a machine does not alter the feeling that you can cause it pain. Kara, a woman in her fifties, reflects on holding a moaning Furby that says it is scared. She finds it distasteful, “not because I believe that the Furby is really scared, but because I’m not willing to hear anything talk like that and respond by continuing my behavior. It feels to me that I could be hurt if I keep doing this.” For Kara, “That is not what I do.... In that moment, the Furby comes to represent how I treat creatures.” When the toy manufacturer Hasbro introduced its My Real Baby robot doll in 2000, it tried to step away from these complex matters. My Real Baby shut down in situations where a real baby might feel pain. This was in contrast to its prototype, a robot called “IT,” developed by a team led by MIT roboticist Rodney Brooks. “IT” evolved into “BIT” (for Baby IT), a doll with “states of mind” and facial musculature under its synthetic skin to give it expression.13 When touched in a way that would induce pain in a child, BIT cried out. Brooks describes BIT in terms of its inner states: If the baby were upset, it would stay upset until someone soothed it or it finally fell asleep after minutes of heartrending crying and fussing. If BIT . . . was abused in any way—for instance, by being swung upside down—it got very upset. If it was upset and someone bounced it on their knee, it got more upset, but if the same thing happened when it was happy, it got more and more excited, giggling and laughing, until eventually it got overtired and started to get upset. If it were hungry, it would stay hungry until it was fed. It acted a lot like a real baby.14 BIT, with its reactions to abuse, became the center of an ethical world that people constructed around its responses to pleasure and pain. But when Hasbro put BIT into mass production as My Real Baby, the company decided not to present children with a toy that responded to pain. The theory was that a robot’s response to pain could “enable” sadistic behavior. If My Real Baby were touched, held, or bounced in a way that would hurt a real baby, the robot shut down.
In its promotional literature, Hasbro marketed My Real Baby as “the most real, dynamic baby doll available for young girls to take care of and nurture.” They presented it as a companion that would teach and encourage reciprocal social behavior as children were trained to respond to its needs for amusement as well as bottles, sleep, and diaper changes. Indeed, it was marketed as realistic in all things—except that if you “hurt” it, it shut down. When children play with My Real Baby, they do explore aggressive possibilities. They spank it. It shuts down. They shake it, turn it upside down, and box its ears. It shuts down. Hasbro’s choice—maximum realism, but with no feedback for abuse—inspires strong feelings, especially among parents. For one group of parents, what is most important is to avoid a child’s aggressive response. Some believe that if you market realism but show no response to “pain,” children are encouraged to inflict it because doing so seems to have no cost. Others think that if a robot simulates pain, it enables mistreatment. Another group of parents wish that My Real Baby would respond to pain for the same reason that they justify letting their children play violent video games: they see such experiences as “cathartic.” They say that children (and adults too) should express aggression (or sadism or curiosity) in situations that seem “realistic” but where nothing “alive” is being hurt. But even these parents are sometimes grateful for My Real Baby’s unrealistic show of “denial.” They do not want to see their children tormenting a screaming baby. No matter what position one takes, sociable robots have taught us that we do not shirk from harming realistic simulations of life. This is, of course, how we now train people for war. First, we learn to kill the virtual. Then, desensitized, we are sent to kill the real. The prospect of studying these matters raises awful questions. Freedom Baird had people hold a whining, complaining Furby upside down, much to their discomfort. Do we want to encourage the abuse of increasingly realistic robot dolls? When I observe children with My Real Baby in an after-school playgroup for eight-yearolds, I see a range of responses. Alana, to the delight of a small band of her friends, flings My Real Baby into the air and then shakes it violently while holding it by one leg. Alana says the robot has “no feelings.” Watching her, one wonders why it is necessary then to “torment” something without feelings. She does not behave this way with the many other dolls in the playroom. Scott, upset, steals the robot and brings it to a private space. He says, “My Real Baby is like a baby and like a doll.... I don’t think she wants to get hurt.” As Scott tries to put the robot’s diaper back on, some of the other children stand beside him and put their fingers in its eyes and mouth. One asks, “Do you think that hurts?” Scott warns, “The baby’s going to cry!” At this point, one girl tries to pull My Real Baby away from Scott because she sees him as an inadequate protector: “Let go of her!” Scott resists. “I was in the middle of changing her!” It seems a good time to end the play session. As the research team, exhausted, packs up to go, Scott sneaks behind a table with the robot, gives it a kiss, and says good-bye, out of the sight of the other children.
In the pandemonium of Scott and Alana’s playgroup, My Real Baby is alive enough to torment and alive enough to protect. The adults watching this—a group of teachers and my research team—feel themselves in an unaccustomed quandary. If the children had been tossing around a rag doll, neither we, nor presumably Scott, would have been as upset. But it is hard to see My Real Baby treated this way. All of this—the Furbies that complain of pain, the My Real Babies that do not—creates a new ethical landscape. The computer toys of the 1980s only suggested ethical issues, as when children played with the idea of life and death when they “killed” their Speak & Spells by taking out the toys’ batteries. Now, relational artifacts pose these questions directly. One can see the new ethics at work in my students’ reactions to Nexi, a humanoid robot at MIT. Nexi has a female torso, an emotionally expressive face, and the ability to speak. In 2009, one of my students, researching a paper, made an appointment to talk with the robot’s development team. Due to a misunderstanding about scheduling, my student waited alone, near the robot. She was upset by her time there: when not interacting with people, Nexi was put behind a curtain and blindfolded. At the next meeting of my graduate seminar, my student shared her experience of sitting alongside the robot. “It was very upsetting,” she said. “The curtain—and why was she blindfolded? I was upset because she was blindfolded.” The story of the shrouded and blindfolded Nexi ignited the seminar. In the conversation, all the students talked about the robot as a “she.” The designers had done everything they could to give the robot gender. And now, the act of blindfolding signaled sight and consciousness. In class, questions tumbled forth: Was the blindfold there because it would be too upsetting to see Nexi’s eyes? Perhaps when Nexi was turned off, “her” eyes remained open, like the eyes of a dead person? Perhaps the robot makers didn’t want Nexi to see “out”? Perhaps they didn’t want Nexi to know that when not in use, “she” is left in a corner behind a curtain? This line of reasoning led the seminar to an even more unsettling question: If Nexi is smart enough to need a blindfold to protect “her” from fully grasping “her” situation, does that mean that “she” is enough of a subject to make “her” situation abusive? The students agreed on one thing: blindfolding the robot sends a signal that “this robot can see.” And seeing implies understanding and an inner life, enough of one to make abuse possible. I have said that Sigmund Freud saw the uncanny as something long familiar that feels strangely unfamiliar. The uncanny stands between standard categories and challenges the categories themselves. It is familiar to see a doll at rest. But we don’t need to cover its eyes, for it is we who animate it. It is familiar to have a person’s expressive face beckon to us, but if we blindfold that person and put them behind a curtain, we are inflicting punishment. The Furby with its expressions of fear and the gendered Nexi with her blindfold are the new uncanny in the culture of computing.
Soon, it may seem natural to watch a robot “suffer” if you hurt it. It may seem natural to chat with a robot and have it behave as though pleased you stopped by. As the intensity of experiences with robots increases, as we learn to live in new landscapes, both children and adults may stop asking the questions “Why am I talking to a robot?” and “Why do I want this robot to like me?” We may simply be charmed by the pleasure of its company. The romantic reaction of the 1980s and 1990s put a premium on what only people can contribute to each other: the understanding that grows out of shared human experience. It insisted that there is something essential about the human spirit. In the early 1980s, David, twelve, who had learned computer programming at school, contrasted people and programs this way: “When there are computers who are just as smart as the people, the computers will do a lot of the jobs, but there will still be things for the people to do. They will run the restaurants, taste the food, and they will be the ones who will love each other, have families and love each other. I guess they’ll still be the only ones who go to church.”15 Adults, too, spoke of life in families. To me, the romantic reaction was captured by how one man rebuffed the idea that he might confide in a computer psychotherapist: “How can I talk about sibling rivalry to something that never had a mother?” Of course, elements of this romantic reaction are still around us. But a new sensibility emphasizes what we share with our technologies. With psychopharmacology, we approach the mind as a bioengineerable machine.16 Brain imaging trains us to believe that things—even things like feelings—are reducible to what they look like. Our current therapeutic culture turns from the inner life to focus on the mechanics of behavior, something that people and robots might share. A quarter of a century stands between two conversations I had about the possibilities of a robot confidant, the first in 1983, the second in 2008. For me, the differences between them mark the movement from the romantic reaction to the pragmatism of the robotic moment. Both conversations were with teenage boys from the same Boston neighborhood; they are both Red Sox fans and have close relationships with their fathers. In 1983, thirteen-year-old Bruce talked about robots and argued for the unique “emotionality” of people. Bruce rested his case on the idea that computers and robots are “perfect,” while people are “imperfect,” flawed and frail. Robots, he said, “do everything right”; people “do the best they know how.” But for Bruce it was human imperfection that makes for the ties that bind. Specifically, his own limitations made him feel close to his father (“I have a lot in common with my father.... We both have chaos”). Perfect robots could never understand this very important relationship. If you ever have a problem, you go to a person.
Twenty-five years later, a conversation on the same theme goes in a very different direction. Howard, fifteen, compares his father to the idea of a robot confidant, and his father does not fare well in the comparison. Howard thinks the robot would be better able to grasp the intricacies of high school life: “Its database would be larger than Dad’s. Dad has knowledge of basic things, but not enough of high school.” In contrast to Bruce’s sense that robots are not qualified to have an opinion about the goings-on in families, Howard hopes that robots might be specially trained to take care of “the elderly and children”—something he doesn’t see the people around him as much interested in. Howard has no illusions about the uniqueness of people. In his view, “they don’t have a monopoly” on the ability to understand or care for each other. Each human being is limited by his or her own life experience, says Howard, but “computers and robots can be programmed with an infinite amount of information.” Howard tells a story to illustrate how a robot could provide him with better advice than his father. Earlier that year, Howard had a crush on a girl at school who already had a boyfriend. He talked to his father about asking her out. His father, operating on an experience he had in high school and what Howard considers an outdated ideal of “macho,” suggested that he ask the girl out even though she was dating someone else. Howard ignored his father’s advice, fearing it would lead to disaster. He was certain that in this case, a robot would have been more astute. The robot “could be uploaded with many experiences” that would have led to the right answer, while his father was working with a limited data set. “Robots can be made to understand things like jealousy from observing how people behave.... A robot can be fully understanding and open-minded.” Howard thinks that as a confidant, the robot comes out way ahead. “People,” he says, are “risky.” Robots are “safe.” There are things, which you cannot tell your friends or your parents, which . . . you could tell an AI. Then it would give you advice you could be more sure of.... I’m assuming it would be programmed with prior knowledge of situations and how they worked out. Knowledge of you, probably knowledge of your friends, so it could make a reasonable decision for your course of action. I know a lot of teenagers, in particular, tend to be caught up in emotional things and make some really bad mistakes because of that.
I ask Howard to imagine what his first few conversations with a robot might be like. He says that the first would be “about happiness and exactly what that is, how do you gain it.” The second conversation would be “about human fallibility,” understood as something that causes “mistakes.” From Bruce to Howard, human fallibility has gone from being an endearment to a liability. No generation of parents has ever seemed like experts to their children. But those in Howard’s generation are primed to see the possibilities for relationships their elders never envisaged. They assume that an artificial intelligence could monitor all of their e-mails, calls, Web searches, and messages. This machine could supplement its knowledge with its own searches and retain a nearly infinite amount of data. So, many of them imagine that via such search and storage an artificial intelligence or robot might tune itself to their exact needs. As they see it, nothing technical stands in the way of this robot’s understanding, as Howard puts it, “how different social choices [have] worked out.” Having knowledge and your best interests at heart, “it would be good to talk to . . . about life. About romantic matters. And problems of friendship.” Life? Romantic matters? Problems of friendship? These were the sacred spaces of the romantic reaction. Only people were allowed there. Howard thinks that all of these can be boiled down to information so that a robot can be both expert resource and companion. We are at the robotic moment. As I have said, my story of this moment is not so much about advances in technology, impressive though these have been. Rather, I call attention to our strong response to the relatively little that sociable robots offer—fueled it would seem by our fond hope that they will offer more. With each new robot, there is a ramp-up in our expectations. I find us vulnerable—a vulnerability, I believe, not without risk.
In April 1999, a month before AIBO’s commercial release, Sony demonstrated the little robot dog at a conference on new media in San Jose, California. I watched it walk jerkily onto an empty stage, followed by its inventor, Toshitado Doi. At his bidding, AIBO fetched a ball and begged for a treat. Then, with seeming autonomy, AIBO raised its back leg to some suggestion of a hydrant. Then, it hesitated, a stroke of invention in itself, and lowered its head as though in shame. The audience gasped. The gesture, designed to play to the crowd, was wildly successful. I imagined how audiences responded to Jacques de Vaucanson’s eighteenth-century digesting (and defecating) mechanical duck and to the chess-playing automata that mesmerized Edgar Alan Poe. AIBO, like these, was applauded as a marvel, a wonder.1 Depending on how it is treated, an individual AIBO develops a distinct personality as it matures from a fall-down puppy to a grown-up dog. Along the way, AIBO learns new tricks and expresses feelings: flashing red and green eyes direct our emotional traffic; each of its moods comes with its own soundtrack. A later version of AIBO recognizes its primary caregiver and can return to its charging station, smart enough to know when it needs a break. Unlike a Furby, whose English is “destined” to improve as long as you keep it turned on, AIBO stakes a claim to intelligence and impresses with its ability to show what’s on its mind. If AIBO is in some sense a toy, it is a toy that changes minds. It does this in several ways. It heightens our sense of being close to developing a postbiological life and not just in theory or in the laboratory. And it suggests how this passage will take place. It will begin with our seeing the new life as “as if ” life and then deciding that “as if ” may be life enough. Even now, as we contemplate “creatures” with artificial feelings and intelligence, we come to reflect differently on our own. The question here is not whether machines can be made to think like people but whether people have always thought like machines. The reconsiderations begin with children. Zane, six, knows that AIBO doesn’t have a “real brain and heart,” but they are “real enough.” AIBO is “kind of alive” because it can function “as if it had a brain and heart.” Paree, eight, says that AIBO’s brain is made of “machine parts,” but that doesn’t keep it from being “like a dog’s brain.... Sometimes, the way [AIBO] acted, like he will get really frustrated if he can’t kick the ball. That seemed like a real emotion . . . so that made me treat him like he was alive, I guess.” She says that when AIBO needs its batteries charged, “it is like a dog’s nap.” And unlike a teddy bear, “an AIBO needs its naps.” As Paree compares her AIBO’s brain to that of a dog, she clears the way for other possibilities. She considers whether AIBO might have feelings like a person, wondering if AIBO “knows its own feelings”—or “if the controls inside know them.” Paree says that people use both methods. Sometimes people have spontaneous feelings and “just become aware” of them (this is “knowing your own feelings”). But other times, people have to program themselves to have the feelings they want. “If I was sad and wanted to be happy”—here Paree brings her fists up close to her ears to demonstrate concentration and intent—“I would have to make my brain say that I am set on being happy.” The robot, she thinks, probably has the second kind of feelings, but she points out that both ways of getting to a feeling get you to the same place: a smile or a frown if you are a person, a happy or sad sound if you are an AIBO. Different inner states lead to the same outward states, and so inner states cease to matter. AIBO carries a behaviorist sensibility.
Keith, seventeen, is going off to college next year and taking his AIBO with him. He treats the robot as a pet, all the while knowing that it is not a pet at all. He says, “Well, it’s not a pet like others, but it is a damn good pet. . . . I’ve taught it everything. I’ve programmed it to have a personality that matches mine. I’ve never let it reset to its original personality. I keep it on a program that lets it develop to show the care I’ve put into it. But of course, it’s a robot, so you have to keep it dry, you have to take special care with it.” His classmate Logan also has an AIBO. The two have raised the robots together. If anything, Logan’s feelings are even stronger than Keith’s. Logan says that talking to AIBO “makes you better, like, if you’re bored or tired or down . . . because you’re actually, like, interacting with something. It’s nice to get thoughts out.” The founders of artificial intelligence were much taken with the ethical and theological implications of their enterprise. They discussed the mythic resonance of their new science: Were they people putting themselves in the place of gods?2 The impulse to create an object in one’s own image is not new—think Galatea, Pygmalion, Frankenstein. These days, what is new is that an off-the-shelf technology as simple as an AIBO provides an experience of shaping one’s own companion. But the robots are shaping us as well, teaching us how to behave so that they can flourish.3 Again, there is psychological risk in the robotic moment. Logan’s comment about talking with the AIBO to “get thoughts out” suggests using technology to know oneself better. But it also suggests a fantasy in which we cheapen the notion of companionship to a baseline of “interacting with something.” We reduce relationship and come to see this reduction as the norm. As infants, we see the world in parts. There is the good—the things that feed and nourish us. There is the bad—the things that frustrate or deny us. As children mature, they come to see the world in more complex ways, realizing, for example, that beyond black and white, there are shades of gray. The same mother who feeds us may sometimes have no milk. Over time, we transform a collection of parts into a comprehension of wholes.4 With this integration, we learn to tolerate disappointment and ambiguity. And we learn that to sustain realistic relationships, one must accept others in their complexity. When we imagine a robot as a true companion, there is no need to do any of this work. The first thing missing if you take a robot as a companion is alterity, the ability to see the world through the eyes of another.5 Without alterity, there can be no empathy. Writing before robot companions were on the cultural radar, the psychoanalyst Heinz Kohut described barriers to alterity, writing about fragile people—he calls them narcissistic personalities—who are characterized not by love of self but by a damaged sense of self. They try to shore themselves up by turning other people into what Kohut calls self objects. In the role of selfobject, another person is experienced as part of one’s self, thus in perfect tune with a fragile inner state. The selfobject is cast in the role of what one needs, but in these relationships, disappointments inevitably follow. Relational artifacts (not only as they exist now but as their designers promise they will soon be) clearly present themselves as candidates for the role of selfobject.
With a price tag of $1,300 to $2,000, AIBO is meant for grown-ups. But the robot dog is a harbinger of the digital pets of the future, and so I present it to children from age four to thirteen as well as to adults. I bring it to schools, to after-school play centers, and, as we shall see in later chapters, to senior centers and nursing homes. I offer AIBOs for home studies, where families get to keep them for two or three weeks. Sometimes, I study families who have bought an AIBO of their own. In these home studies, just as in the home studies of Furbies, families are asked to keep a “robot diary.” What is it like living with an AIBO? The youngest children I work with—the four- to six-year-olds—are initially preoccupied with trying to figure out what the AIBO is, for it is not a dog and not a doll. The desire to get such things squared away is characteristic of their age. In the early days of digital culture, when they met their first electronic toys and games, children of this age would remain preoccupied with such questions of categories. But now, faced with this sociable machine, children address them and let them drop, taken up with the business of a new relationship. Maya, four, has an AIBO at home. She first asks questions about its origins (“How do they make it?”) and comes up with her own answer: “I think they start with foil, then soil, and then you get some red flashlights and then put them in the eyes.” Then she pivots to sharing the details of her daily life with AIBO: “I love to play with AIBO every day, until the robot gets tired and needs to take a nap.” Henry, four, follows the same pattern. He begins with an effort to categorize AIBO: AIBO is closest to a person, but different from a person because it is missing a special “inner power,” an image borrowed from his world of Pokémon. 6 But when I see Henry a week later, he has bonded with AIBO and is stressing the positive, all the things they share. The most important of these are “remembering and talking powers, the strongest powers of all.” Henry is now focused on the question of AIBO’s affection: How much does this robot like him? Things seem to be going well: he says that AIBO favors him “over all his friends.” By eight, children move even more quickly from any concern over AIBO’s “nature” to the pleasures of everyday routines. In a knowing tone, Brenda claims that “people make robots and . . . people come from God or from eggs, but this doesn’t matter when you are playing with the robot.” In this dismissal of origins we see the new pragmatism. Brenda embraces AIBO as a pet. In her robot diary, she reminds herself of the many ways that this pet should not be treated as a dog. One early entry reminds her not to feed it, and another says, “Do not take AIBO on walks so it can poop.” Brenda feels guilty if she doesn’t keep AIBO entertained. She thinks that “if you don’t play with it,” its lights get red to show its discontent at “playing by itself and getting all bored.” Brenda thinks that when bored, AIBO tries to “entertain itself.” If this doesn’t work, she says, “it tries to get my attention.” Children believe that AIBO asks for attention when it needs it. So, for example, a sick AIBO will want to get better and know it needs human help. An eight-year-old says, “It would want more attention than anything in the whole world.”
Yolanda’s feelings about AIBO also go through all the stages. She first sees AIBO as a substitute: “AIBO might be good practice for all children whose parents aren’t ready to take care of a real dog.” But then she takes another step: in some ways AIBO might be better than a real dog. “The AIBO,” says Yolanda, “doesn’t shed, doesn’t bite, doesn’t die.” More than this, a robotic companion can be made as you like it. Yolanda muses about how nice it would be to “keep AIBO at a puppy stage for people who like to have puppies.” Children imagine that they can create a customized AIBO close to their heart’s desire.8 Sometimes their heart’s desire is to have affection when that pleases them and license to walk away, something not possible with a biological pet. Two nine-year-olds—Lydia and Paige—talk through the steps that take a robot from better than nothing to better than anything. Lydia begins by thinking of AIBO as a substitute for a real pet if you can’t have one: “An AIBO, since you can’t be allergic to a robot, that would be very nice to have.” But as she gets to know AIBO better, she sees a more enticing possibility. “Sometimes,” she says, “I might like [AIBO] more than a real living animal, like a real cat or a real dog, because, like if you had a bad day . . . then you could just turn this thing off and it wouldn’t bug you.” Paige has five pets—three dogs, two cats—and when she is sad, she says, “I cuddle with them.” This is a good thing, but she complains that pets can be trouble: “All of them want your attention. If you give one attention you have to give them all attention, so it’s kinda hard.... When I go somewhere, my kitten misses me. He’ll go into my room and start looking for me.” AIBO makes things easy: “AIBO won’t look at you like ‘play with me’; it will just go to sleep if there is nothing else to do. It won’t mind.” Paige explains that the worst thing that ever happened to her was when her family “had to put their dog to sleep.” She hasn’t wanted a new one since. “But the thing about AIBO,” she says, “is that you don’t have to put him to sleep.... I think you could fix [AIBO] with batteries . . . but when your dog actually dies, you can’t fix it.” For now, the idea that AIBO, as she puts it, “will last forever” makes it better than a dog or cat. Here, AIBO is not practice for the real. It offers an alternative, one that sidesteps the necessity of death.9 For Paige, simulation is not necessarily second best. Pets have long been thought good for children because they teach responsibility and commitment. AIBO permits something different: attachment without responsibility. Children love their pets, but at times, like their overextended parents, they feel burdened by their pets’ demands. This has always been true. But now children see a future where something different may be available. With robot pets, children can give enough to feel attached, but then they can turn away. They are learning a way of feeling connected in which they have permission to think only of themselves. And yet, since these new pets seem betwixt and between what is alive and what is not, this turning away is not always easy. It is not that some children feel responsible for AIBO and other do not. The same children often have strong feelings on both sides of the matter.
As soon as children met computers and computer toys in the late 1970s and early 1980s, they used aggression as a way to animate them and to play with ideas about life and death. Children crashed and revived computer programs; they “killed” Merlin, Simon, and Speak & Spell by pulling out their batteries and then made them come back to life. Aggression toward sociable robots is more complex because children are trying to manage more significant attachments. To take only one example, robots disappoint when they do not display the affection children lead themselves to expect. To avoid hurt, children want to dial things down. Turning robots into objects that can be hurt with impunity is a way to put them in their place. Whether we have permission to hurt or kill an object influences how we think about its life. 10 To children, being able to kill spiders without punishment makes spiders seem less alive, and hurting a robot can make it seem less alive as well. But as in the discussion about whether My Real Baby should cry in “pain,” things are complicated. For the idea that you can hurt a robot can also make it seem more alive. Like Henry, twelve-year-old Tamara is aggressive toward AIBO and troubled by what this implies. She wants to play with AIBO in the same way that she plays with her much-loved cat. But she worries that AIBO’s responses to her are generic. She says, “AIBO acts the same to everyone. It doesn’t attach herself to one person like most animals do.” Tamara says that sometimes she stops herself from petting AIBO: “I start to pet it, and then, like, I would start to be, like, ‘Oh wait. You’re not a cat. You’re not alive.’” And sometimes she gives in to an urge to “knock it over because it was just so cute when it was getting up and then it would, like, shake its head, because then it seemed really alive because that’s what dogs do.” She tries to reassure me: “I’m not like this with my animals.” From their earliest experiences with the electronic toys and games of the late 1970s, children split the notion of consciousness and life. You didn’t have to be biologically alive to have awareness. And so, Tamara who knows AIBO is not alive, imagines that it still might feel pain. In the end, her aggression puts her in a tough spot; AIBO is too much like a companion to be a punching bag. For Tamara, the idea that AIBO might “see” well enough to recognize her is frightening because it might know she is hitting it. But the idea of AIBO as aware and thus more lifelike is exciting as well.
Ashley, seventeen, is a bright and active young woman who describes herself as a cat lover. I have given her an AIBO to take home for two weeks, and now she is at my office at MIT to talk about the experience. During the conversation, Ashley’s AIBO plays on the floor. We do not attend to it; it does tricks on its own—and very noisily. After a while, it seems as though the most natural thing would be to turn AIBO off, in the same spirit that one might turn off a radio whose volume interferes with a conversation. Ashley moves toward the AIBO, hesitates, reaches for its off switch, and hesitates again. Finally, with a small grimace, she hits the switch. AIBO sinks to the ground, inert. Ashley comments, “I know it’s not alive, but I would be, like, talking to it and stuff, and then it’s just a weird experience to press a[n off] button. It made me nervous.... [I talk to it] how I would talk to my cat, like he could actually hear me and understand praise and stuff like that.” I am reminded of Leah, nine, who said of her Furby, “It’s hard to turn it off when it is talking to me.” Ashley knows AIBO is a robot, but she experiences it as a biological pet. It becomes alive for her not only because of its intelligence but because it seems to her to have real emotions. For example, she says that when AIBO’s red lights shone in apparent frustration, “it seemed like a real emotion.... So that made me treat him like he was alive.... And that’s another strange thing: he’s not really physically acting those emotions out, but then you see the colors and you think, ‘Oh, he’s upset.’” Artificial intelligence is often described as the art and science of “getting machines to do things that would be considered intelligent if done by people.” We are coming to a parallel definition of artificial emotion as the art of “getting machines to express things that would be considered feelings if expressed by people.” Ashley describes the moment of being caught between categories: she realizes that what the robot is “acting out” is not emotion, yet she feels the pull of seeing “the colors” and experiencing AIBO as “upset.” Ashley ends up seeing AIBO as both machine and creature. So does John Lester, a computer scientist coming from a far more sophisticated starting point. From the early 1990s, Lester pioneered the use of online communities for teaching, learning, and collaboration, including recent work developing educational spaces on the virtual world of Second Life. Lester bought one of the first AIBOs on the market. He called it Alpha in deference to its being “one of the first batch.”12 When Lester took Alpha out of its box, he shut the door to his office and spent the entire day “hanging out with [my] new puppy.” He describes the experience as “intense,” comparing it to the first time he saw a computer or typed into a Web browser. He quickly mastered the technical aspects of AIBO, but this kind of understanding did not interfere with his pleasure in simply being with the puppy. When Sony modified the robot’s software, Lester bought a second AIBO and named it Beta. Alpha and Beta are machines, but Lester does not like anyone to treat them as inanimate metal and plastic. “I think about my AIBOs in different ways at the same time,” Lester says. In the early days of cubism, the simultaneous presentation of many perspectives of the human face was subversive. But at a certain point, one becomes accustomed to looking at a face in this new way. A face, after all, does have multiple aspects; only representational conventions keep us from appreciating them together. But once convention is challenged, the new view of the face suggests depth and new complexities. Lester has a cubist view of AIBO; he is aware of it as machine, bodily creature, and mind. An AIBO’s sentience, he says, is “awesome.” The creature is endearing. He appreciates the programming behind the exact swing of the “floppy puppy ears.” To Lester, that programming gives AIBO a mind.
Lester understands the mechanisms that AIBO’s designers have used to draw him in: AIBO’s gaze, its expressions of emotion, and the fact that it “grows up” under his care. But this understanding does not interfere with his attachment, just as knowing that infants draw him in with their big, wide eyes does not threaten his connection with babies. Lester says that when he is with AIBO, he does not feel alone. He says that “from time to time” he “catches himself ” in engineer mode, remarking on a technical detail of AIBO that he admires, but these moments do not pull him away from enjoying the companionship of his AIBO puppies. This is not a connection he plays at. It is a big step from accepting AIBO as a companion, and even a solace, to the proposals of David Levy, the computer scientist who imagines robots as intimate partners. But today’s fantasies and Levy’s dreams share something important: the idea that after a robot serves as a better-than-nothing substitute, it might become equal, or even preferable, to a pet or person. In Yolanda’s terms, if your pet is a robot, it might always stay a cute puppy. By extension, if your lover were a robot, you would always be the center of its universe. A robot would not just be better than nothing or better than something, but better than anything. From watching children play with objects designed as “amusements,” we come to a new place, a place of cold comforts. Child and adult, we imagine made to measure companions. Or, at least we imagine companions who are always interested in us. Harry, a forty-two-year-old architect, enjoys AIBO’s company and teaching it new tricks. He knows that AIBO is not aware of him as a person but says, “I don’t feel bad about this. A pet isn’t as aware of me as a person might be.... Dogs don’t measure up to people.... Each level of creature simply does their best. I like it that he [AIBO] recognizes me as his master.” Jane, thirty-six, a grade school teacher, is similarly invested in her AIBO. She says she has “adopted my husband’s AIBO . . . because it is so cute. I named it and love to spend time with it.” Early in our conversation, Jane claims that she turns to AIBO for “amusement,” but she ends up saying that she also turns to it when she is lonely. Jane looks forward to its company after a long workday. Jane talks to her AIBO. “Spend[ing] time” with AIBO means sharing the events of her day, “like who I’m having lunch with at school, which students give me trouble.” Her husband, says Jane, is not interested in these topics. It is more comfortable to talk to AIBO than to force him to listen to stories that bore him. In the company of their robots, Jane and Harry are alone in a way that encourages them to give voice to their feelings. Is there harm here?
Wesley knows he is difficult to live with. He once saw a psychiatrist who told him that his “cycles” were out of the normal range. Ex-wives, certainly, have told him he is “too moody.” He sees himself as “pressure” on a woman, and he feels pressure as well because he has not been able to protect women he cared for from his “ups and downs.” He likes the idea of a robot because he could act naturally—it could not be hurt by his dark moods. Wesley considers the possibility of two “women,” one real and the other artificial: “Maybe I would want a robot that would be the perfect mate—less needs—and a real woman. The robot could take some of the pressure off the real woman. She wouldn’t have to perform emotionally at such a high level, really an unrealistic level.... I could stay in my comfort zone.” Rudimentary versions of Wesley’s fantasy are in development. I have spoken briefly of the Internet buzz over Roxxxy, put on the market in January 2010, advertised as “the world’s first sex robot.” Roxxxy cannot move, although it has electronically warmed skin and internal organs that pulse. It does, however, make conversation. The robot’s creator, Douglas Hines, helpfully offers, “Sex only goes so far—then you want to be able to talk to the person.”13 So, for example, when Roxxxy senses that its hand is being held, the robot says, “I love holding hands with you,” and moves into more erotic conversation when the physical caresses become more intimate. One can choose different personalities for Roxxxy, ranging from wild to frigid. The robot will be updated over the Internet to expand its capabilities and vocabulary. It can already discuss soccer. Hines, an engineer, says that he got into the robot business after a friend died in the September 11 attacks on the Twin Towers. Hines wanted to preserve his friend’s personality so that his children could interact with him as they grew up. Like AI scientist and inventor Raymond Kurzweil, who dreams of a robotic incarnation of his father who died tragically young, Hines committed himself to the project of building an artificial personality. At first, he considered building a home health aid for the elderly but decided to begin with sex robots, a decision that he calls “only marketing.” His long-term goal is to take artificial personalities into the mainstream. He still wants to recreate his lost friend. The well-publicized launch of Roxxxy elicits a great deal of online discussion. Some postings talk about how “sad” it is that a man would want such a doll. Others argue that having a robot companion is better than being lonely. For example, “There are men for who attaining a real woman is impossible.... This isn’t simply a matter of preference.... In the real world, sometimes second best is all they can get.”

This book began with a roboticist's dream that struck me as a nightmare. I was reading Hans Moravec's Mind Children: The Future of Robot and Human Intelligence, enjoying the ingenious variety of his robots, when I happened upon the passage where he argues it will soon be possible to download human consciousness into a computer. l To illustrate, he invents a fantasy scenario in which a robot surgeon purees the human brain in a kind of cranial liposuction, reading the information in each molecular layer as it is stripped away and transferring the information into a computer. At the end of the operation, the cranial cavity is empty, and the patient, now inhabiting the metallic body of the computer, wakens to find his consciousness exactly the same as it was before.

Following this thread, I was led into a maze of developments that turned into a six-year odyssey of researching archives in the history of cybernetics, interviewing scientists in computational biology and artificial life, reading cultural and literary texts concerned with information technologies, visiting laboratories engaged in research on virtual reality, and grappling with technical articles in cybernetics, information theory, autopoiesis, computer simulation, and cognitive science. Slowly this unruly mass of material began taking shape as three interrelated stories. The first centers on how information lost its body, that is, how it came to be conceptualized as an entity separate from the materialforms in which it is thought to be embedded. The second story concerns how the cyborg was created as a technological artifact and cultural icon in the years follOwing World War II. The third, deeply implicated with the first two, is the unfolding story of how a historically specific construction called the human is giving way to a different construction called the posthuman. Interrelations between the three stories are extensive. Central to the construction of the cyborg are informational pathways connecting the organic body to its prosthetic extensions. This presumes a conception of information as a (disembodied) entity that can flow between carbon-based organic components and silicon-based electronic components to make protein and silicon operate as a Single system. When information loses its body, equating humans and computers is especially easy, for the materiality in which the thinking mind is instantiated appears incidental to its essential nature. Moreover, the idea of the feedback loop implies that the boundaries of the autonomous subject are up for grabs, since feedback loops can flow not only within the subject but also between the subject and the environment. From Norbert Wiener on, the flow of information through feedback loops has been associated with the deconstruction of the liberal humanist subject, the version of the "human" with which I will be concerned. Although the "posthuman" differs in its articulations, a common theme is the union of the human with the intelligent machine.

What to make of this shift from the human to the posthuman, which both evokes terror and excites pleasure? The liberal humanist subject has, of course, been cogently criticized from a number of perspectives. Feminist theorists have pointed out that it has historically been constructed as a white European male, presuming a universality that has worked to suppress and disenfranchise women's voices; postcolonial theorists have taken issue not only with the universality of the (white male) liberal subject but also with the very idea of a unified, consistent identity, fOCUSing instead on hybridity; and postmodern theorists such as Gilles Deleuze and Felix Guattari have linked it with capitalism, arguing for the liberatory potential of a dispersed subjectivity distributed among diverse desiring machines they call "body without organs."7 Although the deconstruction of the liberal humanist subject in cybernetiCS has some affinities with these perspectives, it proceeded primarily along lines that sought to understand human being as a set of informational processes. Because information had lost its body, this construction implied that embodiment is not essential to human being. Embodiment has been systematically downplayed or erased in the cybernetic construction of the posthuman in ways that have not occurred in other critiques of the liberal humanist subject, espeCially in feminist and postcolonial theories.

In tracing these continuities and discontinuities between a "natural" self and a cybernetic posthuman, I am not trying to recuperate the liberal subject. Although I think that serious consideration needs to be given to how certain characteristics associated with the liberal subject, especially agency and choice, can be articulated within a posthuman context, I do not mourn the passing of a concept so deeply entwined with projects of domination and oppression. Rather, I view the present moment as a critical juncture when interventions might be made to keep disembodiment from being rewritten, once again, into prevailing concepts of subjectivity. I see the deconstruction of the liberal humanist subject as an opportunity to put back into the picture the flesh that continues to be erased in contemporary discussions about cybernetic subjects. Hence my focus on how information lost its body, for this story is central to creating what Arthur Kroker has called the "flesh-eating 90s."11 If my nightmare is a culture inhabited by posthumans who regard their bodies as fashion accessories rather than the ground of being, my dream is a version of the posthuman that embraces the possibilities of information technologies without being seduced by fantasies of unlimited power and disembodied immortality, that recognizes and celebrates finitude as a condition of human being, and that understands human life is embedded in a material world of great complexity, one on which we depend for our continued survival.

Perhaps it will now be clear that I mean my title, How We Became Posthuman, to connote multiple ironies, which do not prevent it from also being taken seriously. Taken straight, this title points to models of subjectivity sufficiently different from the liberal subject that if one assigns the term "human" to this subject, it makes sense to call the successor "posthuman." Some of the historical processes leading to this transformation are documented here, and in this sense the book makes good on its title. Yet my argument will repeatedly demonstrate that these changes were never complete transformations or sharp breaks; without exception, they reinscribed traditional ideas and assumptions even as they articulated something new. The changes announced by the title thus mean something more complex than "That was then, this is now." Rather, "human" and "posthuman" coexist in shifting configurations that vary with historically specific contexts. Given these complexities, the past tense in the title-"became" -is intended both to offer the reader the pleasurable shock of a double take and to reference ironically apocalyptic visions such as Moravec's prediction of a "postbiological" future for the human race. Amplifying the ambiguities of the past tense are the ambiguities of the plural. In one sense, "we" refers to the readers of this book-readers who, by becoming aware of these new models of subjectivity (if they are not already familiar with them), may begin thinking of their actions in ways that have more in common with the posthuman than the human. Speaking for myself, I now find myself saying things like, "Well, my sleep agent wants to rest, but my food agent says I should go to the store." Each person who thinks this way begins to envision herself or himself as a posthuman collectivity, an "I" transformed into the "we" of autonomous agents operating together to make a self. The infectious power of this way of thinking gives "we" a performative dimension. People become posthuman because they think they are posthuman. In another sense "we," like "became," is meant ironically, positioning itself in opposition to the techno-ecstasies found in various magazines, such as Mondo 2000, which customarily speak of the transformation into the posthuman as if it were a universal human condition when in fact it affects only a small fraction of the world's populationa pOint to which I will return.

During the foundational era of cybernetics, Norbert Wiener, John von Neumann, Claude Shannon, Warren McCulloch, and dozens of other distinguished researchers met at annual conferences sponsored by the JOSiah Macy Foundation to formulate the central concepts that, in their high expectations, would coalesce into a theory of communication and control applying equally to animals, humans, and machines. Retrospectively called the Macy Conferences on Cybernetics, these meetings, held from 1943 to 1954, were instrumental in forging a new paradigm. 12 To succeed, they needed a theory of information (Shannon's bailiwick), a model of neural functioning that showed how neurons worked as information-processing systems (McCulloch's lifework), computers that processed binary code and that could conceivably reproduce themselves, thus reinforcing the analogy with biolOgical systems (von Neumann's specialty), and a visionary who could articulate the larger implications of the cybernetic paradigm and make clear its cosmic significance (Wiener's contribution). The result of this breathtaking enterprise was nothing less than a new way oflooking at human beings. Henceforth, humans were to be seen primarily as information-processing entities who are essentially similar to intelligent machines. The revolutionary implications of this paradigm notwithstanding, Wiener did not intend to dismantle the liberal humanist subject. He was less interested in seeing humans as machines than he was in fashioning human and machine alike in the image of an autonomous, self-directed individual. In aligning cybernetiCS with liberal humanism, he was following a strain of thought that, since the Enlightenment, had argued that human beings could be trusted with freedom because they and the social structures they devised operated as self-regulating mechanisms. 13 For Wiener, cybernetics was a means to extend liberal humanism, not subvert it. The point was less to show that man was a machine than to demonstrate that a machine could function like a man.

This definition of reflexivity has much in common with some of the most influential and provocative recent work in critical theory, cultural studies, and the social studies of science. Typically, these works make the reflexive move of showing that an attribute previously considered to have emerged from a set of preexisting conditions is in fact used to generate the conditions. In Nancy Armstrong's Desire and Domestic Fiction: A Political History of the Novel, for example, bourgeOiS femininity is shown to be constructed through the domestic fictions that represent it as already in place. 16 In Michael Warner's The Letters of the RepubliC: Publication and the Public Sphere in Eighteenth-Century America, the founding document of the United States, the Constitution, is shown to produce the very people whose existence it presupposes. 17 In Bruno Latour's Science in Action: How to Follow Scientists and Engineers through Society, scientific experiments are shown to produce the nature whose existence they predicate as their condition of possibility. 18 It is only a slight exaggeration to say that contemporary critical theory is produced by the reflexivity that it also produces (an observation that is, of course, also reflexive).
Reflexivity entered cybernetics primarily through discussions about the observer. By and large, first-wave cybernetics followed traditional scientific protocols in considering observers to be outside the system they observe. Yet cybernetics also had implications that subverted this premise. The objectivist view sees information flOwing from the system to the observers, but feedback can also loop through the observers, drawing them in to become part of the system being observed. Although participants remarked on this aspect of the cybernetic paradigm throughout the Macy transcripts, they lacked a single word to describe it. To my knowledge, the word "reflexivity" does not appear in the transcripts. This meant they had no handle with which to grasp this slippery concept, no Signifier that would help to constitute as well as to describe the changed perspective that reflexivity entails. Discussions of the idea remained diffuse. Most participants did not go beyond remarking on the shifting boundaries between observer and system that cybernetics puts into play. With some exceptions, deeper formulations of the problem failed to coalesce during the Macy discussions.

The second wave of cybernetics grew out of attempts to incorporate reflexivity into the cybernetic paradigm at a fundamental level. The key issue was how systems are constituted as such, and the key problem was how to redefine homeostatic systems so that the observer can be taken into account. The second wave was initiated by, among others, Heinz von Foerster, the Austrian emigre who became coeditor of the Macy transcripts. This phase can be dated from 1960, when von Foerster wrote the first of the essays that were later collected in his influential book Observing Systems. 19 As von Foerster's punning title recognizes, the observer of systems can himself be constituted as a system to be observed. Von Foerster called the models he presented in these essays "second-order cybernetics" because they extended cybernetic principles to the cyberneticians themselves. The second wave reached its mature phase with the publication of Humberto Maturana and Francisco Varela's Autopoiesis and Cognition: The Realization of the Living. 20 Building on Maturana's work on reflexivity in sensory processing and Varela's on the dynamics of autonomous biological systems, the two authors expanded the reflexive tum into a fully articulated epistemology that sees the world as a set of informationally closed systems. Organisms respond to their environment in ways determined by their internal self-organization. Their one and only goal is continually to produce and reproduce the organization that defines them as systems. Hence, they not only are self-organizing but also are autopoietic, or selfmaking. Through Maturana and Varela's work and that of other influential theorists such as German SOCiologist Niklas Luhmann,21 cybernetics by 1980 had spun off from the idea of reflexive feedback loops a theory of autopoiesis with sweeping epistemological implications.
The third wave swelled into existence when self-organization began to be understood not merely as the (re)production of internal organization but as the springboard to emergence. In the rapidly emerging field of artificiallife, computer programs are designed to allow "creatures" (that is, discrete packets of computer codes) to evolve spontaneously in directions the programmer may not have anticipated. The intent is to evolve the capacity to evolve. Some researchers have argued that such self-evolving programs are not merely models oflife but are themselves alive. What assumptions make this claim plausible? If one sees the universe as composed essentially of information, it makes sense that these "creatures" are life forms because they have the form oflife, that is, an informational code. As a result, the theoretical bases used to categorize all life undergo a significant shift. As we shall see in chapters 9 and 10, when these theories are applied to human beings, H onw sapiens are so transfigured in conception and purpose that they can appropriately be called posthuman. The emergence of the posthuman as an informational-material entity is paralleled and reinforced by a corresponding reinterpretation of the deep structures of the phYSical world. Some theorists, notably Edward Fredkin and Stephen Wolfram, claim that reality is a program run on a cosmic computer.22 In this view, a universal informational code underlies the structure of matter, energy, spacetime-indeed, of everything that exists. The code is instantiated in cellular automata, elementary units that can occupy two states: on or off. Although the jury is still out on the cellular automata model, it may indeed prove to be a robust way to understand reality. Even now, a research team headed by Fredkin is working on shOwing how quantum mechanics can be derived from an underlying cellular automata model.

It is this materiality/information separation that I want to contest-not the cellular automata model, information theory, or a host of related theories in themselves. My strategy is to complicate the leap from embodied reality to abstract information by pointing to moments when the assumptions involved in this move were contested by other researchers in the field and so became especially visible. The point of highlighting such moments is to make clear how much had to be erased to arrive at such abstractions as bodiless information. Abstraction is of course an essential component in all theOrizing, for no theory can account for the infinite multiplicity of our interactions with the real. But when we make moves that erase the world's multiplicity, we risk losing Sight of the variegated leaves, fractal branchings, and particular bark textures that make up the forest. In the pages that follow, I will identifY two moves in particular that played important roles in constructing the information/materiality hierarchy. Irreverently, I think of them as the Platonic backhand and forehand.

Whether the enabling assumptions for this conception of information occur in information theory, cybernetics, or popular science books such as Mind Children, their appeal is clear. Information viewed as pattern and not tied to a particular instantiation is information free to travel across time and space. Hackers are not the only ones who believe that information wants to be free. The great dream and promise of information is that it can be free from the material constraints that govern the mortal world. Marvin Minsky precisely expressed this dream when, in a recent lecture, he suggested it will soon be possible to extract human memories from the brain and import them, intact and unchanged, to computer disks.23 The clear implication is that if we can become the information we have constructed, we can achieve effective immortality. In the face of such a powerful dream, it can be a shock to remember that for information to exist, it must always be instantiated in a medium, whether that medium is the page from the Bell Laboratories Journal on which Shannon's equations are printed, the computer-generated topolOgical maps used by the Human Genome Project, or the cathode ray tube on which virtual worlds are imaged. The point is not only that abstracting information from a material base is an imaginary act but also, and more fundamentally, that conceiving of information as a thing separate from the medium instantiating it is a prior imaginary act that constructs a holistic phenomenon as an information/matter duality.
The complex psychological functions a skeuomorph performs can be illustrated by an installation exhibited at SIGGRAPH '93. Called the "Catholic Turing Test," the simulation invited the viewer to make a confession by chOOSing selections from the video screen; it even had a bench on which the viewer could kneel. 28 On one level, the installation alluded to the triumph of science over religion, for the role of divinely authorized interrogation and absolution had been taken over by a machine algorithm. On another level, the installation pointed to the intransigence of conditioned behavior, for the machine's form and function were determined by its religious predecessor. Like a Janus figure, the skeuomorph looks to past and future, Simultaneously reinforcing and undermining both. It calls into a playa psychodynamic that finds the new more acceptable when it recalls the old that it is in the process of displacing and finds the traditional more comfortable when it is presented in a context that reminds us we can escape from it into the new. In the history of cybernetics, skeuomorphs acted as threshold devices, smoothing the transition between one conceptual constellation and another. Homeostasis, a foundational concept during the first wave, functioned during the second wave as a skeuomotph. Although homeostasis remained an important concept in biology, by about 1960 it had ceased to be an initiating premise in cybernetics. Instead, it performed the work of a gesture or an allusion used to authenticate new elements in the emerging constellation of reflexivity. At the same time, it also exerted an inertial pull on the new elements, limiting how radically they could transform the constellation.
Shannon's theory defines information as a probability function with no dimensions, no materiality, and no necessary connection with meaning. It is a pattern, not a presence. (Chapter 3 talks about the development of information theory in more detail, and the relevant equations can be found there.) The theory makes a strong distinction between message and signal. Lacan to the contrary, a message does not always arrive at its destination. In information theoretic terms, no message is ever sent. What is sent is a signal. Only when the message is encoded in a Signal for transmission through a medium-for example, when ink is printed on paper or when electrical pulses are sent racing along telegraph wires-does it assume material form. The very definition of "information," then, encodes the distinction between materiality and information that was also becoming important in molecular biology during this period.
Shannon's approach had other advantages that turned out to incur large (and mounting) costs when his premise interacted with certain predispositions already at work within the culture. Abstracting information from a material base meant that information could become free-floating, unaffected by changes in context. The technical leverage this move gained was considerable, for by formalizing information into a mathematical function, Shannon was able to develop theorems, powerful in their generality, that hold true regardless of the medium in which the information is instantiated. Not everyone agreed this move was a good idea, however, despite its theoretical power. As Carolyn Marvin notes, a decontextualized construction of information has important ideological implications, including an Anglo-American ethnocentrism that regards digital information as more important than more context-bound analog information.33 Even in Shannon's day, malcontents grumbled that divorcing information from context and thus from meaning had made the theory so narrowly formalized that it was not useful as a general theory of communication. Shannon himself frequently cautioned that the theory was meant to apply only to certain technical situations, not to communication in generaP4 In other circumstances, the theory might have become a dead end, a victim of its own excessive formalization and decontextualization. But not in the post -World War II era. The time was ripe for theories that reified information into a free-floating, decontextualized, quantifiable entity that could serve as the master key unlocking secrets of life and death. Technical artifacts help to make an information theoretic view a part of everyday life. From ATMs to the Internet, from the morphing programs used in Terminator II to the sophisticated visualization programs used to guide microsurgeries, information is increasingly perceived as interpenetrating material forms. EspeCially for users who may not know the material processes involved, the impression is created that pattern is predominant over presence. From here it is a small step to perceiving information as more mobile, more important, more essential than material forms. When this impression becomes part of your cultural mindset, you have entered the condition of virtuality.

Nevertheless, I think it is a mistake to underestimate the importance of virtuality, for it wields an influence altogether disproportionate to the number of people immersed in it. It is no accident that the condition of virtuality is most pervasive and advanced where the centers of power are most concentrated. Theorists at the Pentagon, for example, see it as the theater in which future wars will be fought. They argue that coming conflicts will be decided not so much by overwhelming force as by "neocortical warfare," waged through the techno-sciences of information. 37 If we want to contest what these technologies SignifY, we need histories that show the erasures that went into creating the condition of virtuality, as well as visions arguing for the importance of embodiment. Once we understand the complex interplays that went into creating the condition of virtuality, we can demystifY our progress toward virtuality and see it as the result of historically specific negotiations rather than of the irresistible force of technological determinism. At the same time, we can acquire resources with which to rethink the assumptions underlying virtuality, and we can recover a sense of the virtual that fully recognizes the importance of the embodied processes constituting the lifeworld of human beings.38 In the phrase "virtual bodies," I intend to allude to the historical separation between information and materiality and also to recall the embodied processes that resist this division.

A second way to think about the organization of How We Became Posthuman is narratively. In this arrangement, the three divisions proceed not so much through chronolOgical progression as through the narrative strands about the (lost) body of information, the cyborg body, and the posthuman body. Here the literary texts playa central role, for they display the passageways that enabled stories coming out of narrowly focused scientific theories to circulate more widely through the body politic. Many of the scientists understood very well that their negotiations involved premises broader than the formal scope of their theories strictly allowed. Because of the wedge that has been driven between science and values in U.S. culture, their statements on these wider implications necessarily occupied the position of ad hoc pronouncements rather than "scientific" arguments. Shaped by different conventions, the literary texts range across a spectrum ofissues that the scientific texts only fitfully illuminate, including the ethical and cultural implications of cybernetiC technologies.

What does this emphasis on narrative have to do with virtual bodies? FollOwing J ean-Franc:;ois Lyotard, many theorists of postmodernity accept that the postmodern condition implies an incredulity toward metanarrative. 41 As we have seen, one way to construct virtuality is the way that Moravec and Minsky do-as a metanarrative about the transformation of the human into a disembodied posthuman. I think we should be skeptical about this metanarrative. To contest it, I want to use the resources of narrative itself, particularly its resistance to various forms of abstraction and disembodiment. With its chronolOgical thrust, polymorphous digreSSions, located actions, and personified agents, narrative is a more embodied form of discourse than is analytically driven systems theory. By turning the technolOgical determinism of bodiless information, the cyborg, and the posthuman into narratives about the negotiations that took place between particular people at particular times and places, I hope to replace a teleology of disembodiment with historically contingent stories about contests between competing factions, contests whose outcomes were far from obvious. Many factors affected the outcomes, from the needs of emerging technologies for reliable quantification to the personalities of the people involved. Though overdetermined, the disembodiment ofinformation was not inevitable, any more than it is inevitable we continue to accept the idea that we are essentially informational patterns.

The first literary text I discuss in detail is Bernard Wolfe's Limbo. 42 Written in the 1950s, Limbo has become something of an underground classic. It imagines a postwar society in which an ideology, Immob, has developed; the ideology equates aggression with the ability to move. "Pacifism equals passivity," Immob slogans declare. True believers volunteer to banish their mobility (and presumably their aggreSSion) by having amputations, which have come to be regarded as signifiers of social power and influence. These amputees get bored with lying around, however, so a vigorous cyberneticS industry has grown up to replace their missing limbs. As this brief summary suggests, Limbo is deeply influenced by cybernetiCS. But the technical achievements of cybernetics are not at the center of the text. Rather, they serve as a springboard to explore a variety of social, political, and psychological issues, ranging from the perceived threat that women's active sexuality poses for Immob men to global East-West tensions that explode into another world war at the end of the text. Although it is unusually didactic, Limbo does more than discuss cyberneticS; it engages a full range of rhetorical and narrative devices that work both with and against its explicit pronouncements. The narrator seems only partially able to control his verbally extravagant narrative. There are, I will argue, deep connections between the narrator's struggle to maintain control of the narrative and the threat to "natural" body boundaries posed by the cybernetiC paradigm. Limbo interrogates a dynamiC that also appears in Norbert Wiener's work-the intense anxiety that erupts when the perceived boundaries of the body are breached. In addition, it illustrates how the body of the text gets implicated in the processes used to represent bodies within the text.

gs of race, gender, and sexuality. The chapter on contemporary speculative fictions constructs a semiotics of virtuality by shOwing how the central concepts ofinformation and materiality can be mapped onto a multilayered semiotic square. The tutor texts for this analYSis, which include Snow Crash, Blood Music, Galatea 2.2, and Terminal Games, indicate the range of what counts as the posthuman in the age of virtuality, from neural nets to hackers, biolOgically modified humans, and entities who live only in computer simulations.44 In follOwing the construction of the posthuman in these texts, I will argue that older ideas are reinscribed as well as contested. As was the case for the scientific models, change occurs in a seriated pattern of overlapping innovation and replication. I hope that this book will demonstrate, once again, how crucial it is to recognize interrelations between different kinds of cultural productions, specifically literature and science. The stories I tell here-how information lost its body, how the cyborg was created as a cultural icon and technolOgical artifact, and how humans became posthumans-and the waves of historical change I chart would not have the same resonance or breadth if they had been pursued only through literary texts or only through scientific discourses. The scientific texts often reveal, as literature cannot, the foundational assumptions that gave theoretical scope and artifactual efficacy to a particular approach. The literary texts often reveal, as scientific work cannot, the complex cultural, social, and representational issues tied up with conceptual shifts and technological innovations. From my point of view, literature and science as an area of specialization is more than a subset of cultural studies or a minor activity in a literature department. It is a way of understanding ourselves as embodied creatures living within and through embodied worlds and embodied words.
What, finally, are we to make of the posthuman?l At the beginning of this book, I suggested that the prospect of becoming posthuman both evokes terror and excites pleasure. At the end of the book, perhaps I can summarize the implications of the posthuman by interrogating the sources of this terror and pleasure. The terror is relatively easy to understand. "Post," with its dual connotation of superseding the human and coming after it, hints that the days of "the human" may be numbered. Some researchers (notably Hans Moravec but also my UCLA colleague Michael Dyer and many others) believe that this is true not only in a general intellectual sense that displaces one definition of "human" with another but also in a more disturbingly literal sense that envisions humans displaced as the dominant form of life on the planet by intelligent machines. Humans can either go gently into that good night, joining the dinosaurs as a species that once ruled the earth but is now obsolete, or hang on for a while longer by becoming machines themselves. In either case, Moravec and like-minded thinkers believe, the age of the human is drawing to a close. The view echoes the deeply pessimistic sentiments of Warren McCulloch in his old age. As noted earlier, he remarked: "Man to my mind is about the nastiest, most destructive of all the animals. I don't see any reason, ifhe can evolve machines that can have more fun than he himself can, why they shouldn't take over, enslave us, quite happily. They might have a lot more fun. Invent better games than we ever did."2 Is it any wonder that faced with such dismal scenarios, most people have understandably negative reactions? If this is what the posthuman means, why shouldn't it be resisted?

What about the pleasures? For some people, including me, the posthuman evokes the exhilarating prospect of getting out of some of the old boxes and opening up new ways of thinking about what being human means. In positing a shift from presence/absence to pattern/randomness, I have sought to show how these categories can be transformed from the inside to arrive at new kinds of cultural configurations, which may soon render such dualities obsolete if they have not already. This process of transformation is fueled by tensions between the assumptions encoded in pattern/randomness as opposed to presence/absence. In Jacques Derrida's performance of presence/absence, presence is allied with Logos, God, teleology-in general, with an originary plenitude that can act to ground signification and give order and meaning to the trajectory of history. 6 The work of Eric Havelock, among others, demonstrates how in Plato's Republic this view of originarypresence authorized a stable, coherent self that could witness and testifY to a stable, coherent reality. 7 Through these and other means, the metaphysics of presence front-loaded meaning into the system. Meaning was guaranteed because a stable origin existed. It is now a familiar story how deconstruction exposed the inability of systems to posit their own origins, thus ungrounding signification and rendering meaning indeterminate. As the presence/absence hierarchy was destabilized and as absence was privileged over presence, lack displaced plenitude, and desire usurped certitude. Important as these moves have been in late-twentieth-century thought, they still took place within the compass of the presence/absence dialectic. One feels lack only if presence is posited or assumed; one is driven by desire only if the object of desire is conceptualized as something to be possessed. Just as the metaphysics of presence required an originaryplenitude to articulate a stable self, deconstruction required a metaphysics of presence to articulate the destabilization of that self.

Indeed, it is not too much to say that in these and similar models, randomness rather than pattern is invested with plenitude. If pattern is the realization of a certain set of possibilities, randomness is the much, much larger set of everything else, from phenomena that cannot be rendered coherent by a given system's organization to those the system cannot perceive at all. In Gregory Bateson's cybernetiC epistemology, randomness is what exists outside the confines of the box in which a system is located; it is the larger and unknowable complexity for which the perceptual processes of an organism are a metaphor. 11 Significance is achieved by evolutionary processes that ensure the surviving systems are the ones whose organizations instantiate metaphors for this complexity, unthinkable in itself. When Varela and his coauthors argue in Embodied Mind that there is no stable, coherent self but only autonomous agents running programs, they envision pattern as a limitation that drops away as human awareness expands beyond consciousness and encounters the emptiness that, in another guise, could equally well be called the chaos from which all forms emerge.
To explore these resources, let us return to Bateson's idea that those organisms that survive will tend to be the ones whose internal structures are good metaphors for the complexities without. What kind of environments will be created by the expanding power and sophistication of intelligent machines? As Richard Lanham has pOinted out, in the information-rich environments created by ubiquitous computing, the limiting factor is not the speed of computers, or the rates of transmission through fiber-optic cables, or the amount of data that can be generated and stored. Rather, the scarce commodity is human attention. 14 It makes sense, then, that technological innovation will focus on compensating for this bottleneck. An obvious solution is to design intelligent machines to attend to the choices and tasks that do not have to be done by humans. For example, there are already intelligent -agent programs to sort email, discarding unwanted messages and pri- 0ritizing the rest. The programs work along lines similar to neural nets. They tabulate the choices the human operators make, and they feed back this information in recursive loops to readjust the weights given to various kinds of email addresses. After an initial learning period, the sorting programs take over more and more of the email management, freeing humans to give their attention to other matters.
In the posthuman view, by contrast, conscious agency has never been "in control." In fact, the very illusion of control bespeaks a fundamental ignorance about the nature of the emergent processes through which consciousness, the organism, and the environment are constituted. Mastery through the exercise of autonomous will is merely the story consciousness tells itself to explain results that actually come about through chaotic dynamics and emergent structures. If, as Donna Haraway, Sandra Harding, Evelyn Fox Keller, Carolyn Merchant, and other feminist critics of science have argued, there is a relation among the desire for mastery, an objectivist account of science, and the imperialist project of subdUing nature, then the posthuman offers resources for the construction of another kind of account.I8 In this account, emergence replaces teleology; reflexive epistemology replaces objectivism; distributed cognition replaces autonomous will; embodiment replaces a body seen as a support system for the mind; and a dynamic partnership between humans and intelligent machines replaces the liberal humanist subject's manifest destiny to dominate and control nature. Of course, this is not necessarily what the posthuman will mean-only what it can mean if certain strands among its complex seriations are highlighted and combined to create a vision of the human that uses the posthuman as leverage to avoid reinscribing, and thus repeating, some of'the mistakes of the past.


As we have seen, cybernetics was born in a froth of noise when Norbert Wiener first thought of it as a way to maximize human potential in a world that is in essence chaotic and unpredictable. Like many other pioneers, Wiener helped to initiate a journey that would prove to have consequences more far-reaching and subversive than even his formidable powers of imagination could conceive. As Bateson, Varela, and others would later argue, the noise crashes within as well as without. The chaotic, unpredictable nature of complex dynamics implies that subjectivity is emergent rather than given, distributed rather than located solely in consciousness, emerging from and integrated into a chaotic world rather than occupying a position of mastery and control removed from it. Bruno Latour has argued that we have never been modem; the seriated history of cybernetics-emerging from networks at once materially real, socially regulated, and discurSively constructed-suggests, for similar reasons, that we have always been posthuman. The purpose of this book has been to chronicle the journeys that have made this realization pOSSible. If the three stories told here-how information lost its body, how the cyborg was constructed in the postwar years as technological artifact and cultural icon, and how the human became the posthuman-have at times seemed to present the posthuman as a transformation to be feared and abhorred rather than welcomed and embraced, that reaction has everything to do with how the posthuman is constructed and understood. The best possible time to contest for what the posthuman means is now, before the trains of thought it embodies have been laid down so firmly that it would take dynamite to change them.24 Although some current versions of the posthuman point toward the antihuman and the apocalyptic, we can craft others that will be conducive to the long-range survival of humans and of the other life-forms, biological and artificial, with whom we share the planet and ourselves.

A few words on the two neoteric terms, cybertext and ergodic, are in order. Cybertext is a neologism derived from Norbert Wiener's book (and discipline) called Cybernetics, and subtitled Control and Communication in the Animal and the Machine (1948). Wiener laid an important foundation for the development of digital computers, but his scope is not limited to the mechanical world of transistors and, later, of microchips. As the subtitle indicates, Wiener's perspective includes both organic and inorganic systems; that is, any system that contains an information feedback loop. Likewise, the concept of cybertext does not limit itself to the study of computer-driven (or "electronic") textuality; that would be an arbitrary and unhistorical limitation, perhaps comparable to a study of literature that would only acknowledge texts in paper-printed form. While there might be sociological reasons for such a study, we would not be able to claim any understanding of how different forms of literature vary. The concept of cybertext focuses on the mechanical organization of the text, by positing the intricacies of the medium as an integral part of the literary exchange. However, it also centers attention on the consumer, or user, of the text, as a more integrated figure than even reader-response theorists would claim. The performance of their reader takes place all in his head, while the user of cybertext also performs in an extranoematic sense. During the cybertextual process, the user will have effectuated a semiotic sequence, and this selective movement is a work of physical construction that the various concepts of "reading" do not account for. This phenomenon I call ergodic, using a term appropriated from physics that derives from the Greek words ergon and hodos, meaning "work" and "path." In ergodic literature, nontrivial effort is required to allow the reader to traverse the text. If ergodic literature is to make sense as a concept, there must also be nonergodic literature, where the effort to traverse the text is trivial, with no extranoematic responsibilities placed on the reader except (for example) eye movement and the periodic or arbitrary turning of pages.

Typically, these objections came from persons who, while well versed in literary theory, had no firsthand experience of the hypertexts, adventure games, or multi-user dungeons I was talking about. At first, therefore, I thought this was simply a didactical problem: if only I could present examples of my material more clearly, everything would become indisputable. After all, can a person who has never seen a movie be expected to understand the unique characteristics of that medium? A text such as the I Ching is not meant to be read from beginning to end but entails a very different and highly specialized ritual of perusal, and the text in a multi-user dungeon is without either beginning or end, an endless labyrinthine plateau of textual bliss for the community that builds it. But no matter how hard I try to describe these texts to you, the reader, their essential difference will remain a mystery until they are experienced firsthand. In my campaign for the study of cybertextuality I soon realized that my terminology was a potential source of confusion. Particularly problematic was the word nonlinear. For some it was a common literary concept used to describe narratives that lacked or subverted a straightforward story line; for others, paradoxically, the word could not describe my material, since the act of reading must take place sequentially, word for word. This aporia never ceased to puzzle me. There was obviously an epistemological conflict. Part of the problem is easily resolved: hypertexts, adventure games, and so forth are not texts the way the average literary work is a text. In what way, then, are they texts? They produce verbal structures, for aesthetic effect. This makes them similar to other literary phenomena. But they are also something more, and it is this added paraverbal dimension that is so hard to see. A cybertext is a machine for the production of variety of expression. Since literary theorists are trained to uncover literary ambivalence in texts with linear expression, they evidently mistook texts with variable expression for texts with ambiguous meaning. When confronted with a forking text such as a hypertext, they claimed that all texts are produced as a linear sequence during reading, so where was my problem? The problem was that, while they focused on what was being read, I focused on what was being read from. This distinction is inconspicuous in a linear expression text, since when you read from War and Peace, you believe you are reading War and Peace. In drama, the relationship between a play and its (varying) performance is a hierarchical and explicit one; it makes trivial sense to distinguish between the two. In a cybertext, however, the distinction is crucial--and rather different; when you read from a cybertext, you are constantly reminded of inaccessible strategies and paths not taken, voices not heard. Each decision will make some parts of the text more, and others less, accessible, and you may never know the exact results of your choices; that is, exactly what you missed. This is very different from the ambiguities of a linear text. And inaccessibility, it must be noted, does not imply ambiguity but, rather, an absence of possibility--an aporia.

A reader, however strongly engaged in the unfolding of a narrative, is powerless. Like a spectator at a soccer game, he may speculate, conjecture, extrapolate, even shout abuse, but he is not a player. Like a passenger on a train, he can study and interpret the shifting landscape, he may rest his eyes wherever he pleases, even release the emergency brake and step off, but he is not free to move the tracks in a different direction. He cannot have the player's pleasure of influence: "Let's see what happens when I do this." The reader's pleasure is the pleasure of the voyeur. Safe, but impotent. The cybertext reader, on the other hand, is not safe, and therefore, it can be argued, she is not a reader. The cybertext puts its would-be reader at risk: the risk of rejection. The effort and energy demanded by the cybertext of its reader raise the stakes of interpretation to those of intervention. Trying to know a cybertext is an investment of personal improvisation that can result in either intimacy or failure. The tensions at work in a cybertext, while not incompatible with those of narrative desire, are also something more: a struggle not merely for interpretative insight but also for narrative control: "I want this text to tell my story; the story that could not be without me." In some cases this is literally true. In other cases, perhaps most, the sense of individual outcome is illusory, but nevertheless the aspect of coercion and manipulation is real. The study of cybertexts reveals the misprision of the spaciodynamic metaphors of narrative theory, because ergodic literature incarnates these models in a way linear text narratives do not. This may be hard to understand for the traditional literary critic who cannot perceive the difference between metaphorical structure and logical structure, but it is essential. The cybertext reader is a player, a gambler; the cybertext is a game-world or world-game; it is possible to explore, get lost, and discover secret paths in these texts, not metaphorically, but through the topological structures of the textual machinery. This is not a difference between games and literature but rather between games and narratives. To claim that there is no difference between games and narratives is to ignore essential qualities of both categories. And yet, as this study tries to show, the difference is not clear-cut, and there is significant overlap between the two. It is also essential to recognize that cybertext is used here to describe a broad textual media category. It is not in itself a literary genre of any kind. Cybertexts share a principle of calculated production, but beyond that there is no obvious unity of aesthetics, thematics, literary history, or even material technology. Cybertext is a perspective I use to describe and explore the communicational strategies of dynamic texts. To look for traditions, literary genres, and common aesthetics, we must inspect the texts at a much more local level, and I suggest one way to partition the field in chapters 4 through 7, each chapter dealing with a subgroup of ergodic textuality.

However, after the invention of digital computing in the middle of the twentieth century, it soon became clear that a new textual technology had arrived, potentially more flexible and powerful than any preceding medium. Digital systems for information storage and retrieval, popularly known as databases, signified new ways of using textual material. The database is in principle similar to the filing cabinet but with a level of automation and speed that made radically different textual practices possible. On the physical level, the surface of reading was divorced from the stored information. For the first time, this breaks down concepts such as "the text itself" into two independent technological levels: the interface and the storage medium. On the social level, huge texts could be browsed, searched, and updated by several people at once, and from different places on the globe, operations that only superficially seem to resemble what we used to call "reading" and "writing." Armed with a good search engine and a digital library, any college dropout can pass for a learned scholar, quoting the classics without having read any of them. Several new textual genres have emerged with digital computing and automation. Computer programs, complex lists of formal instructions written in specially designed, artificial languages, can be seen as a new type of the rhetorical figure apostrophe, the addressing of inanimate or abstract objects, with the magical difference that it actually provokes a response. Short, simple programs are often linear, but longer programs generally consist of collections of interdependent fragments, with repeating loops, cross-references, and discontinuous "jumps" back and forth between sections. Given the seminatural vocabulary of some modern programing languages, it is not uncommon for programers to write poems in them, often with the constraint that the "poegrams" (or whatever) must make sense to the machine as well. Programs are normally written with two kinds of receivers in mind: the machines and other programers. This gives rise to a double standard of aesthetics, often in conflict: efficiency and clarity. Since speed is a major quality in computer aesthetics, an unreadable program might perform much faster than a comprehensible one. The poetics of computer program writing is constantly evolving, and through paradigms such as object orientation it inspires practical philosophies and provides hermeneutic models for organizing and understanding the world, both directly (through programed systems) and indirectly (through the worldviews of computer engineers).

A related but reverse problem is the tendency to describe the new text media as radically different from the old, with attributes solely determined by the material technology of the medium. In these analyses, technical innovation is presented as a cause of social improvement and political and intellectual liberation, a historical move away from the old repressive media. This kind of technological determinism (the belief that technology is an autonomous force that causes social change) has been refuted eloquently by Langdon Winner (1986), James W. Carey (1988), and others but continues, nevertheless, to dominate the discussion. In the context of literature, this has led to claims that digital technology enables readers to become authors, or at least blurs the (supposedly political) distinction between the two, and that the reader is allowed to create his or her own "story" by "interacting" with "the computer." The ideological forces surrounding new technology produce a rhetoric of novelty, differentiation, and freedom that works to obscure the more profound structural kinships between superficially heterogeneous media. Even the inspiring and perceptive essays of Richard Lanham (1993) are suffused by this binary rhetoric and, ultimately, dominated by politics at the expense of analysis. fragment.. Whether concepts such as "computer literature" or "electronic textuality" deserve to be defended theoretically is by no means obvious, and they will not be given axiomatic status in this book. The idea that "the computer" is in itself capable of producing social and historical change is a strangely ahistorical and anthropomorphic misconception, yet it is as popular within literary-cultural studies as it is in the science fiction texts they sometimes study.Often, in fact, science fiction portrays the technology with an irony that the critical studies lack (see, e.g., William Gibson's short story, "Burning Chrome," in Gibson 1986). Most literary theories take their object medium as a given, in spite of the blatant historical differences between, for instance, oral and written literature. The written, or rather the printed, text has been the privileged form, and the potentially disruptive effects of media transitions have seldom been an issue, unlike semantic transitions such as language translation or intertextual practices. At this point, in the age of the dual ontology of everyday textuality (screen or paper), this ideological blindness is no longer possible, and so we have to ask an old question in a new context: What is a text? In a limited space such as this, it is impossible to recapture the arguments of previous discussions of this question. And since the empirical basis for this study is different from the one assumed in these discussions, the arguments would be of limited value. In the context of this study, the question of the text becomes a question of verbal media and their functional differences (what role does a medium play?), and only subsequently a question of semantics, influence, otherness, mental events, intentionality, and so forth. These philosophical problems have not left us, but they belong to a different level of textuality. In order to deal with these issues responsibly, we must first construct a map of the new area in which we want to study them, a textonomy (the study of textual media) to provide the playing ground of textology (the study of textual meaning).

Strangely, the struggle between the proponents and opponents of "digital literature" deteriorates usually on both sides into material arguments of a peculiar fetishist nature. One side focuses on the exotic hardware of the shiny new technologies, like CD-ROM. Witness especially the computer industry slogan, "information at your fingertips," as if information were somehow a touchable object. The other side focuses on the well-known hardware of the old technology, the "look and feel" of a book, compared to the crude letters on a computer screen. "You can't take it to bed with you" is the sensuous (but no longer true) refrain of the book chauvinists. Isn't the content of a text more important than these materialistic, almost ergonomic, concerns? What these strangely irrelevant exuberances reveal, I think, is that beyond the obvious differences of appearance, the real difference between paper texts and computer texts is not very clear. Does a difference even exist? Instead of searching for a structural divide, this study begins with the premise that no such essential difference is presumed. If it exists, it must be described in functional, rather than material or historical, terms. The alternative, to propose an essential difference and then proceed to describe it, does not allow for the possibility that it does not exist and is, therefore, not an option. Whether it exists or not is not of great importance to this thesis, however, as such knowledge would not make much practical difference in the world. The emerging new media technologies are not important in themselves, nor as alternatives to older media, but should be studied for what they can tell us about the principles and evolution of human communication. My main effort is, therefore, to show what the functional differences and similarities among the various textual media imply about the theories and practices of literature. The exploration is based on the concepts and perspectives of narratology and rhetoric but is not limited to these two disciplines. I argue that existing literary theory is incomplete (but not irrelevant) when it comes to describing some of the phenomena studied here, and I try to show why and where a new theoretical approach is needed. My final aim is to produce a framework for a theory of cybertext or ergodic literature and to identify the key elements for this perspective.

In the current discussions of "computer literacy," hypertext, "electronic language," and so on, there seems to emerge an explicit distinction between the printed, or paper-based, text and the electronic text, both with singular and remarkably opposing qualities. The arguments for this distinction are sometimes historical, sometimes technological, but eminently political; that is, they don't focus on what these textual genres or modes are but on their assumed functional difference from each other. Such a strategy is useful for drawing attention to, but less so for the analysis of, the objects thus constructed. It might have been tempting to follow this rhetoric in my investigation of the concept of cybertext and to describe a dichotomy between it and traditional, conventional literature; but the meaning of these concepts is unstable to the point of incoherence, and my construct would therefore probably have reached a similar degree of uselessness. Cybertext, then, is not a "new," "revolutionary" form of text, with capabilities only made possible through the invention of the digital computer. Neither is it a radical break with old-fashioned textuality, although it would be easy to make it appear so. Cybertext is a perspective on all forms of textuality, a way to expand the scope of literary studies to include phenomena that today are perceived as outside of, or marginalized by, the field of literature--or even in opposition to it, for (as I make clear later) purely extraneous reasons. In this study I investigate the literary behavior of certain types of textual phenomena and try to construct a model of textual communication that will accommodate any type of text. This project is not as ambitious as it might sound, since the model is provisional and empirical and subject to future modification should any "falsificatory" evidence (such as an unpredictable object) appear. This pragmatic model is presented in detail in chapter 3. Multi-User Dungeons The rest of this introductory chapter discusses the conceptual foundations and implications of this approach and establishes the terminology applied in the analytical chapters. These chapters (4 through 7) each takes on a main category (or genre) of cybertext roughly corresponding to the results of the analysis in chapter 3: hypertext, the textual adventure game, computer-generated narrative and participatory world-simulation systems, and the social-textual MUDs of the global computer networks. This pragmatic partitioning, which derives from popular convention rather than from my own theoretical model, is motivated by my strong belief that, in such a newly awakened field, theoretical restraint is imperative. Theories of literature have a powerful ability to co-opt new fields and fill theoretical vacuums, and in such a process of colonization, where the "virgin territory" lacks theoretical defense, important perspectives and insights might be lost or at least overlooked. When we invade foreign ground, the least we can do is to try to learn the native language and study the local customs. Although several studies have already been carried out within most of these subfields, almost none have produced overarching, or universal, perspectives or engaged in a comparative analysis of all the forms of textuality examined here. Therefore, these previous approaches are discussed in their respective chapters rather than in this general introduction.

As can be inferred from its etymology, a cybertext must contain some kind of information feedback loop. In one sense, this holds true for any textual situation, granted that the "text" is something more than just marks upon a surface. A reader peruses a string of words, and depending on the reader's subsequent actions, the significance of those words may be changed, if only imperceptibly. The act of rereading is a crucial example: the second time we read a text, it is different, or so it seems. How can we know the text from the reading? Sometimes, a reader may influence the text for other readers, even if all the "marks on the pages" stay the same: a dramatic example is the ayatollah Khomeiny's reaction to The Satanic Verses. The conventional split between text and reading (between the "intentional object" and the "mental event"), or signifiant and signifié, is not an impermeable membrane: leaks occur constantly; through various stages of reception such as editing, marketing, translation, criticism, rediscovery, canonization, or banishment. These well-known processes are not entirely trivial, however, because they remind us that a text can never be reduced to a stand-alone sequence of words. There will always be context, convention, contamination; sociohistorical mediation in one form or another. Distinguishing between a text and its readings is not only necessary, it is also quite impossible--an ideal, in other words. On the one hand we need the image of "the text" in order to focus on anything at all; on the other hand we use the metaphor of "reading" to signal that our apprehension of a text will always be partial, that we never quite reach the "text itself," a realization that has led certain critics to question the very existence of such an object (see, for instance, Fish 1980). This hermeneutic movement or desire--perhaps better described as asymptotic than circular--holds true for all kinds of textual communication, but the particular organization of a text can make both the reader's strategic approach and the text's perceived teleology very distinctive, perhaps to the point where interpretation is stretched beyond the cognitive bounds of a singular concept. It is this field of varying textual organization that this study attempts to clarify. The differences in teleological orientation--the different ways in which the reader is invited to "complete" a text--and the texts' various self-manipulating devices are what the concept of cybertext is about. Until these practices are identified and examined, a significant part of the question of interpretation must go unanswered.

Previous models of textuality have not taken this performative aspect into account and tend to ignore the medium end of the triangle and all that goes with it. In his phenomenology of literature, Ingarden (1973, 305-13) insists that the integrity of the "literary work of art" depends on the "order of sequence" of its parts; without this linear stability the work would not exist. While Ingarden here certainly acknowledges the importance of the objective shape of the text, he also reduces it to a given. This taken-for-grantedness is hardly strange, since it is only after we have started to notice the "medium" and its recent shifting appearances that we can begin to observe the effect this instability has on the rest of the triangle. As Richard Lanham (1989, 270) observes, literary theorists have for a long time been in the "codex book business," restricting their observations (but not their arguments) to literature mediated in a certain way. Even within the field of codex literature there is room, as experimentalists from Laurence Sterne to Milorad Pavic have demonstrated, for mediational variation, but these attempts have not, apparently, produced sufficient contrast to provoke a systematic investigation of the aesthetic role of the medium (a notable but much too brief exception being McHale 1987, chap 12). There is also the fascinating phenomenon known as "Artists' Books," an art movement that originated in the sixties and dedicated to the creation of unique works of art that challenge the presumed properties of the book from within (cf. Strand 1992b and Lyons 1985). Cybertext, as now should be clear, is the wide range (or perspective) of possible textualities seen as a typology of machines, as various kinds of literary communication systems where the functional differences among the mechanical parts play a defining role in determining the aesthetic process. Each type of text can be positioned in this multidimensional field according to its functional capabilities, as we shall see in chapter 3. As a theoretical perspective, cybertext shifts the focus from the traditional threesome of author/sender, text/message, and reader/receiver to the cybernetic intercourse between the various part(icipant)s in the textual machine. In doing so, it relocates attention to some traditionally remote parts of the textual galaxy, while leaving most of the luminous clusters in the central areas alone. This should not be seen as a call for a renegotiation of "literary" values, since most of the texts drawn attention to here are not well suited for entry into the competition for literary canonization. The rules of that game could no doubt change, but the present work is not (consciously, at least) an effort to contribute to the hegemonic worship of "great texts." The reason for this is pragmatic rather than ethical: a search for traditional literary values in texts that are neither intended nor structured as literature will only obscure the unique aspects of these texts and transform a formal investigation into an apologetic crusade. If these texts redefine literature by expanding our notion of it--and I believe that they do--then they must also redefine what is literary, and therefore they cannot be measured by an old, unmodified aesthetics. I do not believe it is possible to avoid the influence from literary theory's ordinary business, but we should at least try to be aware of its strong magnetic field as we approach the whiter spaces--the current final frontiers--of textuality.

Weibel had above all placed emphasis in his exhibition concept on examining the problematic issues of totalitarian systems such as Fascism, Communist dictatorship or National Socialism and to present in critical terms his long considered views of Austria’s identity as a “country without qualities” (a reference to the title “Man Without Qualities” of the key Robert Musil novel). Austria had never enjoyed a conflict-free relationship to modernity, especially in the interwar years, when its representatives had been banished and exiled, and after the “Anschluss”, the incorporation of Austria into the Third Reich, Austria had denied and suppressed its pernicious role as a guilty party in the Holocaust. This project of negation of criminal involvement and an abysmal identity transfer to that of a victim had been only too gladly passed down to the present time and the problems blithely swept under the carpet. Peter Weibel has rebelled from his earliest youth against this public denial of an “expulsion of the intelligentsia from Austria” both in spectacular art events and performances as also in his theoretical writings and has remained an intellectual authority on contemporary political and topical social issues.
In the meantime we had also engaged an architect, since this exhibition was to be spread among three venues due to the enormous volume of art works covered. The choice fell on Manfred Wolff-Plottegg, who had congenial relations with Weibel in implementing exhibition projects from his youth through to the present. This background naturally meant that he was no easy case to deal with either, and he held with unrelenting firmness to each of his “Utopias,” whether the issue was a deconstruction of the Kuenstlerhaus roof, or pushing through his plan for the development of unconventional wall structures in steel for the Baroque halls of the Neue Galerie, in which he brought our restorer to the point of madness, or even his plan of filling the city with silage bales for use as an advertising medium. The conflict potential rose with the temperature in the course of an unusually hot summer. All holidays were cancelled and any of the usual divisions between the working day and the weekend, day, and night gradually vanished. Our eating habits changed – it became a joy to have the pizza service arrive somewhere around midnight bringing us our “lunch”; Red Bull and Bach-flower drops were used to keep up our concentration through the early hours of the day while reading and editing the galley proofs that were being set manually. Alexandra measured out the dimensions for the illustrations using a ruler, she cut and glued them for the layout that Weibel had specified.
Avant-gardes of the twentieth century fought against the (art) institutions in the name of each individual artist’s freedom – the freedom to overcome the limits of these institutions, to break free from them, flout obsolete standards and create the outrageous. Today people still like to criticize institutions, aiming at their disempowerment and abolition. Meanwhile, however, the meaning of this critique has changed completely. Today criticizing institutions serves only to conceal the fact that these mighty, normative institutions de facto no longer exist. Today’s art institutions are usually weak willed, have no money and are scarcely perceived by the wider public. They cannot and do not want any longer to set up binding aesthetic standards. They would rather chase after fashions spread by the media; they seek to be cool and hip to appeal to “young visitors.” The great, mighty museum director with indisputable power over his collection, or the art critic with a farreaching influence who sets directions and dictates his taste to the public: these are figures that belong to a long-gone age. Today’s criticism regarding cultural institutions can only be interpreted as a nostalgic invocation of this past epoch – an attempt to conceal the real weakness of today’s art institutions. Under these conditions the artist no longer has a chance to orientate himself according to existing art institutions, be it in the sense of adapting to the standards they have established, or in the sense of a revolution against these standards. Basically, the artist is left with but one possibility: to found an institution of his/her own, making themselves an institution. Certainly this way is much more cumbersome than the easier way of common criticism of institutions. Hence there are only very few artists who have been down this road consistently. And these artists deserve our admiration. Amongst these very few, we most assuredly find Peter Weibel. Weibel understood from early on that advanced art has no stable place in our society and that this place has yet to be created – on all levels of topical artistic practice. As an artist, if one does not choose to make oneself a curator, one is dependent on the curator. And when the artist wishes to assume the role of curator, his best move is to keep a museum space constantly available for himself. A space in which he or she can experiment with all kinds of exhibitions. Today’s artists cannot count on a binding comment on their work unless they become theoreticians in their own right and comment on their own work. They could also invite other theoreticians to write for a publication conceived by the artist, and then design and edit the book themselves. Ideally they would print it themselves as well. In this the artist would always be dependent on the technology continuously developing in art production and distribution. If they don’t develop the production on their own account. In this way they could avoid being dependent on using only the things other people had produced as a technological progress. This would mean the artists themselves must create and manage an institution, which would further develop artistic techniques. They must run this institution, employ people and have the means to pay them. So the artists have to ask themselves how they would not only passively reflect and criticize the economical, political and administrative aspects of their artistic practice, but also how they could actively design, organize and manage them on a daily basis. In other words: Once the artists stop to act as a decorator for the state, they have no choice but to become a state within a state, if they choose not just to cater for the art market.

Weibel’s art, however, is not limited to the creation of this image of artistic sovereignty. Beyond art institutions there is another dimension that increasingly determines today’s art world, i.e. the beholder. The avant-garde promised the artists absolute individual freedom regarding all their artistic decisions – the freedom not only to choose freely the theme of their work, but also to dispose freely of all artistic means, independent of all traditions, conventions and criteria of good taste or the mastership that still largely determined artistic creation before the rise of radical avant-garde at the beginning of the twentieth century. This newly promised freedom had a drug-like effect on the artists at first and created an unprecedented intoxication. This is why so many works arose from these early times which we still admire today. At the same time, however, one cannot deny that little has remained of this early euphoria. Today, artists no longer feel free – rather, they feel that their subjectivity has but little or no relevance for the art world. The reason for this new powerlessness on the part of the artist can be put down to the fact that, in the attempt to free themselves, at the same time the artists of the historical avant-garde committed a somewhat fatal act, and one whose importance they themselves did not notice in the beginning: they freed the beholder. The beholder was relieved of any of the criteria he used, or was supposed to use, for his judgment of a piece of art. The artist actively liberated himself from these criteria – and the beholder lost them. This made the beholder extremely insecure. He no longer knew how to react to the pieces of art that were presented to him. He felt helpless vis-à-vis the arbitrariness of the artist, utterly un-able to defend himself by means of a well-founded judgment. This initial insecurity reduced significantly with time. The further modern art developed, the more the beholder began both to cherish and relish the insecurity of freedom given to him by the artist. There are of course more beholders than artists and, according to democratic conditions, we all know that the minority must bend to the will of the majority. In earlier times the artist was able to reject the public’s judgment by asserting that they had not properly understood his work. The artist could call upon his mastership, the historical originality of his work and the strength of his inspiration, etc. Today, however, these justifications no longer apply, as the public feels totally free in their judgment. If they simply do not like something, they are not prepared to let themselves be convinced otherwise; the tables have turned. The avant-garde has directed the artist’s freedom against all of the public’s judgments. Now the freedom of the public’s judgment is directed against all reasoning and explanations on the part of the artist.
Even if our culture no longer believes in the criteria of taste, it certainly believes in knowledge and in technology. Art’s liberation from the beholder’s taste can only happen in terms of a mechanization of art. Or, in other words, its re-mechanization. Art was always, in the first place, a matter of technique. Only the historical avant-garde discredited and then abolished art production’s old techniques. Malevich’s Black Square and Duchamp’s Ready-mades introduced an age of the de-mechanization of art. It was particularly at this point that artists lost their cultural privilege regarding the beholder. If an artistic decision is totally free to declare an object or a random form a work of art, then the beholder must also be free not to accept this decision. Everything becomes a question of personal or individual taste. If the artist’s taste is contrary to the beholder’s taste then, as we all know, the loser is the artist. The avant-garde was an attempt to liberate art by de-mechanizing it, by liberating it from capability and knowledge, by equalizing the act of artistic creation with aesthetic judgment. Today, this de-mechanization of art, which was carried out by the historical avant-garde, increasingly presents itself as the preparation for a new phase of its radical re-mechanization. The avantgarde abolished old techniques such as drawing, painting and sculpture – or relativized them in their meaning. However, after a phase of being freed from technology, the re-mechanization of art has begun. It started with the use of new digital techniques of image production and distribution. The artist has again become a technician, a specialist and a producer, thereby once again establishing cultural distance between himself and the beholder. Peter Weibel had an early understanding of the opportunity offered by the new determination of arts and he seized it. Many artists and theorists have preached the return of art to technology and knowledge in the times of the Post-modernism of the 1970s and 1980s. However, this almost always implied some kind of ironic return to traditional artistic techniques. One wanted to protect oneself from the all too mighty mass media and define other, alternative spaces. For the same reason others use to legitimate their farewell from the avant-garde, as Weibel connects to the avant-garde’s traditions. The opportunity and even the need to restructure the avant-garde program, results for Weibel from the fact that art has definitely left behind its traditionally closed rooms and must act amidst today’s technological, medialized world. Hence the avant-garde was right in letting go of traditional art techniques, directing the view of the beholder onto the new technical world – even with a critical purpose. Today, however, it is widely pronounced that art should give up its critical, elitist, avant-gardist attitude and should be easy to digest. In short: that art, as soon as it starts to be produced in the context of and by using mass-media procedures, should be obliged to adapt aesthetically to the laws of mass-media dissemination too.
Here, the avant-garde is not dismissed as obsolete but rather transcended by means of a re-mechanization of art and continued in its critical impetus. The repertoire of images circulating in mass media networks is very limited. Not only so-called images of the other, but also images created by science, interactive images, purely subjective images, let alone abstract images, usually have no chance of entering large mass-media networks. It is, however, just these denied, excluded images that Weibel works with. Images which can be created using technical media and are hence indeed technically compatible with the networks of massmedia dissemination. At the same time, they display an aesthetic incompatibility with these networks. In this sense Weibel’s artistic practice is a direct continuation of the artistic practice of the historical avant-garde under the conditions of its technical and medial topicality. This would imply a continuation of the critical discussion of mimetic illusion.

This insight, however, would only worry an engineer, not an artist. Weibel is very well informed about the limits of logic, since he has seriously and systematically researched the field. Artists like to use computer programming and simulation because they like to demonstrate how the system loses its own systematic, how strict logic leads to paradoxes and how an ambivalence is created by an obsessive search for unambiguousness. Art is indeed interested in system, structure and program, but mainly where they lead themselves ad absurdum. It is exactly in this drift toward the absurd that it becomes clear how the image of thinking differs from thinking itself and that the living cannot easily be mimetically duplicated. Today’s artists, working with communicative and “intelligent” media, are more interested in the blind alleys, disturbances and absurdities of media communication than in its acquisitions. This is why the re-mechanization of art is certainly not just about naive enthusiasm or worship of technology. On the contrary: it allows not only an ideological claim of the limits of technology but also their technical analyses. But it is not mere pleasure in chaos that makes artists interested in the dysfunctional, in the deceptive element of computer-controlled and communicative processes. It is instead a systematic and critical analysis of mimetic procedure by means of art. This is a task modern art has imposed on itself and in the meantime it has become tradition. Except that today it is the mimesis of thinking, not reality. The analytical, critical dimension of Weibel’s occupation with the operational logic of today’s computer-controlled and communication systems, which he pursues in the context of his work at ZKM Karlsruhe, is already detectable in his early works dealing with the logic of mass media and everyday communication. These earlier art works can be seen mainly as examples of the artist’s critical occupation with the conventions of today’s media culture. These works often have a clear political dimension. They frequently refer to the chances as well as to the shortcomings and absurdities of today’s media world. However, mostly these works display a very specific sense of humor. As we all know, Soren Kierkegaard drew a strict line between irony and humor. For Kierkegaard, irony was the manifestation of a seemingly endless subjectivity aiming for a triumph over the finite character of things. Humor was for him the result of this subjectivity’s insight about its own finiteness.

Don’t worry, this is not an attempt to trace family circumstances or events way back in Odessa. The making of spiteful allegations is not our purpose either but rather to take a leap in the dark and to attempt an investigation of one of the most lucid creative contributions to Austrian art since 1945 – the work of Peter Weibel. Lügt (lies) was the message written on a board the artist held up beneath the sign of a police station in 1971. Let us forget for a moment that it is above all federal institutions, which through their practice contribute in no way at all to the good of certain groups of people. Indeed, these institutional inventions have set themselves objectives such as the freedom of the individual or social progress, or simply, the welfare of mankind, yet, there are also some quite contrary side effects, extending as far as the blatant discrimination for some of the so-called “supported.” If this were not the case, in our representational democracies we would be obliged to assume that the police are ultimately the tool of the representatives of the people (politicians). They again, are elected by the people (us), representing it (us). They are the people (us). So if the policeman lies, this is first of all an incident, maybe resulting from a certain specific personality structure. For instance because the policeman as an individual is given too much personal power which as a consequence he might abuse to the detriment of other individuals. Lies are told, however, also by the institution, the state, the representatives of the people, by the people themselves – and finally by us. In this paradoxical situation you too, dear reader, are lying every bit as much as the author of these lines – and consequently Weibel too. We know what the artist is on about and what he is denouncing, at the same time, however, we understand the limits of linguistic possibilities. Thus in striving for freedom and knowledge, we are forced to contrast the state lie with our own lie, whilst the notion is relativized as soon as the viewpoint changes. In the most radical moment of anti-art, in the 1960’s, avant-garde artists – building their work on the heritage of Dada, Surrealism and Constructivism – were able to shatter the foundations of bourgeois society, and thereby create fear. The traditional self-conception of art at that time was after all based on the assumption that art per se was a part of bourgeois society and thus part of its own system of rule and power. So if art crosses the divide to become an expression of both cultural and political rebellion, its demand for truth will change. For its “counterpart” this becomes a lie that must be fought against. Whilst for the avant-garde, art, now transformed into a weapon, will reveal the lie of the bourgeois capitalist society and hence of the federal power. So we should look at Peter Weibel’s practice with regard to this field of conflict. Seemingly remote sources from anarchism to scientific-theoretical models were suddenly applied in order to help art to get out, to transform, helping it to new relevance.

Polizei lügt (police lie) is one of a series of actions that was documented photographically. Weibel changed the script on public notices. Oberlandesgericht became Oberschandesgericht (Higher Regional Court became Higher Disgrace Court), and Rechtsanwalt became Rechtsgewalt (attorney at law became violence of the law). This “scriptual terrorism” came into existence simply through holding the appropriate letters at the corresponding positions in the word. These interventions were a more direct invasion of public space than the graffiti being created at this time in New York City. As a potential for the denotation of an actual content, a warning, a revolt against state institutions, these “attacks” create feelings of insecurity. The graffiti by contrast no longer contains any direct message, for they have internalized the message and have been radicalized in their formal qualities. “Being neither denotation nor con-notation, they evade the principle of nomenclature and break as empty signifiers into the sphere of the fulfilled signs of the city, which is dissolved in turn by their pure presence.”2 Thus graffiti represent a variation – derived from the sub-culture – on linguistic criticism. Weibel’s series were titled attacks and still speak the language of the protest movement: do it! It also represents a form of subversive poetic statement, reducing content to its opposite through linguistic intervention, while plumbing the limitations of what language in fact can do. Weibel has himself studied and done academic work on the issue of philosophy as criticism of language, culminating in the formal thinking of the mathematician Kurt Gödel. Gödel has provided the quasi salvation of the “liar” paradox in his statement “I cannot be proved.” If Nietzsche took God from us, then Gödel would seem to have robbed us of ourselves. Because where are we if we cannot be proved or demonstrated? There, perhaps, where God is... A connection between linguistic skepticism and individual social rebellion was developed still further in the protest movement of the 1960’s of which Weibel was a part. This connection provides a basic framework, in which the content and socially critical expressions within Weibel’s art can be developed. He did this in the mid 1960’s in literature and as an extension from this into the fine arts. He would not be Weibel if he had not also created a theoretical foundation for his artistic actions. He provides coordinates for the clarification of his procedure in his Sub-geschichte der Literatur (A Sub-History of Literature).3 Here we learn that the official history of literature is not “a history of human invention through, with, and in language” but “rather a collection of felicities and self-help.” Thus no basic or social change is possible without bringing about a change in the formal structure. The hero, in content that is, is not an adequate figure however when presented in traditional costume. The Russian linguist Roman Jakobson provided a formulation of this development in 1921, by describing the poetic procedure as the “sole hero” of literature. The formulation of a sub-history, such as that of the avant-garde, comprises in itself a social-political dimension, precisely analogous to the content which it reports in its special cases.

Weibel’s early experiments began in 1964, downwind of the Vienna Group that had begun to assemble in the 1950’s. Direct work was done by this movement during the 1950’s and 1960’s as a sort of continuation, in the most radical manner possible, of the results achieved by the historical avant-garde (Dada, Futurism, Surrealism, Constructivism u.s.f.). Vienna was by no means ahead of the times in this decade – it was provincial, traditional and marked with the brown striations of a Nazi past. Neo-avant-gardist movements such as the Vienna Group had an absolutely unsettling character; the conviction that the transformation of bourgeois aesthetics would of necessity go hand in hand with the transformation of bourgeois society was here understood as a matter of course. Criticism of both the state and of reality by means of a criticism of language contributed on the one hand to an overstepping of the aesthetic consensus of the times and on the other hand to the transgression of genre frontiers. In anticipation of the “linguistic turn,” the artists expanded into other media such as photography and film, and, even more radical forms of artistic practice such as happenings, Concept Art, and actions. In other words, change-related thinking was by no means solely restricted to language and the arts, and the borderline between art and life was shifting in consequence. The Eight–Points–Proclamation of the Poetic Act, written in 1953 by H.C. Artmann, acts here to a certain extent as godfather.4 The expansion of the definition of language first took place through the inclusion of sounds and words, but continued with another phase of development that included other materials as well, up to the point where the poet could get by without using any kind of verbal language whatsoever. The visual dimension of the written text was rapidly overcome by Weibel as well, whose own work developed along the lines of action texts, motion poems, photo poems, object poems, process poems, and, as a concluding sort of gesture, the radical use of one’s own body. In works such as raum der sprache – skulptur mit angeschlossenem organismus (space of speech – sculpture with a connected living organism) Weibel had his own tongue set in a block of concrete. The point that is articulated in this gesture ranges from general speechlessness to the achievement of a bodily and social effect on people through the use of speech. What else can it be but a lack of freedom, after all, when the letter of the law is interpreted by a judge? What is possible for a judge, in setting his organ of speech in motion, is altogether impossible for the person whose tongue is sealed into a wall of concrete and marked by bodily repression.

Herbert Marcuse spontaneously offers a helping hand here: “Only when art remains negative is it in a position to negate existing reality itself.” And he continues: “Only authentic art is negative and this in the sense that it refuses to obey the existing reality, with its language, its order, its conventions and in its images. (...) To this extent art provides a representation of a free society and of closer human relationships. But art cannot go beyond this.” During the student revolts of the 1960’s, artists in a wide range of fields argued about the commitment of art within the context of the revolutionary process. There was relatively little to hear about these ideas in this respect in Austria, however. The work of the Viennese Actionists (e.g. Kunst und Revolution / Art and Revolution, 1968) represented one of the few contributions to this. Weibel, who was to some extent an active participant in Viennese Actionism and provided theoretical support for it, was once again the exception, however, for no other significant artistic work of Austrian origin provided such thoroughly committed contributions to these various processes. Michail Bakunin should be permitted to have a word here as well. For him, positive means reactionary, an acceptance of the status quo and thus ultimately a sort of quietness. Negativity is for Bakunin, as he wrote in 1842 in his essay Die Reaktion in Deutschland (Reaction in Germany), the democratic principle, which in his day represented the negation of an existing reality, and indeed ultimately represented a sort of movement. He argues from this that the negative alone has the right to existence, closing with the celebrated words: “The lust to destroy is also a creative lust!”6 This sentence has had a great influence of course in artistic and intellectual circles through to the present day. The intensive desire for change in the midst of an unbearable reality sounds like a trumpet call for all avant-garde demands and manifestos. This same phrase applies to the work of Dada, through Surrealism, and even the Viennese Actionists, just as it is to be encountered in linguistic criticism of that time, in its most radical and brutal formulations. In short, Bakunin’s sentence is the legitimizing basis for all such antipositions, and not just in art.

It is true that communicative media already have established a continuity in time and space: telephone, fax, electronic mail, numerical and telematic networks, radio, television, the press, etc. This continuity is still not the continuity of the active and living thought, singular and differentiated, emergent and cohering everywhere, but rather a network for the transportation of information. Do the viewers of a televised transmission share a community? Do they bring together their experiences and their intellectual powers? Do they envisage and perfect new mental models of a situation together? Do they even exchange arguments? No. Their brains are not yet cooperating. The continuity effected by the media is only physical. It is a necessary prerequisite of the intellectual continuity but not sufficient in itself. Until this morning the work of writing was undoubtedly one of the most efficient means for the production of collective thought ever invented. The network of libraries keeps records of the creation and the experience of myriads of dead and living human beings. The fragile filament of memory is re-established, dormant thoughts revivified from generation to generation through the processes of reading and of interpreting. Translations from one language to another, or from one discipline to another, assure the communication between detached spaces of thought.
But by its nature the classical form of writing is a static and discontinuous system of signs. It is an inert, parcelled, dispersed body becoming more and more enormous each moment, and its unification and resuscitation requires that each individual sacrifice years and years to research, interpretation to the establishment of connections. As a remedy to the present situation, virtual worlds of collective intelligence will see the development of new forms of writing: animated pictograms, moving languages that will preserve traces of their interaction with navigators. By itself, the collective memory will organize itself, unfold itself anew for each navigator according to his interests and his previous traversings of the virtual world. The new space of signs will be sensitive, active, intelligent, at the service of its explorers. I ask again: what is interpretation? The subtle mind attempting to invite the inert body of letters into a dance.The evocation of the breath of the author in front of dead signs. The haphazard reconstruction of the knot of affects and of images in which the text originates. And, finally, the production of a new text, that of the interpreter. But what if the signs are alive? What if the image-text or the space-thought continuously grows, proliferates and metamorphoses itself to the beat of the collective intelligence? What if the leaden characters cede their place to some dynamic and translucent substance? What if the opacity of the gigantic stratifications of texts effaces itself in front of a flowing and continuous milieu the center of which is always occupied by its explorer? After the encounter between the vivifying spirit and the dead letter, after the dialectic of the corpus and the oral tradition, comes a new mode of the construction of the continuity of thought, a mode making possible the participation of everyone in the adventure of a nomadic language.

The first nomadic people followed after their flocks searching for nourishment, moving about following the rhytm of seasons and of rains. Today we are nomads following after the future of humanity, the future traversing us and made by us. The human being has become its own climate, an endless season with no return. We are hord and flock intermingled, more and more attached to our instruments and to the world moving with us, strolling on a new steppe each day. Neanderthal men, well adapted to the wonderful hunting expeditions on the glacial tundra, became extinct when the climate abruptly became warmer and more humid. Their natural game disappeared. Despite their intelligence these growling or mute men had no voice, no language with which to communicate with each other. Therefore the solutions found for their problems here and there could not be made more general. They remained dispersed even when they were faced by the transformation of the world surrounding them. They did not change with it. Today the homo sapiens is face to face with a fast modification of its surroundings, a transformation of which it is the collective involuntary agent. We may either cross a new threshold, a new stage in the evolution of man, by inventing some attribute of humanity as essential as language but on a superior level. Or we may continue to "communicate" through the media and to think in institutions detached from one another, organizing moreover the suffocation and division of intelligences. In the second case the only problems we would still be confronting would be problems of survival and of power. But if we were to take the route of the collective intelligence, we would gradually invent techniques, systems of signs, social forms of organization and of regulation permitting us to think together, to concentrate our intellectual and mental power, to multiply our imaginations and our experiences, to work out practical solutions for the complex problems affronting us in real time and on all levels. We would progressively learn to orientate ourselves in a new cosmos, constantly transforming itself and drifting, to become its authors as much as we can, to invent collectively ourselves as a species. Collective intelligence does not aim at the mastery of selves through human collectives but at an essential loosening of the grip changing the very conception of identity, the mechanisms of domination and of the breaking out of conflicts, the unblocking of confiscated communication, the mutual launching of isolated thoughts. So we are now in the same situation as a species whose each member would possess a good memory, would be perceptive and astute, but which would not yet have reached the stage of the collective intelligence of the culture because it would not have been capable of inventing an articulated language. How can one invent language if one has never spoken, if one's ancestors have never pronounced a single phrase, if one has no example to follow, not the slightest idea of what language could be? We are as nearly as possible in the same situation presently: we do not know what it is that we have to create, what we may already have obscurely began to envision. Still it only took a few millenia for the homo habilis to become the homo sapiens, to cross such an imposing threshold; it launched itself in the unknown, inventing the earth, the gods, and the endless world of signification. But languages are made for the "human scale" communication within small communities, perhaps even to guarantee the stability of their relations. Thanks to writing we have reached a new stage. The technique of writing effected the growth of the efficiency of communication and the organization of human groups; its scope was much wider than could ever have been that of shere speech. But this change took place at the expense of the unity of societies: it caused the division of societies into bureaucratic machineries for the handling and manipulation of information with the aid of writing and into those to be "administered". The task of the collective intelligence is to discover, or to invent, the other side of writing, the other side of language, so that the manipulation of information would be distributed everywhere, coordinated everywhere, that it would no longer be the priviledge of separate social organs but, on the contrary, would be naturally integrated into every human activity, as a tool in the hands of everyone. This new dimension of communication should evidently permit the mutuality of our knowledge and the reciprocality of its transmission which is the most rudimentary condition of the collective intelligence. In addition it would open up two major possibilities that would radically transform the fundamental facts of life within societies. First, we would have at our disposal simple and practical means of finding out what it is that we are doing together. Second, we could handle, even more easily than we write today, instruments allowing collective enunciation. And all of this no longer on the scale of paleolithic clans, or on that of States and historical institutions, but with the amplitude and velocity of gigantic turbulences, of deterritorialized processes, and of anthropological nomadism influencing us today. If our societies content themselves with mere intelligent government, they will almost certainly not attain to goals set by them. In order to have some chances of a better life, they will have to become intelligent by the masses. From beyond the media aerial machineries will make the voice of the multiplicity heard. It is still indiscernible, muffled by the mists of the future, bathing another kind of humanity in its murmur, but we are destined for an encounter with superlanguage.
The advantage on the other hand, is that these submicroscopic data can be transmitted without material movement in space and therefore without human accompaniment. Nontechnical media are bound to a physical carrier, they require the same network infrastructure as traffic of people and goods. The earliest wire networks had a point-to-point structure, requiring access points for in- and output, relay or refresher stations at regular intervals, and central nodes where messages are switched to their destination. In 1896, Marconi's wireless telegraphy extended the traffic of signals into the ethereal radio spectrum. The point-to-point cable was supplemented by omnipresent waves that can be intercepted by anyone owning a receiver. Radio extended the distance the voice carries virtually around the globe. With the broadcast networks of radio and TV, the center-to- all structure was invented. One speaks and all listen. These media are an extension of the public sphere, and therefore the radio spectrum is usually considered a public resource and regulated accordingly. Broadcast media create the masses they address,synchronizing millions of non-present, anonymous media recipients. McLuhan points to the origin of technical media in the medium print. In contrast to his own interpretation of the electronic media as fundamentally different from print, in their homogenizing function they are not. Reading this quote against its author, I see the program of 'homogenization of men and materials' rising to its ultimate violent power only in its military form under conditions of mass- mobilization during the Second World War (radio), and in its postwar civilian form under conditions of mass-markets, -media, -automobilization, -tourism etc. (TV). One further important aspect of technical media is that perception of the world shifted from the real thing to its stored mediatizations. Typists took dictation not from their superior's voice but from a gramophone or telegraphone recording. The question if Leland Stanford's horse had all four feet off the ground when in gallop could not be answered by observation through the naked eye, but Muybridge's serial photographs showed that it was in fact the case. The amount of live music we listen to is neglectable in comparison to prerecorded music. Whereas live broadcast implies a co-existence in time, a simultaneity that seems to warrant authenticity, much of what we see on TV is pre-recorded, edited, re-run - if we are not watching out of local storage of video anyway. Personal communication shifted from synchronous to asynchronous with the storage of answering machines, faxes, and email. The Matrix itself is a vast and rapidly growing-library. In short, large and exponentially growing parts of our media horizon are 'canned', and the two essential new operations besides transmission that technical media add to those of the Gutenberg Galaxy - copying and editing - are based on storage media.
The computer has its roots in mathematics which is indistinguishably linked to astronomy. Computing machines were built before Leibniz, like Schickhard's calculating clock (!) (1624) or Pascal's adding machine (1642). Still the primacy goes to Leibniz who produced a great confluence of streams of ideas, and contributed profoundly to symbolic logic, combinatorics, and therefore the history of the computer. Babbage should at least be mentioned in passing. His projected Analytical Engine was to have included most of the characteristics of modern computers realized only a hundred years later: a store, a mill (CPU), a transfer system, in- and output, and he also anticipated automatic operation, external data memories, conditional operations and programming. In 1847, Boole used a binary notation to represent truth-values in formal logic, 0 and 1 representing 'false' and 'true'. Shannon and Weaver's information theory translated the Boolean false and true into off and on states in electronic components. Signals, since this ultimate analytic cut with Ockham's razor, fall apart into basic indivisible yes/no units called bit. Like 'atom' for the material world and 'individuum' for society (both meaning 'indivisible'), 'bit' marks the smallest possible unit, the simplest building block of any possible symbolic system. Having mentioned some of the shoulders he was standing on, I can now turn to Alan Turing. I suggest to name the emerging horizon of binary digital media "Turing Galaxy", because its two central concepts were first formulated by him. One is the Universal Machine, the extremely primitive machine that can emulate any machine, the typewriter that reads and writes an operative text out of no more than two characters which freely models the appearance of the typewriter itself, the Universal Medium that precedes and empowers any possible multimedia to come. From then on every phenomenon and every process that can be described completely and unequivocally (the definition of both algorithm/automaton and the inter-subjectively scrutinizable knowledge of science) can be implemented in the one single machine to end all machines. The problem of building new machines has been replaced by the problem of writing an operational description of this new machine for the universal machine. (5) The other is the thought experiment known as the Turing Test which provided a comprehensive re-definition of man as a symbol processing system on a par with machines, and technically resolved the subjectivity problem. (6) Since then, 'intelligent' modelling, signal processing, and pattern recognition - so called thinking - has turned into a continuum across a range of possible technical or biological implementat ons. 'Mind' and machine have become interconnectable (if not interchangeable). Turing or bit media inherit properties from earlier media. They still operate largely in the mind-frame of the mathematical and the Gutenberg Galaxy. The most essential new operation introduced in the Turing Galaxy seems to me simulation. While models in the Gutenberg Galaxy become operational only after being read into and processed by the cortical CPU, models in the Turing Galaxy run inside a dynamic self-active technical medium. Bit words have the double function of addressing human readers as well as machines, i.e. themselves. Action unfolds and changes according to a script or in response to the action of the user and to its own results. Simulation allows to test hypotheses, to automatically control real life processes, and to construct alternate worlds. Today we observe the collapsing of all media into the universal medium computer. Turing media connect people, libraries, machines, and aartificial communicational entities. We are still exploring what the usage of computers in 'Turing mode' could mean. My suggestion: acting inside of media, and interacting with artificial agents.
form of computer games. Games are simulations. In the earliest form they simulate rules and strategies of board games. Later they simulate technical systems (notably with the military flight simulator reappearing as entertainment product), and social systems ( role playing games, SimCity etc.). In games (as in simulation) the computer takes on the function of agency, of a counter- player, an interlocutor, simulating dragons, enemy aliens, humans, governments, or simply fate. The computer also provides the playing-space into which the human player projects herself as a sprite, avatar, or persona, a marionette of herrself that she flies by the wire of the joystick. This is the first time not only the eye and ear, but the hand reaches through to the other side of the proverbial looking glass. With the emergence of data networks, games shifted from single-player stand-alone games to multi-player networked games or MUDs (Multi-User Dungeons/Dimensions). Here human others are re-introduced into the position of counter-player, next to and on a par with pieces of software simulating game characters. Originally games in the narrow sense of the word built around the sword-wielding-and-monster-slaying world of Dungeons & Dragons, MUDs are developing into common meeting grounds around diverse topics including professional conferencing facilities and educational institutions.
A typical host on the Matrix has a tree structure. It might be presented as a directory listing, menu, or in an index file pointing to the individual texts. One might browse or search by keywords with the search space inside one database, one host, or network-wide. At the final nodes of the tree one might find a text, in itself complete with author, pagination, footnotes, etc. The Gutenbergian resources on the Matrix are vast. Librarians were among the first to inhabit and develop it. Just a few examples are the US Library of Congress, including their Soviet and Vatican online archives, the Project Gutenberg, books.com, the first bookstore on the Net, magazines like Wired, and a sprouting new category of multimedia Metazines like the Electric Eclectic. Do people actually read books on the screen? Do they print them out? How are etexts used? The advantage of having reference books ready for automatic searches is obvious. Same for checking quotes in any sort of text. Maybe people will start to actually read electronic books when screens are light enough to hold, as pleasant to the eye and as 'interactive' as print on paper. Maybe people will have them rread to them by a voice synthesizer. Already now, the ASCII text is driving a Braille interface to allow blind people to read them. But rather than looking at the 'usage' of an individual text it is apparent that 'reading' will take on a different meaning when you imagine a library of 10,000 etexts in the form of a single text corpus available to you at any time. Even though somebody like Borges might be able to store a huge library in his memory and quote from it literally even after going blind, this is not given to most of us mortals. But thanks to the automatized external memory, we have random access to all the stored ideas. We can keyword search, browse with guaranteed serendipity effects, follow through on various threads, all the while creating hyperl inks on the go, leaving tracks inside the Gutenberg horizon that we can follow again next time we touch any of the texts. Every work (say Dante's Divine Comedy (8) appears in its own context, and in any other that we might create. All these operations could be done inside a library, but involving a lot more foot-work. The increased accessibility is already more than a quantitative difference. But what other automatic operations on texts may emerge, most of all what the new faculty of simulation will mean for writing/reading, ie. 'thinking' under conditions of the availability of the virtually complete library (Lyotard) in a dynamic format at the tick of a few keys, will have to be seen. If all these operations have to be done using raw Unix commands or exotic database query languages, the bookish Gutenbergian will likely not feel very at home. Luckily, there have emerged hypertext interfaces that make life a lot easier (World Wide Web under Mosaic). The reader/writer sees a text page complete with graphic design, that can be read like the page of a book. The reader can mouse-click his way around in the labyrinth of the global online library, make annotations, leave 'bookmarks' etc. A special feature is that the 'footnotes' to materials (text, image, sound, and video) outside the present text are active. By clicking on them, the corresponding file is retrieved across the Matrix, and presented immediately. All these are Gutenbergian operations, accelerated to the speed of electricity. Their model is the library.
The Letter. Postal mail is basically a point-to-point network, switched (sorted) at post offices, transported by diverse means of transportation. 'Snail Mail', as it is called in net.land, is usually private, to someone you know, but it can be extended to point-to-many. Mailing •lists exist also in RL for commercial, administrative or grass roots usage, but switching from traffic to transmission is more than a linear change. The Universal Network Medium adds the function of a reflector. A list server (like Majordomus) is an automatic forwarding program that sends every incoming message on to every subscriber, and drops it into his mailbox. Mailing lists can be unmoderated, ie. the information is provided as is, or moderated, ie. preprocessed by a wetware editor agent, which for certain purposes helps to raise the information-to-noise ratio significantly. Whether it holds together a professional special interest group, a hobby club or a speaker's ccorner - the mailing list constitutes a form of public. Electronic fora or bulletin boards - the metaphors reveal the heritage of earlier equivalents in public face-to-face real space. The most impressive are, without doubt, the Usenet newsgroups. Forming another network within the networks, the newsgroups permeate across Usenet, UUCP,Internet and selections also into the commercial networks. One does not subscribe to newsgroups, and the messages are not delivered to one's mailbox. They rather sit on one's local host to be read, browsed, participated in whenever one likes. Mailing lists and newsgroups constitute the basis for a written sense of community. In order to do so, they have to provide some form of space-time coordinates to anchor the social. The placement of a message in one electronic forum creates an unambiguous attribution in index space (an address). Their sequence creates a temporal order, a history of speech acts in which regulars build a sense of group identity. Fora are usually archived, so even though a message was deleted locally you can still look it up. FAQ (Frequently Asked Questions) documents are the collected common sense that is not the lowest common denominator, but common expertise. Like in every form of communal exchange, rules for accepted use and conduct (nettiquette), for the prevention of redundancy, various ideas on how to enforce these rules, etc. are negotiated on the go. Communicational conflicts are solved, of course, also within the interaction, but as a novelty there are technical solutions as well, e.g. the killfile, also sometimes referred to as 'the bozo filter', that locally tunes down or makes invisible unwanted traffic without having to have any censorship at source. Moderated newsgroups and mail- servers, like postal-based news-letters (or the xerox machine-borne mini-komi, as the Japanese call the genre) are already crossing over into publishing. These publications are in the Public Domain, and the moderators are most of the time volunteer editors. Unmoderated newsgroups are a running comment by Everybody attached to Everything. Large events like Tiananmen or the Gulf War, just as small events like a change in the design of Starship Enterprise bring forth their own forums. Strategies have evolved to prevent the slightest idea of censuring an unmoderated newsgroup. Any attempt at dominating or turning it into a PR device will cause a flood of flame - the power of the many.[9] They are specific, global, personal, and very powerful. And again, the total is more than the sum of its parts. MindVox offers many services, among them "a constantly growing library that chronicles the very inceptions of Cyberspace, with timeslices of systems dating all the way back to 1979 - the first bulletin boards ever to exist. "An orgiastic idea for any sociologist, media and market researcher, historian or linguist. The whole problem of sampling that is fundamental to every empirical social science evaporates when you can operate on the complete set. And it comes with a tool box that allows you to do searches, sorts, pattern recognition and other analysis automagically. Fractal algorithms are used to analyze huge amounts of earthquake data out of which it is otherwise very hard to make sense. What collective image might arise if you ran a similar chaotic pattern synthesizer on the subset of, say all utterances on the topic 'Internet' in unmoderated newsgroups, and how it changed over the years? The casual enquiry What's everybody talking about? will receive an unpredictable but mathematically precise answer. The idea of Man, Mina, Everybody, this collective chimera that broadcasting and marketing directors have in mind when they talk about 'the audience' and 'the consumer' - this non-entity will get a voice.
Radio. There were downloadable sound files on the Internet before, but the first regular radio station in cyberspace was pioneered by Carl Malamud in March 1993. You can receive Internet Talk Radio on your desktop or laptop radio either 'live' or 'on tape'. ITR publishes from 30 to 90 minutes of professionally produced radio programming per day. It reaches 100,000 people in 30 countries. TV. Malamud did not choose a TV metaphor, simply because it requires more bandwidth than the majority of the Matrix population has available right now. Also, production of video information still requires an order-of-magnitude larger investment in facilities. As a first step towards general use MIME allows to include little video and sound blips in email. Not yet live broadcast of concerts, but downloadable video clips one finds at mtv.com. Adam Curry, former star host of MTV Networks and net.veteran, created a multimedia site in 1993 that now attracts an average of 35,000 people daily, including music industry professionals. The cable TV on the Internet is the Multicasting Backbone or in short M-Bone. Multicast is a continuous stream of video and audio data packets running over a virtual network layered on top of portions of the physical Internet. It combines a global point-to-point structure with local 'narrowcasting' to everyone who is tuned in. What's on the digital tube? If you belong to the lucky group of power networkers you can watch keynote speeches by John Perry Barlow, co-founder of the EFF, or Vinton G. Cerf, president of the Internet Society, live multicasts from the deep-sea or outer space, footage from NASA satellites and telescopes, eg. Keck, the world's largest pair of binoculars in the solar system. The M-Bone has even emerged a solution to the hotly debated '500 TV channel' problem: 'sd' or session directory is a TV guide where all ongoing events are announced and can be joined on mouse-click. CU-SeeMe, an offshoot of the M-Bone for personal computers, was first developed as a TV metaphor for live video, only later voice was added. Latest news while I am writing this: "Coming Soon - Newscasts on Your PC. Intel and CNN have teamed up to test 'LAN TV' , a system that turns a regular broadcast TV signal into a compressed digital data stream, capable of being received on regular 486-type desktop PCs. While Intel tests the technology, CNN will concentrate on determining what it is people want to watch on their computers, in order to develop a special corporate news service."(10) As with desktop print and radio, desktop TV is not restricted to corporate providers. A CCD camera, a VCR, a video-capture board, and some editing software allow, in principle, TV production and multicasting on every PC. After the telephone answering machine made everyone a radio announcer, the desktop multimedia answering machine will turn everyone or his agent into a celebrity TV announcer. The Hunt. It is here that we see the Dungeon Masters and the Net-Gods at play. The Hunt is a kind of paper-chase, only without the paper. A game to encourage the players to "explore the Net, and traverse little known routes." Huntmaster Rick Gates, Student & Lecturer of the University of Arizona, got the idea "sometime in 1991 when I began to realize the enormous variety and volume of information available via what I will call the Net (Bitnet, Usenet, The Internet, etc.). [...] I suppose my initial ideas were based on the type of search exam that most library-school students have to go through during a class in Basic Reference.' Some of us enjoyed this type of challenge; we called it 'The Thrill of the Hunt". (11) The Hunt is edutainment at its best, "casual instruction in training for information resources. [...] It provides for training in context, which for most people works better than books or chalk on a board." For beginning net.citizens it provides a chance to look over the shoulders of media-literate experts. "It helps more novice users, or Net 'settlers', understand how to move around using the 'trails' that the more experienced Hunt players have 'blazed'. [...] Learning how to learn is critical, and this only comes from experience."
cky part, as always with Oracles, is to formulated the right search command. (12) At the risk of boring you by repeating myself: old media don't go away, they are the content of the new ones, transformed into metaphors. If the 'content' of the Net is magazines, radio, TV etc., then the 'content' of those is the Net. The whole Net is abuzz with questions of where it is heading. Self- reflectiveness is part of the constitution of a new medium. But this is a transient stage. As with Usenet newsgroups we will see that the early bias towards computing and networking itself will shift. Today the comp. groups are far outnumbered by the alt. and rec. groups. There are, of course, differences between the Meta-Medium and the other media it embraces. With text, sound and video editing capabilities on personal computers getting cheaper, one-person desktop publishing and multicasting houses become possible. This was also said when xerox machines spread, and again with laser printers . It did indeed happen to some degree, but it also showed that not everybody has the urge to publish. Most of all, cheap high-quality printing on a laser engine did not solve the problem of distributing and making your product known. This changes with the Universal Medium that is production, transmission and reception medium in one. To multicast does not require the concentration of capital and power necessary to produce a full daily broadcast schedule in one of The Networks. Anybody who finds a friendly host or scrapes up a few thousand dollars to set up her own can be media provider. Combining broadcast tools and communications networks, and private and various forms of public communications, makes all the difference. The implications for the changing nature of work become visible already. Everybody who offers informational products or services can do so - globally, from anywhere, at a price that a private person can afford. This is not to say that capitalism will crumble, and give way to an Anarchist's dream of self-expression for everyone. But it does mean the end of capitalism as we know it. People who are part of what is often called a revolution are very excited about the empowering qualities of the Net.
The oldest medium holds its entrée as the latest, the get-together. The basic social function of going somewhere and hanging out with friends and like-minded people requires a common place and time. While tree-shaped lists, keywords and hyperlinks are appropriate for retrieving data, 'human information' needs conversational tools, an anthropomorphic space not to consult repositories of passive information, but to meet people. Today the theater metaphor (Laurel) re-emerges and with it the idea of actors and agents (Brooks, Maes). It is here that we re-encounter the ars memoria. Cicero suggested to use personae for the memory image that anchors the 'things'. Yates' characterization of a classic memory image: consists of human figures, is active, dramatic, striking, under circumstances that recall the 'whole' thing, can be read directly parallel to Laurel's explications of a desirable human-computer interface. What is not lost in the transition from the art of memory to the art of interface design is most of all the dimension of mental space. The stage where the play is enacted is idea space, regardless whether the mental image is evoked by printed, pictorial or sound signs, or a Wagnerian multimedia Gesamtkunstwerk. An important difference between the two arts isthat the mental space of the ars memoria was not shared. An orator would, of course, share his idea space with his audience, but as he walked around the chambers of his memory, picking the points he wanted to touch upon from the statues where he had deposited them, he was alone. He would never encounter an other there. Net.operations in Gutenberg mode are mostly silent, ASCII, solitary, and asynchronous. Most of the time the netsurfer is not aware of others who 'are inside' the same host. While we have seen that newsgroups can turn into a home on the Net, the potential of the Universal Machine is by no means eexhausted there. The silent and iconoclast world is enlivened by the beginning multimediatization. But hypertext, radio and TV metaphor are precisely that, like horse-less carriage and wire-less radio we now have paper-less libraries and station-less massmedia. They are metaphors for different media, not for the market square in the Global Village (McLuhan). A different approach that does not come from Gutenberg (although it is not illiterate), nor from mathematics, and not from the technical network media, but from game are MUDs. According to one definition they are "detailed and realistic multi-player simulations that present ongoing campaigns and universes with evolving storylines, political systems, and landscapes being imagined into existence as play progresses." MUDs are shared places. You 'telnet' yourself there. Others 'are"there' as well, synchronously, even though from different 'real' time zones. From the theater metaphor we pick up the performing arts and the stage effects. From game/play we get the participatory elements and the challenge for the price at stake: recognition for wit, excellence, style, integrative qualities, for the craziness of thinking up something that nobody has ever done before.
In the Matrix also the physical body has no significance (except for the wetware break). But the mental body can travel along the wires and be re- incarnated in a remote Doppelganger. The same body can be played by a human, just as well as by the machine. On the behavior of human players there is much to be said and studied. Here I would rather take a closer look at the non-human players. Automatons and the game between man and machine carried tremendous fascination ever-since the days of the Ancients, with new boosts during the Renaissance (Maillardet's Magician, Vaucanson's Writer, etc.) and the industrial age. They were only sophisticated toys, but they triggered a philosopher like Descartes to think up a Turing Test avant la lettre. The philosophical and literary (eg. E.T.A. Hoffman) theme continues to fascinate mankind's phantasy. But it was only with Alan Turing and his influence on von Neumann, the cybernetics group, and others that a whole wave of mind-mirroring in Al, neuro nets, piano-playing robots and 'thinking machines' was triggered. This conceptual shift dismissed philosophy and literature, and made the Turing Testable machine the goal of a concrete effort of exact sciences. The first program that passed the Turing Test in a life-like situation was Weizenbaum's Eliza. Since then the Turing horizon has become populated by hosts of talkative and zealous homunculi, women, and daemons. One forum where the best of them come together is the Loebner Turing- Test competition, conducted annually since 1991. The New York business man has donated $100,000 prize money for a program that can pass as human in an unrestricted typed tele-conversation. Entries so far are required to be conversant - in "natural American English" - on one topic only. Entrants may selecttheir own topic areas, but the domains of knowledge must be "within the experience of ordinary people. One of the participants is Julia, a Maas-Neotek bot with an Al engine written by Michael Mauldin at Carnegie Mellon behind it. Between Loebner Turing Tests she logs onto a MUD and behaves like a regular player. She can be summoned, gives useful information, delivers mud.mai I messages to other players who are not currently logged in, dispenses witty quotes, can be nice to you, and kills you when teased too much - and next time you talk to her she will still be angry with you, because she even remembers.
It is envisaged that intelligent robots of the next generation be equipped with various sophistciated capabilities endowing them with desires and intentions, enabling them to perform hypothetical and defeasible reasoning, to solve problems creatively, to appreciate works of art, to achieve some form of cyberpleasure, etc. Understanding and the ability to develop explanations for observations and facts are fundamental for the realization of these capabilities. In fact explanation and understanding are 'two sides of the same coin' in both art and science. Our objective is to highlight techniques used in Artificial Intelligence which could provide mechanisms for modeling the aesthetic response of an intelligent robot, based on the causal explainability of complexity manifested in media such as electronic art. Leyton argues that art is related to explanation, in particular that the aesthetic response is the mind's evaluation of causal explanation. He maintains that the level of aesthetic response to art works is proportional to the level of complexity that an individual observes. He goes further arguing that the desire for art works is part of a general desire that the human mind has for complexity. Barratt also claims that humans seek to explicate complexity, and since the brain is finite, there must be a maximum degree of complexity that the mind is capable of explaining at any one time. If the degree of complexity is increased past this level, it exceeds the mind's capacity to explain it, artistic chaos is reached and consequently the viewer deems the art work to be incoherent. He concludes that the limit is set by the ability to give causal explanation, it is not complexity that is appetitive, but causal explanation itself. Clearly, if our aim is to develop intelligent robots with truly human-like characteristics, then they must be capable of artistic appreciation. For electronic art, appreciation must occur at the conceptual level and not at the physical (pixel) level. In the area of Artificial Intelligence the notion of explanation has been well explored. The complexity of explanations is often a reflection of the richness of the agent's background knowledge, and its ability to discern its surrounding world. Indeed, the aesthetic response to artistic chaos is equivalent to an explanation of a contradiction. Central to such an explanatory capability is the need for mechanisms supporting the modification or revision of knowledge, that is, learning. Belief revision models the process of accepting new information in such a way that an intelligent agent's epistemic state remains logically consistent, or coherent. Frameworks for explanation within the area of Artificial Intelligence can be used to support the aesthetic response of an intelligent agent. In particular, two important parameters of an explanation may assist in gauging an aesthetic response, namely the plausibility and the specificity of the explanation. In summary, if aesthetic response is the evaluation of causal explanation, then we can endow an intelligent robot with aesthetic responses which ebb and flow in accordance with the complexity of the causal explanation achieved.
I have always thought of computers as dynamic tools for introspection, exploration and discovery. Computer programming is instrumental in the externalization of ideas and algorithms are formal descriptions of what one hypothesis constitute the production of creative statements. The computer is a playground to speculate on the generative potential of ideas. As a matter of fact, the physical, tangible management of purely conceptual constructs becomes possible. However, the paradox is that while algorithmic specification allows the artist to touch the essence of his ideas it also creates a distance since all specification is indirect and seems to exclude spontaneous action. The idea is to view computers as partners in the process of creative decision-making. By way of algorithms we can explore various man- machine relations in this partnership: from studying total autonomy in computer programs to systems designed for expl icit interaction. The development of personal algorithms is the key to exploration and the gradual specification of objectives from incomplete knowledge, in sharp contrast to view the computer as slave, as a medium for deterministic visualization. I have characterized the interactive method where man and machine collaborate in a common effort and with common objectives as conceptual navigation; the artist-programmer gets feedback, his expectations are confirmed or contradicted by the program's behavior. Eventually, unexpected results may signal new and promising routes exposing unknown territories. Thus, man and machine contribute both to the creation of a computational climate that favours invention and to the development of a critical attitude towards the often complex relationships between programmed intention and actual result.
Writing algorithms has also forced me to evaluate experience vs. speculation. If one relies on models that have proven to be successful in the past, one 60 confirms what is already known. Algorithms that use rules reflecting this knowledge produce predictable results. Otherwise, designing processeswith the greatest possible freedom in pure speculation is like working outside of any known context making evaluation very hard indeed. The creation of new contexts for growing algorithmic activity mixing memories of the past and an open imagination is, I think, perhaps the most interesting challenge to algorithmic art.
Almost as if by magic - whatever procedure you dream of - you can probably extend the power of your dream to the computer and let it develop the dream beyond your wildest expectations. You may identify procedures for improvising with color, scale, and position - which is what artists have always done. Given sufficient definition you could develop a form generator and from your new vantage point see new possibilities for further elaboration on your routine. Through trial and error - interacting with the algorithm itself you proceed further into the new frontier. So what can we learn from this? We learn what artists have always known - that "CAD" programs, paint brush programs, paint brushes and drawing paraphernalia do not make art. Neither do artists or designers simply "make art". The one over-riding essential element to the process, "a developed artistic procedure", is necessarily unique for each artist and for each work of art. The procedure addresses a singular conjunction of elements for which there is no "universal" rule. The "calculus of form" may be placed in the service of such procedures but should not be confused with the art-making procedure. For the artist who writes the code the artistic procedure is the act of "writing the code", pretty much like the creative work of the composer when the composer writes a musical score. Making art does indeed require a "calculus of form". But the artist's instructions on how to employ the "calculus of form" precede the "calculus". One needs an "artistic procedure" which addresses the entire complex of elements for each specific work. The final form, unique and specific to each work, embraces more than the "calculus". While it embraces and grows from a "calculus" it might employ any of an infinite number of approaches to deliver the form. These may include metaphor, improvisations of the form phenomenon in and of itself, or reference to some other phenomenon or idea - historical, literary, political, mathematical or philosophic. Can an artist write an algorithm then for an artistic procedure? Emphatically yes! Such algorithms provide the artist with self-organizing form generators which manifest his or her own artistic concerns and interests. We are looking to an interesting time ahead of us when artists will be able to exchange and interact with each other's form-generating tools in ways we never dreamed. There are procedures yet to be developed to make this kind of interactive expression accessible - a time ahead when we will literally see an evolution of form including a genealogy associated with its creators.

Despite, or perhaps because of, a healthy skepticism, Artificial Intelligence (Al) has been making quiet progress in electronic arts. Artificial Intelligence has inspired traditional fields of electronic arts as well as it has developed new horizons for many artists working in electronic composition environments. Building on the success and shortcomings of previous experiences with computers in arts, the attempt to extend the paradigm of artificial intelligence systemsto the domain of electronic sonic arts is made now. Musicians are increasingly using intelligent machines to deal with tasks for which they are better equipped than humans. Computers are increasingly being used to address the brain-numbing complexity of modern electronic music products and processes, thereby allowing people to concentrate on their music and ideas. Expert systems, for example, help people by searching a book of rules to decide what to do in a particular situation; as machines do not forget, these systems can manage rules more consistently than people. Some musicians are using neural nets, which can recognize complex patterns, to apply precedents that are difficult to express in numbers or words. The real challenge facing technology is to recognize the uniqueness of machine intelligence and learn to work with it. Given enough memory, a computer can remember everything that ever happened to it or to anyone else. Furthermore, when faced with a logical problem or a theoretical model of how compositions or sounds should be, computers can deduce more results more quicklythan humans. Their complementary strengths should allow man and computer to work together and do things that neither can do separately.
Whereas the impossibility of physical death in cyberspace is one of its main attractions (certainly for the flight simulators used by the military), this absence of death and of death's possibility does not emasculate the project. For death becomes the ultimate ground for the cybernaught, not in terms of individual death, nor even death of the planet, but according to Lyotard, in the death of the solar system. On a number of occasions Lyotard mentions the inevitalbe the destruction of the solar system estimated to occur in 4.5 billion solar years. The task of technology, is to create an alternate non organic system that will survive this catastrophe. Not only does the certainty of this event constitute perhaps the most sublime of deaths, but the end of the solar system represents a finality, a resolution, that puts ultimate limits on human endeavour. Such closure however, comes at the end of a narrative space in which all the utopian and apocalyptic concerns that have defined twentieth century culture's relationship to technology, are able to play out their fictions. As a way of representing the body in space, according to a perspective that the logocentric apparatus has inherited form the renaissance, futurity is also associated with frontality, and opposed to anteriority. As a radiant, or irradiated subject, the cybernaught may transmit from a centre in all directions, nonetheless s/he is literally always looking in front. In front - to the absence of distance between the organic eye and the simulated scene, to the absence of difference between the real and the repesentation, to the unfolding in sequence of the virtual narrative, and to the future as a narrative of progress. This future space thus stands in for all the physical spaces which go missing in virtual worlds, and this future death defers the resolution of corporeality and the promise of transcendence that individual death promises. More than this, the future impossibility of organic embodiment provides the ultimate rationale for the numerical constitution, Cartesian co-ordination, and digital storage of the subject, who then shines with the necessity of survival. This is the radiant subject of art - the channel to the sublime, now irradiated. The subject who shares with radiant sound, the security of identity with the eventfullness, change and flux of the event. As Baudrillard says, we no longer need the VR glove or suit because 'we have swallowed our microphones' and 'internalized our aesthetic image.' 28 We have become the post holocaust meaning of radiant sound - transmissive but rotten at the core. And the realization of this subjectivity occurs, not at the point of solar explosion as radiance would suggest, but at the point of total computation. At this point, the signal continues to survive in outer space; the space of the future, but sound, and any vibrational body, is immediately extinguished by silence.
My first visit to virtual reality— a cartoon-like 'Virtual Seattle' at VPL Labs in California a number of years ago—indicated that for me at least, the great attraction was not the lure of computer technology or of interface devices, which included a cumbersome helmet ('eyephones') which put little video monitors over my eyes; and, the coarsely rendered, neon-colored artificial world, in which I had the illusion of being immersed was not a convincing imitation of the physical Seattle, or for that matter, any other landscape which could possibly have drawn me in. The allure of this cyberscape was the impression that it was responsive to me, as if my gaze itself was creating (or performing) this world and that I was to some extent enunciating it according to my own desire. My most abiding memory was of exhilarating ability to fly through the artificial world at great speed simply by cocking my hand like a gun—'navigation' is a poor term for this experience. Best of all, I had a sense of the weightlessness and super-power that I had imagined in childhood and had read about in myths and comic books, but had never before experienced, not even in my dreams. (My childhood friends in first and second grade and I tried fruitlessly to fly day after day by flapping blankets while jumping off walls and out of trees.) It is this feeling of transcendence of the mortal body and the gravity of earth that for me is a key to the desire and media attention which has been focused on 'cyberspace' and the subculture which has grown up around it.
The primordial virtual space is an utterly empty display, unlike the physical world, which is always 'full' and readymade. So far at least, cyberspace worlds are sparsely stocked with metaphors, now largely constituted from scratch with considerable graphic effort. Once these graphics are out of sight, it is easy to get lost in a void that is uniformly colored (usually black) and that wears infinity at its edges if not at a vanishing point. My first flight revealed Virtual Seattle, like most other virtual environments, to be relatively void but for a crude symbolscape of geometric objects. I remember my panic at flying through and out the swimming-pool-like image-space of Puget Sound and getting lost in utter emptiness. (I have also flown too far from the landingstrip metaphor of a Wall Street stock market program and have fallen off the checker-board world of 'Dactyl Nightmare.' The stock market program has an arrow function which points the way back to civilization.) What a comfort it was to find the traces of the human imagination in the spacescape near me again. On the other hand, why are these cyber-traces, the externalized imagination of electronic producers, filled with so little of our cultural legacy? I am thinking of metaphorically and graphically impoverished architectural flythroughs or crude male-centered fantasies of pornotopia ('Virtual Valerie') or a pseudo-prehistoric past wherein the only activity is relentless killing, (for instance, the aforementioned 'Dactyl Nightmare.') One task of art that commodity culture apparently eschews is to resituate the disengaged space of virtuality into a socio-historical context. For instance, Jeffrey Shaw's interactive city installations, such as virtual New York or Amsterdam, are richly symbolic, suggesting how the built environment may be refigured in a image-space as a kind of alphabet. Multiple and interlacing historical narratives are traced in a kind of writing motion over the display area via a bicycle interface. The Biblical reference in Shaw's piece at ars electronics 94, The Golden Calf, made what was otherwise a clever piece—a statue visible only through a mirror-like electronic display—into a commentary on electronic art itself. The uncanny and more sinister implications of my first flight occured to me later: A virtual space it is not just the ground or background or the landscape at which I look, or even that my look calls forth—that space looks at me, following my every move. Indeed, space constituted itself in response to various indices of my intention, for instance, the vectors of my gaze and the motion of my body or head. That is, in a virtual world, not just objects but space itself is interactive. As a consequence the virtual environment that surrounds the visitor itself can appear to be something 'live' or animate, 'that we cannot acknowledge as subject or persona in the traditional european sense, and which nonetheless constantly demonstrates that it sees us without revealing itself.
Of the many artistic responses to the Gulf War, I remember Frances Dyson's and Doug Kahn's sound and sculptural installation for its condensation of the sounds and images of birds in flight with the resonances of the air-war on Iraq. A more recent installation by Laura Kurgan explores the actual operation of several satellites in Global Positioning System or GPS by using them to trace the position of the New Museum gallery in New York. The installation was effectively demystifying, not only in revealing how this surveillance system works, but its material fallibility resulted in wavy deviations from geometric accuracy. Julia Scher has explored the psychical and cultural implications of electronic and computer surveillance in work spanning over a decade, including her 1993 installation, Predictive Engineering, at the San Francisco Museum of Modern Art, mixing live and recorded video on a two chiastically arranged and elegantly situated surveillance camera and monitor set-ups. The interest of art then may not be in the seamless operation of electronic culture nor in the production of realistic virtual worlds—like Icarus, that may be flying so low as to be dragged into the sea. The often mentioned desire for photographic resolution in virtual displays may also have as much to do with the goal of controlling physical objects and events as it does with aesthetics. An art of virtual spaces which simply aims toward realism of fit or of appearance with a physical landscape may then risk merely serving the instrumental or hegemonic purposes of military and business interests in an information society. On the other hand, art that surrenders to the allure of the mysterious or that seems to offer transcendence may find the wax that holds its feathers together melted by the sun. Exploiting the magical aura of virtual spaces risks satisfying the commodity and entertainment functions of information and nothing more. For, unlike prior illusion- producing modes, cyberspace is a means of enchanting not only liminal realms, but everyday reality. Even though it is has been discredited as a popular rather than scientific term, 'cyberspace' is appropriately built on the analogy of Norbert Wiener's cybernetics, or the study of feedback systems. In computers, feedback is elaborated into a programmed responsiveness which Sherry Turkle has noted, can captivate the user as a kind of 'second self'.2 But feedback is not restricted to the space of the monitor, for material artifacts and even a physical space itself can be 'cyberized,' or granted agency by programming it to simulate some form of human interaction, in the process ultimately lending it qualities associated with human personality. As Jay David Bolter, explains in Writing Space: The Computer, Hypertext and the History of Writing,' 'Artificial intelligence leads almost inexorably to a kind of animism, in which every technological device (computers, telephones, wristwatches, automobiles, washing machines) writes and in which everything that reads and writes also has a mind.' One futuristic vision of the personified or smart home proclaims, 'Once your house can talk to you, you may never feel alone again,14 suggesting this animism and a quasi-subjecthood can extend to even physical space, once it has been 'cyberized.' A utopia of ubiquitous computing would enchant the entire world, distributing magical powers to the most mundane aspects of existence.
Of course, business interests are far more concerned with 'information' as a resource and an exchange value, than with virtual environments, even 'smart houses' per se. 'Information' is knowledge decontextualized and stored as data (that is, as virtual objects.) In order to be retrieved and placed in a new context, that data must take on symbolic or metaphoric form in an interactive and to some degree immersive display. The value of information is realized not just in any one state, but as a passage from the conceptual to the virtual to the material and back again, crossing through a variety of reality statuses. For instance, virtual money or credit demands a passage through material objects in order to increase itself as interest. Jeff Schulz, for instance, has made the credit system the material of his performance art and of commentary in his essay, 'Virtu-Real Space: Information Technologies and the Politics of Consciousness:15 If virtual environments are best understood in connection with other social and cultural processes, as one stage in the unfolding of metaphors across a variety of reality-statuses and degrees of materiality, this suggests that the electronic arts are themselves part of a range or spectrum of interactive and immersive media and are not well-served by isolating them from art using other media, that is also concerned with the transformation of information societies into electronic culture. Artists from the ex-Eastern block or what was once the Third World are likely to suffer the consequences of this global change, even if they are excluded from its benefits. That is, there are artistic issues and perspectives which have a bearing on the global economic and cultural transformation we are undergoing that may be posed by those who have little access to computers or even to electricity —they must be welcomed into the discourse as full partners. Artists and cultural activists—for instance, Paper Tiger/Deep Dish and Ponton—have also not forgotten the issue of public access to the material and technical level where information is processed, stored and transmitted. It is real estate in terms of data space on computer disks and in main-frames, personal space in seats in front of computer work- stations, frequencies on the broadcast spectrum, satellite space off which to bounce signals, and room in the bandwidth of fiberoptic cables that global corporations struggle among themselves to own and control. The scarcity and costliness of these material gates of entry limit the number and types of subjects we can find in the virtual gathering spaces of an electronic culture. What we to some extent have and need more of is art which figures relationships between the virtual and the physical world, which demystifies the relation of the body to the virtual environment, and which is both a meta-commentary and an aesthetic statement. On the other hand, the technological ability to recreate the acoustic space of a medieval cathedral in one's living room, or to merge movie stars and tourists into the same image and have them interact, merely exploits the ability to superimpose the virtual over physical space: it is entertainment. The following section concludes by making some generalizations about virtual environments as virtual space, based in reflections on experience in cyberspace, from virtual realities to CD Rom work-stations to electronic networks.
The very idea of space becomes self-contradictory, when it is applied to virtual realms, especially the maze-like vectors and links which compose the paradoxical 'space' of networks. Virtual space is not so much space as 'nonspace,' for it need not occupy ground, nor be a continuous linear extension, area or void, nor even constitute the interval between things; and, unlike the material Lebensraum of earth, it not be perceived as limited or scarce. If the virtual space in question is the discontinuous, yet communal space of isolated computer network users, it can expand ad infinitum, like the text-based 'rooms' which make up a M.U.D. or multi-user dimension. But where is that noplace in which, for instance, two people talking by telephone meet? Where is the room and where is the display in which the hundreds who belong to the same M.U.D. (Multi-User Dungeon or Dimension) or M.O.O. (M.U.D., Object- Oriented) may gather? The reality-status of any one virtual environment is also unclear, seemingly in-between an exteriorized mental space, the apparatus of the image-display and the material world. The many different levels and degrees of virtuality in an information society add complexity to mystery. What, for instance, is the 'space' of a virtual object in a computer program? Even if it can be quantified as data in megabytes or ultimately in bandwidth or pixels, a virtual object itself remains an imperceptible potentiality, which occupies no space at all until it is accessed and displayed. Can one even say the object is 'inside' the opaque casing of the computer or hidden under the obscure machine language of programming?' Even if one could break into the black box or extract and analyze the program, one wouldnUt expose the virtual object, only the mechanism that has the potential to produce it. Yet, the virtual space on display is still a realm of cause-and-effect, though the consequences of any one action may seem more magical than logical, for they need not be proportionate to the results to which we are accustomed in physical space. Space is ordinarily conceived of as continuous or at least, at its most abstract, as a homogenous void. Yet, virtual non-space or cyberspace can be distributed discontinuously over physical space (in a way that is usually imagined as supported by ubiquitous computing.) Furthermore, physical separation between the users and objects of physical space need has little bearing on the seams which separate and link virtual spaces. What remains somewhat clumsy are the figural conventions which ease the passages between virtual 'worlds': the vortex, the window and the door are given too much work to do as metaphorical thresholds and passageways. The additive aesthetic principle of the Internet, the global network of networks, is an extremely elegant, non-hierarchical, rhizomatic global web of relatively independent yet connecting nodes. Though it was conceived out of militaristic considerations, it might be compared with Panofsky's analysis of the gothic cathedral. This comparison is not trivial, for combined as an infrastructural and virtual entity, the Internet is among the greatest architecture the world has every known, far greater than the material reference point of the information highway metaphor, the freeway system.
Such compression of space and time finds an exponent in Jeffrey ShawUs interactive installation, Revolution. The user's effort turns a grindstone interface, which churns out pictorial representations of hundreds of social revolutions in the historical record onto a video monitor. Revolution is then not a representational space of linear histories or of geographical areas but the presentational space of a metaphor and its recurring metahistorical patterns. The visitor to the installation stands for the protagonist and motive force of this social phenomenon, a spontaneously acting group called at times the 'mass,' the 'crowd' or the 'people.' Then the vocation of an art of the kind that reflects on electronic crowds and networks is not the representation of the visible world, but the visualization of what is otherwise inaccessible to perception and is difficult to imagine because of its scale, its discontinuity in space and or time or its impenetrability—from the insides of the body, the atom, or the black box to the outside of our galaxy and our universe. All the linking devices which create virtual spaces of greater and greater, albeit ephemeral unities—text-based networks, MUDUs and MOOUs, telecommunication satellite links and cables, but also protocyberspace like the nets which unify physical space—railroads and highways are understood, paradoxically enough as 'spaces.' Such virtual environments of discontinuous and overlapping jurisdictions would tax any political imagination capable of ethnic cleansing or of resolving ethnic conflict by dedicating bounded areas to one homogeneous culture. If virtual space were our model of political space, there would be no struggle for nationhood as a geographical entity. What would remain a nagging material problem is opening the gateway of induction into the virtual realm wide. The concept of 'space' applied to computer- and other machine- generated virtual realms is a metaphor that invokes something quite different than the fundamental experience of being in the space of the physical world in a body rooted to the ground by gravity, in view of a horizon. Cyberspace is heterogeneous and dispersed, it can be experienced in various degrees of person and immersion and in different symbolic modes as a virtually embodied metaphor where the flesh (or meat body) can't go, but into which disengaged spectral bodies and multiple personas be inducted, fly and interact, alone in an electronic crowd. The scene itself can move and is responsive to the user in ways which promote performative and/or magical experiences, loosely covered in scientific and socio-economic alibis. That is, electronically produced liminal realms and induced experiences are only superficially about technology, they are about transcendence (even when in degraded forms of sex, shopping, high-speed driving, mortal combat, etc.) Some of the organizing metaphors of cyberspace (frontier, highway, spaceflight, cave, net, theater, game, etc.) are propositions which should be scrutinized carefully as to the way they define the control, access, reality status and experiences assigned to the virtual and symbolic realm which is increasingly our everyday world.
Post-biological technologies enable us to become directly involved, body and mind, in our own transformation, and they are bringing about a qualitative change in our being. The emergent faculty of cyberception, our artificially enhanced interactions of perception and cognition, involves the transpersonal technology of global networks and cybermedia. We are learning to see afresh the processes of emergence in nature, the planetary media-flow, the invisible forces and fields of our many realities, while at the same time re-thinking possibilities for the architecture of new worlds. Cyberception not only implies a new body and a new consciousness but a redefinition of how we might live together in the interspace between the virtual and the real, calling for a wholly new social environment and a reconsideration of every aspect of our ways of being. Western architecture shows too much concern with surface and structures - an arrogant "edificiality" - and is too little aware of the human need for transformative systems. There is no biology of building. A city should offer its citizens the opportunity to participate in the process of cultural emergence. Its infrastructure, like its buildings, must be both intelligent and publicly intelligible, comprising systems that anticipate and react to our individual desires and needs as much as we interact with them. A "grow bag" culture is required in which seeding replaces designing, and where architecture finds its guiding metaphors in microsystems and horticulture rather than in monumentality and warfare. Currently, architecture has no response to the realities of cyborg living, or the distributed self, or to the ecology of digital interfaces and network nodes. It has produced a shopping cart world of pre-packed products wheeled around the sterile post-modernity of a mall culture. Buildings, like cities, should grow. As products of creative cyberception, they must become the matrix of new forms of consciousness and of the rhythms and realisations of post-biological life.

Techno music is an aggressive, technology and future oriented genre of youth culture and popular music. The historical background of this musical form lies in the avantgarde groups of 60's and 70's; especially Fluxus and Kraftwerk. From a philosophical point of view, techno can also be seen as a continuation to the modernist avantgarde movements such as futurism, surrealism and dadaism of the early 20th century. Techno music is especially popular in Europe. What used to be pure underground five years ago has become evidently mainstream. The recent commercial success of artists like Sven Vaeth, Westbam, LFO, Orbital, The Orb and Aphex Twin has proved techno to be a fast growing youth movement. Pop journalists and music experts have claimed techno to be "rock of the 90's". Concerning this, its is not surprising that the massive party concepts of Mayday and Love Parade have been called "Woodstocks of the 90's". In my paper I will introduce and analyse the latest developments of techno music and aesthetics. During the recent years techno has divided into several sub-genres such as ambient, trance, hardcore and gabber. A clear turning point can be seen. At its current status quo, techno seems to be a cultural phenomenon with a fascinating mixture of experimental avantgarde music and transnational pop culture. Techno music has been said to be "a soundtrack of the information age". Juergen Laarmann, the editorin-chief of German Frontpage techno magazine, has also written that techno music is only a small part of a broader concept of techno culture. In this case, we have to ask what is techno culture? In Laarmann's opinion all the computer based technologies from computer networks to video games and hypermedia programs represent techno culture. Concerning this point of view, it is interesting to bring up a citation from Bill Nichols' remarkable article "The Work of Culture in the Age of Cybernetic Systems": The Computer is more than an object; it is also an icon and a metaphor that suggests new ways of thinking about ourselves and our environment, new ways of constructing images of what it means to be human and to live in a humanoid world. Cybernetic systems include an entire array of machines and apparatuses that exhibit computational power. Such systems contain a dynamic, even if limited, quotient of intelligence. Telephone networks, communication satellites, radar systems, programmable laser videodiscs, robots, biogenetically engineered cells, rocket guidance systems, videotex networks - all exhibit a capacity to process information and execute actions. They are all "cybernetic" in that they are self-regulating mechanisms or systems within predefined limits and in relation to predefines tasks. Just as the camera has come to symbolise the entirety of the photographic and cinematic processes, the computer has come to symbolise the entire spectrum of networks, systems and devices that exemplify cybernetic of "a utomated but intelligent" behaviour.
Electronic artists rely on technologies developed by disciplines which did not exist just a few decades ago: computer graphics, image processing, computer vision, human-computer interface design, virtual reality and so on. The paper traces the history of these currently prominent image disciplines. My analysis begins in the 1920s when avant-garde artists, inspired by modern engineering, tried to systematically apply its principles to visual communication. To engineer vision meant to be able to affect the viewer with engineering precision, predictability, and effectiveness. Thus, Dziga Vertov championed montage as the most economical kind of communication while Sergei' Eisenstein searched for units to measure communication's efficiency. In its desire to engineer vision, the avant-garde was ahead of its time. The systematic engineering of vision took place only after World War II with the shift to post-industrial society. In post-industrial society, the mental labor of information processing is more important than manual labor. In contrast to a manual worker of the industrial age an operator in a humanmachine system is primarily engaged in the observation of displays which present information in real time about the changing status of a system or an environment, real or virtual: a radar screen tracking a surrounding space; a computer screen updating the prices of stocks; a video screen of a computer game presenting an imaginary battlefield, etc. In short, vision becomes the major instrument of labor, the most productive organ of a worker in a human-machine system. The research into human- machine interfaces — from first computer graphics displays of the late 1940s to today's VR — can be seen as attempts to make the use of vision in this new role as efficient as possible. The importance of information processing for post-industrial society also leads to the necessity to automate as much of it as possible. The ultimate aim is the complete replacement of human cognitive functions by a computer, including the substitution of human vision by computer vision. This is the second trajectory of image research in post-industrial society; from pattern recognition systems of the 1950s to today's computer vision systems. In summary, most of the new research into imaging and vision after World War II can be understood as following two directions: on the one hand, making human vision in its new role of human-machine interface as efficient and as productive as possible; on the other hand, transferring vision from a human to a computer. Why should this historical analysis be of concern to electronic artists? The notion that the artist functions outside of society, history, and industry is a modernist myth. Modernist artists were not only the pioneers of the utilitarian aesthetics of modern industrial design and the techniques of modern advertisement and political propaganda, but they have also pioneered the post-modern engineering of vision, the integration of human and machine in human-machine systems, and the replacement of human vision by computer vision. Today, computer graphics industry is one of the sites of this engineering. Whether computer artists acknowledge or ignore their relationship to this industry, it exists. Acknowledging rather than ignoring this relationship is the first step toward a critical computer art practice.
Art and Technology as the New Avant-garde Machine Vertebral Animal The status and understanding of technology in the computer epoch is very different from 'optic-engine based technology'. As opposed to the human body, an engine is a different body/construction. The human body was understood as a unit, as an undebatable organism. Computer technology can't be separated in reflective categories from the subject and, in a way, from the body. A machine now is not a structure that is alienated from the subject. (Structures don't go on the streets! – slogan of 1968). Technology is intermingled with intimate human life as a part of the 'molecular structure'. Technology seems to be saturated with desire, seduction, 'automata of the body'. It is supposed to be combined with desire with functions of the body and the filters of perception. Assemblage of Representations, Body When making a comparison between the body and the subject we work on the side of the subject. Making analyzable the bodily practices and the unconscious, we are continuing to dissociate the body, from one side and to incorporate it into forms of representations from the other side. The only territory of the body is the terrain of transgression, affect, death, sex. The body is incorporated into language and viewed through a multitude of practices. The practices could be understood as an assemblage of verbal and visual possibilities taken from past and present (marginal and dominant) culture. The Art of the Disembodied Subject a) dislocation. A virtual portrait (in VR games, for example) could include the following: mind, age , character, temperament, style, design, sex. Everything that was articulated, analyzed inside the subject could be terminated and artificially used. A subject is a landscape, open for a multitude of subjects, that can be recombined or segmented for different needs and functions. A subject can't live beyond the cultural media: literature, film, TV. It needs to be disembodied, moved to interfere with other life forms and to be dislocated from the automatic 'natural' body. It has a multitude of images and a freedom of recombining and choosing itself. b) Segmentation and interactivity. Interactivity is different from communication and information. An interactive technology needs a special subject and atactics, that avoids stable codes and emphasizes the process of collaborative acting. The paper will be illustrated with conceptual and video installations by Russian artists and by experts from experimental TV in Russia and the Piazza Virtuale in Kassel.
In his report on the study of pictorial perception among African subjects, William Hudson (1967) says that we take it very much for granted that methods which are only moderately successful in our own cultures will prove equally, if not highly, successful in an alien culture: "We fall into the error of thinking of the black man's mind as a tabula rasa, which we have only to fill with the benefits of our own cultural experience in order to promote whatever objectives we may have in mind. We forget or ignore the fact that the black man possesses his own indigenous culture." During recent years, many artists have addressed the issue of cultural diversity as part of their discussions on Electronic Art. Although the vast majority of artists claim the need for a transcultural approach, most of them have taken a superficial look at this complex problem, turning attention away from some of its more crucial points. Their discourse focuses on the possibilities for providing artistic bridges across different cultures, while their attitudes and works reflect, in many cases, a typical ethnocentric view. The discussion aims at promoting a debate on transcultural issues, as one of the major challenges electronic artists face today. In a world of social, cultural and economic disparities, how can technology meet basic human needs in both developed and developing countries? Which are the dominant cultural values that underlie computer-related technologies today? What is the impact of new electronic technologies on Third World nations? How can we minimize technological dependence and cultural domination, when 30 developed countries – with less than 30% of the world's population – account for approximately 95% of the world's scientific and technological production?
Some artists using electronics take for granted that the art that will be significantly new is going to emerge through new technology. If they look at a painting, they see a medium that doesn't do very much except sit on the wall. Old medium, old ideas. The new media involve intelligent and ambitious systems, radical shifts in our thinking. So it's natural to expect radical and impressive art, too. Working as a painter who also uses computers, I am more sceptical. The art of painting is built on asking questions about what you see, and the process has the feel of a stumbling search. Obsolete? During the sixties and seventies we had exhibitions with "beyond painting" in the title. Kinetic, Op, Minimal, Conceptual, all mixed make-believe and pseudo-science to suggest a future where only "de-materialized" art would be possible. In fact what evaporated wasn't the "art object" but the credibility of this way of thinking, discredited and soon forgotten because the work with real punch and ambition proved to come from painting. As well as finding another country for art – albeit a virtual country – the visual creativity of computing can function just as well within traditional media. The given technology of a painting – flat surface, nothing moving, no sound, no buttons or head-set, not even a plug required – is unimpressive, but it can whirl into life through the touch of colour, the dance of line, the stare of a face. At the Minneapolis conference last year the neighbouring museum held a small exhibition of Matisse's graphic work, its vitality and simplicity a reminder of how far the computer graphic exhibits (mine included) fell short. The technophobia of the mainstream art world is the routine excuse for the failures of computer works to be as impressive as they should be. But on exciting, sophisticated technology is just the starting point. Picasso on an Apple II might still be interesting. Whatever else is possible, a fusion of computer techniques and painterly sensibility shouldn't be discounted too hostily. If there are frontiers in art they certainly aren't where you expect them to be.
Remember Vincent Van Gogh's Painter on His Way to Work, carrying it all on his back? That's where art education is heading. I don't mean the canvas and easel. I mean carrying it all on your back, in the clothes that you wear and in the headband in your hair. 50% pure natural wool 50% optical fibre. I am talking about the interface moving onto and, eventually, into the body. That's your electronic media artist on her way to school. She's wearing the university on her sleeve. We're not talking about a few curriculum changes here. We're not talking about the gradual replacement of some of the library stacks with a few computers. We are talking about the total dissolution, disintegration, and dispersal of Higher Education. From real estate to cyber estate. The university is becoming the interversity. Ask the students. Hundreds of thousands use the Internet daily. When Larry Smart first issued NCSA Mosaic, the network interface to hypermedia browsing, there were ten thousand users in the first three weeks. Now there are over two million. Students are half in school and half in cyberspace. They live between the virtual and the real. They are in the Net more often than out of it. This is the advent of Inter Reality, the space we are most likely to inhabit for the next many years. The ethics of the net, its integrity and inclusiveness, are creating a social behaviour, a morality, which will bring huge bonuses to the real world. I am with Esther Dyson of the Electronic Frontier Foundation when she says that organised political parties won't be needed if open networks "enable people to organise ad hoc, rather than get stuck in some rigid group". The end is to reverse-engineer government, to hack Politics down to its component parts and fix it. She echoes the words of Hazel Henderson writing twenty years before her: "Networks are a combination of invisible college and a modern version of the Committees of Correspondence that our revolutionary forefathers used as vehicles for political change". This post-political process also involves the student in learning to browse, to graze, to hunt for ideas, projects, data, as well as intellectual and artistic collaboration and friendship in all kinds of electronic places, virtual libraries, telecommon rooms and cybercolleges. The students' time in telepresence and virtual learning mode is increasing rapidly. Have you noticed in the studios, libraries and computer suites how every terminal, every interface is occupied, all the time. There are 50 billion adults in the world seeking education in one form or another. That form will be on-line. CD ROM is migrating to big disks at a server near you. The future of education lies in the function of integrated multimedia telecommunication services. But that future could be solely in the hands of big business who simply see "content" as the "value-added" they've got to include to get "market share". I foresee a completely crazy take- over of education by these commercial telecommunications industries unless we can provide models of on-line collaborative creativity and learning whose originality, effectiveness and appeal outshine the more cynical manipulations of the market. We cannot hope to do that in isolation, in our separate colleges, just meeting occasionally, even at conferences as dynamic as this. I want to invoke the sense of a group in which each member has more or less equal power and authority in both access to knowledge and in the means of its reconfiguration and distribution; a group concerned with art and the advancement of learning through collaborative inquiry and shared experience. I want to propose the creation of a Planetary Collegium: non hierarchical , non-linear, and intrinsically interactive; a gathering together, a connecting, an integration of people and ideas. Combining cognition and connectivity, what better creative learning organism could serve our unfolding telematic culture. But by definition such an organism cannot be planned and implemented top down. In fact it is already emerging, bottom-up from the infinity of interactions within the net.
What will be the consequences for art and for education when the digital image is no longer box-dependant? When we no longer have to sit up and beg for information with a typewriter keyboard and TV set? When whole walls of building inside and out can be digitally flooded with sound, colour and light, images and texts flowing in endless transformations, when whole environments respond to our body movements and the articulations of our voices?. When the printed page no longer regiments our thinking into orderly rows of linear data? If the poets, artists and musicians of the world are not ready with strategies to effect this environmental and ecological digitalisation, the politicians, merchants and entrepreneurs will. In this context, Art schools have a clear necessity to put up or shut down. But college is a place for social experiment as much as artistic and intellectual growth. Nothing is more human, warm and convivial than a bunch of kids hanging out on the Internet. As networked virtual reality transports our telepresence, and gives us the tools to reconfigure our own identities, social life will become not only more complex but more imaginative, the scenarios of conviviality outstripping no doubt those of the most fecund scriptwriters of the old movie era. I am happy to admit to possessing a butterfly mind. I'm constantly on the move, physically and virtually, between nodes, between people, between data, between cities, between images, between channels, between texts. I have a psychic restlessness called connectivity. I blame the technology! But then, everyone blames the technology whilst everybody knows that technology has imposed nothing upon us that we did not first desire. Technology arises from our longing to be out-of-body, to see beneath the surface of things and events, to break the bounds of authorised perception, to exceed language, to transform the material world, to recreate ourselves. Technology represents a further embodiment of mind. Minds of course can be vacuous, coldly calculating and analytical, mean and narrow. The same is the case for technology. Minds also can be open, inclusive, loving, spiritual, transcendent. The hope for a cognitive technology lies in this , indeed the hope for a truly human electronic, multimedia and interactive art precisely lies here. The context of this embodiment, the ecology of mind, is at the root of all our considerations about art in the era of interactivity and transformation. With the bionic revelation of our cyborg nature now well rehearsed and understood, it is clear that our art is post-biological also. Again, the educational provision for the development of this post-biological culture must form the overarching agenda of a planetary collegium.
For me, Gelernter can be usefully triangulated with Varela and Stafford because of his project to build a spiritual computer. An emotional computer. Gelernter has valuable things to say about computers and creative thought. "A computer that can't feel can't think". His new book speaks our language. And from his vantage point at Yale, he "rejects the traditional academic subject divisions", and feels "especially at home in the no man's land between art and science". Professor Stafford must feel the same, I would guess, navigating between aesthetics and medicine to chart the emergent revolution in seeing and imaging. Equally, Francisco Varela is not fettered by academic boundaries and roams over an extensive cultural terrain, combining neuroscience with Buddhist theory, and speaking to issues in art as much as society. From these three vantage points, a map of consciousness and the reality we actively construct can be defined. Such vantage points and the new perspectives they cast upon our understanding of ourselves, must become endemic of the learning landscape our Collegium will provide. Not only are students redefining who they are and what they may become, but we too must redefine our identity as teachers, collaborators and guides relative to them. Similarly our tools are changing. While the printed book will continue to be employed, the question becomes how and for what purpose, since it is clear that hypermedia is in many areas set to replace it. The book has come to be the embodiment of authority and its obsolescence as a primary academic tool will cause considerable problems in the academic world. The book is a medium which is fixed and frozen while interactive media are fluid. Post-modernism with its relativist doctrine of layered realities and the slippage of codes has prepared us for the shifting uncertainties of authority, indeed of authorship and ownership of ideas whatever they might constitute, either in science or in art. But the scripting, negotiation and critical evaluation of a hypertext present demands for revolutionary pedagogical change . It's not simply that many colleges are haunted by the ghosts of culture past but that apparitions of the future are emerging on every screen, from every disk, in every network. These apparitions are the constructions of distributed mind, the coming-into-being of new forms of human presence, half real, half virtual, new forms of social relationships, realised in telepresence set in cyberspace. They are challenging the old discarded forms of representation and hermeneutics which still haunt the lecture halls. The students are beginning to treat the university as an interface to Inter Reality as a doorway to a radical constructivism, the way into building their own world. What could be more hopeful than a world designed by the young tested against the on-line wisdom of a global community. This is education in its hyper-Socratic form. There will be no easy transition from the past stability of tradition to the dynamic uncertainty of the immediate future. New priorities must be set in the fiscal affairs of universities. In academic networking and on-line research, change is imminent and difficult times are ahead. "As the Internet expands something will have to give: either the government will stop paying, or politicians will notice that the government is paying and will impose controls, like those imposed by school boards on textbook content or by the FCC on radio and TV broadcasts". The Clipper chip, the cryptography issue, poses serious problems for academic freedom. As Bruce Sterling recently reported from the Conference on Computers, Freedom and Privacy: when the audience was asked by a White House representative who they feared would abuse cryptography more, the US government or criminals, three quarters voted against the government.
Cyberspace: A word from the pen of William Gibson, science fiction writer, circa 1984. An unhappy word, perhaps, if it remains tied to the desperate, dystopic vision of the near future found in the pages of Neuromancer (1984) and Count Zero (1987)—visions of corporate hegemony and urban decay, of neural implants, of a life in paranoia and pain—but a word, in fact, that gives a name to a new stage, a new and irresistible development in the elaboration of human culture and business under the sign of technology. Cyberspace: A new universe, a parallel universe created and sustained by the world’s computers and communication lines. A world in which the global traffic of knowledge, secrets, measurements, indicators, entertainments, and alter-human agency takes on form: sights, sounds, presences never seen on the surface of the earth blossoming in a vast electronic night. Cyberspace: Accessed through any computer linked into the system; a place, one place, limitless; entered equally from a basement in Vancouver, a boat in Portau-Prince, a cab in New York, a garage in Texas City, an apartment in Rome, an office in Hong Kong, a bar in Kyoto, a cafe in Kinshasa, a laboratory on the Moon. Cyberspace: The tablet become a page become a screen become a world, a virtual world. Everywhere and nowhere, a place where nothing is forgotten and yet everything changes. 1 2 Michael Benedikt Cyberspace: A common mental geography, built, in turn, by consensus and revolution, canon and experiment; a territory swarming with data and lies, with mind stuff and memories of nature, with a million voices and two million eyes in a silent, invisible concert of enquiry, dealmaking, dream sharing, and simple beholding. Cyberspace: Its corridors form wherever electricity runs with intelligence. Its chambers bloom wherever data gathers and is stored. Its depths increase with every image or word or number, with every addition, every contribution, of fact or thought. Its horizons recede in every direction; it breathes larger, it complexifies, it embraces and involves. Billowing, glittering, humming, coursing, a Borgesian library, a city; intimate, immense, firm, liquid, recognizable and unrecognizable at once. Cyberspace: Through its myriad, unblinking video eyes, distant places and faces, real or unreal, actual or long gone, can be summoned to presence. From vast databases that constitute the culture’s deposited wealth, every document is available, every recording is playable, and every picture is viewable. Around every participant, this: a laboratory, an instrumented bridge; taking no space, a home presiding over a world . . . and a dog under the table. Cyberspace: Beneath their plaster shells on the city streets, behind their potted plants and easy smiles, organizations are seen as the organisms they are—or as they would have us believe them be: money flowing in rivers and capillaries; obligations, contracts, accumulating (and the shadow of the IRS passes over). On the surface, small meetings are held in rooms, but they proceed in virtual rooms, larger, face to electronic face. On the surface, the building knows where you are. And who. Cyberspace: From simple economic survival through the establishment of security and legitimacy, from trade in tokens of approval and confidence and liberty to the pursuit of influence, knowledge, and entertainment for their own sakes, everything informational and important to the life of individuals—and organizations— will be found for sale, or for the taking, in cyberspace.
Cyberspace: The realm of pure information, filling like a lake, siphoning the jangle of messages transfiguring the physical world, decontaminating the natural and urban landscapes, redeeming them, saving them from the chain-dragging bulldozers of the paper industry, from the dieselsmoke of courier and post office trucks, from jet fuel fumes and clogged airports, from billboards, trashy and pretentious architecture, hour-long freeway commutes, ticket lines, and choked subways. . .from all the inefficiencies, pollutions (chemical and informational), and corruptions attendant to the process of moving information attached to things—from paper to brains—across, over, and under the vast and bumpy surface of the earth rather than letting it fly free in the soft hail of electrons that is cyberspace. Cyberspace as just described—and, for the most part, as described in this book—does not exist. But this states a truth too simply. Like Shangri-la, like mathematics, like every story ever told or sung, a mental geography of sorts has existed in the living mind of every culture, a collective memory or hallucination, an agreed-upon territory of mythical figures, symbols, rules, and truths, owned and traversable by all who learned its ways, and yet free of the bounds of physical space and time. What is so galvanizing today is that technologically advanced cultures—such as those of Japan, Western Europe, and North America—stand at the threshold of making that ancient space both uniquely visible and the object of interactive democracy. Sir Karl Popper, one of this century’s greatest philosophers of science, sketched the framework in 1972. The world as a whole, he wrote, consists of three, interconnected worlds. World 1, he identified with the objective world of material, natural things and their physical properties—with their energy and weight and motion and rest; World 2 he identified with the subjective world of consciousness—with intentions, calculations, feelings, thoughts, dreams, memories, and so on, in individual minds. World 3, he said, is the world of objective, real, and public structures which are the not-necessarily-intentional products of the minds of living creatures, interacting with each other and with the natural World 1. Anthills, birds’ nests, beavers’ dams, and similar, highly complicated structures built by animals to deal with the environment, are forerunners. But many World 3 structures, Popper noted, 4 Michael Benedikt are abstract; that is, they are purely informational: forms of social organization, for example, or patterns of communication. These abstract structures have always equaled, and often surpassed, the World 3 physical structures in their complexity, beauty, and importance to life. Language, mathematics, law, religion, philosophy, arts, the sciences, and institutions of all kinds, these are all edifices of a sort, like the libraries we build, physically, to store their operating instructions, their “programs.” Man’s developing belief in, and effective behavior with respect to, the objective existence of World 3 entities and spaces meant that he could examine them, evaluate, criticize, extend, explore, and indeed make discoveries in them, in public, and in ways that could be expected to bear on the lives of all. They could evolve just as natural things do, or in ways closely analogous. Man’s creations in this abstract realm create their own, autonomous problems too, said Popper: witness the continual evolution of the legal system, scientific and medical practice, the art world, or for that matter, the computer and entertainment industries. And always these World 3 structures feed back into and guide happenings in Worlds 1 and 2. For Popper, in short, temples, cathedrals, marketplaces, courts, libraries, theatres or amphitheaters, letters, book pages, movie reels, videotapes, CDs, newspapers, hard discs, performances, art shows. . . are all physical manifestations—or, should one say, the physical components of—objects that exist more wholly in World 3. They are “objects,” that is, which are patterns of ideas, images, sounds, stories, data. . . patterns of pure information. And cyberspace, we might now see, is nothing more, or less, than the latest stage in the evolution of World 3, with the ballast of materiality cast away—cast away again, and perhaps finally. This book explores the consequences and limits of such a development. But let it be said that, in accordance with the laws of evolution, and no matter how far it is developed, cyberspace will not replace the earlier elements of World 3. It will not replace but displace them, finding, defining, its own niche and causing the earlier elements more closely to define theirs too. This has been the history of World 3 thus far. Nor will virtual reality replace “real reality.” Indeed, real reality—the air, the human body, nature, books, streets. . . who could finish such a list?—in all its exquisite design, history, quiddity, and meaningfulness may benefit from both our renewed appreciation and our no longer asking it to do what is better done “elsewhere.”
I have introduced Popper’s rather broad analysis to set the stage for a closer examination of the origins and nature of our subject, cyberspace. I discern four threads within the evolution of World3. These intertwine. Thread One This, the oldest thread, begins in language, and perhaps before language, with a commonness-of-mind among members of a tribe or social group. Untested by dialogue—not yet brought out “into the open” in this way—this commonnessof-mind is tested and effective nonetheless in the coordinated behavior of the group around a set of beliefs held simply to be “the case:” beliefs about the environment, about the magnitude and location of its dangers and rewards, what is wise and foolhardy, and about what lies beyond; about the past, the future, about what lies within opaque things, over the horizon, under the earth, or above the sky. The answers to all these questions, always “wrong,” and always pictured in some way, are common property before they are privately internalized and critiqued. (The group mind, one might say, precedes the individual mind, and consensus precedes critical exception, as Mead and Vygotsky pointed out.) With language and pictorial representation, established some ten to twenty thousand years ago, fully entering the artifactual world, World 3, these ideas blossom and elaborate at a rapid pace. Variations develop on the common themes of life and death, the whys and wherefores, origins and ends of all things, and these coalesce ecologically into the more or less coherent systems of narratives, characters, scenes, laws, and lessons that we now recognize, and sometimes disparage, as myth. One does not need to be a student of Carl Jung or Joseph Campbell to acknowledge how vital ancient mythological themes continue to be in our advanced technological cultures. They inform not only our arts of fantasy, but, in a very real way, the way we understand each other, test ourselves, and shape our lives. Myths both reflect the “human condition” and create it. Now, the segment of our population most visibly susceptible to myth and most productive in this regard are those who are “coming of age,” the young. Thrust inexorably into a complex and rule-bound world that, it begins to dawn on them, they did not make and that, further, they do not understand, adolescents are apt to reach with some anger and some confusion into their culture’s “collective unconscious”—a world they already possess—for anchorage, guidance, and a base for 6 Michael Benedikt resistance. The boundary between fiction and fact, between wish and reality, between possibility and probability, seems to them forceable; and the archetypes of the pure, the ideal, the just, the good, and the evil, archetypes delivered to them in children’s books and movies, become now, in their struggle towards adulthood, both magnified and twisted. It is no surprise that adolescents, and in particular adolescent males, almost solely support the comic book, science fiction, and videogame industries (and, to a significant extent, the music and movie industries too). These “media” are alive with myth and lore and objectified transcriptions of life’s more complex and invisible dynamics. And it is no surprise that young males, with their cultural bentindeed mission-to master new technology, are today’s computer hackers and so populate the on-line communities and newsgroups. Indeed, just as “cyberspace” was announced in the pages of a science fiction novel, so the young programmers of on-line “MUDS” (Multi-User Dungeons) and their slightly older cousins hacking networked videogames after midnight in the laboratories of MIT’s Media Lab, NASA, computer science departments, and a hundred tiny software companies are, in a very real sense, by their very activity, creating cyberspace. This is not to say that cyberspace is for kids, even less is it to say that it is for boys: only that cyberspace’s inherent immateriality and malleability of content provides the most tempting stage for the acting out of mythic realities, realities once “confined” to drug-enhanced ritual, to theater, painting, books, and to such media that are always, in themselves, somehow less than what they reach for, mere gateways. Cyberspace can be seen as an extension, some might say an inevitable extension, of our age-old capacity and need to dwell in fiction, to dwell empowered or enlightened on other, mythic planes, if only periodically, as well as this earthly one. Even without appeal to sorcery and otherworldy virtual worlds, it is not too farfetched to claim that already a great deal of the attraction of the networked personal computer in general-once it is no longer feared as a usurper of consciousness on the one hand, nor denigrated as a toy or adding machine on the otheris due to its lightning-fast enactment of special “magical” words, instruments, and acts, including those of induction and mastery, and the instant connection they provide to distant realms and buried resources. For the mature as well as the young, then, and for the purposes of art and self-definition as well as rational communications and business, it is likely that cyberspace will retain a good measure of mytho-logic, the exact manifestations of which, at this point, no one can predict. Three of the authors in this book—Michael Heim, Allucquere Rosanne Stone, and David Tomas—take up the cultural-anthropological theme, the latter two with special reference to the changing meaning of the “technophilic” physical body. Chip Morningstar and F. Randall Farmer describe their experiences with on-line games, in particular, LucasFilm’s Habitat. William Gibson’s short piece also makes its contribution at this level, if more directly, as an allegorical work of fiction itself. Thread Two Convolved with the history of myth is the thread of the history of media technology as such, that is, the history of the technical means by which absent and/or abstract entities—events, experiences, ideas—become symbolically represented, “fixed” into an accepting material, and thus conserved through time as well as space. Again, this a fairly familiar story, one whose detailed treatment is far beyond the scope of this introduction and this book. Nevertheless it is one worth rehearsing. It is also a topic that is extremely deep, for the secret of life itself is wrapped up in the mystery of genetic encoding and the replication and motility of molecules that orchestrate each other’s activity. Genes are information; molecules are media as well as motors, so to speak ... But we cannot begin here, where the interests of computation theorists and biologists coincide. Our story best begins with evolved man’s conscious co-option of the physical environment, specifically those parts, blank themselves, that best receive markings-such as sand, wood, bark, bone, stone, and the human body-for the purpose of preserving and delivering messages: signs, not unlike spoors, tracks, or tell-tale colors of vegetation or sky, but now intentional, between man and man, and man and his descendants. What a graceful and inspired step it was, then, to begin to produce the medium, to create smooth plastered walls, thin tablets, and papyrus, and to reduce the labor of marking-carving, chiseling—to the deft movement of a pigmented brush or stylus. As society elaborated itself and as the need to keep records and to educate grew, how much more efficient it was to shrink and conventionalize the symbols themselves, then to crowd them into rows and layers, "paper-thin" scrolls and stacks. 8 Michael Benedikt At this early stage already, the double movement towards the dematerialization of media on the one hand and the reification of meanings on the other is well underway. Against the ravages of time, nonetheless, and to impress the illiterate masses, only massive sculptures, friezes, and reliefs in stone would do. These are what we see today; these are what survive of ancient cultures and impress us still. But it would be wrong therefore to underestimate the traffic of information in more ephemeral media that must have sustained day-to-day life: the scratched clay tablets, the bark shards, graffitied walls, counters, papyri, diagrams in the sand, banners in the wind, gestures, demonstrations, performances, and, of course, the babble of song, gossip, rumor, and instruction that continuously filled the air. Every designed and every made thing was also the story of its use and its ownership, of its making and its maker. This world sounds strangely idyllic. Many of its components, in only slightly updated forms, survive today. It was a period perhaps four thousand years long when objects, even pure icons and symbols, were not empty or ignorable but were real and meaningful, when craftsmanship, consensus, and time were involved in every thing and its physical passage through society. But first, with the development of writing and counting and modes of graphic representation, and then, centuries later, with the invention of the printing press and the spread of literacy beyond the communities of religious scholars and noblemen, the din of ephemeral communications came to be recorded at an unprecedented scale. More important for our story, these “records” came to be easily duplicable, transportable, and broadcastable. Life would never be the same. The implications of the print revolution and the establishment of what Marshall McLuhan called the “Gutenberg galaxy” (in his book of the same name) for the structure and function of technologically advancing societies can hardly be overestimated. Not the least of these implications were (1) the steady, de facto, democratization of the means of idea production and dissemination, (2) the exponential growth of that objective body of scientific knowledge, diverse cultural practices, dreams, arguments, and documented histories called World 3, and (3) the fact that this body, containing both orthodoxies and heresies, could neither be located at any one place, nor be entirely controlled.
However, our double movement did not stop there, as we are all witness today. Although “printed matter” from proclamations to bibles to newspapers could, in principle, be taken everywhere a donkey, a truck, a boat, or an airplane could physically go, there was a limit, namely, time. No news could be fresh days or weeks later. The coordination of goods transportation in particular was a limiting case, for if no message could travel faster than that whose imminent arrival it was to announce.. . then of what value the message? Hence the telegraph, that first “medium” after semaphore, smoke signals, and light-flashing, to connect distant “stations” on the notion of a permanent network. Another related limit was expense: the sheer expenditure of energy required to convey even paper across substantial terrain. The kind of flexible common-mindedness made possible in small communities by the near-simultaneity and zero-expense of natural voice communications, or even rumor and leaflets, collapses at the larger scale. Social cohesion is a function of ideational consensus, and without constant update and interaction, such cohesion depends crucially on early, and strict, education in—and memory of—the architectures, as it were, of World 3. With the introduction of the telephone, both the problem of speed and the problem of expense were largely eliminated. Once wired, energy expenditure was trivial to relay a message, and it was soon widely realized (interestingly only in the 1930s and 40s) that the telephone need not be used like a “voice-telegraph,” which is to say, sparingly and for serious matters only. Rather, it could be used also as an open channel for constant, meaningful, community-creating and business-running interchanges; “one-on-one” interchanges, to be sure, but “many-to-many” over a period of time. Here was a medium, here is a medium, whose communicational limits are still being tested, and these quite apart from what can be accomplished using the telephone system for computer networks. Of course, the major step being taken here, technologically, is the transition, wherever advantageous, from information transported physically, and thus against inertia and friction, to information transported electrically along wires, and thus effectively without resistance or delay. Add to this the ability to store information electromagnetically (the first tape recorder was demonstrated commercially in 1935), and we see yet another significant and evolutionary step in dematerializing the medium and conquering—as they say—space and time. 10 Michael Benedikt But this was paralleled by a perhaps more significant development: wire-less broadcasting, that is, radio and television. Soon, encoded words, sounds, and pictures from tens of thousands of sources could invisibly saturate the world’s “airwaves,” every square millimeter and without barrier. What poured forth from every radio was the very sound of life itself, and from every television set the very sight of it: car chases, wars, laughing faces, oceans, volcanos, crying faces, tennis matches, perfume bottles, singing faces, accidents, diamond rings, faces, steaming food, more faces. . . images, ultimately, of a life not really lived anywhere but arranged for the viewing. Critic and writer Horace Newcomb (1976) calls television less a medium of communication than a medium of communion, a place and occasion where nightly the British, the French, the Germans, the Americans, the Russians, the Japanese.. .settle down by the million to watch and ratify their respective national mythologies: nightly variations on a handful of dreams being played out, over and over, with addicting, tireless intensity. Here are McLuhan’s acoustically structured global villages (though he wished there to be only one), and support for the notion that the electronic media, and in particular television, provide a medium not unlike the air itself—surrounding, permeating, cycling, invisible, without memory or the demand for it, conciliating otherwise disparate and perhaps antagonistic individuals and regional cultures. With cordless and then private cellular telephones, and “remote controls” and then hand-held computers communicating across the airwaves too, the very significance of geographical location at all scales begins to be questioned. We are turned into nomads . . . who are always in touch. All the while, material, print-based media were and are growing more sophisticated too: “vinyl” sound recording (a kind of micro-embossing), color photography, offset lithography, cinematography, and so on. . . the list is long. They became not only more sophisticated but more egalitarian as the general public not only “consumed” ever greater quantities of magazines, billboards, comic books, newspapers, and movies but also gained access to the means of production: to copying machines, cameras, movie cameras, record players, and the rest, each of which soon had its electronic/digital counterpart as well as a variety of hybrids, extensions, and cross-marriages: national newspapers printed regionally from satellite-transmitted electronic data, facsimile transmission, digital preprint and recording, and so on. The end of our second narrative thread is almost at hand. With the advent of fast personal computers, digital television, and high bandwidth cable and radio-frequency networks, so-called postindustrial societies stand ready for a yet deeper voyage into the “permanently ephemeral” (by which I mean, as the reader is well aware, cyberspace). As a number of chapters in this book observe, so-called online community, electronic mail, and information services (USENET, the Well, Compuserve, and scores of others) already form a technological and behavioral beginning. But the significance of this voyage is perhaps best gauged by the almost irrational enthusiasm that today surrounds the topic of virtual reality. Envisaged by science fiction writer/promoter Hugo Gernsback as long ago as 1963 (see Stashower 1990) and explored experimentally by Ivan Sutherland (1968), the technology of virtual reality (VR) stands at the edge of practicality and at the current limit of the effort to create a communication/communion medium that is both phenomenologically engulfing and yet all but invisible. By mounting a pair of small video monitors with the appropriate optics directly to the head, a stereoscopic image is formed before the “user’s” eyes. This image is continuously updated and adjusted by a computer to respond to head movements. Thus, the user finds himself entirely surrounded by a stable, three-dimensional visual world. Wherever he looks he sees what he would see were the world real and around him. This virtual world is either generated in real time by the computer, or it is preprocessed and stored, or it exists physically elsewhere and is “videographed” and transmitted in stereo, digital form. (In the last two cases the technique is apt to be named telepresence rather than virtual reality.) In addition, the user may be wearing stereo headphones. Tracked for head movements, a complete acoustic sensorium is thus added to the visual one. Finally, the user may wear special gloves, and even a whole body suit, wired with position and motion transducers to transmit to others—and to represent to himself—the shape and activity of his body in the virtual world. There is work underway also to provide some form of forcefeedback to the glove or suit so that the user will actually feel the presence of virtual “solid” objects—their weight, texture, and perhaps 12 Michael Benedikt even temperature (see Stewart 1991a for a recent survey, and Rheingold 1991). With a wishful eye cast towards such fictional technologies as the Holodeck, portrayed in the television series “Star Trek, the Next Generation,” devices sketched in such films as Total Recall and Brainstorm, and, certainly, the direct neural connections spoken of in Gibson’s novels, virtual reality/telepresence technology is as close as one can come in reality to entering a totally synthetic sensorium, to immersion in a totally artificial and/or remote world. Much turns on the question of whether this is done alone or in the company of others; and if the latter, of how many, and how. Most of the chapters in this book tackle the question in one form or another. For, engineering questions aside, as the population of a virtual world increases, with it comes the need for consensus of behavior, iconic language, modes of representation, object “physics,” protocols, and design—in a word, the need for cyberspace as such, seen as a general, singularat-some-level, public, consistent, and democratic “virtual world.” Herein lies the very power of the concept. In this volume, the chapters by Wendy A. Kellogg, John M. Carroll, and John T. Richards, by Steve Pruitt and Tom Barrett, by Meredith Bricken, and, again, by Michael Heim look specifically at the remarkable phenomenon of telepresence or “virtuality” as a prime component of the experience of cyberspace. Other authors in this volume imagine a viable cyberspace operating with less completely immersive techniques, although these nonetheless are thought of as considerably advanced over today’s rather simple, low-resolution, two-dimensional graphical and textual interfaces. Thread Two, then, is drawn from the history of communication media. The broad historical movement from a universal, preliterate actuality of physical doing, to an education-stratified, literate reality of symbolic doing loops back, we find. With movies, television, multimedia computing, and now VR, it loops back to the beginning with the promise of a postliterate era, if such can be said; the promise, that is, of “post-symbolic communication” to put it in VR pioneer Jaron Lanier’s words (Lanier 1989, Stewart 1991b). In such an era, languagebound descriptions and semantic games will no longer be required to communicate personal viewpoints, historical events, or technical information. Rather, direct—if “virtual”— demonstration and interactive experience of the “original” material will prevail, or at least be a universal possibility. We would become again “as children,” but this time with the power of summoning worlds at will and impressing speedily upon others the particulars of our experience. In future computer-mediated environments, whether or not this kind of literal, experiential sharing of worlds will supersede the symbolic, ideational, and implicit sharing of worlds embodied in the traditional mechanisms of text and representation remains to be seen. While pure VR will find its unique uses, it seems likely that cyberspace, in full flower, will employ all modes. Thread Three Another narrative, this one is spun out of the history of architecture. The reader may remember that Popper saw architecture as belonging to World 3. This it surely does, for although shelter, beauty, and meaning can be found in “unspoiled” nature, it is only with architecture that nature, as habitat, becomes co-opted, modified, and codified. Architecture, in fact, begins with displacement and exile: exile from the temperate and fertile plains of Africa two million years ago—from Eden, if you will, where neither architecture nor clothing was required—and displacment through emigration from a world of plentiful food, few competitors, and no more kin than the earth would provide for. Rapid climatic change, increasing competition, and exponential population growth was to change early man’s condition irreversibly. To this day, architecture is thus steeped in nostalgia, one might say; or in defiance. Architecture begins with the creative response to climatic stress, with the choosing of advantageous sites for settlements (and the need to defend these), and the internal development of social structures to meet population and resource pressure, to wit: with the mechanisms of privacy, property, legitimation, task specialization, ceremony, and so on. All this had to be carried out in terms of the availability of time, materials, and design and construction expertise. Added to these were the constraints and conventions manufactured by the culture up to that point. These were often arbitrary and inefficient. But always, even as conventions and constraints transformed, and as man passed from hunting and gathering to agrarianism to urbanism, the theme of return to Eden endured, the idea of return to a time of (presumptive) innocence and tribal/familial/national oneness, with each other and with nature. 14 Michael Benedikt I bring up this theme not because it “explains” architecture, but because it is a principle theme driving architecture’s self-dematerialization. Dematerialization? The reader may be surprised. What is architecture, after all, if not the creation of durable physical worlds that can orient generations of men, women, and children, that can locate them in their own history, protect them always from prying eyes, rain, wind, hail, and projectiles.. . durable worlds, and in them, permanent monuments to everything that should last or be remembered? Indeed these are some of architecture’s most fundamental charges; and most sacred among them, as I have argued elsewhere (Benedikt 1987), is architecture’s standard bearing, along with nature, for our sense of what we mean by “reality.” But this should not blind us to a significant countercurrent, one fed by a resentment of quotidian architecture’s bruteness and claustrophobia, which itself is a spilling over of the resentment we feel for our own bodies’ cloddishness, limitations, and final treachery: their mortality. Reality is death. If only we could, we would wander the earth and never leave home; we would enjoy triumphs without risks, eat of the Tree and not be punished, consort daily with angels, enter heaven now and not die. In the name of these unreasonable desires we revere finery and illumination, and reward bravery, goodness, and learning with the assurance of eternal life. As though we could grow wings! As though we could grow wings, we erect gravity-defying cathedrals resplendent with colored windows and niches crowded with allegorical life, create paradisiacal gardens such as those at Alhambra, Versailles, the Taj Mahal, Roan-Ji, erect stadia for games, create magnificent libraries, labyrinths, and observatories, build on sacred mountain tops, make enormous, air conditioned greenhouses with amazing flying-saucer elevators, leap from hillsides strapped to kites, dazzle with gold, chandeliers, and eternally running streams; we scrub and polish and whiten. . . all in a universal, crosscultural act of reaching beyond brute nature’s grip in the here and now. And this with the very materials nature offers us. In counterpoint to the earthly garden Eden (and even to that walled garden, Paradise) then, floatsthe image of the Heavenly City, the new Jerusalem of the book of Revelation. Like a bejeweled, weightless palace it comes down out of heaven itself “its radiance like a most rare jewel, like jasper, transparent” (Revelation 21:9). Never seen, we know its geometry to be wonderfully complex and clear, its twelves and fours and sevens each assigned a set of complementary cosmic meanings. A city with streets of crystalline gold, gates of solid pearl, and no need for sunlight or moonlight to shine upon it for “the glory of God is its light.” In fact, all images of the Heavenly City-East and West-have common features: weightlessness, radiance, numerological complexity, palaces upon palaces, peace and harmony through rule by the good and wise, utter cleanliness, transcendence of nature and of crude beginnings, the availability of all things pleasurable and cultured. And the effort at describing these places, far from a mere exercise in superlatives by medieval monks and painters, continues to this day on the covers and in the pages of innumerable science fiction novels and films. (Think of the mother ship in Close Encounters of the Third Kind.) Here is what it means to be “advanced,” they all say. From Hollywood Hills to Tibet, one could hardly begin to list the buildings actually built and projects begun in serious pursuit of realizing the dream of the Heavenly City. If the history of architecture is replete with visionary projects of this kind, however, these should be seen not as naive products of the fevered imagination, but as hopeful fragments. They are attempts at physically realizing what is properly a cultural archetype, something belonging to no one and yet everyone, an image of what would adequately compensate for, and in some way ultimately justify, our symbolic and collective expulsion from Eden. They represent the creation of a place where we might re-enter God’s graces. Consider: Where Eden (before the Fall) stands for our state of innocence, indeed ignorance, the Heavenly City stands for our state of wisdom, and knowledge; where Eden stands for our intimate contact with material nature, the Heavenly City stands for our transcendence of both materiality and nature; where Eden stands for the world of unsymbolized, asocial reality, the Heavenly City stands for the world of enlightened human interaction, form and information. In Eden the sun rose and set, there were days and nights, wind and shadow, leaf and stone, and all perfumed. The Heavenly City, though it may contain gardens, breathes the crystalline gleam of its own lights, sparkling, insubstantial, laid out like a beautiful equation. Thus, while the biblical Eden may be imaginary, the Heavenly City is doubly imaginary: once, in the conventional sense, because it is not actual, but once again because even if it became actual, because it is information, it could come 16 Michael Benedikt into existence only as a virtual reality, which is to say, fully, only “in the imagination.” The image of The Heavenly City, in fact, is an image of World 3 become whole and holy. And a religious vision of cyberspace. I must now return briefly to the history of architecture, specifically in modern times. After a century of the Industrial Revolution, the turn of the twentieth century saw the invention of hightensile steels, ofsteel-reinforced concrete, and of high-strength glass. Very quickly, and under economic pressure to do more with less, architects seized and celebrated the new vocabulary of lightness. Gone were to be the ponderous piers, the small wooden windows, the painstaking ornament, the draughty chimneys and lanes, the chipping and smoothing and laying! Instead: daring cantilevers, walls reduced to reflective skins, openness, light, swiftness of assembly, chromium. Gone the stairs, the horse-droppings in the street, and the cobbles. Instead, the highway, the bulletlike car, the elevator, the escalator. Gone the immovable monument, instead the demountable exhibition; gone the Acropolis, instead the World’s Fair. In 1924, the great architect Le Corbusier proposed razing half of Paris and replacing it with La Ville Radieuse, the Radiant City, an exercise in soaring geometry, rationality, and enlightened planning, unequaled since. A Heavenly City. By the late 1960s, however, it was clear that the modern city was more than a collection of buildings and streets, no matter how clearly laid out, no matter how lofty its structures or green its parks. The city became seen as an immense node of communications, a messy nexus of messages, storage and transportation facilities, a massive education machine of its own complexity, involving equally all media, including buildings. To no one was this more apparent than to a group of architects in England calling themselves Archigram. Their dream was of a city that built itself unpredictably, cybernetically, and of buildings that did riot resist television and telephones and air conditioning and cars and advertising but accommodated and played with them; inflatable buildings, buildings on rails, buildings like giant experimental theaters with video cameras gliding like sharks through a sea of information, buildings bedecked in neon, projections, lasers beams. . . . These were described in a series of poster-sized drawings called architectural telegrams, which were themselves, perhaps not incidentally, early examples of what multimedia computer screens might look like tomorrow (Cook 1973). Although the group built nothing themselves, they were and are, nonetheless, very influential in the world of architecture.
Now, a complete treatment of the signs of the ephemeralization of architecture and its continuing capitulation to media is outside the scope of this introduction. It occurs on many fronts, from the wild “Disneyfication” of form, to the overly meek accommodation of services. Most interesting, however, is a thread that arises from thinking of architecture itself as an abstraction, a thread that has a tradition reaching back to ancient Egypt and Greece and the coincidence of mathematical knowledge with geometry and hence correct architecture. As late as the eighteenth century, architects were also scientists and mathematicians; witness Andrea Palladio, Sir Christopher Wren, and before them, of course, Leonardo da Vinci and Leon Battista Alberti. From the 1920s till the 1960s, the whole notion that architecture is about the experiential modulation of space and time—that it is “four dimensional”—captivated architectural theory, just as it had captivated a generation of artists in the 20s and 30s (Henderson 1983). This was something conceptually far beyond the simple mathematics of good proportions, even of structural engineering. It is an idea that still has force. Then too there is the tradition of architecture seen for its symbolic content; that is, for not only the way it shapes and paces information fields in general (the emanations of faces, voices, paintings, exit signs, etc.) but the way buildings carry meaning in their anatomy, so to speak, and in what they “look like.” After five thousand years, the tradition is very much alive as part of society’s internal message system. In recent years, however, the architectural “message system” has taken on a life of its own. Not only have architectural drawings generated an art market in their own right—as illustrated conceptual art, if you will—but buildings themselves have begun to be considered as arguments in an architectural discourse about architecture, as propositions, narratives, and inquiries that happen, also, to be inhabitable. In its most current avant-garde guise, the movement goes by the name of Deconstructivism, or Post-Structuralism (quite explicitly related to the similarly named movements in philosophy and literary criticism). Its interests are neither in the building as an object of inhabitation nor as an object of beauty, but as an object of information, a collection of ciphers and “moves,” junctions and disjunctions, reversals and iterations, metaphorical woundings and healings, and so on, all to be “read.” This would be of little interest to us here were it not an 18 Michael Benedikt indication of how far architecture can go towards attempting to become pure demonstration, and intellectual process, and were it not fully a part of the larger movement I have been describing. (And we should remember that, as a rule, today’s avantgarde informs tomorrow’s practice. See Betsky 1990.) But there is a limit to how far notions of dematerialization and abstraction can go and still help produce useful and interesting, real architecture. That limit has probably been reached, if not overshot (Benedikt 1987). And yet the impetus toward the Heavenly City remains. It is to be respected; indeed, it can usefully flourish. . . in cyberspace. The door to cyberspace is open, and I believe that poetically and scientifically minded architects can and will step through it in significant numbers. For cyberspace will require constant planning and organization. The structures proliferating within it will require design, and the people who design these structures will be called cyberspace architects. Schooled in computer science and programming (the equivalent of “construction”), in graphics, and in abstract design, schooled also along with their brethren “real-space” architects, cyberspace architects will design electronic edifices that are fully as complex, functional, unique, involving, and beautiful as their physical counterparts if not more so. Theirs will be the task of visualizing the intrinsically nonphysical and giving inhabitable visible form to society’s most intricate abstractions, processes, and organisms of information. And all the while such designers will be rerealizing in a virtual world many vital aspects of the physical world, in particular those orderings and pleasures that have always belonged to architecture. Two chapters in this volume “come out of” architecture, my own and Marcos Novak’s. My chapter attempts to discuss cyberspace in terms of certain basic design principles and then show some visualized examples; Novak discusses the idea of cyberspace as a poetic mediuim that, among other things, creates a “liquid architecture,” an architecture of information, being less a proposition about designing buildings, of course, than a prelude as to how we might evolve legible forms in the context of a user-driven and self-organizing cyberspace system. Thread Four This thread is drawn from the larger history of mathematics. It is the line of arguments and insights that revolve around (1) the propositions of geometry and space, (2) the spatialization of arithmetical/algebraic operations, and (3) reconsideration of the nature of space in the light of (2). Since Artistotle, operating alongside this “spatial-geometrical” thread in mathematics has been a complementary one, that is, the development of symbolic logic, algebraic notation, calculus, finite mathematics, and so on, to modern programming languages. I say “complementary” because these last-named subjects could (and can still) proceed purely symbolically, with little or no geometrical, spatial interpretation; algebra, number theory, computation theory, logic. . . these are symbolic operations upon symbolic operations and have a life of their own. In practice, of course, diagrams, which are spatial and geometrical, and symbol strings (mathematical notation, language) are accepted as mutually illuminating representations and are considered together. But the distinction between them, and the tension, still remain. There are those who think most easily and naturally in symbolic sequences, and linear operations upon them; there are those who think most easily and naturally in shapes, actions, and spaces. Apparently more than one type of intelligence is involved here (West 1991, Gardner 1983, Hadamard 1945). Be this as it may, cyberspace clearly is premised upon the desirability of spatialization per se for the understanding of information. Certainly, it extends the current paradigm in computing of “graphic user interfaces” into higher dimensions and more involving experiences, and it extends current interest, as evidenced by the popularity of Edward Tufte’s books (1983, 1990), in “data cartography” in general and in the field of scientific visualization. But, more fundamentally, cyberspace revivifies and then extends some of the more basic techniques and questions having to do with the spatial nature of mathematical entities, and the mathematical nature ofspatial entities, that lie at the heart of what we consider both real and measurable. Rigorous reasoning with shape—deductive geometry—began, as we all know, in ancient Greece with Thales around 600 B.C., continuing through 225 B.C. with Pythagoras, Euclid, and Apollonius. The subject was twin: (1) the nature (and methods of construction) of the idealized forms studied—basically lines, circles, regular polygons and polyhedra, although Apollonius began work on conic sections—and (2) the nature of perfect reasoning itself, which the specifiability and universality of 20 Michael Benedikt geometrical operations seemed to exemplify. The results of geometrical study had practical use in building and road construction, land surveying, and what we today call mechanical engineering. Its perfection and universality also supported the casting of astrological/cosmological models along geometrical lines. The science and art of geometry has developed sporadically since, receiving its last major “boost” of renewed interest—after Kepler and Newton—in the late nineteenth century, with Bolyai and Lobatchevsky’s discovery of non-Euclidean geometry. Soon, however, with the concept of pure topology and the discovery of consistent geometries of higher dimensionality than three, first Euclidean geometry and then geometry in general began to lose something of its luster as a science wherein significant new discoveries could be made. All statements of visual geometrical insight, it seemed, could be studied more generally and accurately in the symbolic/algebraic language of analytical mathematics—final fruit of Descartes’ project in La Géométrie, which was precisely to show how the theorems of geometry could be transcribed into analytical (algebraic) form. Of course the linkage, once made, between geometry and algebra, space and symbol, form and argument, is a two-way one. Descartes had both “algebraized” geometry and “geometrized” algebra. (And it is this second movement that is of most interest to us here.) With one profound invention, he had built the conceptual bridge we today call the Cartesian coordinate system. Here was the insight: just as the positions of points in natural, physical space could be encoded, specified, by a trio of numbers, each referring to a distance from a common but arbitrary origin in three mutually orthogonal directions, so too could the positions of points in a “mathematical space” where the “distances” are not physical distances but numerical values, derived algebraically, of the solution of equations of (up to) three variables. In this way, thousands of functions could accurately be “graphed” and made visible. Today, procedures based on Descartes’ insight are a commonplace, taught even at good elementary schools. But this should not mask the power of the implicit notion that space itself is something not necessarily physical: rather that it is a “field of play” for all information, only one of whose manifestations is the gravitational and electromagnetic field of play that we live in, and that we call the real world. Perhaps no examples are more vivid than the beautiful forms that emerge from simple recursive equations—the new science of “fractals”—and recent discoveries of “strange attractors,” objects of coherent geometry and behavior that “exist” only in mathematical spaces (coordinate systems with specially chosen coordinates) and that economically map/describe/prescribe the behavior of complex, chaotic, physical systems. Which reality is the primary one? we might fairly ask. Actually, why choose? Modern physicists are sanguine: Minkowski had shown the utility of mapping time together with space, Hamiltonian mechanics lent themselves beautifully to visualizing the dynamics of a physical system in n-dimensional state or phase space where a single point represents the entire state of the system, and quantum mechanics seems to play itself out in the geometrical behavior of vectors in Hilbert space, in which one or more of the coordinates are “imaginary” (see Penrose 1989 for a recent explication). In the meantime, the more common art of diagrams and charts proliferatedfrom old maps, schedules, and scientific treatises, to the pages of modern economics primers, advertisements, and boardroom “business graphics.” Many of these representations are in fact hybrids, mixing physical, energic or spatiotemporal, coordinates with abstract, mathematical ones, mixing histories with geographies, simple intervallic scales with exponential ones, and so on. The practice of diagramming (surely one whose origins are earlier than writing) continues too, today enhanced by the mathematics of graph theory with its combinatorial and network techniques to analyze and optimize complex processes. What, we may ask, is the ontological status of such representations? All of them—from simple bar charts and organizational “trees” through matrices, networks, and “spreadsheets” to elaborate, multidimensional, computer-generated visualizations of invisible physical processes—all of these, and all abstract phase-, state-, and Hilbert-space entities, seem to exist in a geography, a space, borrowed from, but not identical with, the space of the piece of paper or computer screen on which we see them. All have a reality that is no mere picture of the natural, phenomenal world, and all display a physics, as it were, from elsewhere. What are they, indeed? Neither discoveries nor inventions, they are of World 3, entities themselves evolved by our intelligence in the world of things and of each other. They represent first evidence of a continent about which we have hitherto communicated only in sign language, 22 Michael Benedikt a continent “materializing,” in a way. And at the same time they express a new etherealization of geography. It is as though, in becoming electronic, our beautiful old astrolabes, sextants, surveyor’s compasses, observatories, orreries, slide rules, mechanical clocks, drawing instruments and formwork, maps and plans—physical things all, embodiments of the purest geometry, their sole work to make us at home in space—become environments themselves, the very framework of what they once only measured. The contributions by Tim McFadden, Carl Tollander, and Alan Wexelblat are partially woven from this thread, as is a good part of my own. McFadden examines the idea of cyberspace as an informational Indra’s Net, a universe of pointlike beads, infinite in number, each of which reflects all the others. Here cyberspace is an evolving, fourdimensional hologram of itself. (It was this ancient Hindu image of Indra’s Net that also informed Leibniz’s Monadology, as Heim discusses.) Tollander introduces Edelman’s Neuronal Group Selection theory into the design of a noncentralized system of computational “engines” to create cyberspaces that can evolve in a “natural” way. (Novak also discusses this notion). Wexelblat examines the nature of coordinates in abstract spaces in general, in modern personal computing, and then, extrapolated, in terms of cyberspace specifically considered as an outgrowth of these. My account of the intertwining “threads” that seem to lead to cyberspace is, of course, impressionistic and incomplete, and not just for lack of space in this introduction. Cyberspace itself is an elusive and future thing, and one can hardly be definitive at this early stage. But it is also clear that the “threads” themselves are made of threads, and that there are others. For example, the history of art into modern times tells a related story, fully involving mythology, changing media, a relationship to architecture, logic, and so on. It is a thread I have not described, and yet the contribution of artists—visual, musical, cinematic—to the design of virtual worlds and cyberspace promises to be considerable, as Nicole Stenger, poet and animation artist, attests in this volume. Similarly, the story of progress in telecommunications and computing technology—the miniaturizations, speeds, and economies, the new materials, processes, interfaces and architectures—is a thread in its own right, with its own thrusts and interests in the coming-to-be of cyberspace. This story is well chronicled elsewhere (Rheingold 1985, Gilder 1988). Then there is the sociological story, and the economic one, the linguistic one, even the biological one . . . and one begins to realize that every discipline can have an interest in the enterprise of creating cyberspace, a contribution to make, and a historical narrative to justify both. How could it be otherwise? We are contemplating the arising shape of a new world, a world that must, in a multitude of ways, begin, at least, as both an extension and a transcription of the world as we know it and have built it thus far. Another reason that my account is impressionistic and incomplete, however, is that the very metaphor of threads is too tidy and cannot support all that needs to be said. Scale aside, something deeper and more formless is going on. Consider: if information is the very stuff of space and time, what does it mean to manufacture information, and what does it mean to transfer it at ever higher rates between spatiotemporally distinct points, and thus dissolve their very distinctness? With mature cyberspaces and virtual reality technology, this kind of warpage, tunneling, and lesioning of the fabric of reality will become a perceptual, phenomenal fact at hundreds of thousands of locations, even as it falls short of complete, quantum level, physical achievement. Today intellectual, tomorrow practical, one can only guess at the implications. Finally, my “narrative of threads” has not done justice to the authors represented in this volume. Each has their own perspective, expertise, and interest, and each draws inspiration from matters I have not mentioned, and stories I have not sketched or have only touched upon. Rather than extend this introduction with fuller discussion of each chapter, however, I recommend that the reader turn to them forthwith! Many are expanded and revised versions of presentations made at The First Conference on Cyberspace.1 Others are written especially for the present collection.2 All the authors address themselves to the topic with extraordinary seriousness, acumen, and enthusiasm, even though—and perhaps because—the varieties of cyberspace they imagine, describe, and sometimes criticize, do not yet exist. Indeed, the very definition of cyberspace may well be in their hands (or yours, dear reader). Of this much, one can be sure: the advent of cyberspace will have profound effects on so-called postindustrial culture, and the 1Held on May 4 and 5, 1990, at The University of Texas at Austin. The Second (International) Conference on Cyberspace was held April 18-19, 1991, at The University of California at Santa Cruz. 2Gibson, Tomas, Stone, and Wexelblat. 24 Michael Benedikt material and economic rewards for those who first and most properly conceive and implement cyberspace systems will be enormous. But let usset aside talk of rewards. With this volume, with these “firststeps,” let us begin to face the perplexities involved in making the unimaginable imaginable and the imaginable real. Let the ancient project that is cyberspace continue.

Codes—by name and by matter—are what determine us today, and what we must articulate if only to avoid disappearing under them completely. They are the language of our time precisely because the word and the matter code are much older, as I will demonstrate with a brief historical regression. And have no fear: I promise to arrive back at the present. Imperium Romanum Codes materialize in processes of encryption, which is, according to Wolfgang Coy’s elegant defi nition, “from a mathematical perspective a mapping of a fi - nite set of symbols of an alphabet onto a suitable signal sequence.”1 This defi nition clarifi es two facts. Contrary to current opinion, codes are not a peculiarity of computer technology or genetic engineering; as sequences of signals over time they are part of every communications technology, every transmission medium. On the other hand, much evidence suggests that codes became conceivable and feasible only after true alphabets, as opposed to mere ideograms or logograms, had become available for the codifi cation of natural languages. Those alphabets are systems of identically recurring signs of a countable quantity, which map speech sounds onto letters more or less one- to- one and, hopefully, completely. A vocalic alphabet of a type such as Greek,2 justly praised for being the “fi rst total analysis of a language,”3 does appear to be a prerequisite for the emergence of codes, and yet, not a suffi cient one. For what the Greeks lacked (leaving out of consideration sporadic allusions in the work of Aischylos, Aenas, Tacticus, and Plutarch to the use of secret writing4 was that second prerequisite of all coding, namely, developed communications technology. It is anything but coincidental that our reports of the fi rst secret message systems coincide with the rise of the Roman Empire. In his Lives of the Caesars, Suetonius—who himself served as secret scribe to a great emperor—recounts discovering encrypted letters among the personal fi les left behind by both the divine Caesar and the divine Augustus. Caesar contented himself with moving all the letters of the Latin alphabet by four places, thus writing D instead of A, E instead of B, and so forth. His adoptive son Augustus, by contrast, is reported to have merely skipped one letter, but a lack of mathematical discernment led him to replace the letter X, the last in his alphabet, by a double A.5 The purpose was obvious: When read aloud by those not called upon to do so (and Romans were hardly the most literate of people), a stodgy jumble of consonants resulted. And as if such innovations in matters of encryption were not suffi cient, Suetonius attributes to Caesar another invention immediately beforehand—that of having written in several columns, or even separate pages, reports to the Roman Senate on the Gallic campaign. Augustus is credited with the illustrious deed of creating, with riders and relay posts, Europe’s fi rst strictly military express- mail system.6 In other words, the basis on which command, code, and communications technology coincided was the Empire, as opposed to merely the Roman Republic or shorthand writers like Cicero. Imperium is the name of both the command and its effect: the world empire. “Command, control, communications, intelligence” was also the Pentagon’s imperial motto until very recently, when, due to the coincidence of communication technologies and Turing machines it was swapped for C4 —“command, control, communication, computers”—from Orontes to the Scottish headland, from Baghdad to Kabul. It was the case, however, that imperia, the orders of the Emperor, were also known as codicilla, the word referring to the small tablets of stripped wood coated with wax in which letters could be inscribed. The etymon codex for its part—caudex in Old Latin and related to the German verb hauen (to hew)—in the early days of the Empire assumed the meaning of “book,” whose pages could, unlike papyrus scrolls, for the fi rst time be leafed through. And that was how the word that interests us here embarked on its winding journey to the French and English languages. From Imperator Theodosius to Empereur Napoleon, “code” was simply the name of the bound book of law, and codi- fi cation became the word for the judicial- bureaucratic act needed to arrest in a single collection of laws the torrents of imperial dispatches or commands that for centuries had rushed along the express routes of the Empire. Message transmission turned into data storage,7 pure events into serial order. And even today the Codex Theodosius and Codex Iustinianus continue to bear a code of ancient European rights and obligations in those countries where Anglo- American common law does not happen to be sweeping the board. In the Corpus Iuris, after all, copyrsights and trademarks are simply meaningless, regardless of whether they protect a codex or a code. Nation- States The question that remains is why the technical meaning of the word “code” was able to obscure the legal meaning to such a degree. As we know, contemporary legal systems regularly fail to grasp codes in the fi rst place and, in consequence, to protect them, be it from robbers and purchasers or, conversely, from their discoverers and writers. The answer seems to be simple. What we have been calling a code since the secret writings of Roman emperors to the arcana imperii of the modern age was known as a “cipher” from the late Middle Ages onward. For a long time the term code was understood to refer to very different cryptographic methods whereby words could still be pronounced, but obscure or innocuous words simply replaced the secret ones. Cipher, by contrast, was another name for the zero, which at that time reached Europe from India via Baghdad and put sifr (Arabic: “emptiness”) into mathematical- technical power. Since that time, completely different sets of characters have been devised (in sharp contrast to the invention of Greek for speech sounds and numbers: on one side of language the alphabet of the people, on the other the numbers of the bearers of secrets—the name of which spelled the Arabic sifr once again. Separate character sets, however, are productive. Together they brew wondrous creatures that would never have occurred to the Greeks or Romans. Without modern algebra there would be no encoding; without Gutenberg’s printing press, no modern cryptology. In 1462 or 1463, Battista Leone Alberti, the inventor of linear perspective, was struck by two plain facts. First, that the frequency of occurrence of phonemes or letters varies from language to language, a fact which is proved, according to Alberti, by Gutenberg’s letter case. From the frequency of shifted letters as they were written by Caesar and Augustus, cryptanalysis can heuristically derive the clear text of the encrypted message. Second, it is therefore insuffi cient to encrypt a message by shifting all the letters by the same number of places. Alberti’s proposal that every new letter in the clear text be accompanied by an additional place- shift in the secret alphabet was followed up until World War II.8 One century after Alberti, François Viète, the founder of modern algebra, and also a cryptologist in the service of Henry IV, intertwined number and letter more closely still. Only since Viète have there been equations containing unknowns and universal coef- fi cients written with numbers encoded as letters.9 This is still the work method of anybody who writes in a high- level programming language that likewise allocates variables (in a mathematically more or less correct manner) to alpha numeric signs, as in equations. On this basis—Alberti’s polyalphabetic code, Viète’s algebra, and Leibniz’ differential calculus—the nation- states of the modern age were able to technically approach modernity. Global Message Traffi c Modernity began, however, with Napoleon. As of 1794, messengers on horseback were replaced by an optical telegraph which remote- controlled France’s armies with secret codes. In 1806, the laws and privileges surviving from the old days were replaced by the cohesive Code Napoléon. In 1838, Samuel Morse is said to have inspected a printing plant in New York in order—taking a leaf from Alberti’s book—to learn from the letter case which letters occurred most frequently and therefore required the shortest Morse signals.10 For the fi rst time a system of writing had been optimized according to technical criteria—that is, with no regard to semantics—but the product was not yet known as Morse code. The name was bestowed subsequently in books known as Universal Code Condensers, which offered lists of words that could be abbreviated for global cable communications, thus reducing the length, and cost, of telegrams, and thereby encrypting the sender’s clear text for a second time. What used to be called deciphering and enciphering has since then been referred to as decoding and encoding. All code processed by computers nowadays is therefore subject to Kolmogorov’s test: Input is bad if it is longer than its output; both are equally long in the case of white noise; and a code is called elegant if its output is much longer than itself. The twentieth century thus turned a thoroughly capitalist money- saving device called “code condenser” into highest mathematical stringency. The Present Day—Turing All that remains to ask is how the status quo came about or, in other words, how mathematics and encryption entered that inseparable union that rules our lives. That the answer is Alan Turing should be well known today. The Turing machine of 1936, as the principle controller of any computer, solved a basic problem of the modern age: how to note with fi nitely long and ultimately whole numbers the real, and therefore typically infi nitely long, numbers on which technology and engineering have been based since Viète’s time. Turing’s machine proved that although this task could not be accomplished for all real numbers, it was achievable for a crucial subset, which he dubbed computable numbers.11 Since then a fi nite quantity of signs belonging to a numbered alphabet which can, as we know, be reduced to zero and one, has banished the infi nity of numbers. No sooner had Turing found his solution than war demanded its cryptanalytical application. As of spring 1941 in Britannia’s Code and Cipher School, Turing’s proto- computers almost decided the outcome of the war by successfully cracking the secret codes of the German Wehrmacht, which, to its own detriment, had remained faithful to Alberti. Today, at a time when computers are not far short of unravelling the secrets of the weather or the genome—physical secrets, that is to say, and increasingly often biological ones, too—we all too often forget that their primary task is something different. Turing himself raised the question of the purpose for which computers were actually created, and initially stated as the primary goal the decoding of plain human language: Of the above possible fi elds the learning of languages would be the most impressive, since it is the most human of these activities. This fi eld seems, however, to depend rather too much on sense organs and locomotion to be feasible. The fi eld of cryptography will perhaps be the most rewarding. There is a remarkably close parallel between the problems of the physicist and those of the cryptographer. The system on which a message is enciphered corresponds to the laws of the universe, the intercepted messages to the evidence available, the keys for a day or a message to important constants which have to be determined. The correspondence is very close, but the subject matter of cryptography is very easily dealt with by discrete machinery, physics not so easily.12 Conclusions Condensed into telegraphic style, Turing’s statement thus reads: Whether everything in the world can be encoded is written in the stars. The fact that computers, since they too run on codes, can decipher alien codes is seemingly guaranteed from the outset. For the past three- and- a- half millennia, alphabets have been the prototype of everything that is discrete. But it has by no means been proven that physics, despite its quantum theory, is to be computed solely as a quantity of particles and not as a layering of waves. And the question remains whether it is possible to model as codes, down to syntax and semantics, all the languages that make us human and from which our alphabet once emerged in the land of the Greeks. This means that the notion of code is as overused as it is questionable. If every historical epoch is governed by a leading philosophy, then the philosophy of code is what governs our own, and so code—harking back to its root, “codex”—lays down the law for one and all, thus aspiring to a function that was, according to the leading philosophy of the Greeks, exercised exclusively by Aphrodite.13 But perhaps code means nothing more than codex did at one time: the law of precisely that empire which holds us in subjection and forbids us even to articulate this sentence. At all events, the major research institutions that stand to profi t most from such announcements proclaim with triumphant certainty that there is nothing in the universe, from the virus to the Big Bang, which is not code. One should therefore be wary of metaphors that dilute the legitimate concept of code, such as when, for instance, in the case of DNS, it was not possible to fi nd a one- to- one correspondence between material elements and information units as Lily Ray discovered in the case of bioengineering. As a word that in its early history meant “displacement” or “transferral”—from letter to letter, from digit to letters, or vice versa—code is the most susceptible of all to faulty communication. Shining in the aura of the word code one now fi nds sciences that do not even master their basic arithmetic or alphabet, let alone cause something to turn into something different as opposed to merely, as in the case of metaphors, go by a different name. Therefore, only alphabets in the literal sense of modern mathematics should be known as codes, namely one- to- one, fi nite sequences of symbols, kept as short as possible but gifted, thanks to a grammar, with the incredible ability to infi nitely reproduce themselves: Semi- Thue groups, Markov chains,14 Backus- Naur forms, and so forth. That, and that alone, distinguishes such modern alphabets from the familiar one that admittedly spelled out our languages and gave us Homer’s poetry15 but cannot get the technological world up and running the way computer code now does. For while Turing’s machine was able to generate real numbers from whole numbers as required, its successors have—in line with Turing’s daring prediction—taken command.16 Today, technology puts code into the practice of realities, that is to say: it encodes the world. I cannot say whether this means that language has already been vacated as the House of Existence. Turing himself, when he explored the technical feasibility of machines learning to speak, assumed that this highest art, speech, would be learned not by mere computers but by robots equipped with sensors, effectors, that is to say, with some knowledge of the environment. However, this new and adaptable environmental knowledge in robots would remain This means that the notion of code is as overused as it is questionable. If every historical epoch is governed by a leading philosophy, then the philosophy of code is what governs our own, and so code—harking back to its root, “codex”—lays down the law for one and all, thus aspiring to a function that was, according to the leading philosophy of the Greeks, exercised exclusively by Aphrodite.13 But perhaps code means nothing more than codex did at one time: the law of precisely that empire which holds us in subjection and forbids us even to articulate this sentence. At all events, the major research institutions that stand to profi t most from such announcements proclaim with triumphant certainty that there is nothing in the universe, from the virus to the Big Bang, which is not code. One should therefore be wary of metaphors that dilute the legitimate concept of code, such as when, for instance, in the case of DNS, it was not possible to fi nd a one- to- one correspondence between material elements and information units as Lily Ray discovered in the case of bioengineering. As a word that in its early history meant “displacement” or “transferral”—from letter to letter, from digit to letters, or vice versa—code is the most susceptible of all to faulty communication. Shining in the aura of the word code one now fi nds sciences that do not even master their basic arithmetic or alphabet, let alone cause something to turn into something different as opposed to merely, as in the case of metaphors, go by a different name. Therefore, only alphabets in the literal sense of modern mathematics should be known as codes, namely one- to- one, fi nite sequences of symbols, kept as short as possible but gifted, thanks to a grammar, with the incredible ability to infi nitely reproduce themselves: Semi- Thue groups, Markov chains,14 Backus- Naur forms, and so forth. That, and that alone, distinguishes such modern alphabets from the familiar one that admittedly spelled out our languages and gave us Homer’s poetry15 but cannot get the technological world up and running the way computer code now does. For while Turing’s machine was able to generate real numbers from whole numbers as required, its successors have—in line with Turing’s daring prediction—taken command.16 Today, technology puts code into the practice of realities, that is to say: it encodes the world. I cannot say whether this means that language has already been vacated as the House of Existence. Turing himself, when he explored the technical feasibility of machines learning to speak, assumed that this highest art, speech, would be learned not by mere computers but by robots equipped with sensors, effectors, that is to say, with some knowledge of the environment. However, this new and adaptable environmental knowledge in robots would remain obscure and hidden to the programmers who started them up with initial codes. The so- called “hidden layers” in today’s neuronal networks present a good, if still trifl ing, example of how far computing procedures can stray from their design engineers, even if everything works out well in the end. Thus, either we write code that in the manner of natural constants reveals the determinations of the matter itself, but at the same time pay the price of millions of lines of code and billions of dollars for digital hardware; or else we leave the task up to machines that derive code from their own environment, although we then cannot read—that is to say: articulate—this code. Ultimately, the dilemma between code and language seems insoluble. And anybody who has written code even only once, be it in a high- level programming language or assembly, knows two very simple things from personal experience. For one, all words from which the program was by necessity produced and developed only lead to copious errors and bugs; for another, the program will suddenly run properly when the programmer’s head is emptied of words. And in regard to interpersonal communications, that can only mean that self- written code can scarcely be passed on with spoken words. May myself and my audience have been spared such a fate in the course of this essay.


My semi-technical introduction to computer graphics will, however, provide only a half-answer, one that, in particular, cannot address the necessary comparison between paintings and computer images or between subtractive and additive color mixing. Simplified accordingly, a computer image is a two-dimensional additive mixture of three base colors shown in the frame, or parergon, of the monitor housing. Sometimes the computer image as such is less apparent, as in the graphic interface of the newfangled operating systems, sometimes rather more, as in "images" in the literal sense of the word. At any rate, the generation of 2000 likely subscribes to the fallacy-backed by billions of dollars-that computers and computer graphics are one and the same. Only aging hackers harbor the trace of a memory that it wasn't always so. There was a time when the computer screen's display consisted of white dots on an amber or green background, as if to remind us that the techno-historical roots of computers lie not in television, but in radar, a medium of war.
The computer image derives precisely this addressability from early-warning systems, even if it has replaced the polar coordinates _ I of the radar screen with Cartesian coordinates. In contrast to the semi-analog medium of television, not only the horizontal lines but also the vertical columns are resolved | lI . . into basic units. The mass of these so-called- 1 I "pixels" forms a two-dimensional matrix - that assigns each individual point of the " i* ] image a certain mixture of the three base colors: red, green, and blue. The discrete, or digital, nature of both the geometric coordinates and their chromatic values makes possible the magical artifice that separates computer graphics from film and television. Now, for the first time in the history of optical media, it is possible to address a single pixel in the 849th row and 720th column directly without having to run through everything before and after it. The computer image is thus prone to falsification to a degree that already gives television producers and ethics watchdogs the shivers; indeed, it is forgery incarnate. It deceives the eye, which is meant to be unable to differentiate between individual pixels, with the illusion or image of an image, while in truth the mass of pixels, because of its thorough addressability, proves to be structured more like a text composed entirely of individual letters. For this reason-and for this reason only-it is no problem for a computer monitor to switch between text and graphics modes. The twofold digitality of coordinates and color value, however, creates certain problem areas, of which at least three should be mentioned.
Third, the digitality of computer graphics creates a problem unknown to computer music. In an essay on time axis manipulation, I have previously tried to show the leeway produced by the fact that the digital sampling of any given musical sequence falls into three elements (a triad is familiar to us through Giuseppe Peano's theory of natural numbers): an event or state of a millisecond's duration, its predecessor, and its successor.2 These three can be integrated or differentiated, exchanged or scrambled until the limits of modern academic and popular music are truly explored. In principle-and that means, unfortunately, given an exponentially higher processing time-these tricks could be adapted from digital music's single dimension to the two dimensions of digital images. The result, however, tends to be so chaotic that it is as if perception were regressing to pure sensation a la David Hume or Kaspar Hauser. The reason for this is as fundamental as it is non-trivial. Every image (in the sense of art, not of mathematics) has a top and a bottom, a left and a right. Pixels, insofar as they are constructed algebraically as two-dimensional matrices and geometrically as orthogonal grids, necessarily have more than one neighbor. In the heroic beginnings of computer science, great mathematicians had to begin by formulating truisms, whence arose W. Ross Ashby's and John von Neumann's concepts of neighboring elements. In the former, a given element is considered to be surrounded only by a cross of neighbors: above, below, left, and right; in the latter, it is surrounded by a square of the above-mentioned orthogonal elements plus four additional diagonal neighbors. A difference that could perfectly describe, if you like, the difference between the urban fabrics of Manhattan and Tokyo, respectively.
Heidegger posed the riddle of perception thus: "in the appearing of things, never do we, either preliminarily or essentially, perceive an onrush of sensations."3 For beings that dwell in language, anything seen or heard shows itself always already as something. For computer-supported image analysis, however, this something-assomething remains a distant theoretical goal, the achievement of which is not even assured. Therefore I would postpone the question of automatic image analysis for symposia on perception to take place not sooner than a decade from now, and limit myself in the following to the problem of automatic image synthesis. I am not concerned, then, with how computers simulate optical perception, but rather only with how they deceive us. For it seems to be precisely this exorbitant capacity that elevates the medium of the computer above all optical media in Western history.
Computer graphics are to these optical media what the optical media are to the eye. Just as the camera lens, literally as hardware, simulates the eye, which is literally wetware. so does software, as computer graphics, simulate hardware. The optical laws of reflection and refraction remain in effect for output devices such as monitors or LCD screens, but the program whose data directs these devices transposes such optical laws as it obeys into algebraically pure logic. These laws are generally, it should be noted from the outset, by no means all the optical laws valid for fields of vision and surfaces, shadows and effects of light; what is played out are these selected laws themselves and not, as in the optical media, just the effects they produce. It's no wonder, then, that art historian Michael Baxandall can go so far as to suggest that computer graphics provide the logical space of which any given perspective painting forms a more or less rich subset.
Conversely, computer graphics, because it is software, consists of algorithms and only of algorithms. The optimal algorithm for automatic image synthesis can be determined just as easily as non-algorithmic image synthesis. It would merely have to calculate all optical, i.e. electromagnetic, equivalencies that quantum electrodynamics recognizes for measurable spaces, for virtual spaces as well; or, to put it more simply, it would have to convert Richard Feynman's three-volume Lectures on Physics into software. Then a cat's fur, because it creates anisotropic surfaces, would shimmer like cat's fur; then streaks in a wine glass, because they change their refraction index at each point, would turn the lights and things behind them into complete color spectra. Theoretically, nothing stands in the way of such miracles. Universal discrete machines, which is to say, computers, can do anything so long as it is programmable. But it is not just in Rilke's Malte Laurids Brigge but also in quantum electrodynamics that "realities are slow and indescribably detailed."7 The perfect optics could be programmed just barely within a finite time, but, because of infinite monitor waiting times, would have to put off rendering the perfect image. Computer graphics are differentiated from the cheap real-time effects of the visual entertainment media by a capacity to waste time that would rival that of good old painters if its users were just more patient. It is only in the name of impatience that all existing computer graphics are based on idealizations-a term that functions here, unlike in philosophy, as a pejorative.

In all historical accuracy I shall begin with raytracing, if only because it, for the best or worst reasons in the world, is much older than the radiosity algorithm. As Axel Roch will soon make public, the concept of raytracing derives not at all from computer graphics, but rather from its military predecessor: the tracking of enemy airplanes with radar. And as the computer graphics expert Alan Watt has recently shown, raytracing is in fact even more venerable. The first light ray whose refraction and reflections generated a virtual image was constructed in the year of our Lord 1637 by a certain Rene Descartes.9 Eighteen years earlier, in the wartime of November 1619, Descartes had received one illumination and three dreams. The illumination was about a wondrous science-perhaps the analytic geometry he would go on to develop later. The dreams, however, began with a storm that spun Descartes, who was lame on his right side, around his own left leg three or four times. I suspect, however, that the dream and the science are one and the same. In the dream the subject becomes an unextendable point or, better, midpoint, around which one's own body, as a three-dimensional res extensa, describes the geometric figure of a circle. Cartesian philosophy, as is well known, deals with the res cogitans and the res extensa; as is far less well known, analytic geometry deals with algebraically describable movements or surface areas. Descartes made it possible, for the first time in the history of mathematics, not to produce figures like the circle as the drawn likeness of a celestial-geometrical given but rather to construct them as functions of an algebraic variable. The subject as res cogitans took a wild ride, so to speak, through all the functional values of an equation, until in Descartes's initial dream of 1619 the circle (or, in Miinchhausen's ride on the cannonball, the parabola) was described.

To be sure, Heron of Alexandria had already formulated the law of reflection, Willibrord Snell the law of refraction. It remained to Descartes, however, to piece together the path of a single ray of light through the repeated application of both laws. The Cartesian subject comes about through self-application, or, to put it in the terms of computer science, through recursion. Precisely for this reason, Cartesian raytracing never inspired any painter, let alone any optical analog medium. Only computers and, more precisely, computer languages that allow for recursive functions have the processing power to even trace the countless alternative cases or fates of a single light ray in a virtual space full of virtual surfaces. Raytracing programs begin, in the most elementary case, by defining the computer screen as a two-dimensional window onto a virtual three-dimensionality. Then, two iteration loops follow all the lines and columns of this screen until the ray of vision of a virtual eye situated in front of the screen has reached all the pixels. These virtual rays, though, keep wandering behind the pixels in order to explore the various different outcomes. Most of these have the fortune not to collide with a surface, and thus can quickly execute their task of rendering a mere background color such as that of the sky. Other rays, however, find themselves trapped in a transparent glass globe like Descartes's, where they would be subject to an endless series of refractions and reflections if the impatience of computer graphics programs did not limit the maximum allowable recursions. This is necessary if only because a light ray, should it play between two parallel and perfect mirrors, would never stop, while algorithms are all but defined by a finite use of time.

Radiosity is consequently, in contrast to raytracing, an algorithm born of necessity. Only when seen in its formal elegance can integration be defined as the reverse function of differentiation, for the bitter empirical and numerical truth is that it consumes dramatically higher processing time. Radiosity programs have only become feasible since they have stopped promising to solve their linear equation system in a single run-through.1 In more prosaic terms: one starts up the algorithm, contemplates the as yet completely black screen, takes one of the coffee breaks so famous among computer programmers, then returns after one or two hours to have a look at the first passable results of the global light energy distribution. What so-called nature can accomplish in nanoseconds with its parallel calculation drives its alleged digital equivalent to overload.

Had I promised mere recipes instead of a semi-technical introduction to computer graphics, this short text could end here. Fans of interiors would download some radiosity programs, while fans of the open horizon would surf the Net for some raytracing programs. And now that, at least with LINUX, we have the Blue Moon Rendering Tools, the very decision has become moot. This software, no less wondrous than a blue moon, calculates virtual image worlds in the first run-through following global dependencies in the sense of radiosity, but in the second runthrough follows local singularities in the sense of raytracing. It thus promises a coincidentia oppositorum, which cannot be a matter of simple addition given all that has been said above. It would be going too far afield if I were to try to explain why, in the case of such two-step processes, not only the second step must orient itself to the first but, what is nearly impossible, the first must already orient itself to the second. Otherwise, the four possible cases of optical energy transmission couldn't possibly all be taken into consideration.

The partnership between computers and the visual arts is now at a singular point in its development, emerging from a state of infancy into a first level of maturity. Many artists have used the new technology to enhance or facilitate work on lines established by other media, primarily paint and photographic film. Many are working away from these preexisting genres into modes that could only be created with computers. Though it's always fun to speculate, it's impossible to say with even a slight bit of certainty what this new alliance between art and technics may bring. We may see massive changes in all the arts, perhaps coming on us so rapidly that we won't know what hit us. Perhaps the alliance will result primarily in techniques that will allow artists to do what they'd be doing anyway, but with greater ease and speed. We may see bio chips and neural interfaces allowing us to experience all art simultaneously and internally, and take it from there to wherever our own personal capabilities allow. Or maybe we'll just see a few good pieces and some snappier special effects in the movies. Whatever the case, a lot is going on right now, and it would be a shame to miss out on the marvelous advent of computer art — this coming of age will not happen again.

Cynthia Goodman's Digital Visions is an excellent survey of the state of the art at the time of publication. Despite the rapid changes in computer technology, this book will probably be the best survey available for several years and remain a landmark after it has been superseded. The book includes about 150 samples of computer related art, reproduced as well as images often meant to be seen on a different scale or in a different context or illuminated from behind can be printed in an affordable edition. Goodman's commentary is just what a survey should be: descriptive, impersonal, nonjudgemental and pluralistic. Her documentation is sufficiently detailed in her listing of hardware and software used in samples to satisfy those who are knowledgeable, but her commentaries are free from the technical argot that would make it difficult reading for those unfamiliar with computers.

One of the fascinating phenomena of the present state of the alliance is the way that computers can be used to make standard functions easier and quicker. Using a keyboard or any one of a number of input devices, including light pens that can be used directly on a computer terminal, an artist can create a basic design, save the original on a magnetic disk, and then rework it, changing existing forms, adding new ones, deleting others, and shifting color around. If one color doesn't work, it can be dropped and substituted by another by pressing a few keys. At the present state of the art, this need not produce the clunky images and lifeless colors often associated with computers -- resolutions so fine that disjunctions are imperceptible to the human eye allow a delicacy of shading fully comparable with anything a brush can achieve; and with a palette of some sixteen million colors (about all a human eye can discern) available on some of the most powerful units, it could be argued that computers offer more color options than any other medium. At present, some artists use this sort of technique as a means of making sketches for work to be completed in other media. Others print out their work directly from the images composed on their computer monitors.

In many cases the results are so much like easel paintings or photographs that the use of the computer seems comic, a great hooplah made over nothing. Used in this mimetic way, the value of computers can only be assessed by the artists using them. With the advent of inexpensive micro-computers we can assume that more artists will try these convenience functions and accept or reject them. If this usage becomes common practice, it probably won't make much difference to viewers -- it will simply become part of the professional bag of tricks. The majority of the works in Goodman's book use techniques of this sort. Whether the works are interesting or not, the many elaborate techniques are fascinating and, again, now is the time to be enthralled by them — the magic won't last.

Emerging from these convenience functions are some interesting shifts that move away from computer assistance to possibilities unattainable with traditional techniques. Perhaps the most promising is a shift from printing out the final work to creating art meant to be seen on computer terminals or other illuminated devices. These works, seen by radiant rather than reflected light may be the stained glass windows of a future age.

Among the artists who've gone beyond the level of simple convenience, I'd like to bring special attention to two who represent computer art's first level of maturity. Their work goes in different directions, suggesting the versatility of computer usage. In both we see a strong basis in techniques and aesthetics that have nothing to do with computers, and at the same time move the state of computer art beyond simple housekeeping.

Manfred Mohr has for some fifteen years been exploring the possibilities of restructuring the twelve sides of the most basic of forms, the cube, in two dimensional, black and white images. Mohr begins by designing a non-visual program based on algorithms (calculations with cyclic regularities) which are transformed into signs by the computer. Mohr then reworks the signs to his satisfaction and has a plotter (a computer driven drawing device) produce the final image on canvas or paper. The result is a large opus of dynamic images and sequences that can be read as narrative or analyzed by semiotic method. Both the program and the plotter put some distance between Mohr and the finished work, allowing geometry, mathematics, and chance to play an independent role in the work, and minimizing personal or idiosyncratic elements. Mohr's art seems to have raised Constructivism to a level unattainable by his predecessors, Malevich and Mondrian.

Harold Cohen has designed an artificial intelligence program called AARON and he has been able to teach this program to draw clearly legible human figures, plant forms, and other objects, as well as clearly conceived abstractions. AARON produces lively, fluid, energetic drawings with much of the expressiveness you would expect from an artist coming out of a tradition that emphasizes human individualism and prizes natural mysticism. The program has a capacity to learn, it is not simply repeating preexisting drawings but making drawings that could not have been anticipated by Cohen when he wrote or refined the program. Cohen's interactions with AARON occur on several levels: he refines his program as he goes along, taking cues and challenges from what the program has accomplished. AARON is limited to monochrome productions and Cohen often radically alters the program's drawings by adding color. The artificial intelligence of this program is a far cry from the advanced sort of A.I. that technocrats and sci fi buffs forecast, but here we have the first real example of man and computer communicating and interacting constructively, producing art that goes beyond simple mechanical gimicry.

Conventional wisdom has it that computers are inherently dehumanizing devices, the product of mad scientists working in isolation, unaware that their machines are foisting their alienation and solipsism on everyone else. That's more a product of the movies than of computers. Though a new generation of artists turned hackers and scientists turned artist is now emerging, most computer artists have had to form alliances with the scientists who are often perceived as their polar opposites in temperament and personality — and often enough both have had to use equipment owned by great corporate beasts like IBM, Phillips and the pentagon. This collaboration sometimes functions on an intimate level: many computer art producers are married couples or lovers working in tandem, as often as not initially brought together by their need to share skills. (Maybe we could think of this as a form of computer dating that actually works!) Among producers, the computer has encouraged community rather than alienation, perhaps beginning to exorcise the "two cultures" boogie man still seen by many as part of our collective schizophrenia.

Perhaps participatory works will also bring viewers together and encourage community. In popular culture, their cognates are already doing so -- how many kids have met each other for the first time in video arcades since you started reading this article? On the level of self conscious art, computers tend to encourage participatory work, and it's my hunch that computer art will most distinguish itself in this area. A few of the many examples in Goodman's book illustrate directions in which this trend is going.

Wen-Ying Tsai's compositions of moving fiberglass rods illuminated by strobes are simple and elegant examples. Audio feedback devices speed up or slow down the movement of the strobes in response to sounds made by people around them, moving slowly when the environment is quiet, frantically when it is noisy. People around these pieces can control the apparent movement of the rods by making noises ranging from whispers to speech to laughter to clapping, or the units may simply reflect the sounds of people who are not trying to interact with the sculptures.

In the collaborations between Otto Piene and Paul Earls, the frequencies of Earls's electronic music guide the images of Piene's computer drawing program. The images are created by a laser which can project them in all sorts of environments, including projections into the sky, where their three dimensional quality takes on the character of constantly changing monumental sculpture. In work like this, what you see could only be created by computer and laser. The maximum so far attained in collaboration between media and artists is in dance performances such as Phosphones, which use the CORTLI system designed by computer sculptor James Seawright, electronic music composer Emmanuel Ghent, programer William Hemsath, and choreographer Mimi Garrard. This system presents complex interactions between music and lighting, which in turn interact with the movements of the dancers in Ms. Garrard's company. This is just a few steps away from a total art form in which everyone dances and the audience and the work are reintegrated. And it's not far from massive works in which thousands of people participate, and a final "product" is never achieved or desired.

The basis of the partnership between computers and the arts is a human partnership. How much it can grow through its interaction with the nonhuman may be a partial test of its value. But ultimately this new technology will be a test of our cooperative and conceptual capacities and of our imagination and courage.

This volume of essays is the happy result of contacts and collaborations established during the three years devoted to the preparation of 'Cybernetic Serendipity'. Cybernetic Serendipity was an exhibition mounted at the Institute of Contemporary Arts in the summer of 1968, which dealt with the relationship of the computer and the arts. The exhibition, like this book, was concerned with the exploration and demonstration of connexions between creativity and technology (and cybernetics in particular), the links between scientific or mathematical approaches, intuitions, and the more irrational and oblique urges associated with the making of music, art and poetry. The title itself was intended to convey the fact that through the use of cybernetic devices we have made many fortunate discoveries for the arts. The exhibition Cybernetic Serendipity was mounted in a gallery of 6500 square feet, involved 325 participants and was seen by 60,000 people. The exhibits showed how man can use the computer and new technology to extend his creativity and inventiveness. These consisted of computer graphics, computercomposed and -played music, computer-animated films, computertexts, and among other computergenerated material, the first computer sculpture. There were also cybernetic machines such as Gordon Pask's 'colloquy of mobiles', television sets converting sound into visual patterns, Peter Zinovieff's electronic music studio with a computer which improvised on tunes whistled into a microphone by the visitors; there were robots, drawing machines and numerous constructions which responded to ambient sound and light. Six IBM machines demonstrated the uses of computers, and a visual display provided information on the history of cybernetics. Two aspects of this whole project are particularly significant. The first is that at no point was it clear to any of the visitors walking around the exhibition, which of the various drawings, objects and machines were made by artists and which were made by engineers; or, whether the photographic blow-ups of texts mounted on the walls were the work of poets or scientists. There was nothing intrinsic in the works themselves to provide information as to who made them. Among the contributors to the exhibition there were forty-three composers, artists and poets, and eighty-seven engineers, doctors, computer systems designers and philosophers. The second significant fact is that whereas new media inevitably contribute to the changing forms of the arts, it is unprecedented that a new tool should bring in its wake new people to become involved in creative activity, whether composing music, painting or writing. Graphic plotters, cathode-ray tube displays and teleprinters have enabled engineers, and others, who would never even have thought of putting pen to paper, to make images for the sheer pleasure of seeing them materialize. Many of the computer graphics made by engineers in Europe, Japan and the USA, approximate very closely to what we have learned to call art and put in our public galleries. This raises a very real question - should these computer graphics hang side by side with drawings by artists in museums and art galleries, or should they belong to another, as yet unspecified, category of creative achievement? There are certain classifications to which we are all assigned according to what we do. These categories which relate solely to our work, or our professional titles, inform the outside world about our way of life, our abilities and creative propensities. The deductions based on these classifications are not necessarily accurate but they suffice to colour the picture of an individual sufficiently for him to be irrevocably labelled. These labels provide information which is accepted without question and without protest. Thus it is assumed that the electronic engineers represent a clever but an uncreative branch of society, whereas artists are exceptionally creative but it is unlikely that they should possess any technological skills. It is also widely assumed that to the engineer, scientist and mathematician, art is magic, and to the composer, painter and poet, technology is a mystery. These rough assumptions are very broadly true but not altogether true. Since the middle 1950s the relationship between art and technology has been increasingly in evidence through the advent of computer-aided creative design. Today these categorical assumptions about our various talents, functions and possibilities are less accurate than ever. Thus Cybernetic Serendipity was not an art exhibition as such, nor a technological fun fair, nor a programmatic manifesto - it was primarily a demonstration of contemporary ideas, acts and objects, linking cybernetics and the creative process.
The question what is life, says Norman O. Brown, turns out to be the question what is sleep. We perceive that the sky exists only on earth. Evolution and human nature are mutually exclusive concepts. We're in transition from the Industrial Age to the Cybernetic Age, characterized by many as the post-Industrial Age. But I've found the term Paleocybernetic valuable as a conceptual tool with which to grasp the significance of our present environment: combining the primitive potential associated with Paleolithic and the transcendental integrities of "practical utopianism" associated with Cybernetic. So I call it the Paleocybernetic Age: an image of a hairy, buckskinned, barefooted atomic physicist with a brain full of mescaline and logarithms, working out the heuristics of computer-generated holograms or krypton laser interferometry. It's the dawn of man: for the first time in history we'll soon be free enough to discover who we are.

Radical Evolution and Future Shock
in the Paleocybernetic Age It is perhaps not coincidental that Western youth has discovered the
I Ching, or Book of Changes, on a somewhat popular level as wemove into the final third of the twentieth century. Change is now ouronly constant, a global institution. The human ecological biosphere isundergoing its second great transition, destined to be even moreprofound than the invention of agriculture in the Neolithic Age. If wecan't see the change, at least we can feel it. Future shock affects ourpsyche and our economy just as culture shock disorients the PeaceCorps worker in Borneo.It is said that we are living in a period of revolution. But nothingsells like freedom: Revolution is big business. As the physicist P. W.Bridgman once said, the true meaning of a term is found byobserving what a man does with it, not what he says about it. Sincethe phenomenon we call revolution is worldwide, and since it's felt inevery human experience, perhaps we might think of it not asrevolution but as radical evolution. Revolution is basically the samewhether defined by Marx or the I Ching: removal of the antiquated.But revolution replaces one status quo with another. Radicalevolution is never static; it's a perpetual state of polarization. Wecould think of it as involuntary revolution, but whatever terminologywe apply that's the condition of the world today, the environment withwhich the artist must work. Radical evolution would be kinder if itwere better understood; but it won't be so long as commercialentertainment cinema continues to representa "reality" that doesn'texist.Sociologist Alvin Toffler has stressed ephemerality as a chiefaspect of radical evolution: "Smith Brothers Cough Drops, CalumetBaking Soda, Ivory Soap, have become institutions by virtue of theirlong reign in the marketplace. In the days ahead, few products willenjoy such longevity. Corporations may create new productsknowing full well they'll remain on the market for only a matter of afew weeks or months. By extension, the corporations themselves—as well as unions, government agencies and all other organizations—may either have shorter life-spans or be forced to undergoincessant and radical reorganization. Rapid decay and regenerationwill be the watchwords of tomorrow."8 Toffler observes that noreasonable man should plan his life beyond ten years; even that, hesays, is risky. When parents speak of their sons becoming lawyersthey are deceiving themselves and their sons, according to thesociologist, "Because we have no conception of what being a lawyerwill mean twenty years hence. Most probably, lawyers will becomputers." In fact, we can't be sure that some occupations willeven exist when our children come of age. For example, thecomputer programmer, a job first created in the 1950's, will be asobsolete as the blacksmith within a decade; computers will reprogram and even regenerate themselves (IBM recently announceda new computer that repairs itself).John McHale, coauthor of the World Design Science Decadedocuments with Buckminster Fuller, emphasizes expendability andimpermanence in radical evolution: "Use value is replacingownership value. For example, the growth of rental and services—not only in automobiles and houses, but from skisto bridal gowns toheirloom silver, castles and works of art... our personal and household objects, when destroyed physically or outmoded symbolically,may be replaced by others exactly similar. A paper napkin, a suit, achair, an automobile, are items with identical replacement value.Metals in a cigarette lighter today may be, within a month or year,part of an auto, lipstick case or orbiting satellite... the concept ofpermanence in no way enables one to relate adequately to ourpresent situation." 9McHale has seen the need for a totally new world view as radicalevolution speeds farther from our grasp. "There's a mythologyabroad which equates the discovery and publication of newfacts with new knowledge. Knowledge is not simply accumulated facts butthe reduction of unrelated and often apparently irrelevant facts intonew conceptual wholes."10 He's talking about completely new ways of looking at the world and everything in it. This is proposition farmore profound than mere political revolution, which Krishnamurti hascharacterized as "The modification of the right according to the ideasof the left.''11 The new consciousness transcends both right and left.We must redefine everything.What happens to our definition of "intelligence" when computers,as an extension of the human brain, are the same size, weight, andcost as transistor radios? They're being developed through themicroelectronics process of Large-Scale Integration.What happens to our definition of "morality" when biochemists areabout to unravel the secrets of the DNA/RNA interaction mechanismto create human life?What happens to our definition of "man" when our next doorneighbor is a cyborg (a human with inorganic parts)? There areseveral crude cyborgs in the world today.What happens to our definition of "environment" when our videoextensions bring us the reality of the solar system daily? What do wemean by "nature" under these circumstances? (McLuhan: "The firstsatellite ended nature in the conventional sense.")What happens to our definition of "creativity" when a computerasks itself an original question without being programmed to do so?This has occurred several times.What happens to our definition of "family" when the intermedianetwork brings the behavior of the world into our home, and whenwe can be anywhere in the world in a few hours?What happens to our definition of "progress" when, according toLouis Pauwels: "For the really attentive observer the problems facingcontemporary intelligence are no longer problems of progress. Theconcept of progress has been dead for some years now. Today it is aquestion of a change of state, a transmutation.''12 Or Norbert Wiener:"Simple faith in progress is not a conviction belonging to strength butone belonging to acquiescence and hence to weakness.'' What happens to our definitions of "material" and "spiritual" whenscience has found no boundary between the two? Although it is stillpopularly assumed that the world is divided into animate andinanimate phenomena, virologists working at the supposed thresholdbetween life and nonlife at the virus level have in fact discovered nosuch boundary. "Both animate and inanimate have persisted rightacross yesterday's supposed threshold in both directions...subsequently what was animate has become foggier and foggier...no life, per se, has been isolated.''14 Indeed, what becomes of "reality" itself as science expands itsmastery of the forces of the universe? "The paradox of twentiethcentury science consists of its unreality in terms of sense impressions. Dealing as it does in energy transformation and submicroscopic particles, it has become a kind of metaphysics practiced by adevoted priestly cult—totally as divorced from the common-sensenotions of reality as was the metaphysics practiced by witch doctorsand alchemists. It is not at all odd, then, to discover that the closerwe come via the scientific method to 'truth,' the closer we come tounderstanding the 'truth' symbolized in myths.''15 This, then, is merely a superficial glimpse at some of the phenomena that characterize the Paleocybernetic Age. Quite clearly manis in the paradoxical position of existing in a state of consciousnesswithout being able to understand it. Man does not comprehend hisrelationship to the universe, either physical or metaphysical. Heinsists on "doing his thing" without the slightest notion of what his"thing" might be. This cosmic credibility gap exists primarily betweenthe facts of scientific experience and the illusions of environmentalconditioning as manifested in the global intermedia network.
The Intermedia Network as Nature The point I wish to make here is obvious yet vital to anunderstanding of the function of art in the environment, even thoughit is consistently ignored by the majority of film critics. It's the ideathat man is conditioned by his environment and that "enviromnent"for contemporary man is the intermedia network. We are conditionedmore by cinema and television than by nature. Once we've agreedupon this, it becomes immediately obvious that the structure andcontent of popular cinema is a matter of cardinal importance, at leastas serious as most political issues, and thus calls for comment notfrom journalists but from those who work at the matter, artiststhemselves.The cinema isn't just something inside the environment; theintermedia network of cinema, television, radio, magazines, books,and newspapers is our environment, a service environment thatcarries the messages of the social organism. It establishes meaningin life, creates mediating channels between man and man, man andsociety. "In earlier periods such traditional meaning and valuecommunication was carried mainly in the fine and folk arts. But todaythese are subsumed amongst many communicating modes. Theterm 'arts' requires expansion to include those advanced technological media which are neither fine nor folk.''16 We've seen the need for new concepts regarding the nature ofexistence; yet concepts are expanded or constricted in direct relationto the relevancy of prevailing languages. In a world where change isthe only constant, it's obvious we can't afford to rely on traditionalcinematic language. The world has changed immeasurably in theseventy years since the birth of cinema: for one thing "world" nowincludes the microcosm of the atom and the macrocosm of the universe in one spectrum. Still popular films speak a languagedeveloped by Griffith, Lumière, Méliès, derived from traditions ofvaudeville and literature.In the Agricultural Age man was totally passive, conditioned andvictimized by the environment. In the Industrial Age man's role was participatory; he became more aggressive and successful in hisattempts to control his environment. We're now moving into theCybernetic Age in which man learns that to control his environmenthe must cooperate with it; he not only participates but actuallyrecreates his environment both physical and metaphysical, and inturn is conditioned by it.To be free of the toil of old relationships we must first be free of theconditioning that instills it within us. As radical evolution gainsmomentum the need to unlearn our past becomes increasingly clear:contemporary life is a process of miseducation/uneducation/reeducation, at a cost of much precious time. McLuhan has noted thatthe true significance of Pavlov's experiments was that any controlledman-made environment is a conditioner that creates "non-perceptivesomnambulists." Since then science has discovered that "molecularmemory" is operative in single-celled and some multi-celledorganisms, and there's evidence that memory-in-the-flesh exists inhumans as well. Biochemists have proven that learned responses toenvironmental stimuli are passed on phylogenetically fromgeneration to generation, encoded in the RNA of the organism'sphysical molecular structure.17 And what could be a more powerfulconditioning force than the intermedia network, which functions toestablish meaning in life?Science has proven that there's no such thing as "human nature."Just as water takes the shape of its container, so human nature isrelative to its past and present conditioning. Optimum freedom ofbehavior and increased self-awareness are implicit in the industrialequation that is trending toward physical success for all men;Paleocybernetic man, however, has not learned to control the environment he creates. "The content of what is available for emulationon the part of the young in each society is itself culturally shapedand limited... the individual typically remains, throughout his lifetime,unaware of how his own habits, which to him appear 'only natural,' infact result from a learning process in which he never had anopportunity to attempt alternative responses.''18 This process is fortunate to have a tool that makes him awareof his own enculturation and thus he enjoys greater psychic freedomthan his ancestors. This tool is what Teilhard de Chardin has calledthe noosphere, the film of organized intelligence that encircles theplanet, superposed on the living layer of the biosphere and thelifeless layer of inorganic material, the lithosphere. The minds ofthree-and-a-half-billion humans—twenty-five percent of all humanswho ever lived—currently nourish the noosphere; distributed aroundthe globe by the intermedia network, it becomes a new "technology"that may prove to be one of the most powerful tools in man's history.John McHale: "World communications... diffuse and interpenetratelocal cultural tradition, providing commonly-shared culturalexperience in a manner unparalleled in human history. Within thisglobal network the related media share and transmit man's symbolicneeds and their expression on a world scale. Besides theenlargement of the physical world, these media virtually extend ourpsychical environment, providing a constant stream of moving,fleeting images of the world for our daily appraisal. They provide
psychic mobility for the greater mass of our citizens. Through thesedevices we can telescope time, move through history, and span theworld in a great variety of unprecedented ways.'' 19Like all energy sources the noosphere can be used for negativepurposes. Its resources can be manipulated to disguise craft ascreativity, especially in these Paleocybernetic days when we're stillimpressed by the sudden influx of information. Fuller hasdifferentiated craft from industry by demonstrating that craft isinherently local in technique and effect whereas industry is inherentlycomprehensive and universal in technique and effect. One mightmake a similar analogy between entertainment and art: entertainment is inherently "local," that is, of limited significance, whereasart is inherently universal and of unlimited significance. Too oftentoday we find that so-called artists working in the intermedia network are little more than adroit imitators, collectors of data andphenomena, which they glean from the noosphere and amalgamateinto packages that are far from whole. They're clever and glib;they've made an art of selling themselves, but they know only effect,not cause; they are merchants of mannerisms.It is precisely this confusion that clouds critical appraisal of"content" in the popular arts. All too frequently eclectic thinking isconfused with creative thinking. The distinction is subtle to be sure:integrative thinking can be the highest form of creativity. Indeed bothart and science function to reveal similarities within an a prioriuniverse of apparent dissimilarities. As with all else, however, there'san art and a craft to thinking, and the popular entertainments remainat the craft level by the very nature of their purpose.The intermedia network has made all of us artists by proxy. Adecade of television-watching is equal to a comprehensive course indramatic acting, writing, and filming. Compressed in such constantand massive dosage, we begin to see the methods and clichés moreclearly; the mystique is gone—we could almost do it ourselves.Unfortunately too many of us do just that: hence the glut of submediocre talent in the entertainment industry. Paradoxically thisphenomenon carries with it the potential of finally liberating cinemafrom its umbilical to theatre and literature, since it forces the moviesto expand into ever more complex areas of language andexperience. Evidence of television's effect on the cinema is alreadyapparent, as we shall see in our discussion of synaesthetic cinema.From another more immediate perspective, however, it is quiteunfortunate. We live in an age of hyperawareness, our sensesextended around the globe, but it's a case of aesthetic overload: ourtechnological zeal has outstripped our psychic capacity to cope withthe influx of information. We are adrift on the surface of radicalevolution unable to plumb the depths of its swift and turbulentcurrent.
The current generation is engaged in an unprecedented questioningof all that has been held essential. We question traditional conceptsof authority, ownership, justice, love, sex, freedom, politics, eventradition itself. But it's significant that we don't question ourentertainment. The disenfranchised young man who dropped out ofcollege, burned his draft card, braids his hair, smokes pot, and digsDylan is standing in line with his girl, who takes the pill, waiting tosee The Graduate or Bonnie and Clyde or Easy Rider— and they'rereacting to the same formulas of conditioned response that lulledtheir parents to sleep in the 1930's.We've seen the urgent need for an expanded cinematic language. Ihope to illustrate that profit-motivated commercial entertainment, byits very nature, cannot supply this new vision. Commercialentertainment works against art, exploits the alienation and boredomof the public, by perpetuating a system of conditioned response toformulas. Commercial entertainment not only isn't creative, it actuallydestroys the audience's ability to appreciate and participate in thecreative process. The implications become apparent when werealize that, as leisure time increases, each human will be forced tobecome a creative, self-sufficient, empirical energy laboratory.D. H. Lawrence has written: "The business of art is to reveal therelation between man and his circumambient universe at this livingmoment. As mankind is always struggling in the toil of oldrelationships, art is always ahead of its 'times,' which themselves arealways far in the rear of the living present." Jean-Jacques Lebelstated the same idea in different terms when he described art as "thecreation of a new world, never seen before, imperceptibly gaining onreality."
We've seen that man is conditioned by, and reacts to, certainstimuli in the man-made environment. The commercial entertainer isa manipulator of these stimuli. If he employs a certain triggermechanism, we're guaranteed to react accordingly, like puppets,providing he manipulates the trigger properly. I'm not saying theartist doesn't resort to audience manipulation; we know he oftendoes. The point, however, is the motivation in doing so. If the artistmust resort to trigger mechanisms to make himself clear, he will; butit's only a means to his end. In the case of the commercialentertainer, however, it's the end in itself.Plot, story, and what commonly is known as "drama" are thedevices that enable the commercial entertainer to manipulate hisaudience. The very act of this manipulation, gratifying conditionedneeds, is what the films actually are about. The viewer purchases itwith his ticket and is understandably annoyed if the film asks him tomanipulate himself, to engage in the creative process along with theartist. Our word poetry derives from the Greek root poiein meaning"to make" or "to work." The viewer of commercial entertainmentcinema does not want to work; he wants to be an object, to be actedupon, to be manipulated. The true subject of commercialentertainment is this little game it plays with its audience.By perpetuating a destructive habit of unthinking response toformulas, by forcing us to rely ever more frequently on memory, thecommercial entertainer encourages an unthinking response to dailylife, inhibiting self-awareness. Driven by the profit motive, thecommercial entertainer dares not risk alienating us by attemptingnew language even if he were capable of it. He seeks only to gratifypreconditioned needs for formula stimulus. He offers nothing wehaven't already conceived, nothing we don't already expect. Artexplains; entertainment exploits. Art is freedom from the conditionsof memory; entertainment is conditional on a present that isconditioned by the past. Entertainment gives us what we want; artgives us what we don't know we want. To confront a work of art is toconfront oneself—but aspects of oneself previously unrecognized.The extent to which blatant audience manipulation not only istolerated but extolled is alarming. Alfred Hitchcock, for example, inhis interview with François Truffaut, finds merit in his ability tomanipulate preconditioned needs for formula stimulus. Speaking of
Psycho, Hitchcock frankly admits: "It wasn't a message that stirred them, nor was it a great performance, or their enjoyment of thenovel... they were aroused by the construction of the story, and theway in which it was told caused audiences all over the world to reactand become emotional.''21 It is essential to understand that Hitchcock openly admits that hedidn't even try to expand awareness or to communicate somesignificant message, but only exploited a universal tradition ofdramatic manipulation in order to supply his audience with thegratification it paid for. The audience sees itself and its dreamsreflected in the film and reacts according to memory, whichKrishnamurti has characterized as being always conditioned."Memory," says Krishnamurti, "is always in the past and is given lifein the present by a challenge. Memory has no life in itself; it comesto life in the challenge [preconditioned formula stimulus]. And allmemory, whether dormant or active, is conditioned."22 It is thisprocess that the entertainment industry calls audience identification.To a healthy mind, anything that is primarily art is also immenselyentertaining. It seems obvious that the most important things shouldbe the most entertaining. Where there's a difference between whatwe "like" and what we know to be vital, we have a condition ofschizophrenia, an unnatural and destructive situation. I speakdeliberately of a "healthy" mind as one capable of creative thinking.Filmmaker Ken Kelman: "The old cinema removes experience,making us see things along with (or through) a protagonist withwhom we identify, and a plot in which we are caught. Such anapproach tends toward not only a lack of viewpoint, of definition of
whose experience it is, but also filters the power of sight into merehabit, dissolves insight into vicariousness. The spectator is reducedto a voyeur—which is, increasingly, the individual's role in society atlarge."23 Minimalist painter David Lee: "When people do not trust theirsenses they lack confidence in themselves. For the last fewcenturies people have lacked confidence. They have not trusted their experience to provide a standard for knowing how to act." 24 It isquite obvious that most of us not only don't know much about art, wedon't even know what we like. Krishnamurti: "One of the fundamentalcauses of the disintegration of society is copying, which is theworship of authority."25 Imitation is the result of inadequate information. Information resultsin change. Change requires energy. Energy is the result of adequateinformation Energy is directly proportional to the amount ofinformation about the structure of a system. Norbert Wiener: "Information is a name for the content of what is exchanged with theouter world as we adjust to it and make our adjustment felt upon it …to live effectively is to live with adequate information."26 From thecinema we receive conceptual information (ideas) and designinformation (experiences). In concert they become one phenomenon, which I've described as the experiential information of aestheticconceptual design. This information is either useful (additive) or redundant. Useful information accelerates change. Redundant information restricts change. If sustained long enough redundant information finally becomes misinformation, which results in negativechange.In communication theory and the laws of thermodynamics thequantity called entropy is the amount of energy reversiblyexchanged from one system in the universe to another. Entropy alsois the measure of disorder within those systems. It measures thelack of information about the structure of the system. For ourpurposes "structure of the system" should be taken to mean "thehuman condition," the universal subject of aesthetic activity. Entropyshould be understood as the degree of our ignorance about thatcondition. Ignorance always increases when a system's messagesare redundant. Ignorance is not a state of limbo in which noinformation exists, but rather a state of increasing chaos due to
misinformation about the structure of the system.The First Law of Thermodynamics states that energy is constant: itcannot be created or destroyed; its form can change, but not its quantity.
The Second Law states that the amount of energy within a localsystem is naturally entropic—it tends toward disorder, dissipation,incoherence. And since energy is defined as "a capacity to rearrange elemental order," entropy, which runs counter to thatcapacity, means less potential for change. We've learned fromphysics that the only anti-entropic force in the universe, or what iscalled negentropy (negative entropy), results from the process offeedback. Feedback exists between systems that are not closed butrather open and contingent upon other systems. In the strictestsense there are no truly "closed" systems anywhere in the universe;all processes impinge upon and are affected by other processes insome way. However, for most practical purposes, it is enough to saythat a system is "closed" when entropy dominates the feedbackprocess, that is, when the measure of energy lost is greater than themeasure of energy gained.The phenomenon of man, or of biological life on earth taken as aprocess, is negentropic because its subsystems feed energy backinto one another and thus are self-enriching, regenerative. Thusenergy is wealth, and wealth according to Buckminster Fuller is "thenumber of forward days a given system is sustainable." BiologistJohn Bleibtreu arrived at a similar conclusion when he noted that theconcept of time can best be viewed as a function of the Second Lawof Thermodynamics—that the measure of entropy in a system is ameasure of its age, or the passage of time since the systemoriginated.27 In other words the degree of a system's entropy isequal to redundancy or stasis whereas its negentropy is equal tokinesis or change. So information becomes energy when itcontributes to the self-enriching omni-regenerative wealth of thesystem. When it's not contributing (i.e., redundant) it is allowing thenatural entropy to increase."It is possible to treat sets of messages as having an entropy likesets of states of the external world... in fact, it is possible to interpretthe information carried by a message as essentially the negative ofits entropy... that is, the more probable the message the lessinformation it gives. Clichés, for example, are less illuminating thangreat poems." 28 Thus the more information concerning the human condition that the artist is able to give us, the more energy we havewith which to modify ourselves and grow in accord with theaccelerating accelerations of the living present.Commercial entertainment may be considered a closed systemsince entropy dominates the feedback process. To satisfy the profitmotive the commercial entertainer must give the audience what itexpects, which is conditional on what it has been getting, which isconditional on what it previously received, ad infinitum. Inherent inthe term "genre," which applies to all entertainment, is that it must beprobable. The content of westerns, gangster movies, romances, etc.,is probable in that it can be identified and comprehended simply byclassification. The phenomenon of drama itself usually is notconsidered a genre, but is in fact the most universal and archetypicalof all genres. Drama, by definition, means conflict, which in turnmeans suspense. Suspense is requisite on the expectation of knownalternatives. One cannot expect the unknown. Thereforeexpectation, suspense, and drama are all redundant probablequalities and thus are noninformative.Drama requires a plot that forces the viewer to move from point Ato point B to point C along predetermined lines. Plot does not mean"story" (beginning-middle-end). It simply indicates a relatively closedstructure in which free association and conscious participation arerestricted. Since the viewer remains passive and is acted upon bythe experience rather than participating in it with volition, there's nofeedback, that vital source of negentropy. Norbert Wiener:"Feedback is a method of controlling a system by reinserting into itthe results of its past performance... if the information whichproceeds backward from the performance is able to change thegeneral method and pattern of performance, we have a processwhich may well be called learning."29 Fuller: "Every time man makesa new experiment he always learns more. He cannot learn less.”30 In the cinema, feedback is possible almost exclusively in what I callthe synaesthetic mode, which we'll discuss presently. Because it isentirely personal it rests on no identifiable plot and is not probable.The viewer is forced to create along with the film, to interpret forhimself what he is experiencing. If the information (either concept or design) reveals some previously unrecognized aspect of the viewer'srelation to the circumambient universe—or provides language withwhich to conceptualize old realities more effectively— the viewerrecreates that discovery along with the artist, thus feeding back intothe environment the existence of more creative potential, which mayin turn be used by the artist for messages of still greater eloquenceand perception. If the information is redundant, as it must be incommercial entertainment, nothing is learned and change becomesunlikely. The noted authority on communication theory, J. R. Pierce,has demonstrated that an increase in entropy means a decrease inthe ability to change. 31 And we have seen that the ability to changeis the most urgent need facing twentieth-century man.The notion of experimental art, therefore, is meaningless. All art isexperimental or it isn't art. Art is research, whereas entertainment isa game or conflict. We have learned from cybernetics that inresearch one's work is governed by one's strongest points, whereasin conflicts or games one's work is governed by its weakestmoments. We have defined the difference between art and entertainment in scientific terms and have found entertainment to beinherently entropic, opposed to change, and art to be inherentlynegentropic, a catalyst to change. The artist is always an anarchist,a revolutionary, a creator of new worlds imperceptibly gaining onreality. He can do this because we live in a cosmos in which there'salways something more to be seen. When finally we erase thedifference between art and entertainment—as we must to survive—we shall find that our community is no longer a community, and weshall begin to understand radical evolution.
The image I would offer as representative of the PaleocyberneticAge is that of the dying man whose life passes before him: a Retrospective Man who discovers the truth about himself too late to makeuse of it. The information explosion is not a window on the future somuch as a mirror of the past catching up with the present. Theintermedia network, or global communications grid, taps knowledgeresources that always have existed in discrete social enclavesaround the planet and saturates them into the collective consciousness. Suddenly the mass public "discovers" African culture,East Indian and American Indian cultures, folk music, politics.Knowledge previously the domain of scholars becomes commonknowledge, and precisely at that point when the old order is about tofade it sees itself clearly for the first time. William Burroughs hascalled it the Age of Total Confront, noting that all the heretoforeinvisible aspects of our condition have quite suddenly becomevisible.Through Duchamp, Cage, and Warhol, for example, we haverediscovered art in the ancient Platonic sense in which there's nodifference between the aesthetic and the mundane. Although thesemen certainly fulfill an avant-garde function in present society, theyin fact conform to the most universal and enduring definition of art. Ifthey've been rejected as artists by the majority of our citizens it'sbecause we've been conditioned by an economic system in whichaesthetic concerns must assume a secondary position if the systemis to survive. Thus art is separated from common experience and anelite hierarchy is established, which seems only natural to everyonecaught up in the economic struggle. John Dewey: "When art attainsclassic status it becomes isolated from the human conditions underwhich it was brought into being and from the human consequences itengenders in actual life experience... when, because of their remoteness, the objects acknowledged by the cultivated to be works offine art seem anemic to the mass of people, aesthetic hunger islikely to seek the cheap and the vulgar."32 Twentieth-century man is retrospective also because the symbolic and value content of his messages—most of which take the form ofcommercial entertainment—is predominantly redundant. NorbertWiener: "Society can only be understood through a study of themessages and the communication facilities which belong to it."33 Almost without exception, these messages tend to be concernedwith what is known as the "human condition." The history of popularentertainment, in terms of its conceptual content, can be divided intothree general categories: (1) idealization, which corresponds tostates of happiness in which life is seen as a heavenly experienceand man is characterized by his most noble deeds; (2)frustration, anexpression of the conflict between inner and outer realities, whenwhat is is not what should be; (3) demoralization, generallyexpressed as "the blues." In commercial entertainment cinema thesethree formulas are followed religiously, almost without exception, andusually comprise the nature of the message. They are the humancondition, that which is taken for granted, the given, the facts of life.Everyone has ideals, everyone is frustrated, everyone gets theblues. But this information is redundant; we must go on from there.Commercial entertainment is "popular" and not what we call artbecause it doesn't go on from there. To insure the widest possibleacceptance of his message, the commercial entertainer must speaka common language. He copies, repeats, or imitates that whichalready exists within the grasp of the so-called average man. Andthe majority of us embrace it because it offers security, a crutch, inthe knowledge that the miseries we suffer are shared by others. Butart transcends the human condition. The artist doesn't want to hearour problems and our dreams—he already knows them. Instead hewants to know what we're doing about them, and he gives us theinstruments we need for the task. The symbol is the basic instrumentof thought; those who create new symbols—artists, scientists, poets,philosophers—are those who, by giving us new instruments to thinkwith, give us new areas to explore in our thinking.A rather indignant woman once asked me how I could have thenerve to suggest that an "abstract" film like Brakhage's Dog Star
Man could be more important than an immortal classic like Renoir's  like Renoir's do not contain one single insight into the nature of thehuman condition that has not already been absorbed by the collective consciousness. Bob Dylan: "How many times must a man lookup before he can see the sky? How many ears must one man havebefore he can hear people cry?" And my own question: how manytimes must we acknowledge the human condition before it becomesredundant? How long must we tolerate the same facts of life beforewe begin seeking new facts? We intuit that the human condition hasexpanded since yesterday, but the popular arts aren't telling us. Thehuman condition does not stop with what we know about ourselves.Each genuinely new experience expands the definition of the humancondition that much more. Some are seeking those new facts, thosenew experiences, through the synaesthetic research of expandedcinema.Barbara Rose: "The new art... posits an entirely new world viewwhich shifts cultural values from a death-oriented, commemorative,past-enshrining culture to a life-oriented, present-oriented civilization... In this sense [Claes] Oldenburg's monuments represent, as hecontended, not the appearance of something, but its disappearance... the tomb, the memorial, the shrine, the monument, all belongto cultures that commemorate."34 John McHale: "The problem now is that those areas of our formaleducation which deal with the symbolic and value content of ourculture do so almost entirely in terms of the past35 ... The neweducational technologies are largely being used as twentieth-centurychannels to convey a conceptual context which is still nineteenthcentury or earlier. The most recent example was mathematics,where the Sputnik-inspired 'second look' revealed that mathematicsas generally taught was quite out of date. Science has begun to takea second look at its contents as currently taught. But the arts andhumanities remain relatively unaware of any need to revise theconceptual framework of studies little removed from the politeeducation of eighteenth-century gentry."
The entropy of commercial entertainment is the chaos that resultsfrom its retrospective nature, forever commemorating past events,historical figures, social eras, life-styles, or the memory of the viewer,while the living present speeds farther from our grasp. Alvin Toffler:"We offer children courses in history; why not also make a course in'future' a prerequisite for every student? A course in which thepossibilities and probabilities of the future are systematically explored exactly as we now explore the social system of the Romansor the rise of the feudal manor?
Our discussion obviously has excluded many important works of artthat function completely within the genres of drama, plot, and story.
Citizen Kane, L'Avventura, Pierrot le Fou, and 8½ are dramatic, plotfilms, yet no one denies their greatness. We know also that most ofthe truly significant films such as Beauty and the Beast or Pather
Panchali operate entirely within parameters of the human conditionas generally recognized. Moreover, common sense tells us that theartist must work with what exists, with the given, the humancondition; he could produce no art at all if he relied exclusively oninformation that is totally new.Yet the undeniable aesthetic value of these works does notcontradict what I have said about art and entertainment. These filmstranscend their genres. They are not important for their plots orstories but rather for their design. Susan Sontag: "If there is any'knowledge' to be gained through art, it is the experience of the formor style of knowing the subject, rather than a knowledge of thesubject itself."38 To perceive that the artist functions as design scientist we mustfirst understand that in their broadest implications art and scienceare the same. Eddington's classic definition of science, "The earnestattempt to set in order the facts of experience," corresponds withBronowski's view of science as "The organization of knowledge insuch a way that it commands more of the hidden potential innature…all science is the search for unity in hidden likenesses."39 It'sthe same in art: to set in order the facts of experience is to reveal therelation between man and his circumambient universe with all itshidden potential.Herbert Read: "Only in so far as the artist establishes symbols forthe representation of reality can mind, as a structure of thought, takeshape. The artist establishes these symbols by becoming conscious of new aspects of reality and by representing his consciousness inplastic or poetic form... it follows that any extension ofawareness ofreality, any groping beyond the threshold of present knowledge,must first establish its sensuous imagery." 40Our word "design" is composed of "de" and "sign," indicating that itmeans "to remove the symbol of." In this context "symbol" signifiesideas distinct from experiences. As design scientist the artistdiscovers and perfects language that corresponds more directly toexperience; he develops hardware that embodies its own softwareas a conceptual tool for coping with reality. He separates the imagefrom its official symbolic meaning and reveals its hidden potential, itsprocess, its actual reality, the experience of the thing. (A. N.Whitehead: "Process and existence pre-suppose each other.") Heestablishes certain parameters that define a discrete "special case"phenomenon, principle, or concept known as the subject. The work,in effect, poses this "problem" of perception and we as viewers mustdraw from this special case all the "general case" metaphysicalrelationships that are encoded within the language of the piece.This language is the experiential information of aestheticconceptual design; it is addressed to what Wittgenstein termed the"inarticulate conscious," the domain between the subconscious andthe conscious that can't be expressed in words but of which weconstantly are aware. The artist does not point out new facts somuch as he creates a new language of conceptual design information with which we arrive at a new and more completeunderstanding of old facts, thus expanding our control over theinterior and exterior environments.The auteur theory of personal cinema indicates those instanceswhen the filmmaker's design science transcends the parameters ofhis genre; our comprehension of that genre, that human condition isthus expanded. But cybernetics has demonstrated that the structureof a system is an index of the performance which may be expectedfrom it.41 That is, the conceptual design of a movie determines thevariety and amount of information we're likely to obtain from it. Andsince we've seen that the amount of information is directly proportional to the degree of available choices we can seethat drama, story, and plot, which restrict choice, also restrict information. So the auteur is limited to developing new designs for oldinformation, which we all know can be immensely enjoyable andinstructive. There are no "new" ideas in L'Avventura, for example,but Antonioni voiced the inarticulate conscious of an entiregeneration through the conceptual and structural integrity of histranscendental design science, merging sense and symbol, form andcontent.Rudolph Arnheim: "Perceiving achieves at the sensory level whatin the realm of reasoning is known as understanding... eyesight isinsight."42 If we realize that insight means to see intuitively, weacknowledge that Arnheim's assertion is true only when ordinaryvision—conditioned and enculturated by the most vulgar of environments—is liberated through aesthetic conceptual design information. Film is a way of seeing. We see through the filmmaker's eyes.If he's an artist we become artists along with him. If he's not,information tends toward misinformation.The artist's intuitive sense of proportion corresponds to thephenomenon of absolute pitch in musicians and answers a fundamental need in comprehending what we apprehend. In the finalanalysis our aptitudes and our psychological balance are a result ofour relation to images. The image precedes the idea in the development of consciousness: an infant doesn't think "green" when itlooks at a blade of grass. It follows that the more "beautiful" theimage the more beautiful our consciousness.The design of commercial entertainment is neither a science noran art; it answers only to the common taste, the accepted vision, forfear of disturbing the viewer's reaction to the formula. The viewer'staste is conditioned by a profit-motivated architecture, which hasforgotten that a house is a machine to live in, a service environment.He leaves the theatre after three hours of redundancy and returnshome to a symbol, not a natural environment in which beauty andfunctionality are one. Little wonder that praise is heaped on filmswhose imagery is on the level of calendar art. Global man stands onthe moon casually regarding the entire spaceship earth in a glance, yet humanity still is impressed that a rich Hollywood studio can lugits Panavision cameras over the Alps and come back with prettypictures. "Surpassing visual majesty!" gasp the critics over A Man
and a Woman or Dr. Zhivago. But with today's technology andunlimited wealth who couldn't compile a picturesque movie? In factit's a disgrace when a film is not of surpassing visual majesty because there's a lot of that in our world. The new cinema, however,takes us to another world entirely. John Cage: "Where beauty endsis where the artist begins."
"The final poem will be the poem of fact in the language of fact. But it will be the poem
of fact not realized before." WALLACE STEVENSExpanded cinema has been expanding for a long time. Since it leftthe underground and became a popular avant-garde form in the late1950's the new cinema primarily has been an exercise in technique,the gradual development of a truly cinematic language with which toexpand further man's communicative powers and thus his awareness. If expanded cinema has had anything to say, the message hasbeen the medium.1 Slavko Vorkapich: "Most of the films made so farare examples not of creative use of motion-picture devices andtechniques, but of their use as recording instruments only. There areextremely few motion pictures that may be cited as instances ofcreative use of the medium, and from these only fragments and shortpassages may be compared to the best achievements in the otherarts."2 It has taken more than seventy years for global man to come toterms with the cinematic medium, to liberate it from theatre andliterature. We had to wait until our consciousness caught up with ourtechnology. But although the new cinema is the first and only truecinematic language, it still is used as a recording instrument. Therecorded subject, however, is not the objective external human condition but the filmmaker's consciousness, his perception and its process. If we've tolerated a certain absence of discipline, it has been infavor of a freedom through which new language hopefully would bedeveloped. With a fusion of aesthetic sensibilities and technologicalinnovation that language finally has been achieved. The new cinemahas emerged as the only aesthetic language to match theenvironment in which we live.Emerging with it is a major paradigm: a conception of the nature ofcinema so encompassing and persuasive that it promises todominate all image-making in much the same way as the theory ofgeneral relativity dominates all physics today. I call it synaesthetic
cinema. In relation to traditional cinema it's like the science of bionicsin relation to previous notions of biology and chemistry: that is, itmodels itself after the patterns of nature rather than attempting to"explain" or conform nature in terms of its own structure. The newartist, like the new scientist, does not "wrest order our of chaos."Both realize that supreme order lies in nature and traditionally wehave only made chaos out of it. The new artist and the new scientistrecognize that chaos is order on another level, and they set about tofind the rules of structuring by which nature has achieved it. That'swhy the scientist has abandoned absolutes and the filmmaker hasabandoned montage.Herbert Read: "Art never has been an attempt to grasp reality as awhole—that is beyond our human capacity; it was never even anattempt to represent the totality of appearances; but rather it hasbeen the piecemeal recognition and patient fixation of what is significant in human experience."3 We're beginning to understand that"what is significant in human experience” for contemporary man isthe awareness of consciousness, the recognition of the process ofperception. (I define perception both as "sensation" and "conceptualization," the process of forming concepts, usually classified as"cognition." Because we're enculturated, to perceive is to interpret.)Through synaesthetic cinema man attempts to express a total phenomenon—his own consciousness.
Synaesthetic cinema is the only aesthetic language suited to thepost-industrial, post-literate, man-made environment with its multidimensional simulsensory network of information sources. It's theonly aesthetic tool that even approaches the reality continuum ofconscious existence in the nonuniform, nonlinear, nonconnectedelectronic atmosphere of the Paleocybernetic Age. "As visual spaceis superseded," McLuhan observes, "we discover that there is nocontinuity or connectedness, let alone depth and perspective, in anyof the other senses. The modern artist—in music, in painting, inpoetry—has been patiently expounding this fact for decades."5 Themodern synaesthetic filmmaker has been patiently expounding thisfact for decades as well, and with far more success than painters orpoets.Finally, I propose to show that synaesthetic cinema transcends therestrictions of drama, story, and plot and therefore cannot be called agenre. In addition to matching McLuhan's view of contemporaryexistence, it also corresponds to Buckminster Fuller's observationson natural synergetics and consequently is negentropic. Beforediscussing specifics, however, we must first understand why synaesthetic cinema is just now being developed into a universallanguage, more than seventy years after the birth of the medium.Like most everything else, it's because of television.
Just as every fact is also metaphysical, every piece of hardwareimplies software: information about its existence. Television is thesoftware of the earth. Television is invisible. It's not an object. It's nota piece of furniture. The television set is irrelevant to the phenomenon of television. The videosphere is the noosphere transformed into a perceivable state. "Television," says video artist LesLevine, "is the most obvious realization of software in the generalenvironment. It shows the human race itself as a working model ofitself. It renders the social and psychological condition of the environment visible to the environment."A culture is dead when its myths have been exposed. Television isexposing the myths of the republic. Television reveals the observed,the observer, the process of observing. There can be no secrets inthe Paleocybernetic Age. On the macrostructural level all televisionis a closed circuit that constantly turns us back upon ourselves.Humanity extends its video Third Eye to the moon and feeds its ownimage back into its monitors. "Monitor" is the electronic manifestationof superego. Television is the earth's superego. We become awareof our individual behavior by observing the collective behavior asmanifested in the global videosphere. We identify with persons innews events as once we identified with actors or events in fictionfilms. Before television we saw little of the human condition. Now wesee and hear it daily. The world's not a stage, it's a TV documentary.Television extends global man throughout the ecological biospheretwenty-four hours a day. By moving into outer space, televisionreveals new dimensions of inner space, new aspects of man'sperception and the results of that perception.This implosive, self-revealing, consciousness-expanding process isirreversible. Global information is the natural enemy of local government, for it reveals the true context in which that government is operating. Global television is directly responsible for the politicalturmoil that is increasing around the world today. The politicalestablishments sense this and are beginning to react. But it's toolate. Television makes it impossible for governments to maintain theillusion of sovereignty and separatism which is essential for theirexistence. Television is one of the most revolutionary tools in theentire spectrum of technoanarchy.We recognize television's negative effect on the popular arts: that itinduces a kind of sedentary uniformity of expression and generates afalse sense of creativity. In its broader consequences, however,television releases cinema from the umbilical of theatre andliterature. It renders cinema obsolete as communicator of the objective human condition. It has affected cinema in much the same wayas the invention of photography affected sculpture and painting.Cubism and other means of abstracting the realistic image were bornwith the photographic plate because painting no longer provided themost realistic images. The plastic arts abandoned exterior reality forinterior reality. The same has happened to cinema as a result oftelevision: movies no longer provide the most realistic images sothey've turned inward.We're in direct contact with the human condition; there's no longerany need to represent it through art. Not only does this releasecinema; it virtually forces cinema to move beyond the objectivehuman condition into newer extra-objective territory. There aremanifold trends that indicate that virtually all cinema has felt theprofound impact of television and is moving inevitably towardsynaesthesis. The progression naturally includes intermediary stepsfirst toward greater "realism," then cinéma-vérité, before the final andtotal abandon of the notion of reality itself. The fact that we're nowapproaching the peak of the realism stage is demonstrated byWarhol, for example, whose recent work contrasts "reality" with"realism" as manifested in the spontaneous behavior of actors pretending to be acting. In addition there's virtually all of Godard's work,as well as John Cassavetes' Faces, James McBride's David
Holzman's Diary, Peter Watkins' The War Game, Gillo Pontecorvo's
The Battle of Algiers, Paul Morrissey's Flesh, and Stanton Kaye's
Georg and Brandy in the Wilderness.
Most of this work is characterized by an astute blending of scriptedand directed acting with spontaneous improvisation, in which theactor randomly fills in the parameters of a characterization predetermined and predestined by the director. Yet precisely becausethey attempt to approximate objective reality without actually beingreal, places them firmly in the tradition of conventional Hollywoodpretend movies, with the exception of camera presence or whatmight be called process-level perception.It's only natural that contemporary filmmakers should be moresuccessful at imitating reality since the intermedia network makes usmore familiar with it. But there's a curious and quite significantaspect to the nature of this new realism: by incorporating a kind ofbastardized cinéma-vérité or newsreel style of photography andbehavior, the filmmaker has not moved closer to actual unstylizedreality itself but rather a reality prestylized to approximate ourprimary mode of knowing natural events: television. We accept it asbeing more realistic because it more closely resembles the processlevel perception of TV watching, in which unstylized reality is filteredand shaped through the process of a given medium.The traditional dramatic structure of these films becomes moreeasily discernible in contrast with pure cinéma-vérité work such asJean Rouch's Chronicle of a Summer, Pennebaker's Don't Look
Back, or Chris Marker's brilliant Le Joli Mai. A comparison of Facesor David Holzman's Diary with Warhol's Nude Restaurant is evenmore revealing: the difference between prestylized and predestinedrealities on the one hand, and Warhol's totally random and onlypartially prestylized reality on the other, is brought into sharp focus.Warhol has expressed regret that a camera cannot simply beswitched on and left running for twenty-four hours, since the "important" (naturally-revealing) events seem to occur at that momentjust after it stops turning. Godard disclosed similar sentiments whenhe said: "The ideal for me is to obtain right away what will work. Ifretakes are necessary it falls short of the mark. The immediate ischance. At the same time it is definitive. What I want is the definitiveby chance."
Simultaneous Perception of Harmonic Opposites Time, said St. Augustine, is a threefold present: the present as weexperience it; the past as present memory; the future as presentexpectation. Hopi Indians, who thought of themselves as caretakersof the planet, used only the present tense in their language: past wasindicated as "present manifested," and the future was signified by"present manifesting.”6 Until approximately 800 B.C., few culturesthought in terms of past or future: all experience was synthesized inthe present. It seems that practically everyone but contemporaryman has intuitively understood the space-time continuum.Synaesthetic cinema is a space-time continuum. It is neither subjective, objective, nor nonobjective, but rather all of these combined:that is to say, extra-objective. Synaesthetic and psychedelic meanapproximately the same thing. Synaesthesis is the harmony ofdifferent or opposing impulses produced by a work of art. It meansthe simultaneous perception of harmonic opposites. Its sensorialeffect is known as synaesthesia, and it's as old as the ancientGreeks who coined the term. Under the influence of mindmanifesting hallucinogens one experiences synaesthesia in additionto what Dr. John Lilly calls "white noise," or random signals in thecontrol mechanism of the human bio-computer.7 Any dualism is composed of harmonic opposites: in/out, up/ down,off/on, yes/no, black/white, good/bad. Past aesthetic traditions,reflecting the consciousness of their period, have tended toconcentrate on one element at a time. But the Paleocyberneticexperience doesn't support that kind of logic. The emphasis of traditional logic might be expressed in terms of an either/or choice, whichin physics is known as bistable logic. But the logic of the CyberneticAge into which we're moving will be both/and, which in physics is called triadic logic. Physicists have found they can no longer describe phenomena with the binary yes/no formula but must operatewith yes/no/maybe.The accumulation of facts is no longer of top priority to humanity.The problem now is to apply existing facts to new conceptual wholes,new vistas of reality. By "reality" we mean relationships. PietMondrian: "As nature becomes more abstract, a relation is moreclearly felt. The new painting has clearly shown this. And that is whyit has come to the point of expressing nothing but relations."8 Synaesthetic cinema is an art of relations: the relations of the conceptual information and design information within the film itselfgraphically, and the relation between the film and the viewer at thatpoint where human perception (sensation and conceptualization)brings them together. As science gropes for new models to accommodate apparent inconsistencies and contradictions, the need forseeing incompatibles together is more easily discerned. For example, the phenomenon of light is conceived in both/and terms: bothcontinuous wave motions and discontinuous particles. And we havenoted our incapacity for observing both movement and position ofelectrons.This is but one of many reasons that synaestheticcinema is theonly aesthetic language suited to contemporary life. It can function asa conditioning force to unite us with the living present, not separateus from it. My use of the term synaesthetic is meant only as a way ofunderstanding the historical significance of a phenomenon withouthistorical precedent. Actually the most descriptive term for the newcinema is "personal" because it's only an extension of the filmmaker's central nervous system. The reader should not interpret"synaesthetic" as an attempt to categorize or label a phenomenonthat has no definition. There's no single film that could be calledtypical of the new cinema because it is defined anew by each individual filmmaker.I've selected about seven films that are particularly representativeof the various points I wish to make. I'm using them only to illuminatethe nature of synaesthetic cinema in general, not as specific archetypal examples. Sufficient literature exists on Brakhage's Dog Star
Man to preclude any major expository analysis here, but it is exemplary of virtually all concepts involved in the synaesthetic mode, inparticular syncretism and metamorphosis. Will Hindle's Chinese
Firedrill is an outstanding example of the evocative language ofsynaesthetic cinema as distinct from the expositional mode ofnarrative cinema. Pat O'Neill's 7362, John Schofill's XFilm, andRonald Nameth's Exploding Plastic Inevitable provide some insightinto kinaesthetics and kinetic empathy. Carolee Schneemann's
Fuses, in contrast with Warhol's Blue Movie and Paul Morrissey's
Flesh, illustrates the new polymorphous eroticism. And, finally,Michael Snow's Wavelength has been chosen for its qualities ofextra-objective constructivism.
Montage as Collage The harmonic opposites of synaesthetic cinema are apprehendedthrough syncretistic vision, which Anton Ehrenzweig has characterized as: "The child's capacity to comprehend a total structure ratherthan analyzing single elements... he does not differentiate the identityof a shape by watching its details one by one, but goes straight forthe whole."9 Syncretism is the combination of many different formsinto one whole form. Persian tapestries and tile domes aresyncretistic. Mandalas are syncretistic. Nature is syncre-tistic. Themajority of filmgoers, conditioned by a lifetime of conven-tionalnarrative cinema, make little sense of synaesthetic cinema becausetheir natural syncretistic faculty has suffered entropy and atrophy.Buckminster Fuller: "All universities have been progressivelyorganized for ever-finer specialization. Society assumes that specialization is natural, inevitable and desirable. Yet in observing a littlechild we find it is interested in everything and spontaneously apprehends, comprehends and coordinates an ever-expanding inventory of experience.''10 It has been demonstrated that all species of life on earth that havebecome extinct were doomed through overspecialization, whetheranatomical, biological, or geological. Therefore conventional narrative cinema, in which the filmmaker plays policeman guiding oureyes here and there in the picture plane, might be described as"specialized vision," which tends to decay our ability to comprehendthe more complex and diffuse visual field of living reality.The general impression that syncretism, and therefore synaestheticcinema, is empty of detail or content is an illusion: "… it is highlysensitive to the smallest of cues and proves more efficient in identifying individual objects. It impresses us as empty, vague and generalized only because the narrowly-focused surface consciousnesscannot grasp its wider more comprehensive structure. Its precise,concrete content has become inaccessible and ‘unconscious.’11 ''Synaesthetic cinema provides access to syncretistic contentthrough the inarticulate conscious. Similarly, it contradicts theteachings of Gestalt psychology, according to which we must makean either/or choice: we can choose either to see the "significant"figure or the "insignificant" ground. But when the "content" of themessage is the relationship between its parts, and when structureand content are synonymous, all elements are equally significant.Ehrenzweig has suggested that syncretism is "Gestalt-free perception," and indeed this must be the case if one expects any visual"meaning" from synaesthetic cinema.Paul Klee, whose syncretistic paintings closely resemble certainworks of synaesthetic cinema, spoke of the endotopic (inside) and
exotopic (outside) areas of a picture plane, stressing their equalimportance in the overall experience.12 Synaesthetic cinema, primarily through superimposition, fuses the endotopic and exotopic byreducing depth-of-field to a total field of nonfocused multiplicity.Moreover, it subsumes the conventional sense of time by interconnecting and interpenetrating the temporal dimension with images thatexist outside of time. The "action" of Dog Star Man, for example,could be an entire life-span or merely a split second in the inarticulate conscious of Stan Brakhage. I stress "action" as commonlyunderstood in the cinema because synaesthetic syncretism replacesmontage with collage and, as André Bazin has observed, "montageis the dramatic analysis of action." Bazin was perceptive enough torealize that "only an increased realism of the image can support theabstraction of montage.''13 Synaesthetic cinema subsumes Eisenstein's theory of montage-ascollision and Pudovkin's view of montage-as-linkage. It demonstratesthat they were certainly correct but didn't follow their own observations to their logical conclusions. They were restricted by the consciousness of their times. Synaesthetic cinema transcends the notionof reality. It doesn't "chop the world into little fragments," an effectBazin attributed to montage, because it's not concerned with theobjective world in the first place. The new filmmaker is showing ushis feelings. Montage is indeed an abstraction of objective reality;that's why, until recently, Warhol did not cut his films at all. Butsynaesthetic syncretism is the only mode in which the manifestationsof one's consciousness can be approximated without distortion.There's no conflict in harmonic opposites. Nor is there anything thatmight be called linkage. There is only a space-time continuum, amosaic simultaneity. Although composed of discrete elements it isconceived and edited as one continuous perceptual experience. Asynaesthetic film is, in effect, one image continually transforming intoother images: metamorphosis. It is the one unifying force in all ofsynaesthetic cinema. The notion of universal unity and cosmicsimultaneity is a logical result of the psychological effects of theglobal communications network.If montage is the dramatic analysis of action, a film without classicmontage thus avoids at least the structural element of dramainherent within the medium. All that remains to avoid drama entirelyis to exclude dramatic (i.e., theatrical) content by making content andstructure the same. Warhol's films are not dramatic, and neither arefilms at the extreme opposite end of the spectrum, synaesthesia. Theclassical tension of montage is dissolved through overlappingsuperimposition. For example: we have shots A, B. and C. First wesee A, then B is superimposed over it to produce AB. Then A fadesas C fades in. There's a brief transitional period in which we'reseeing ABC simultaneously, and finally we're looking only at BC. Butno sooner has this evolved than B begins to fade as D appears, andso on.This is a physical, structural equivalent of the Hopi "presentmanifested" and "present manifesting" space-time continuum. It's theonly style of cinema that directly corresponds to the theory of generalrelativity, a concept that has completely transformed all aspects ofcontemporary existence except traditional Hollywood cinema. Theeffects of metamorphosis described above become more apparent ifshots A, B. and C happen to be of the same image but from slightly different perspectives, or with varied inflections of tone and color. Itis through this process that a synaesthetic film becomes, in effect,one image constantly manifesting.And finally we're forced to admit that the pure art of cinema existsalmost exclusively in the use of superimposition. In traditionalcinema, superimposition usually gives the impression of two moviesoccurring at once in the same frame with their attendant psychological and physiological connotations coexisting separately. In synaesthetic cinema they are one total image in metamorphosis. This doesnot imply that we must relinquish what Eisenstein called "intellectualmontage." In fact, the conflict-juxtaposition of intellectual effects isincreased when they occur within the same image. Fiction, legend,parable, myth, traditionally have been employed to make comprehensible the paradoxes of that field of nonfocused multiplicity that islife. Synaesthetic cinema, whose very structure is paradox, makesparadox a language in itself, discovering the order (legend) hiddenwithin it.
Stan Brakhage: Dog Star Man
Dog Star Man is a silent, seventy-eight-minute film divided into
Prelude and Parts One through Four. It was shot in 1959-60 andedited during the next four years. Prelude is an extremely fastcollage of multiple-level superimpositions and compounded imagesthat emerge from a blurry diaphanous haze and slowly take form,only to be obscured by other images and countermotions. We beginto discern specific objects, patterns, and finally a motif or theme: theelements of Earth, Air, Fire, and Water; a childbirth; a man climbinga mountain with his dog; the moon; the sun throwing off huge solarprominences; lovemaking; photomicrography of blood vessels; abeating heart; a forest; clouds; the faces of a man and a woman; andliterally thousands of other images to appear in the rest of the film.These images exist essentially autonomously and are superimposed or compounded not for "dramatic" effect but rather as a kind ofmatrix for psychic exercise on the part of the viewers. For example,over an expanding solar prominence we might see Brakhage'sleonine face or a line of snow-covered fir trees in the mountains ofColorado. We are not asked to interpret or find "meaning" in these combinations, though vastly rich experiences arepossible. When theimages emerge from a hazy blur, for example, we are not asked tointerpret this as the creation of life or some similar dramatic notion,but rather as a perceptual experience for its own sake, in addition tothe contextual relationship of this image to the rest of the film, orwhat Eisenstein indicated by the term "intellectual montage."Whereas Prelude is a rapid barrage of multiple overlays, Part Oneis superimposed sparingly, concentrating on interface relationshipsbetween individual shots. However, every effort is made to subdueany effect that might be considered montage. The shots fade in andout very slowly, often fading into a color such as red or green. Thefragments of Prelude fall into place and an overwhelming sense ofoceanic consciousness evolves. We begin to realize that Brakhageis attempting to express the totality of consciousness, the realitycontinuum of the living present. As his solitary figure climbs thesnow-covered mountain, we see images of man's world from themicrospectrum of the bloodstream to the macrospectrum of the sun,moon, and universe. Both time and space are subsumed in thewholeness of the experience. Superimposition is not used as aneconomical substitute for "parallel montage"—indicating simultaneous but spatially separate events—for spatio-temporal dimensionsdo not exist in the consciousness. Brakhage is merely presenting uswith images orchestrated in such a way that a new reality arises outof them.When we see the sun superimposed over a lovemaking scene, it'snot an invitation to interpret a meaning such as cosmic regenerationor the smallness of man in the universe, but rather as an occasion toexperience our own involuntary and inarticulate associations. Theimages are not symbolic, as in The Seventh Seal, or artfully composed as in Last Year at Marienbad. Brakhage does not manipulateus emotionally, saying: "Now I want you to feel suspense" or "Now Iwant you to laugh" or "Now is the time to be fearful." This is the ployof the commercial entertainer: an arrogant degradation of cinema,using film as a tool for cheap sensationalism. This is not to say thatspatio-temporal experiences, or suspense, humor, or any emotioncannot be found in synaesthetic cinema. Quite the contrary: becausewe're dealing with our own personal associations, emotion is guaranteed. And it will be more genuinely profound than the formulatriggered gratification of conditioned response that we receive fromcommercial entertainment.Brakhage has spoken of "restructuring" vision through his films,and often refers to the "untutored" vision of the child before he'staught to think and see in symbols. In what he calls "closed-eyevision," Brakhage attempts to simulate, by painting and scratching onfilm, the flashes and patterns of color we perceive when our eyes areclosed. Approximately midway through Dog Star Man, otherwisemundane images take on wholly new meanings and in some casesnew appearances. We stop mentally labeling images and concentrate instead on the synaesthetic/kinaesthetic flow of color,shape, and motion.This is not to suggest a nonobjective experience. The imagesdevelop their own syntactical meaning and a "narrative" line isperceived, though the meaning of any given image may change inthe context of different sequences. This constitutes a creative use ofthe language itself, over and above any particular "content" conveyed by that language. (Wallace Stevens: "A new meaning isequivalent to a new word.") The effect of synaesthetic cinema is tobreak the hold that the medium has over us, to make us perceive itobjectively. Art is utter folly unless it frees us from the need of art asan experience separate from the ordinary.Wittgenstein has described art as a game whose rules are madeup as the game is in process. The exact meaning of words (images)becomes known only in the context of each new statement.14 E. H.Gombrich, on the other hand, demonstrates that objective realismalso is a game, but one whose schema is established prior to its useand is never altered. Artists and society thus learn to read theschema as though it were objective reality. But since the languageitself is not used creatively, the viewer is seduced beyond form intoan abstract content with an illusion of being externally objective.15 Thus the viewer is captive under the hold, or spell, of the mediumand is not free to analyze the process of experience.
14 Ludwig Wittgenstein, Philosophical Investigations (Oxford: Blackwell Press, 1963).
Brakhage expressed this concept with respect to his own work:"Imagine an eye unruled by man-made laws of perspective, an eyeunprejudiced by compositional logic, an eye which must know eachobject encountered in life through a new adventure of perception.Imagine a world alive with incomprehensible objects and shimmeringwith an endless variety of movement and gradations of color.Imagine a world before the beginning was the word.''16
16 Stan Brakhage, "Metaphors on Vision," ed. P. Adams Sitney, Film Culture (Fall, 1963).
Evocation and Exposition:
Toward Oceanic Consciousness There is an important distinction to be made betweenevocation, thelanguage of synaesthetic cinema, primarily poetic in structure andeffect, and exposition, the language of narrative cinema, whichchiefly conforms to traditional, literary narrative modes. Intermediaartist and filmmaker Carolee Schneemann has characterized evocation as the place between desire and experience, the interpenetrations and displacements which occur between various sense stimuli."Vision is not a fact," Miss Schneemann postulates, "but anaggregate of sensations. Vision creates its own efforts towardrealization; effort does not create vision.”17 Thus, by creating a new kind of vision, synaesthetic cinema createsa new kind of consciousness: oceanic consciousness. Freud spokeof oceanic consciousness as that in which we feel our individualexistence lost in mystic union with the universe. Nothing could bemore appropriate to contemporary experience, when for the first timeman has left the boundaries of this globe. The oceanic effect ofsynaesthetic cinema is similar to the mystical allure of the naturalelements: we stare in mindless wonder at the ocean or a lake orriver. We are drawn almost hypnotically to fire, gazing as thoughspellbound. We see cathedrals in clouds, not thinking anything inparticular but feeling somehow secure and content. It is similar to theconcept of no-mindedness in Zen, which also is the state of mantraand mandala consciousness, the widest range of consciousness.Miss Schneemann defines perception as eye-journey or empathydrawing. It is precisely through a kind of empathy-drawing that thecontent of synaesthetic cinema is created jointly by the film and theviewer. The very nature of evocation requires creative effort on thepart of the viewer, whereas expository modes do all the work and theviewer becomes passive. In expositional narrative, a story is being figure of Stan Brakhage in Dog Star Man actually moves through apsychic environment created by the viewer, whose deeply-hiddencreative resources and hungers have been evoked by the film.With typical poetic eloquence, Hermann Hesse has summarizedthe evocative effects of oceanic consciousness in this memorablepassage from Demian: "The surrender to nature's irrational, strangelyconfused formations produces in us a feeling of inner harmony withthe force responsible for these phenomena... the boundaries separating us from nature begin to quiver and dissolve... we are unable todecide whether the images on our retina are the result ofimpressions coming from without or from within... we discover towhat extent we are creative, to what extent oursoul partakes of theconstant creation of the world.'
Will Hindle: Chinese FiredrillThere have been essentially three generations of personal filmmakers in the United States. The first began with the invention of themedium and continued in various stages through the 1940's. Thesecond began approximately in the mid-1950's with the increasingavailability of inexpensive 8mm. and 16mm. equipment. It represented the first popular movement toward personal cinema as a wayof life. The third generation has evolved since the mid-1960's, primarily in the San Francisco area, where the latest trend is toward ablending of aesthetics and technology. One reason personal cinemais more eloquent than commercial cinema is that the filmmaker isforced into a closer interaction with his technology.Will Hindle is exemplary of this recent technological awareness, acombination of engineering and aesthetics. Trained in art, literature,and professional television filmmaking, Hindle has applied hisknowledge to personal cinema in a singularly spectacular fashion.His ability to invest a technical device with emotional or metaphysical content is truly impressive. He has, for example, developedthe technique of rear-projection rephotography to a high degree ofeloquence. He shoots original scenes with wide-angle lenses, then"crops" them by projecting and rephotographing this footage using aspecial single-frame projector. Thus extremely subtle effects are achieved that would be prohibitively expensive, if not impossible, ifdone through conventional laboratory optical printing.Although many synaesthetic films are wonderfully evocative,Hindle's recent works are especially notable for their ability to generate overwhelming emotional impact almost exclusively from cine matic technique, not thematic content. Hindle has an uncanny talentfor transforming spontaneous unstylized reality into unearthly poeticvisions, as in Billabong (1968), a wordless impressionistic "documentary" about a boy's camp in northern California, and Watersmith (1969), a spectacular visual fantasy created from footage of anOlympic swimming team at practice.
Chinese Firedrill, unique in Hindle's work, was prestylized and"performed" almost in the traditional sense of a scripted, directed,and acted movie. The difference is that Hindle used the images notfor their symbolic or theatrical content but as ingredients of an almosticonographic nature, to be compounded and manipulated through theprocess of the medium. Although there are "actors" (Hindle plays theprincipal role), there is no characterization. Although there are sets,we're not asked to suspend our disbelief.
Chinese Firedrill is a romantic, nostalgic film. Yet its nostalgia is ofthe unknown, of vague emotions, haunted dreams, unspoken words,silences between sounds. It's a nostalgia for the oceanic presentrather than a remembered past. It is total fantasy; yet like the bestfantasies—8½, Beauty and the Beast, The Children of Paradise— itseems more real than the coldest documentary. The "action" occursentirely within the mind of the protagonist, who never leaves thesmall room in which he lives. It's all rooms everywhere, all cubicleswherever we find man trapped within his dreams. Through thedoor/mirror is the beyond, the unreachable, the unattainable, thebeginning and the end. Not once in the film's twenty minutes can wepinpoint a sequence or action that might be called "dramatic" in theusual sense. Yet almost immediately an overwhelming atmosphereof pathos is generated. There are moments of excruciating emotionalimpact, not from audience manipulation but from Hindle's ability torealize metaphysical substance, stirring the inarticulate conscious.Every effort is made to distance the viewer, to keep us aware of ourperceptions, to emphasize the purely cinematic as opposed to thetheatrical.
We find Hindle kneeling on the floor of his surrealistic room stuffingthousands of IBM cards into boxes. Over this we hear a strangemonologue of fragmented words and sentences in an odd foreignaccent. This is punctuated by fierce thunderclaps and howling windthat evolve into ethereal music and tinkling bell sounds. Periodicallythe screen is slashed across with blinding white flashes while thecentral images constantly are transformed through lap-dissolves andmultiple superimpositions. There are flash-forwards of images to beencountered later, though we don't recognize them and thereforedon't interpret them. We see nude lovers, a small boy bathing, abeautiful woman with candles, a huge eyeball, a battery of glaringlights. These are noted for their inherent psychological connotationsand not as narrative devices.The most memorable sequence of Firedrill, possibly one of thegreat scenes in the history of film, involves Hindle lying in anguish onhis floor and slowly reaching out with one hand toward theglimmering void beyond his door. Suddenly a mirror-like reflection ofhis arm and hand appears on the opposite side of the mirror. Whenhe removes his hand we see the vague shadowy figure of a nudewoman silhouetted ghostlike, her skin sparkling. In slow motion thesilhouette of a nude man enters from an opposite direction and thetwo gossamer figures embrace in a weightless ballet of gracefulmotion in some dream of bliss. In the film's final image, the hauntedman has become a child once again, splashing in his bath in a seriesof freeze-frames that grow ever fainter until they vanish.
Synaesthetics and Kinaesthetics:
The Way of All Experience The term kinetic generally indicates motion of material bodies andthe forces and energies associated with it. Thus to isolate a certaintype of film as kinetic and therefore different from other films meanswe're talking more about forces and energies than about matter. Idefine aesthetic quite simply as: the manner of experiencing something. Kinaesthetic, therefore, is the manner of experiencing a thingthrough the forces and energies associated with its motion. This iscalled kinaesthesia, the experience of sensory perception. One whois keenly aware of kinetic qualities is said to possess a kinaestheticsense.The fundamental subject of synaesthetic cinema—forces andenergies—cannot be photographed. It's not what we're seeing somuch as the process and effect of seeing: that is, the phenomenon ofexperience itself, which exists only in the viewer. Synaestheticcinema abandons traditional narrative because events in reality donot move in linear fashion. It abandons common notions of "style"because there is no style in nature. It is concerned less with factsthan with metaphysics, and there is no fact that is not also metaphysical. One cannot photograph metaphysical forces. One cannoteven “represent" them. One can, however, actuallyevoke them in theinarticulate conscious of the viewer.The dynamic interaction of formal proportions in kinaestheticcinema evokes cognition in the inarticulate conscious, which I call kinetic empathy. In perceiving kinetic activity the mind's eye makesits empathy-drawing, translating the graphics into emotionalpsychological equivalents meaningful to the viewer, albeit meaningof an inarticulate nature. "Articulation" of this experience occurs inthe perception of it and is wholly nonverbal. It makes us aware offundamental realities beneath the surface of normal perception:forces and energies.

Social media content shared today in cities, such as Instagram images, their tags and descriptions, is the key form of contemporary city life. It tells people where activities and locations that interest them are and it allows them to share their urban experiences and selfrepresentations. Therefore, any analysis of urban structures and cultures needs to consider social media activity. In our paper, we introduce the novel concept of social media inequality. This concept allows us to quantitatively compare pattern in social media activities between parts of a city, a number of cities, or any other spatial areas. We define this concept using an analogy with the concept of economic inequality. Economic inequality indicates how some economic characteristics or material resources, such as income, wealth or consumption are distributed in a city, country or between countries. Accordingly, we define social media inequality as unequal spatial distribution of social media sharing in a particular geographic area or between areas. To quantify such distributions, we can use many characteristics of social media such as number of people sharing it, the number of photos they have shared, their content, and user assigned tags. We propose that the standard inequality measures used in other disciplines, such as the Gini coefficient, can also be used to characterize social media inequality. To test our ideas, we use a dataset of 7,442,454 public geo-coded Instagram images shared in Manhattan during five months (March-July) in 2014, and also selected data for 287 Census tracts in Manhattan. We compare patterns in Instagram sharing for locals and for visitors for all tracts, and also for hours in a 24 hour cycle. We also look at relations between social media inequality and socio-economic inequality using selected indicators for Census tracts. The inequality of Instagram sharing in Manhattan turns out to be bigger than inequalities in levels of income, rent, and unemployment.

Social media content shared today in cities, such as Instagram images, their tags and descriptions, is the key form of contemporary city life. It tells people where activities and locations that interest them are and it allows them to share their urban experiences and selfrepresentations. Social media also has become one of the most important representations of city life to both its residents and the outside world. One can argue that any city today is as much media content shared in that city on social networks as its infrastructure and economic activities. For these reasons, any analysis of urban structures and cultures needs to consider social media activity and content. While the industry developed many concepts and measurement tools to analyze social media, these concepts and tools were not developed with the view for the comparative urban analysis. Therefore, we need to develop our own concepts that bridge the perspectives of urban studies and design and quantitative analysis of social networks that uses computational methods and “big data.” In the last few years, one of the most frequently discussed public issues has been the rise in income inequality (Stiglitz 2012, Piketty 2014, Atkinson 2015). But inequality does not only refer to distribution of income. It is a more general concept, and it has been used for decades in a number of academic disciplines besides economics, such as urban planning, sociology, education, engineering, and ecology. The quantitative measurements of inequality allow researchers to characterize a set of numbers or compare multiple sets, regardless of what the data represents. In addition to income inequality, we can measure inequality in wealth, education levels, social well-being, and numerous other social characteristics. In our paper, we introduce the novel concept of social media inequality. We define this concept using an analogy with the concept of economic inequality. Economic inequality refers to how some economic characteristics or material resources, such as income, wealth or consumption are distributed in a city, country or between countries (Ray 1998, Milanovic 2007, OECD 2011). Accordingly, we can define social media inequality as the measures of distribution of characteristics of social media content shared in a particular geographic area or between areas. An example of such characteristics is the number of photos shared by all users of a social network such as Instagram in a given city or city area. Another example is the number of hashtags – how many hashtags users added to the photos, and how many of these hashtags are unique. Other examples include average number of tweets shared by a user in a particular period; numbers of tweets shared per month, per week or per hour of a day; the proportions of tweets that were retweeted, and so on. Of course, we can computer and analyze features of content itself - for example, how many different subjects appear the photos, and what are their proportions. In fact, any metric of social media can be used to compare inequality in social 5 media activity between areas - for example, number of likes, length of text messages, most frequent and least frequent words, number of unique topics, number of distinct photographic styles, image compositions, styles of video editing, and so on. We propose that the standard inequality measures used in other disciplines, such as the Gini coefficient, can also be used to characterize social media inequality. We can also compare these measures between content shared on various social networks (Instagram, Twitter, etc.) in the same area or areas. We can do these comparisons for social networks where the main content is text (e.g., Twitter, VK), images (e.g., Instagram, Tumblr), video (e.g., YouTube), or combination of different media (e.g. Facebook, QZone, Sina Weibo, Line, etc.). Finally, we can also compare characteristics of shared content with various social and economic characteristics in the same areas, such as income, rent, the level of education, or ethnic mix. The paper tests some of these ideas using a large dataset of Instagram images shared in Manhattan borough of New York City. This dataset, which we created for this study, contains 7,442,454 public geo-coded Instagram images shared in Manhattan during five months (February 1, 2014 July 31, 2014). Among these images, 1,524,046 were shared by 515,608 city visitors; the remaining 5,918,408 images were shared by 375,876 city residents. Our analysis of the images shared by two types of users in this paper is inspired by the pioneering project Locals and Tourists created by Eric Fischer (Fisher, 2010.) We used the following method to divide users into “visitors” and “locals.” If a user posted all her photos within a single 12-day period out of the total five months, we consider this person a “visitor”. If a user shared a minimum of two photos within any interval larger than 12 days, we consider this person a local. Although this very simple method is expected to produce some errors, we felt that they are acceptable given the size of our dataset. (Such method is also used a number of published papers that analyze patterns in social media activity.) Comparing the locations of images shared by visitors (figure 1) and locals (figure 2) gives us an intuition for social media inequality concept. We can immediately notice that in each case these locations are not distributed evenly. Some parts of the city have many more images than other parts. These figures also suggest that the big proportion of images by city visitors are shared only in a few areas, while the locals share images in most areas of the city. Note that we use the term “shared” rather than “captured” because Instagram allows sharing of any image from user’s phone and not only the ones captured within Instagram app. So users can upload images taken previously in other locations. However, since Instagram captured the geolocation and time when an image was shared (for users who allowed Instagram access to this data), the metadata of images in our dataset tells us about people’s presence at particular place in the city at a particular time.
While the U.S. Census collects data on individuals, it only reports the data aggregated by geographic areas at different scales. We follow a similar logic in our analysis of spatial social media inequality by dividing a city into hundreds of small areas and aggregating characteristics of social media content shared in each area - as opposed to comparing individuals to each other. The way we measure social media inequality is comparable to how Milanovic defines one of the measures of global economic inequality (Milanovic 2006, Concept 1). This measure uses countries as the units of observation. Milanovic does not directly compare the income of people worldwide. Instead he compares average income across different countries to calculate global inequality. In our case, the Census tracts are our units of observation. We aggregate social media characteristics at the tract level in order to analyze social media inequality across all of Manhattan. Social media content shared in a given area may combine contributions from different kinds of users: people who reside in this area, people who live in different parts of the city or in suburbs but spend significant time in this area for work during weekdays; international or domestic tourists visiting a city; companies located in this area, and so on. Together, the content shared by all these users create a collective “voice” of a particular area of a city. A city as a whole can be compared to an orchestra of all these voices (although, of course, they are not necessary performing the same composition.) Applying the concept of inequality to a collection of these urban voices can give us new ways of understanding a city, and provide an additional metric for comparing numerous cities around the world. Social media inequality as we define it refers to the unequal distribution of social media content and its metadata and their characteristics in any type of geographic area – a city, a region, a country, or any other type of area. However, as Fischer’s maps show visually, the density of social media contributions in larger cities is much higher than in non-urban areas, which makes these cities particularly convenient areas of study. We think that our proposed measurements of social media inequality can be useful for urbanism studies, urban planning, urban design, public administration, economics, and other professional and academic fields. While researchers in the fields of social computing, spatial analytics, and “science of cities” have published many quantitative studies analyzing urban data of many kinds (Batty 2013, Goldsmith and Crawford 2014, Townsend 2014, Pucci et al. 2015, Ratti et al. 2006), a significant portion of this analysis cannot be approached without having a degree in computer science. In contrast, social media inequality measurement is a concept that is easy to understand and also easy to calculate. The locations of social media contributions reflect the presence of people in a particular part of a city at a particular time. However, in comparison to pure location data captured by mobile phones or other body sensors, social media images are much more than simple coordinates and time stamps. The content of these contributions can also tell us what people find interesting and how they are spending their time. Therefore, mapping and measuring inequality in 9 characteristics of social media can help us understand how social, economic, and urban design characteristics of cities influence life patterns and the overall “dynamism” and “vitality” of a city. Researchers have never observed perfect equality in any natural, biological or social system or population. In using the term “social media inequality,” we are not suggesting that the goal of urban planners or city administration should be to reduce differences in social media use between various areas to a minimum, or to some optimal level. If people are sharing the same amount of social media in every area of the city, it means that this city does not have any centers or attractions that stand out, or places where many people gather. In terms of modern housing, large American-type suburbs with the same density of houses and same demographics of families and income would probably generate least amount of social media inequality. Today such suburbs are common around the world, from Mexico to China. Given the wide criticism of this classical suburb type, we can assume that some level of spatial social media inequality is desirable. In this case, inequality stands for variety and differentiation while complete equality stands for sameness and lack of variety. But is extreme social media inequality a good thing? For example, do we really want all people living in a city to spend their weekends in a single place? There are certain situations where reducing extreme spatial social media inequality would be very desirable. For example, if city authorities find that most tourists’ social media activity is concentrated in just a few areas surrounding only a few landmarks (like Times Square in New York City), they can change the way the city is promoted to visitors to diversify where tourists go, what they look at, and what they experience. Being able to quantify inequality of social media would allow for better planning and evaluation of such changes. Formulated as a type of spatial analysis, our study compares the parts of the city that attract more people and generate more content shared on social media networks and thus are “social media rich” with parts of the city that are “social media poor.” What are the relationships between such social media rich and social media poor areas? Is social media inequality larger or smaller than economic or social inequality in the same areas? Does social media inequality increase worldwide, similar to how economic inequality has been growing recently? Which parts of the world have the highest social media inequality and which are the most equal? Although our analysis is focusing on one part of a single megacity (i.e., Manhattan in New York City), it can be expanded to consider hundreds of cities around the world to consider such questions.

I present a number of core concepts from data science that are relevant to digital art history and the use of quantitative methods to study any cultural artifacts or processes in general. These concepts are objects, features, data, feature space, and dimension reduction. These concepts enable computational exploration of both large and small visual cultural data. We can analyze relations between works on a single artist, many artists, all digitized production from a whole historical period, holdings in museum collections, collection metadata, or writings about art. The same concepts allow us to study contemporary vernacular visual media using massive social media content. (In our lab, we analyzed works by van Gogh, Mondrian, and Rothko, 6000 paintings by French Impressionists, 20,000 photographs from MoMA photography collection, one million manga pages from manga books, one million artworks of contemporary non-professional artists, and over 13 million Instagram images from 16 global cities.) While data science techniques do not replace other art historical methods, they allow us to see familiar art historical material in new ways, and also to study contemporary digital visual culture. In addition to their relevance to art history and digital humanities, the concepts are also important by themselves. Anybody who wants to understand how our society “thinks with data” needs to understand these concepts. They are used in tens of thousands of quantitative studies of cultural patterns in social media carried out by computer scientists in the last few years. More generally, these concepts are behind data mining, predictive analytics and machine learning, and their numerous industry applications. In fact, they are as central to our “big data society” as other older cultural techniques we use to represent and reason about the world and each other – natural languages, material technologies for preserving and accessing information (paper, printing, digital media, etc.), counting, calculus, or lens-based photo and video imaging. In short, these concepts form the data society’s “mind” – the particular ways of encountering, understanding, and acting on the world and the humans specific to our era.

Will art history fully adapt quantitative and computational techniques as part of its methodology? While the use of computational analysis in literary studies and history has been growing slowly but systematically during 2000s and first part of 2010s, this has not yet happened in the fields that deal with the visual (art history, visual culture, film, and media studies). However, looking at the history of adoption of quantitative methods in the academy suggests that these fields sooner or later will also go through their own “quantitative turns.” Writing in 2001, Adrian Raftery points out that psychology was the first to adopt quantitative statistical methods in 1920s-1930s, followed by economics in 1930s-1940s, sociology in 1960s, and political science in 1990s.2 Now, in 2015, we also know that humanities fields dealing with texts and spatial information (i.e., already mentioned literary studies and history) are going through this process in 2000s- 2010s. So I expect that “humanities of the visual” will be the next to befriend numbers.  This adaption will not, however, simply mean figuring out what be counted, and then using classical statistical methods (developed by the 1930s and still taught today to countless undergraduate and graduate students pretty much in the same way) to analyze these numbers. Instead, it will take place in the context of a fundamental social and cultural development of the early 21st century – the rise of “big data,” and a new set of methods, conventions, and skills that came to be called “data science.” Data science includes classical statistical techniques from the 19th and early 20th century, additional techniques and concepts for data analysis that were developed starting in 1960s with the help of computers, and concepts from a number fields that also develop in the second part of the 20th century around computers: pattern recognition, information retrieval, artificial intelligence, computer science, machine learning, information visualization, data mining. Although the term "data science" is quite recent, it is quite useful as it acts as an umbrella for currently most frequently used methods of computational data analysis. (Alternatively, I could have chosen machine learning or data mining as the key term for this article, but since data science includes their methods, I decided that if I am to refer to all computational data analysis using a single term, data science is best right now.)
Data science includes many ideas developed over many decades, and hundreds of algorithms. This sounds like a lot, and it is. It is much more than can be learned in one or two graduate methods classes, or summarized in a single article, or presented in a single textbook. But rather than simply picking particular algorithms and techniques from a large arsenal of data science, or borrowing whatever technique happens to be the newest and therefore is currently in fashion (for example, “topic modeling” or “deep learning”) and trying to apply them to art history, it is more essential to fist understand the most fundamental as sumption of the field as a whole. That is, we in art history (or any other humanities field) need to learn the core concepts that underlie the use of data science in contemporary societies. These concepts do not require formulas to explain, and they can be presented in one article, which is what I will attempt here. (Once we define these core concepts, a variety of terms employed in data science today can also become less confusing for the novice.)
Surprisingly, after reading thousands of articles and various textbooks over last eight years, I have not found any short text that presents these core concepts together in one place. While many data science textbooks, of course, do talk about them, their presentation often takes place in the context of mathematically sophisticated techniques or particular applications which can make it hard to understand the generality of these ideas.3 These textbooks in general can be challenging to read without computer science background. Since my article is written for humanities audience, it is on purpose biased–my examples of the application of the core concepts of data science come from humanities as opposed to economics or sociology. And along with an exposition, I also have an argument. I will suggest that some parts of data science are more relevant to humanities research than others, and therefore beginning “quantitative humanists” should focus on learning and practicing these techniques first.
If we want to use data science to “understand” some phenomenon (i.e., something outside of a computer), how do we start? Like other approaches that work on data such as classical statistics and data visualization, data science starts with representing some phenomenon or a process in a particular way. This representation may include numbers, categories, digitized texts, images, audio, spatial locations, or connections between elements (i.e., network relations). Only after such a representation is constructed, we can use computers to work on it. In most general terms, creating such a representation involves making three crucial decisions: What are the boundaries of this phenomenon? For example, if we are interested to study “contemporary societies,” how can we make this manageable? Or, if we want to study “modern art,” how we will choose what time period(s), countries, artist(s), and artworks, or other information to include? In another example, let’s say that we are interested in contemporary “amateur photography.” Shall we focus on studying particular groups on Flickr that contain contributions of people who identify themselves as amateur or semi-pro photographers, or shall we sample widely from all of Flickr, Instagram, or other media sharing service since everybody today with a mobile phone with a built-in camera automatically becomes a photographer. What are the objects we will represent? For example, in modern art example, we may include the following “objects” (in data science they can be also called data points, records, samples, measurements, etc.): individual artists, individual artworks, correspondence between artists, reviews in art journals, passages in art book, auction prices. (For example, 2012 Inventing Abstraction exhibition in MoMA in NYC featured a network visualization showing connections between artists based on the number of letters they exchanged.4 In this representation, modernist abstract art was represented by a set of connections between artists, rather than any other kind of object I listed above.) In a “society” example, we can for instance choose a large set of randomly chosen people, and study social media they share, their demographic and economic characteristics, their connections to each other, and biological daily patterns as recorded by sensors they wear. If we want to understand patterns of work in a hospital, we may use as elements people (doctors, nurses, patients, and any others), also medical procedures to be performed, tests to be made, written documentation and medical images produced, etc. What characteristics of each object we will include? (These are also referred to as metadata, features, properties, or attributes.). In humanities, we usually refer to characteristics that are already available as part of the data (because somebody already recorded them) and characteristics we have added (for example, by tagging) as metadata. In social science, the process of manually adding descriptions of data is called coding. In data science, people typically use algorithms to automatically extract additional characteristics from the objects, and they are referred as features (this process is called “feature extraction”). For example, artists’ names is an example of metadata; average brightness and saturation of their paintings, or the length of words used in all titles of their works are examples of features that can be extracted by a computer. Typically features are numerical descriptions (whole or fractional numbers) but they can also take other form. For example, a computer can analyze an image and generate a few words describing content of the image. In general, both metadata and features can use various data types: numbers, categories, free text, network relations, spatial coordinates, dates, times, and so on.  Although it is logical to think of the three questions above as three stages in the process of creating a data representation– limiting the scope, choosing objects, and choosing their characteristics – it is not necessary to proceed in such linear order. At any point in the research, we can add new objects, new types of objects and new characteristics. Or we can find that characteristics we wanted to use are not practical to obtain, so we have to abandon our plans and try to work with other characteristics. In short, the processes of generating a representation and using computer techniques to work on it can proceed in parallel and drive each other.
The author and several colleagues studied cultural differences using these computerized patterns of Instagram postings—arranged by hue and brightness—from Tokyo, New York, Bangkok, and San Francisco.

In 2002, I was in Cologne, Germany, and I went into the best bookstore in the city devoted to humanities and arts titles. Its new-media section contained hundreds of books. However, not a single title was about the key driver of the "computer age": software. I started going through indexes of book after book: No "software."

Yet in the 1990s, software-based tools were adopted in all areas of professional media production and design. In the 2000s, those developments have made their way to the hundreds of millions of people writing blogs and tweeting, uploading photos and videos, reading texts on Scribd, and using free tools that 10 years earlier would have cost tens of thousands of dollars.

Thanks to practices pioneered by Google, the world now operates on web applications that remain forever in beta stage. They can be updated anytime on remote servers without consumers having to do anything—and in fact, Google is revising its search-algorithm code as often as 600 times a year. Welcome to the world of permanent change—a world defined not by heavy industrial machines that are modified infrequently, but by software that is always in flux.

Software has become a universal language, the interface to our imagination and the world. What electricity and the combustion engine were to the early 20th century, software is to the early 21st century. I think of it as a layer that permeates contemporary societies. If we want to understand today's techniques of communication, representation, simulation, analysis, decision making, memory, vision, writing, and interaction, we must understand software.

But while scholars and media and new-media theorists have covered all aspects of the IT revolution, creating fields like cyberculture studies, Internet studies, game studies, new-media theory, and the digital humanities, they have paid comparatively little attention to software, the engine that drives almost all they study.

It's time they did.

Consider the modern "atom" of cultural creation: a "document," i.e. content stored in a physical form delivered to consumers via physical copies (books, films, audio records) or electronic transmission (television). In software culture, we no longer have "documents." Instead, we have "software performances."

If you are a scholar working inside Google or Facebook, you have a major advantage over colleagues in academe.

I use the word "performance" because what we are experiencing is constructed by software in real time. Whether we are exploring a website, playing a video game, or using an app on a mobile phone to locate nearby friends or a place to eat, we are engaging with the dynamic outputs of computation.

Although static documents may be involved, a scholar cannot simply consult a single PDF or JPEG file the way 20th-century critics examined a novel, movie, or TV program. Software often has no finite boundaries. For instance, a user of Google Earth is likely to experience a different "earth" every time he or she uses the application. Google could have updated some of the satellite photographs or added new Street Views and 3D buildings. At any time, a user of the application can also load more geospatial data created by other users and companies.

Google Earth is not just a "message." It is a platform for users to build on. And while we can find some continuity here with users' creative reworking of commercial media in the 20th century—pop art and appropriation, music, slash fiction and video, and so on—the differences are larger than the similarities.

Even when a user is working only with a single local media file stored in his or her computer, the experience is still only partly defined by the file's content and organization. The user is free to navigate the document, choosing both what information to see and the sequence in which to see it. (In Google Earth, I can zoom in and out, switching between a bird's-eye view of the area, and its details; I can also switch between different kinds of maps.)

Most important, software is not hard-wired to any document or machine: New tools can be easily added without changing the documents themselves. With a single click, I can add sharing buttons to my blog, thus enabling new ways to circulate its content. When I open a text document in Mac OS Preview media viewer, I can highlight, add comments and links, draw and add thought bubbles. Photoshop allows me to save my edits on separate "adjustment layers," without modifying the original image. And so on.

All that requires a new way to analyze media and culture. Since the early 2000s, some of us (mostly from new-media studies and digital arts) have been working to meet that challenge. As far as I know, I was the first to use the terms "software studies" and "software theory" in 2001. The field of software studies gradually took shape in the mid-2000s. In 2006, Matthew Fuller, author of the pioneering Behind the Blip: Essays on the Culture of Software(Sagebrush Education Resources, 2003), organized the first Software Studies Workshop in Rotterdam. "Software is often a blind spot in the theorization and study of computational and networked digital media," Fuller wrote in introducing the workshop. "In a sense, all intellectual work is now 'software study,' in that software provides its media and its context, but there are very few places where the specific nature, the materiality, of software is studied except as a matter of engineering."

In 2007, we started the Software Studies Initiative at the University of California at San Diego, and in 2008 we held the second software-studies workshop. The MIT Press offers a software-studies series, and a growing number of books in other fields (media theory, platform studies, digital humanities, Internet studies, game studies) also help us better understand the roles software plays in our lives. In 2011, Fuller and other researchers in Britain began Computational Culture, an open-access peer-reviewed journal.

T

here is much more to do. One question that particularly interests me is how software studies can contribute to "big data"—analyzing vast data sets—in fields like the digital humanities, computational social science, and social computing. Here are some of the key questions related to big cultural data that software studies could help answer:

What are interactive-media "data"? Software code as it executes, the records of user interactions (for example, clicks and cursor movements), the video recording of a user's screen, a user's brain activity as captured by an EEG or fMRI? All of the above, or something else?

To use terms from linguistics, rather than thinking of code as language, we may want to study it as speech.

Over the past few years, a growing number of scholars in the digital humanities have started to use computational tools to analyze large sets of static digitized cultural artifacts, such as 19th-century novels or the letters of Enlightenment thinkers. They follow traditional humanities approaches—looking at the cultural objects (rather than peoples' interaction with these objects). What has changed is the scale, not the method.

The study of software culture calls for a fundamentally different humanities methodology. We need to be able to record and analyze interactive experiences, following individual users as they navigate a website or play a video game; to study different players, as opposed to using only our own game-play as the basis for analysis; to watch visitors of an interactive installation as they explore the possibilities defined by the designer—possibilities that become actual events only when the visitors act on them.

In other words, we need to figure out how to adequately represent "software performances" as "data." Some answers can come from the field of human-computer interaction, where researchers in academe and private enterprise study how people engage with computer interfaces. The goals of that research, however, are usually practical: to identify the problems in new interfaces and to fix them. The goals of digital humanities' analysis of interactive media will be different—to understand how people construct meanings from their interactions, and how their social and cultural experiences are mediated by software. So we need to develop our own methods of transcribing, analyzing, and visualizing interactive experiences. Together with the Experimental Game Lab, directed by Sheldon Brown, for example, my lab analyzed the experiences of hundreds of users of Scalable City, a large-scale, complex virtual-world art installation created in Brown's lab. One of our goals was to help future users have more challenging interactive experiences.

Who has access to detailed records of user interactions with cultural artifacts and services on the web, and what are the implications of being able to analyze these data?

From the early days of interactive human-computer interfaces, tracking users' interaction with software was easy. Why? Because software continuously monitors inputs like key presses, mouse movements, menu selections, finger gestures over a touch surface, and voice commands.

The shift from desktop to web computing in the 1990s has turned the already existing possibility of recording and storing users' inputs into a fundamental component of a "software-media complex." Since dynamic websites and services (Amazon's online store, personal blogs that use Google's Blogger system, online games, etc.) are operated by software residing on company's servers, it is easy to log the details of user interactions. Each web server keeps detailed information on all visits to a given site. A separate category of software and services exemplified by Google Analytics has emerged to help fine-tune the design of a website or blog.

Today social-media companies make available to their users some of the recorded information about visitors' interactions with the sites, blogs, or accounts they own; the companies also provide interactive visualizations to help people figure out which published items are most popular, and where their visitors are coming from. However, usually the companies keep the really detailed records to themselves. Therefore, if you are one of the few social scientists working inside giants such as Facebook or Google, you have an amazing advantage over your colleagues in the academy. You can ask questions others can't. This could create a real divide in the future between academic and corporate researchers. While the latter will be able to analyze social and cultural patterns on both supermicro and supermacro levels, the former will have only a normal "lens," which can neither get extremely close nor zoom out to a planetary view.

Who benefits from the analysis of the cultural activities of hundreds of millions of people?Automatic targeting of ads on Google networks, Facebook, and Twitter already uses both texts of users' posts or emails and other data, but learning how hundreds of millions of people interact with billions of images and social-network videos could not only help advertisers craft more-successful visual ads but also help academics raise new questions.

Can we analyze the code of software programs? It's not as easy as you may think. The code itself is "big data."

Early software programs such as 1970s video games were relatively short. However, in any contemporary commercial web service or operating system, the program code will simply be too long and complex to allow you to read and interpret it like a short story. While Windows NT 3.1 (1993) was estimated to contain four to five million source lines of code, Windows XP (2001) had some 40 million. MAC OS turned out even bigger, with OS X 10.4 (2005) code at 86 million lines. The estimated number of lines in Adobe Creative Suite 3 (which includes Photoshop, Illustrator, and a number of other popular applications to produce media) is 80 million.

The gradual move of application software to the web also brings with it a new set of considerations. Web services, apps, and dynamic sites often use multi-tier software architecture, where a number of separate modules (for example, a web client, application server, and a database) work together. Especially in the case of large commercial sites like amazon.com, what the user experiences as a single web page may involve continuous interactions between dozens or even hundreds of separate software processes.

The complexity and distributed architecture of contemporary large-scale software poses a serious challenge to the idea of "reading the code." However, even if a program is relatively short and a cultural critic understands exactly what the program is supposed to do, this understanding of the logical structure of the program can't be translated into envisioning the actual user experience.

The attraction of "reading the code" approach for the humanities is that it creates an illusion that we have a static and definite text we can study—i.e., a program listing. But we have to accept the fundamental variability of the actual "software performances." So rather than analyzing the code as an abstract entity, we may instead trace how it is executed, or "performed," in particular user sessions. To use the terms from linguistics, rather than thinking of the code as language, we may want to study it as speech.

Some researchers, like Mark Marino and others working in "critical code studies," have been promoting nuanced, theoretically rigorous, and rich ideas about what it means to "read the code," so my critique is aimed only at a naïve version of the idea that I sometimes encounter in the humanities.

The development of methods to study contemporary software in a way that can be discussed in articles, conferences, and public debates by nonprogrammers, is a key task for software studies. However, given both the complexity of software systems and the fact that, at least at present, only a very small number of media and cultural researchers are trained in software engineering, I don't expect that we can solve this problem in a short time.

And yet, confronting it is crucial, not just for the academy but also for society at large. How can we discuss publicly the decisions made by Google Search algorithms, or Facebook's algorithms controlling what is shown on our news feeds? Even if these companies made all their software open source, its size and complexity would make public discussion very challenging.

While some of the details from popular web companies are published in academic papers written by researchers working at these companies, only people with computer-science and statistics backgrounds can understand them. Moreover, many popular software services use machine-leaning technology that often results in "black box" solutions. (While the software achieves desired results, we don't know the rules it follows.)

As more and more of our cultural experiences, social interactions, and decision making are governed by large-scale software systems, the ability of nonexperts to discuss how these systems work becomes crucial. If we reduce each complex system to a one-page description of its algorithm, will we capture enough of software behavior? Or will the nuances of particular decisions made by software in every particular case be lost?

The role of software studies is not to answer these and many other questions about our new interactive world, but rather to articulate them and offer examples of how they can be approached. And to encourage people across all disciplines to think about how software changes what they study and how they study it.

In the Phototrails project (phototrails.net), created by myself, Nadav Hochman, and Jay Chow, we visualized patterns in the use of Instagram across 2.3 million photos from 13 global cities. In the paper accompanying the project, we attempted to combine two mirror sides of software studies—thinking about software interfaces and how they influence what we do and at the same time studying large-scale behaviors of many software users. One of the key questions we raised: How much of the differences among the cities can we find, given that everybody uses the same Instagram app that comes with its own strong "message" (all photos have the same square size, and all users have access to the same set of build-in filters to make their photos more aesthetic in the same ways). While we did find small but systematic differences in the photos from each city, the use of Instagram software itself was remarkably consistent.

How does the software we use influence what we express and imagine? Shall we continue to accept the decisions made for us by algorithms if we don't know how they operate? What does it mean to be a citizen of a software society? These and many other important questions are waiting to be analyzed.

Hundreds of people stuck in a giant swimming pool passively floating to the rhythm of artificial waves. The poor resolution of the found footage muddles them into a contextless and faceless crowd. Nobody tries to escape the crowd, or go against the current. They are trapped but happy enough. It’s like Dante’s Inferno but without the drama. Just the people floating in the mud.

The final scene of Mainsqueeze captures “a contemporary atmosphere or mood” which sets the present as a time out of joint, encapsulated by the washing machine that tears itself apart over the course of the film. Rafman poses the present escape from the real towards the simulated as the result of a general feeling of turmoil that leads to flight rather than revolt. In the video, the first readable line of text is written on the forehead of a sleeping drunk man at the beginning of the film: “LOSER”. He smiles, and we are led to wonder who the loser really is.

Yet Rafman is not making a particular ethical statement: “Mainsqueeze expresses a moral condition or atmosphere without making a moral judgment. I gravitate towards communities like 4chan because I see in them a compelling mix of attraction and repulsion. This ambivalence is reflected in the current cultural moment.”

Surfing the deep web, Rafman collects, orders, observes, and makes his source material visible to us: “Mainsqueeze is entirely composed of footage found through my online explorations. The voice over text is a combination of modified quotes from literature, Tumblr, and comments on various message boards. I feel less of a need to create original material from scratch due to the sheer abundance of material out in the world to work with. The craft is found in the searching, selecting or curating, and editing together of the materials pulled from far-flung corners of the web.” Yet, he insists “it is not about fetish tourism or shocking people about what exists in the dark corners of the net, rather, I am giving the sourced material a poetic treatment.”

Rafman assumes, and in turn invites us to assume, a difficult position: he is simultaneously the drunk man, the one who paints his face and the one passerby who thinks they are both stoned. Sometimes he himself indulges in degradation, and we as viewers are equally implicated.

An original use of voice-over contributes to this sense of viewer involvement. It is used neither to generate empathy nor to signify a complete alterity (as when synthesized voices are used). Rafman explains that “This particular tone came about through experimenting with a montage of a wide range of material; moments of philosophical epiphany, pseudo-intellectual quotes from tumblr, banal confessional message boards, comments from reddit threads, etc.” InMainsqueeze, this montage is turned into a profound yet familiar voice that often addresses the viewer directly, dragging you into the world depicted on screen. Let yourself in.

Our story begins between the end of the 1950s and the beginning of the 1960s, when technological progress on one hand and developments in art on the other created the conditions for art, science and technology to intertwine once more. Such an encounter was anything but new in the history of art, having been vigorously embraced by the avant-garde movements: see Lazlo Moholy-Nagy, often invoked as one of the founding fathers of New Media Art, above all for his Licht-raum-modulator (1930), a kinetic sculpture that produces fascinating light effects. And it was the historic avant-garde movements that informed the new artistic experiences that sought to go beyond what then looked like the dead end of Abstract Expressionism: New Dada, Nouveau Réalisme, Gutai, Happening, Fluxus, Kinetic Art, Arte Programmata, Optical Art, Pop Art and Video Art. Reality, in the shape of real or represented objects, entered artworks; the pop culture conveyed by the media began to capture the attention of artists; art appropriated all media, from the human body to consumer products, from advertising to television sets to cars, and theoretical developments like cybernetics and information theory informed the lexicon of art. This is, for example, what John Brockman says about John Cage.


For the first and only time in the history of art, the implicit perspective in the most generic interpretation of the expression New Media Art became a mass strategy, common to all the avantgarde art of the period. This situation was short-lived: while a few “new media” and artistic strategies, from assemblage to photography, performance and conceptual interventions on mechanically reproduced images rapidly became the stuff of the establishment, the more radically technophile or science-based expressions, like Kinetic and Optical art, were put out of action, and video entered a splendid isolation of its own that was to last until the early 1990s. At the same time, in the States, the spectre of permanent war gave an incredible boost to scientific and technological research. In 1946 the University of Pennsylvania presented the first digital calculator, ENIAC (Electronic Numerical Integrator and Computer); 1951 saw the launch of UNIVAC, the first computer to hit the market, capable of processing both numerical data and text. These were huge machines without any kind of user interface, that accepted programs in the shape of perforated cards and could only be operated by highly skilled users. Accessibility was also very restricted: developed for military applications, they resided mostly in research centers and universities. It was in Bell Laboratories in Murray Hill (New Jersey) in particular that the first studies on the algorithmic production of text, music and images were carried out, and not by artists, but engineers and researchers who saw these experiments as more or less necessary diversions to their research work. The electronic engineer A. Michael Noll, for example, was taken on by Bell Labs in 1961 and worked there for 15 years. In the summer of 1962 he created his first works of “Computer Art”, abstract images generated by algorithms and mathematical functions that were an evident tribute to Piet Mondrian and Cubism. Around 1963 many pioneers began working in this direction, including Lillian Schwartz, Herbert Franke, Manfred Mohr, Jean-Pierre Hébert and Roman Verotsko. In April 1965 the Howard Wise Gallery in New York, the same venue that brought Gruppo Zero and Kinetic Art to America, staged the exhibition Computer-Generated Pictures by Bela Julesz and Michael Noll. Computer Art appeared in a number of group shows, including Cybernetic Serendipity (ICA, London 1968), Tendencija 4 (Zagreb 1969) and Computerkunst (Hannover 1969). [2] At the same time, potential uses of computers in literature and music were also being studied: on one hand there was the combinatory literature developed by Alison Knowles at Bell Labs and the members of the European group OuLiPo (Ouvroir de Littérature Potentielle), founded in 1960 by Raymond Queneau and François Le Lionnais; and on the other the work of the composer James Tenney at Bell Labs. [3] This initial foray into Computer Art therefore came about in an extremely restricted context, in both sociological and technological terms. From an aesthetic point of view the massive mainframes of the sixties placed great limitations on artists and were extremely difficult to use, and the result was that in this niche engineers vastly outnumbered genuine artists. In view of this, much Computer Art of the sixties is exceptionally ingenuous aesthetically speaking – in the words of Jim Pomeroy, it rolled out «flashy geometric logos tunneling through twirling wire-frames,’ graphic nudes, adolescent sci-fi fantasies, and endless variations on the Mona Lisa». [4] A. Michael Noll candidly confesses.

Yet dismissing Computer Art as merely ingenuous would be a simplistic way of looking at things. Even supposing that the only achievement of Noll and the first computer artists was to show it was possible to make art with a computer, their contribution to the evolution of the medium was crucial. For Computer Art not only paved the way for New Media Art, but the whole of computer graphics, which over the years has progressed to photorealistic videogames and 3D animation. Even considering merely this dual legacy we can appreciate the scope of its contribution to the culture of the twentieth century. And the success, however fleeting, of Computer Art also points up something else: the openness of the art world of the sixties to the most advanced, precarious fringes of cultural experimentation, its acceptance of ideas that would be hard pressed to find a welcome elsewhere. The best demonstration of this was probably the 1968 exhibition Cybernetic Serendipity curated by Jasia Reichardt at the Institute of Contemporary Art in London. This show was part of the work of the Independent Group and resulted from the 1965 encounter between Reichardt and Max Bense, the German philosopher, a key figure of the Stuttgart school, who studied the relationships between maths, language and art, and coined the term “information aesthetics”. According to Brent MacGregor, it was Bense who told Reichardt to “look into computers”. [6] In 1966 the exhibition was announced at a public conference, and fundraising began. Despite initial expectations, the only private company to invest significantly in it was IBM; the rest was covered by the Arts Council. Cybernetic Serendipity was not an exhibition of Computer Art, but a multidisciplinary event that explored the impact of information technology and cybernetic theory on life and contemporary creativity. It was divided into three sections: the first featured works – images, but also music, animations and texts – generated by computers, the second contained cybernetic robots and “painting machines”, and the third explored the social uses of computers and the history of cybernetics. Alongside the pioneers of Computer Art and cybernetic art, from Charles Csuri to Michael Noll, John Whitney to Edward Ihnatowicz to the Computer Technique Group of Tokyo, were artists who shared aesthetic, thematic or formal characteristics with the latter (Nam June Paik, Jean Tinguely and his machines, James Seawright, the Optical painter Bridget Riley, and avant-garde musicians like John Cage and Jannis Xenakis). But there were also explanatory elements and even a computer, provided by IBM, that offered a service for booking flights. According to the curator.

Cybernetic Serendipity came about in a context, the British context, which was of great interest. Catherine Mason’s research [8] has in fact shown that Britain’s distinctive education system facilitated the development of relationships between art, science and technology between the sixties and the eighties. A legacy of the Victorian education system, Britain’s design schools provided both artistic education and training in the applied arts. In the 1950s, the Independent Group addressed, among other things, the implications of science, technology and the mass media on art and society, culminating in the exhibition This Is Tomorrow (Whitechapel Art Gallery, 1956). In 1953 Richard Hamilton went to teach at King’s College in Newcastle, where, together with Victor Pasmore, he held a Basic Design course. Among their students was Roy Ascott, who was encouraged to cultivate his interest in communication, interactivity and cybernetics. In 1961, Ascott was asked by the Ealing Art School to create a two-year course based on the principles of cybernetics: his Ground Course, along with his subsequent appointments, was to play a crucial role in the education of a new generation of artists and designers. In 1967 the first polytechnics appeared, thanks to sizeable government investments in technology in the post war period, which also led to the creation of a Ministry of Technology. In the polytechnics, as Catherine Mason notes, an art student could also learn programming. In the seventies this led to a wide network of schools engaged in Computer Art, yielding interesting results above all in computer graphics for television and advertising. At the same time, these academic roots enabled students and lecturers to develop their own creative work, despite the relative lack of interest in digital art from the art world. And while British Computer Art survived in the world of academe, it soon developed systems of support and critical debate. In 1968, in connection with the British Computer Society, the Computer Arts Society (CAS) was founded. In 1969, CAS launched its own publication, Page, as a platform for debate and critical engagement. Equally early on, CAS began to look beyond the United Kingdom, setting up chapters in various European countries and coming to the States in 1971. In 1970 the association had 377 members, including libraries and institutions, in 17 countries. In this period it put together a collection that included works by pioneers like Manuel Barbadillo, Charles Csuri, Herbert W. Franke, Edward Ihnatowicz, Ken Knowlton, Manfred Mohr, Georg Nees, Frieder Nake, Lillian Schwartz and Alan Sutcliffe, and in 2007, with Mason’s involvement, this collection was bought by the Victoria and Albert Museum in London. Just how receptive the art world of the sixties was to the “art and technology” pairing is also proved by the milieu that sprung up around the distinctive figure of Billy Klüver (1927 – 2004). An electronic engineer of Swedish origin, in 1958 Klüver was hired by Bell Labs in Murray Hill. With a life-long interest in art, in the early seventies he began to work with artists. In 1960 he provided technical support to the Swiss artist Jean Tinguely (after being introduced to him by Pontus Hultén) for his spectacular Homage to New York (1960), a kinetic machine that self-destructed in the Sculpture Garden of the MoMA in New York. Robert Rauschenberg was also involved in this project. Following that, Klüver provided technical support to various artists: he worked with Rauschenberg on the installation Oracle (1962 – 1965) supplying the artist with remote controlled radios, and he helped Jasper Johns and Andy Warhol, providing the material for the latter’s famous Silver Clouds, the helium-filled pillows that accompanied his temporary break from painting, presented in a solo show at the Leo Castelli gallery in 1966. 1966 also saw Klüver’s first major production, the outcome of a collaboration with Rauschenberg. From 14 to 23 October 1966, at the 69th Regiment Armory in New York, he presented the event 9 Evenings: Theatre and Engineering, a series of multimedia performances featuring ten artists working with thirty engineers and scientists from Bell Labs. Participants included Robert Rauschenberg, John Cage, David Tudor, Yvonne Rainer, Robert Whitman and Öyvind Fahlström. During the event Klüver discussed the idea of giving this collaboration between artists and engineers more stable foundations, and this was what led to the establishment of Experiments in Arts and Technology (E.A.T.), a no-profit association launched at the start of the following year that promoted collaborations between artists and engineers with both technical and financial input, thanks to ongoing links with the technology industry. By 1969 E.A.T boasted 4,000 members and various chapters throughout the United States. [9] Klüver’s collaborative model was in fact a two-way process: while on one hand he was convinced that technicians could help artists achieve their objectives, on the other he believed that artists, as visionaries and active agents of social change, could influence the development of technology. This is Barbara Rose’s take on the matter.

If some kind of follow-up had materialized, these early experiments, and the model pursued by E.A.T. – to get acknowledged exponents of the artistic avant-garde working in close contact with engineers, while keeping their respective roles distinct – could feasibly be attributed a key role in the history of contemporary art. So how did it come to pass that the great emphasis placed in the sixties on the “art and technology” pairing by key figures like Jasia Reichardt, Roy Ascott, Billy Klüver, Robert Rauschenberg and Pontus Hultén, as well as Jack Burnham, gradually waned in subsequent years, leaving only the faintest of traces in the official historiography of art? How was it that one of the most significant components of the neo avant-garde ended up as an underground phenomenon, carving out a niche that enabled it to go unnoticed for the next thirty years? There is no single answer to this question: we must rather look to a series of circumstances that emerged during the seventies. In the first place, in this period the “art and technology” pairing found itself up against ideological and political opposition connected to the military purposes of technological research and the considerable financial interests involved. The Vietnam war, and the protests against it from artistic and intellectual quarters, fuelled opposition to the “art and technology” model. “Technology is what we do to the Black Panthers and Vietnamese”, Richard Serra asserted in 1969. [13] Beyond the political sphere, other academics have highlighted the emergence in the late sixties of “anti-computer” sentiment, bound up with enduring concepts such as the romantic vision of the artists and the fear that technology might supersede the individual and undermine the central role of the artist in the creative act. [14] It has also been observed that the critical model underpinning the acknowledgement of the importance of the “art and technology” pairing has encountered varying fortunes. In a 2007 essay, [15] in line with Jack Burnham, Edward A. Shanken asserts that the hermeneutic approach imposed by Alois Riegl and summed up in the concept of Kunstwollen, quashed the theories of Gottfried Semper, according to whom art reflects “economic, technical and social relationships”. In Shanken’s opinion, this approach still endures today, helping to keep New Media Art outside the canons of contemporary art. In the short term, these two prejudices conspired against the operative and interpretative model of the “art and technology” pairing, with a number of significant results: video retired into a niche, despite continuing to have (limited) critical success, above all in works that put formal exploration of the medium in second place, as per the “narcissistic” line plotted by Rosalind Krauss; Kinetic Art and Optical Art, also steeped in technophile rhetoric, vanished completely from the scene, after an initial period of great success, to be rediscovered only relatively recently; even a certain interpretative approach to Conceptual Art – as put forward by Jack Burnham in Software (New York, Jewish Museum 1970) and Kynaston McShine in Information (New York, MoMA 1970) – that relates conceptual work to the advent of information technologies, surrendered to other approaches with less of a technological vein. As for the nascent field of New Media Art, the collaborative model developed by Klüver was well suited to the organization of one-off events, but less to facilitating continuity in artists’ work. Lastly, Computer Art had to come to terms with its aesthetic limitations and the problems involved in actually accessing the machines, which continued to be expensive and bulky. During the seventies computers became more accessible, albeit gradually. Research into increasingly intuitive forms of manmachine interaction made enormous progress, and in 1969 the first distributed network made its appearance, in the shape of Arpanet. In 1971, thanks to the creation of a common protocol among various university and corporate networks, the internet was born. In parallel to this, alongside the cumbersome mainframes, cheaper, more manageable computers appeared: minicomputers (like the PDP-8, distributed as of 1968); microcomputers, like the famed Altair 8800, distributed as of 1975; and home computers, headed up by the equally legendary Apple II (1977), produced by the startup Apple Computer (founded by Steve Wozniak and Steve Jobs in 1976). With the arrival of home computers on the scene, computing branched out of research centers and universities and entered offices and households. A complex, variegated culture sprung up around them, with contributions not only from engineers and high level researchers, but also amateurs and enthusiasts. Many of them had radical political ideas, influenced by Californian counterculture. Much of the New Media Art of the seventies was an expression of this complex cultural milieu. In this context it is not easy to identify figures who can be described simply as “artists”: most of them worked across the disciplines, researchers and employees of the hi-tech industry with an artistic sideline. Douglas Kahn relates, for example, that the first serious attempt to make music with an Altair 8800 was undertaken between 1970 and 1975 by Ned Lagin, who was doing astronaut training at the MIT, but also studied jazz and composition. This work earned him a temporary collaboration with the Grateful Dead. In the same enclave of enthusiasts in the Bay Area there was Paul De Marinis, who worked with Jim Pomeroy and David Tudor on a number of sound installations before starting out on his own artistic career. [16] Visual experimentation received impetus from university and corporate circles. In Stanford University in 1970, the Xerox Corporation opened the Palo Alto Research Centre (PARC), devoted to the development of graphic applications; in the same year, General Electric presented Genigraphics, a graphic system designed for the business world, but used extensively by artists. In 1973, the main computing association in the United States, the ACM (Association for Computing Machinery), set up SIGGRAPH, its “Special Interest Group on GRAPHics and Interactive Techniques”, which organized its first conference in 1974. From then on SIGGRAPH became the main international showcase for developments in computer graphics. This field was to be heavily influenced by the discovery of fractals, described in 1975 by the French-American mathematician Benoît Mandelbrot, then researcher at IBM, as geometric forms that can be split into parts, each a small scale copy of the whole. [17] Throughout the decade, thanks to institutional and corporate support, research into the algorithmic generation of images thus developed, between the more aesthetically and conceptually conscious work of such artists as Charles Csuri, Manfred Mohr and Vera Molnar on the one hand, and the simple deployment of the productive and aesthetic potential of the new tools on the other. Something similar happened with robotics. In 1973 at the University of California San Diego (UCSD), Harold Cohen launched the AARON project, which consisted in developing a form of artificial intelligence capable of painting. Having trained as a painter, over the years Cohen attempted to teach AARON the basic rules of painting, developing its “aesthetic tastes” and decision-making power. The painting done by AARON naturally closely resembles that of Cohen, though the machine did gradually develop its own style over time. In Britain Edward Ihnatowicz, who in 1971 began working as a Research Assistant in the Department of Mechanical Engineering at University College in London, produced his most ambitious project, the cybernetic sculpture The Senster (1970 – 1974), thanks to a commission from Philips, which exhibited it for four years in its permanent exhibition space in Eindhoven, before dismantling it. The sculpture, a 4 meter aluminium structure controlled by a computer, responded to the voices and movements of viewers. In the late seventies and early eighties it was above all telecommunications that lent New Media Art a presence and a profile outside of the corporate/university world. While one to one communication systems (like the telephone) and one to many systems (like mail) elicited the attention of the avant-garde movements and Fluxus, before the advent of the Internet satellite broadcasting was the technology that afforded concrete opportunities to explore the field of communications. In 1973, for the first time in history, satellite technology succeeded in broadcasting a cultural event – Elvis Presley’s concert in Hawaii – to the whole world. On 29 December 1976, with the support of the Contemporary Arts Museum in Houston, the video artist Douglas Davis broadcast the closing minutes of his performance Seven Thoughts to all the IntelSat channels. The following year, thanks to funding from NASA, the Californian artists Kit Galloway and Sherrie Rabinowitz produced Satellite Arts Project ’77, which connected two NASA centers, one on the East Coast and one on the West Coast, via satellite: images of dancers performing in the two centers were filmed and edited, using a simple chroma-key, to form a single live image. In this way, performers physically 3,000 miles apart could act as if dancing together on the same stage. Dance was adopted as a traditional performing art capable of exploring the limitations and potential of technology. [18] In the same year Documenta 6, curated by Manfred Schneckenburger, was devoted to means of communication, with the aim of exploring the position of art in the media society. The exhibition presented photography, video and video installations, and opened up to television by means of satellite broadcasts of performances by Davis, Nam June Paik and Joseph Beuys.

It was above all in the 1980s that artistic work on communication gathered pace, extending to telematics too. 1980 saw two major events, the conference Artists’ Use of Telecommunications, organized by Carl Eugene Loeffler at the Museum of Modern Art in San Francisco, and Hole in Space, a public art project by Galloway and Rabinowitz. The former was an international event that connected up participants in different areas of the globe by satellite, Slow-Scan TV (video broadcast via telephone) or telematic network: from the Center for Advanced Visual Studies at the M.I.T. in Cambridge (USA) to Japan’s Tsukuba University; from the Alternative Media Center of New York to the Trinity Video and Ontario College of Art in Toronto; from the Western Front Society in Vancouver to the Museum des 20 Jahrhunderts in Vienna. Participants included Robert Adrian, Bill Bartlett, Douglas Davis, Carl Loeffler, David Ross, Aldo Tambellini, Norman White, Gene Youngblood and Peter Weibel. The event highlighted the presence of a solid network of traditional art institutions, research centers and media centers. Hole in Space, on the other hand, created a satellite bridge between public areas in two cities (New York and Los Angeles), with large screens installed at the Lincoln Center for the Performing Arts in New York City and the Broadway Department Store in Century City, Los Angeles, respectively. The screens showed live footage from a camera placed beside each one, enabling people in the street, most of whom were unaware that the event was taking place, to interact with others thousands of miles away. The result was a highly participative, spectacular event, that attracted various audiences who explored different levels of interaction and remote communication: relational aesthetics ante-litteram – but also, as it has been defined on YouTube, “the mother of all video chats”. In 1982 it was the turn of The World in 24 Hours, coordinated by Robert Adrian from the Ars Electronica Festival in Linz and featuring a wide range of communications technologies: from phone to fax, Slow-Scan TV and telematic networks, followed in 1983 by La Plissure du Texte by Roy Ascott (Paris, Musée d’Art Moderne de la Ville de Paris), a collaborative text produced by various users connected by BBS, and in 1984 by Good Morning Mr Orwell, a satellite broadcast of video pieces and live performances coordinated by Nam June Paik and produced by WNET TV in New York in collaboration with the Pompidou Center in Paris, seen by more than 10 million people. All these events reveal both the upsurge in interest from traditional art institutions and the great ferment of the field, with the involvement of both companies and specialized centers, some of which came into being in that very decade. The interest from traditional art institutions must however be seen in context. The nascent technologies were the hot topic of the day, and it was not difficult to get sponsorship from the hi-tech industry and television networks. By the early eighties the latter enjoyed an unprecedented presence in society, and critical reflections on the media and their power to manipulate were advanced by artists and intellectuals, and reached the public at large (Sidney Lumet’s film Network, on the power of television, was released in 1976). Moreover, in the decade that saw the return of painting and the explosion of the art market, the institutions took it upon themselves to support less stable, less marketable artistic genres like video, photography and performance. In other words, while conditions were favorable, the reappearance of New Media Art in the establishment art world during the 1980s was conditioned by external factors and was on the whole too fleeting to lead to lasting continuity. All of this emerges clearly if we consider two key events in this decade: the exhibition Les Immateriaux, curated by Jean Francois Lyotard and Thierry Chaput for the Pompidou Center in Paris in 1985; and the 1986 Venice Biennale, coordinated by Maurizio Calvesi and entitled “Art and Science”. The first was not actually an exhibition devoted to the New Media or art numerique, as it is known in France. It started life as a project on the “new materials of creativity”, but the involvement – at a late stage – of Lyotard transformed it into an exploration of post-modern sensibility. As Lyotard said: «It is not our intention to sum up the new technologies in this exhibition [...] or to explain how they work. All it attempts is to discover and raise a sensibility that is specific to post-modernism, and we assume that it exists already». [19] Its press release described it as a “non-exhibition”, and one of its stated aims was to challenge the modern, “prescriptive” model of the exhibition, connected to the 19th century salon and the gallery. In Les Immateriaux works were not hung on the walls: cables attached to the floor and ceiling divided up a decentralised setting, which could be explored in various ways. Visitors were given a walkman with the soundtrack of the exhibition, which played according to their position in the venue: this collage of music, sounds and texts, only some of which actually related to the exhibition, aimed to create a powerful sensation of instability. The event also featured works by conceptual and minimal artists, from Joseph Kosuth to Dan Flavin and Robert Ryman, precursors like Marcel Duchamp and MoholyNagy, and artists working with communication technologies, such as Roy Ascott and Rolf Gelhaar; yet it was the exhibition itself that was designed “as a work of art”, to the point that the actual works on show are rarely mentioned in the numerous comments that the event elicited. [20] Once again, we are faced with a singular contrast: while on one hand Les Immateriaux was of seminal importance for New Media Art, configuring the aesthetic and philosophic categories that were to be its focus in subsequent decades, on the other hand it showed the art crowd that, as Jasia Reichardt commented with regard to Cybernetic Serendipity, this area was yet to produce any definitive outcomes, comparable with those of other artistic tendencies, and was as yet mainly to be appreciated for its aspect of research and experimentation. Similar observations could be made with regard to the 1986 Venice Biennale, where the “Technology and Computing” section curated by Roy Ascott, Don Foresta, Tom Sherman and Tommaso Trini was given a deliberately “workshop” style layout. The central nucleus of this was the Planetary Network, coordinated by Roy Ascott: for three weeks in this workshop in the heart of the Corderie venue, the artists present conducted communicational exchanges of various kinds with other artists in twenty different locations, from Canada to Australia, using three communications protocols: email, fax and Slow-Scan TV. The networking aspect – artists across the globe working together – clearly prevailed over the actual material exchanged: video, images faxed with manual interventions by the artists involved, computer-generated images and texts. According to Ascott, networking and working within a telematic network – with meetings, interactions, negotiations, and visualizations in the electronic arena – was at the core of this show. [21] In the exhibition catalogue, Tom Sherman [22] also returns to the idea of interaction as a founding element of the electronic arts, in an illuminating text that also dwells on their exclusion from the art world in the 1970s and their radical “difference” that continues to make them unpalatable today: their love of machines, feared by the public at large; their propensity for collaborations, which clashes with the rampant careerism of the art world, and the notion of interaction (between artist and machine, between artists via machine, and between machine and public). The 1986 Biennale was undoubtedly a great platform for New Media Art, which in Venice found a unique opportunity to network and succeeded in exploring a large part of its potential. Around the Planetary Network the event featured the most groundbreaking work in computer graphics, as well as less technological, more amateur images; the “first interactive art videodisc” by Lynn Hershman Leeson; a fascinating installation of sounds and coloured lights by Brian Eno, and the sound environment Very Nervous System (1984) by the Canadian David Rokeby: a space controlled by a system of sensors that perceived the presence of the viewer and his or her movements in the area, translated into sounds by a computer. From the 1980s onwards this vast, variegated scene found its first, privileged point of encounter at the Ars Electronica festival in Linz, Austria. [23] Ars Electronica came about in 1979 as a renewed version of the Bruckner Festival, an event devoted to contemporary music accompanied by an academic symposium. The initial idea was to dedicate the symposium to electronic music. But the involvement of the Austrian Broadcasting Corporation (ORF), directed locally by Hannes Leopoldseder, raised the bar. Leopoldseder proposed going beyond the limits of the symposium and creating a permanent festival devoted to technology and its impact on art and society. On 18 September 1979 the first edition of the Ars Electronica festival opened with a spectacular open-air event, in front of an audience of 100,000. The success of this first edition excited the organisers, who began to think about making it a stable thing. The business model behind it had not yet firmed up, and the following editions, up to 1986, took place on a biennial basis. In the meantime the Austrian artist and curator Peter Weibel joined the artistic committee, and from 1986 the event was scheduled to take place every year, with a common theme for the festival and symposium. 1987 saw the launch of the Prix Ars Electronica, a prize – divided into different categories – that was to play a fundamental role in stimulating creativity, as well as establishing a series of critical and qualitative criteria, and developing a hierarchy of merit within the artistic community. In the early 1990s, feasibility studies were undertaken into founding a permanent center, the Ars Electronica Center in Linz, which got off the ground in 1995, accompanied by Ars Electronica Futurelab. The former was conceived as a “Museum of the Future”, gathering and hosting emerging results from the digital medium, while the latter was devoted to production and research, involving artists in courses and workshops and putting the most advanced technologies at their disposal. As emerges from this brief overview, Ars Electronica and the people involved in it were to play a decisive role in establishing New Media Art world as an independent arena. By stimulating debate, proposing categories and criteria of value, facilitating the production and circulation of works, developing a strategic network with other centers, universities and companies and contributing to the development of an economy and model of sustainability for New Media Art, Ars Electronica became its undisputed mecca. Locally, the Ars Electronica model was made possible by the fact that the post-industrial city of Linz was attempting to reinvent itself as the cultural and technological capital of Austria and central Europe. But its success was above all linked to the existence of a flourishing art scene in search of a stable platform for producing and exhibiting its work, not linked to one-off events like the aforementioned 1986 Biennale, and to the slow but ongoing development of an alternative system of festivals and centers like V2_, launched in Hertogenbosch, Holland in 1981 before moving to Rotterdam in 1994, where it stages a biennial festival called the Dutch Electronic Art Festival (DEAF). All these developments are obviously a product of the inexorable progress of technology, which was gradually seeping into everyday life. After the Apple II, various models of home computer appeared on the market: from the Atari 400 to the Commodore VIC-20, the first computer to achieve sales of over a million; from the Sinclair ZX Spectrum to the Commodore 64 and the IBM PC. In 1984, Apple Computer launched the Macintosh, a genuine revolution in the history of the personal computer: relatively cheap (at almost 2,500 dollars), the computer functioned with keyboard and mouse, and featured a graphic interface that replaced the customary green text against a black background. This graphic interface heralded the introduction of common metaphors inspired by the world of the office that the computer was destined for: desktop, wastebasket, windows, files and documents. Lastly, the computer featured a modem, a device that enabled it to connect up to a telematic network via a simple telephone line. Telematic networks also began to spread, and while Internet remained mainly linked to the American university system, some countries (like France with Minitel) created a national network, and on an amateur level BBS (Bulletin Board Systems) took off. These computer systems functioned like electronic noticeboards, with users connecting to them to share or download files and exchange messages. BBS technology first appeared in 1977 and became popular above all thanks to Fidonet, (invented by the American Tom Jennings in 1984), a network of different BBS. But computing did not make its way into households (and the everyday lives of millions) only by means of home computers and networks. In 1961 the MIT labs created Spacewar!, the first videogame in history. It did not take long for the business world to realise that this very basic interactive interface could be the start of a profitable sector of cultural entertainment. In the second half of the 1970s arcade games took off, along with the first home platforms for videogames. From Pong (1972) to Space Invaders (1978) and Pacman (1980), the videogames industry expanded exponentially, and the advent in 1983 of the NES (Nintendo Entertainment System) was to make an indelible mark on the collective consciousness. These developments had conspicuous consequences on the cultural sphere. The 1980s were the decade of hackers, cyberpunk, basic telematics, virtual reality and the start of the free software movement: phenomena which are too complex to be explored in detail here. Cyberpunk, for example, came about as a literary movement in the United States in the early 80s, thanks to the science fiction successes of William Gibson and Bruce Sterling, and the rediscovery of Philip K. Dick, but in Italy it developed as a political movement, attaching onto the substrate of punk, the ferment of the social centers and the left-wing protest movements in 1977. [24] Likewise in California, where a pivotal role was played by figures like Timothy Leary, exponent of counterculture and advocate for psychedelic drugs, who went on to develop videogames, use BBS and become a leading figure of “cyberculture”, and scholar of virtual reality. Both the hacker movement and the Free Software philosophy were rooted in this complex milieu. Artists played an active role in shaping this culture, and enriching its imagery with their works. It is often difficult, if not impossible, to separate the art from the context it is an active, integral part of. The association between New Media and New Media Art formed in the previous decades, but consolidated in the 1980s. This arose perhaps because on one hand, these artists were excluded from – or deliberately avoided – traditional artistic contexts, and on the other because there was a proliferation of hybrid, multidisciplinary figures who did not separate their art from their political activism, or their contribution to the network. In 1986, reviewing an Italian festival, Vittorio Fagone wrote about a “third culture”, distinguishing digital culture from humanistic and scientific culture: a culture in which «engineers, mathematicians, information technologists, architects, musicians and artists (or, if we wish, “visual operators”) and graphic designers live and work together, often exchange not roles but models and objectives. Electronic art occupies this space». [25] In parallel, the system of relationships, events and production centers that conveyed and supported “electronic art”, also firmed up. While in previous decades New Media Art was rooted in the universities and research centers, in the 80s New Media Art became an independent “art world” in its own right and laid the foundations for its continued existence. On the networks debate was conveyed above all on the BBS, while in the real world New Media Art was distributed at temporary events like technology and electronic art festivals, in line with the Linz model. Towards the end of the decade the first “New Media Centers” appeared, really taking off in the early 90s. The advent of these new distribution channels outside of the traditional art world gave the “third culture” fairly sound foundations in terms of visibility, critical debate and preservation. Yet in this regard Italy remained a fairly isolated case. Despite the presence of an active, vibrant art scene (with artists and groups like Tommaso Tozzi, the Giovanotti Mondani Meccanici, Correnti Magnetiche, Mario Canali, Studio Azzurro, Giacomo Verde and, later on, Piero Gilardi and Maurizio Bolognini), the lack of institutional involvement led to a proliferation of autonomous, isolated initiatives, the result of voluntary efforts by curators like Mario Costa and Maria Grazia Mattei, conducted mostly in private venues or peripheral institutional settings. Even now Italy has no Media Centers, and its few active festivals struggle to make a name for themselves internationally.

1989 is a pivotal year in terms of gaining insight into the subsequent fate of New Media Art, and could indeed be taken as the symbolic date in its process of institutionalisation. The initial setting for this was Europe, where specialized institutions (art centers, museums, workshops, archives and festivals) flourished at an unprecedented rate. It was in 1989 that the ZKM (Zentrum für Kunst und Medientechnologie) of Karlsruhe (Germany) was founded, a center that could, broadly-speaking, be seen as the leader of this process. In the same year the fall of the Berlin Wall and the Soviet empire opened an entirely new season, for art too. Russia, together with the countries of Eastern Europe, was obliged to speedily institutionalize contemporary art, which to date had been developing in unofficial situations like squats and private homes. This process was heavily influenced by the billionaire philanthropist George Soros with his Soros Centers of Contemporary Art (SCCA). As Lioudmila Voropai writes, [26] there were some interesting aspects to this process of institutionalization. In the first place, New Media Art had always stressed its “social utility” and contribution to the creative development of the New Media, thus adding to the legacy of confusion between the development of the medium and its use for artistic purposes, between “New Media” and “New Media Art”. This confusion was accompanied by the ambiguous and conflictual relationship between New Media Art and contemporary art, and was indeed one of the reasons behind the conflict: the social utility of New Media Art implicitly opposed the non-utility of contemporary art, which not coincidentally bases its economy on a luxury market.

The conflict between the two became even more pronounced when they were made to coexist in the same institution. The institution in question was the ZKM, the very notion of which speaks volumes about the nature of the relationship between contemporary art and New Media Art in the early 1990s. The two different art worlds coexist here, like a separated couple still sharing the same roof, thanks to an apparently virtuous division into a series of “institutes” and departments, coordinated since 1999 by the director Peter Weibel: the Museum of Contemporary Art, founded in 1999 and also a venue for temporary exhibitions; the Media Museum, which has a permanent, and unique, collection of “interactive media art”, accompanied in recent years by a number of “permanent exhibitions” on the latest developments in New Media Art; the Institute for Visual Media, the center’s “research and development” division (founded and directed by the artist Jeffrey Shaw until 2003); the Institute for Music and Acoustics, the Institute for Media, Education, and Economics, and the Filminstitute. In reality the ZKM only opened its premises, in a converted industrial area, in 1997, but it prepared the terrain with a series of temporary initiatives, like the Multimedia festival of 1989. Its vocation, linked to the orientation of its director (or rather the duo Weibel – Shaw) and its origins in the early 90s, made it into a temple for the interactive, immersive and technologically groundbreaking installations of the last decade of the century, so much so that in Europe the expression “ZKM art” is normally used, tongue in cheek, to refer to this kind of art. [27] Criticism aside, the ZKM has the undisputed merit of being the first in the 90s to raise the question of the “museification” of New Media Art, and issues related to how to preserve it and create a canon, in this way establishing a model for other international players, like Tokyo’s Intercommunication Center (ICC), founded in 1990 and given a permanent venue in 1997. Back in Europe, we have already seen how in the 90s various long-standing institutions like Ars Electronica and V2_ reinforced their position. In the Netherlands sizeable institutional investments in the new media led to the foundation in 1990 of the Inter-Society for the Electronic Arts, or ISEA, that organizes the International Symposium on Electronic Art. This association, which moved its headquarters to Montreal in Québec from 1996 to 2001, before returning to Holland, has an extremely international outlook, as evinced by the itinerant nature of the symposium, always staged in a different location. In Germany, the Institute for New Media (INM) in Frankfurt was set up in 1989 as an experimental workshop in the context of the Art School, before evolving into an independent research platform for post-graduate students. 1988 saw the founding in Britain of the FACT in Liverpool (then known as Moviola), which remains the country’s most important New Media Art institution. These are just a few examples on an international panorama in constant expansion. In this context it is inevitable to take a brief look at what was going on in Eastern Europe, not only for the significant contribution it gave to the development of New Media Art in the 90s, but because what went on there in the space of a decade appears to encapsulate the entire history of New Media Art. In Eastern Europe, up to the 90s, avant-garde art existed entirely outside of the institutional sphere. The Open Society Institute & Soros Foundation Network was the first to make a serious move in this direction. As of 1991 SCCAs were set up in 17 former Soviet block countries. These were relatively shortlived: in 1999, after the Soros foundations were restructured, all the SCCAs became independent non-governmental organizations. For many of them this meant tackling the crucial issue of funding, not always an easy task where public funds for culture were in relatively short supply. But some managed to survive. Supporting New Media Art was one of the key missions of the SCCAs. This came about because in an area where the personal computer was still a rarity and a status symbol, the social utility of the centers lay in their ability to guarantee the population (and the artists) access to the network and the new technologies. In postsocialist countries there was no tradition of New Media Art: information technology was linked to military uses and scientific research, and the embargo which followed the war with Afghanistan effectively prevented Western-made technologies from arriving in Russia. Yet the networking that got under way, and the widespread use of the network, enabled New Media Art to flourish. In 1993 the SCCA in Moscow set up its New Media Art Laboratory, led by Alexei Isaev and Olga Shishko. In 1994 the artist Alexei Shulgin established the Moscow-WWW-Art-Lab, and in the same year Gallery 21, a no-profit venue in the famous quarter of Pushkinaskaya 10 – a squat converted into an art center – opened its doors in St. Petersburg. Leaving Russia, Budapest saw the opening of the C3, the Center for Culture and Communication, which is still up and running, and which combined the traditional functions of an art center with teaching activities, holding courses and workshops on Internet and the new technologies, while Ljubljana opened the Ljudmila Digital Media Lab, promoting festivals and events, and supporting the artistic activities of Vuk osi, one of the pioneers of Net Art. As Voropai notes, the post-Soros era began during the golden age of New Media Art in the West. 1999 was the year of net_condition, a travelling exhibition organised by the ZKM, which opened the season of the major museum exhibitions, destined to continue – above all in the States – until 2002. In Russia the decline of the New Media institutions gave rise to a difficult situation. The affirmation of an uncertain, poorly regulated art market, buoyed up by the new rich, who saw art as a way of laying claim to elite status, did not favour New Media Art, which was held – rightly or wrongly – to be an institutional art form.

This is the situation that has come to pass, in a more recent period, and with the same dynamics, in the West. Here the development of a system of New Media Art, by means of the dynamics we have attempted to illustrate, has gone hand in hand with increasing interest from traditional artistic institutions. Yet the latter tend to be uninterested in the underground tradition of New Media Art, and focus their attention on its most recent results, connected to the mass spread of digital technologies and the advent of the web in the second half of the 90s. Indeed at the start of the decade there were as yet few artists using “domestic” technologies with some degree of awareness, to make art: figures like the Italian Maurizio Bolognini, who in the early 1990s produced installations in a highly conceptual vein by reprogramming and “sealing” personal computers in such a way that their vitality and continued functioning, perceptible as a monotonous hum, could be detected but not visualised through any output devices; [28] or like the German artist Wolfgang Staehle, who in New York in 1991 used various BBS to found The Thing, conceived as a “social sculpture” à la Beuys. And while home computing remained the main arena for the formation of the digital cultures of the 90s, at the start of the decade New Media Art focused above all on immersive systems and virtual reality, telepresence and interactivity (with figures like Jeffrey Shaw, David Rokeby, Paul Sermon and, back in Italy, Mario Canali, Piero Gilardi and Studio Azzurro), technological prostheses and robotics (Eduardo Kac, Stelarc), and 3D graphics and generative algorithms (Karl Sims). But this work involved the use of cutting edge technologies, and was too focused on the latest developments in technology and too detached from the developments in contemporary art in that period to be properly interesting in this context. With the advent of the World Wide Web (Mosaic, the first commercial browser, appeared in 1994), and the mass distribution of the personal computer (1995), this situation changed radically. The computers of the 90s were cheap and featured an intuitive interface; anyone, with a minimum of instruction (which was often undertaken in universities, in the workplace or, for the young generations, by means of videogames) could use them. Processing text, modifying images, and creating sound and video files were relatively simple matters. At the same time the web gave the internet network a multimedia, hypertext interface based on a programming language (html), the basics of which can be picked up in a few days. Making art with a computer no longer required technological training, access to research labs, collaborations with engineers and professionals. Anyone could do it, and not necessarily to make art that was accessible only via computer. So while on one hand computers could be used by any artist, they could also be employed by anyone wishing to exploit the extraordinary communicative, aesthetic and narrative potential of the web. Net Art came about in this very way. It was no longer a question of creating the finest image possible with a given tool, or generating an immersive interface, but about exploring and subverting an elementary language, creating a short circuit in communication, infiltrating a global communications medium. The first net artists did not come from the New Media Art of previous years, but from photography (Alexei Shulgin), post-conceptual art (Vuk osi), film (Olia Lialina), street art (Heath Bunting), painting (Mark Napier) and video (Jodi); they had an artistic, rather than a technological training; some turned to the web out of frustration with the contemporary art world, others were fresh out of art school, and others had links with political activism, which in that very period was beginning to realise the web’s unprecedented potential for media impact (Ricardo Dominguez). Net Art was ironic, subversive and played with the limits of meaning; it looked to the avant-garde and neo avant-garde movements; it practiced pastiche, collage and linguistic games, and it was the output of an era of cultural production that eliminated the difference between original and copy. Net Art originated between 1995 and 1997. In 1997 Documenta, one of the most important dates in the contemporary art calendar, had a section devoted to Net Art. The year before, the Swiss collective etoy won a Golden Nica at the Prix Ars Electronica, in the “World Wide Web” category, for the work Digital Hijack, a spectacular operation of search engine manipulation that diverted hundreds of thousands of internet users onto their site. [29] In the “Computer Animation” category, the first prize went to Pixar, for the animated movie Toy Story (1995), the first movie produced entirely using computer graphics. In the photograph that commemorates the event, an etoy agent with a shaved head and mirror sunglasses, in an orange jacket and black trousers, shares the stage with Japanese interactive artist Masaki Fujihata, Canadian electroacoustic music composer Robert Normandeau, and writer and film director Pete Docter from Pixar: they are all smiling, but they seem to be wondering what they are doing on the same stage. And the question is by no means irrelevant: while 1989 was the key year for consolidating the New Media Art world, 1997 was the annus horribilis of the split between the art and its world: the moment when so called “new media artists” started wondering what they had in common, besides the medium and their under-recognition by mainstream art worlds. The events that we have described, from the eighties onwards, appear to be entirely concentrated in Europe. So what was going on with the States, the homeland of the new technologies and the first artistic experiments in this direction? Lev Manovich accounts for [30] the American delay on this front with two simple considerations. In the first place, the rapidity with which the new technologies were assimilated in the States made them invisible in a very short space of time. In other words, in the US there was no hiatus between the arrival of a new technology and its normalization, the hiatus that enables artists to develop a critical distance from the medium. Secondly, Manovich blames the lack of institutional support, at least compared to areas like Western Europe, Australia and Japan, where the New Media Art world leaned heavily on public funding in the 80s and 90s. In the States the art world is market-driven, and in that context an artistic practice that had always professed its unsaleability had trouble getting by for many years. This, at least, was the case until the late 90s, when the situation changed completely. Universities and art schools set up courses and programs of New Media Art and New Media Design; prestigious academic publishers like the MIT Press began producing books on the subject; renowned institutions like the Princeton Institute for Advanced Studies, the Rockefeller Foundation and the Social Science Research Council set about organizing conferences, prizes and funding, and the major contemporary art museums, from the Whitney Museum of American Art in New York to MoMA, from the San Francisco Museum of Modern Art to the Walker Art Center in Minneapolis to the Guggenheim in New York, together with numerous university museums, got involved with exhibitions, programs and curatorial positions. Even some private galleries, like the Postmasters Gallery in New York, staged solo and group shows of New Media Art. Various no-profit organizations (often led by artists) also appeared, along with specialized institutions like Eyebeam in New York, while existing structures like the Electronic Arts Intermix (EAI) founded by Howard Wise in 1971 and mainly focussed on video, opened up more substantially to the digital media. In other words, interest in New Media Art exploded in the States at a period in which the New Media sector was gaining financial thrust, and New Media Art was becoming financially and technically sustainable for any artist. This phenomenon, however, was fairly short-lived: after the collapse of the New Economy, and the consequent disappearance of the funding that had boosted interest in it, the enthusiasm of American museum system cooled off considerably. At this point the American New Media Art scene was faced with two alternatives, both of which it explored. On one hand it attempted to tackle the arduous task of integrating into the contemporary art system and its market. On the other it looked to Europe with interest, attempting to come up with an alternative model for survival that would enable it to preserve its specific characteristics.

As Arthur Danto wrote, [5] from the sixties onwards (namely from the acceptance of the new “paradigm” introduced by Marcel Duchamp in the 1910s with his first readymades) anything and everything could be art, as long as there was an internal reason for which a given thing should be considered art. Identifying this reason, however, is not always easy. Francesco Bonami, in a book that sets out to explain to the man in the street “why contemporary art really is art”, spectacularly fails in this mission by adopting oblique strategies that constantly avoid the question. In the introduction, Bonami explains that to understand a work of art «all you need is an open-minded approach», curiosity and courage, and that the important thing in art is not the technique, but the idea, which has to be “new” and “right”: «The important thing, in any case and if possible before others get there before you, is to think the right thing at the right time». [6] Yet Bonami does not explain the concept of “new”. In this complete absence of rules, the only one that appears to withstand scrutiny, and that Bonomi returns to frequently, is the central role of the idea. The “right idea”, “good contents”, is the only thing that links Duchamp, who «learned how to generate hot air better than others», and the “reactionary” art of Lucian Freud, who paints «as if Duchamp and Warhol had never existed». I have mentioned Bonami’s dumbed-down aesthetics, rather than more structured theories, because I think it reveals something significant about the arena we are analyzing. One of the most renowned international critics and curators, Bonami does not seem to base his work on a specific “idea of art”. He seems to operate more like a water-diviner, who can see art where others cannot – and is almost always in the right place. Obviously this is possible because when Bonami makes his choice, he has the authority and the means to impose it as the “right” choice to other members of the art world: a consideration that implies a contextual definition of art, according to which art is art because there is a surrounding context that says it is. As Blais and Ippolito explain, [7] this idea is nothing more than intellectual provocation (that of Duchamp) turned intellectual inertia (that of today’s art world). If a work of art is defined by its aura, and if in the age of its technical reproducibility that aura is no longer an integral part of it, the process of “conferring” that aura – namely the work of critics, museums, gallerists and dealers – does not follow but actually precedes the recognition of an object as a work of art. Art is art because critics write about it, museums exhibit it and collectors collect it, not vice versa; the aura is the consequence of this intellectual attention, the interest of the museums, the investments made by collectors, and so on, rather than the cause. [8] This theory, which crops up not infrequently among both those within the art world, and those criticizing it from the outside, is undoubtedly an enthralling one. Also because, once embraced, it is very easy to find evidence to back it up, and very difficult to find arguments against it. By way of example, it is all too easy to look at Damien Hirst, one of the stars of today’s art world, and see the results of canny investments made by an advertising mogul (Charles Saatchi), an extremely solid art world (the English establishment), an unprecedented eye for business (that of the artist) and the concerted efforts of museums, collectors, galleries, critics and curators. It is more difficult to explain why his colored dots mesmerize us, why his butterfly wings fascinate us and why his pharmacies and animals in formaldehyde embody our angst more than many other present day works of art. In other words, it is more difficult to understand whether we would have recognised these pieces as works of art before the art world lent them an aura, variously boosted by the torrents of words used to describe them, the floods of money spent on buying them and the sacral ambiance of the white cube. This problem obviously arises from the weak nature of the few attempts that have been made to come up with a definition of art that transcends the contextual theory. Bonami’s “theory of the right idea” encapsulates this weakness fairly well. Even a vastly more sophisticated theory, like that of the philosopher Mario Perniola (2000) does not seem to yield the results hoped for. Today «we consider it “natural” that some objects are works of art and that some people are artists; any other question seems superfluous», [9] Perniola writes. But just what is it, aside from economic worth and communicative value, that makes art art? According to the philosopher, the answer to this question lies in art’s shadow, «a shady form which contains the most unsettling and enigmatic elements that belong to it». Yet Perniola refuses to define this shadow, conscious that by nature it «disappears when exposed to the light». We can at best identify only a few components of that shadow – the “splendour of the real”, the “sex appeal of the inorganic”, the “logic of dissent”. But shedding light on it necessarily means making it vanish. What seems to emerge from all these “weak” theories is the need for strong contents, art’s ability to home in on an issue, objectivize it and present it for our analysis. This also gives rise to prejudice against media specificity, and art that is not “just art”. This prejudice is linked on one hand to the “damnatio memoriae” that struck Clement Greenberg in the States, and on the other to the fact that art appears to have entered a “postmedia” phase that best manifests itself in multimedia installations, and the nomadic shifting between different media that characterizes the work of many artists. In particular, according to Rosalind Krauss, medium specificity was overcome around the 1970s, on one hand by Marcel Broodthaers with his “eagle principle”, that «simultaneously implodes the idea of an aesthetic medium and turns everything equally into a readymade that collapses the difference between the aesthetic and the commodified»; [10] and on the other by video that, sharing the «television’s “constitutive heterogeneity”», proclaimed the end of medium specificity. «In the age of television, so it broadcast – Krauss writes – we inhabit a post-medium condition». [11] Which does not mean that staying with one medium is inappropriate, or that exploring the specific characteristics of that medium is a cardinal sin. Krauss tries to explain this in another essay, significantly entitled “Reinventing the medium”. According to Krauss, a medium can be rediscovered and reinvented by artists in the post-medium phase when it has fallen into obsolescence: not to explore its creative and aesthetic potential, but to examine it as a “theoretical object” of art.
In Remainder, the first novel by the English artist and writer Tom McCarthy, the main character has survived an accident, followed by a grueling rehabilitation process, that has left him with partial memory loss, but compensation of several million pounds. With this money the character attempts relentlessly to regain the authenticity of some brief episodes of his past and present life by faithfully reconstructing and reenacting them. His first project involves reproducing the atmosphere of a house he believes he has lived in. The setting is reconstructed in great detail (down to the cracks in the walls, the black cats on the roof in front, the sounds and the smells), and various “reenactors” are hired full-time to enable him to relive these moments whenever he feels like it. This is followed by other “projects”, staged with the involvement of hundreds of professionals and “reenactors”: the obsessive reconstruction of a minor accident he once had in a gas station, a murder, a bank robbery. All of this is done to enable him to relive the tingling feeling he experiences when authenticity is achieved. At one point someone asks him: «Does he, perhaps, […] consider himself to be some kind of artist?» To which he replies: «No. I wasn’t any good at art. In school». [13] These lines are telling. They reveal that today’s art is not something you learn at school, and is not necessarily associated with traditional artistic techniques. They also say that art is something visionary and gratuitous; it is not to do with objects, but projects, and it does not produce anything of use, but requires total dedication, generous funds and the involvement of many different kinds of professionals. The artist figure that emerges from this picture is still firmly anchored to the romantic vision of the genius, obviously updated to today’s standards. Figures like Olafur Eliasson, who created waterfalls cascading down the struts of New York’s bridges, and Matthew Barney, who spent five years of his life producing an unprecedented cycle of films, conceived in its entirety as a sophisticated allegory of male genitalia, embody this idea to perfection. The romantic genius acquires celebrity status, and is required to be an excellent entrepreneur of him or herself: think of figures like Damien Hirst, Maurizio Cattelan and Francesco Vezzoli, and further back Jeff Koons and Andy Warhol. If we descend gradually from art’s lofty pinnacles into the complex, variegated fauna of artists, many of these aspects fade away, but the one constant, the one thing we always expect from an artist, is absolute devotion to a project, an idea. With this one lodestar established, everything else is up for discussion, renegotiation. The mythos of complete freedom also admits the option of choosing an entirely reactionary path – that of manual skill, technical prowess, obsessively nurturing a single language. Artists can hide their identities behind a pseudonym or a collective: in this way an academic painter like John Currin can rub shoulders with the likes of Jeff Koons, who has skilled craftsman producing his marble busts. And while the latter, who places himself at the center of many of his works, explores – and reinforces – the cult of the personality of the artist, in contemporary art it is not difficult to come across collaborative platforms, in which individual contributions merge into collective output: the existence of collectives like the Indian RAQS Media Collective – a platform that operates on an artistic, critical and curatorial level – comes as no surprise.
In Mercanti d’aura, Alessandro Dal Lago and Serena Giordano assert that the notion of “purpose” represents an insurmountable barrier to an object being a work of art. If an object has a purpose, it cannot be art, because art serves no purpose; it exists unto itself. And the writers go one further, maintaining that objects created to serve a purpose (therefore the products of worlds such as that of fashion, design and the entire cultural industry) possess disturbing properties that make opposition to them particularly vehement. These objects disturb us because they are artworks in all respects, but also «services marked by the stigma of subordinate work». [14] This theory is undoubtedly a fairly convincing one. Conceived by the aesthetes of the late nineteenth century, the idea of art for art’s sake has stayed with us, in various different forms, in the art and criticism of the twentieth century. Yet continuing to envisage the world of contemporary art as an ivory tower under constant threat from base, secondary practices, is frankly anachronistic. All of the arts have their own “art world”, and most of the artifacts they generate can only be appreciated according to the canons of those worlds. Yet each of these worlds can produce – has produced and continues to produce – a series of artifacts (usually a fairly limited series) able to fulfil the conditions of another world, for example that of contemporary art. This happens for various reasons: because the historic schism between some of these “art worlds” is actually a fairly recent thing, and because certain phenomena that are part of the mythology of contemporary art, like modernism, envisaged a reconciliation that continues to crop up at regular intervals – and, lastly – because the contemporary art world, intended as an arena of free experimentation, unfettered by ulterior motives, has always been particularly receptive to approaches and figures viewed as anomalous by the other art worlds. In other words, the skin of the contemporary art world is much more porous and permeable than that of other worlds, and while it may have proved slightly less porous at some periods in its history, the period that began in 1989, with the fall of the Berlin wall and the recovery of the art market after the recession at the end of the 80s, was undoubtedly particularly open to contamination. In his critical and curatorial work Germano Celant has often highlighted this.
This situation has given rise to two movements: one of appropriation, which encourages artists to engage with other media, be it importing them into the contemporary art world or shifting towards those others worlds, and one of convergence, which sees many hybrid, borderline figures (filmmakers, designers, musicians, etc.) bringing their works into the arena of contemporary art. This does not happen, as might be expected, only on the “borders of the empire”, but at its summit, involving figures of prime importance. Think of Matthew Barney and Shirin Neshat, who have taken works to the Venice Film Festival; think of the numerous artists who have directed Hollywood movies (from Robert Longo to Kathryn Bigelow to Julian Schnabel); or Pierre Bismuth, who won an Oscar for his screenplay for the film Eternal Sunshine Of The Spotless Mind (2004), written with the director Michael Gondry. And think, too, of Takashi Murakami’s collaboration with Vuitton, the double identity of Carsten Nicolai (who also works as a musician, going by the name of Alva Noto), and Peter Greenaway’s nomadism. All of this is also facilitated by internal developments in the contemporary art world, which is increasingly forging a presence as one of the sectors of the cultural industry and show business. And museums and institutions, traditionally more conservative, are facilitating this process, hosting exhibitions devoted to fashion and design, in ways that can be debatable and are indeed debated, but are undeniably forging a trend
The question of how all this is to be reconciled with the traditional conception of the visual work of art, intended as an artifact that is unique (or reproduced in limited editions), collectible, and therefore financially valuable, is constantly being renegotiated, and obviously entails some interesting compromises. In the contemporary art world value is attributed by means of a complex system that includes criticism, museums and other institutions, prizes, exhibitions and the market. Not being able to deal with each of these players singly, I will consider above all the market, which, in my analysis, represents the missing link in the world of New Media Art. The art market has played a key role in the world of visual arts since the nineteenth century, when the arts began gradually severing their ties with the nobility and institutional powers, becoming a private activity mainly destined for the cultured bourgeoisie in search of the social prestige that only a productive relationship with the world of culture can confer. Particularly after the Second World War, art became increasingly bound up with the market: in this way, while the “dematerialization of art” became possible in a period when the market was relatively weak, when the market recovered in the 1980s, and there was a resurgence in demand for marketable artifacts, traditional practices like painting and sculpture rose to the fore once more. The collapse of the stock market in 1989, together with other crucial factors – the new geopolitical situation, and AIDS wiping out an entire generation of artists – played a key role in changing the lie of the land in the early nineties. The phase which followed this, and which is still under way, is a complex one for various reasons. Globalization is bringing forth new art scenarios, new exhibiting platforms and new markets; major temporary art events, like the biennales, are springing up, creating new destinations for cultural tourism; contemporary art museums are being revamped, testing the terrain of the global museum, and becoming artistic objects in their own right, with containers that are often more appealing than their contents, boosting the number of services on offer and becoming focal points of a society in which the services sector, media and culture play a key role; and lastly, the advent of the information society has generated an exponential increase in platforms for criticism, with the launch of dozens of new magazines. The art market spearheads this transformation. Private galleries stage events; by means of contemporary art fairs they increasingly condition the construction of museum collections; by paying for advertising space in art magazines they finance art criticism, and even if the relationship forged between the two is not, at least in the most virtuous cases, a genuine exchange, they inevitably end up conditioning the choices made. Art fairs have grown exponentially in the last decade and some of them (like Art Basel, Frieze or New York’s Armory Show) have established themselves as primary cultural events, key destinations for global tourism, on a par with museum exhibitions and biennales. Lastly, auctions, the main arena for the so-called “secondary market”, have gradually opened up to contemporary art and the so-called “primary market”, their fluctuations influencing the careers of artists. In The Art Fair Age (2008), the Spanish critic Paco Barragán defines art fairs as «Urban Entertainment Centers», [16] and contemporary collecting as a pyramid: on the bottom layer, art is sought after as “social capital”, a source of prestige and affirmation; on the next level art is collected as “financial capital”, namely for its investment value; on the third level of the pyramid we find companies who view art as a “brand” of sure-fire appeal, and include it in their market strategies, while at the top we come to private collectors who seek intellectual fulfillment from art. And the latter are increasingly putting their collections into the public domain, by means of donations to museums (like Giuseppe Panza di Biumo), taking over established institutions (like the new Palazzo Grassi owned by the French entrepreneur François Pinault) or setting up their own foundations (like the Fondazione Sandretto Re Rebaudengo in Turin), thus boosting their influence over the process of institutionalization. The close bond between the contemporary art world and its economy was incisively analysed by the English critic Julian Stallabrass in his book Art Incorporated (2004), which explicitly focuses on the «regulation and incorporation of art in the new world order». [17] According to Stallabrass, art’s micro-economy, governed by a handful of dealers, critics and collectors, is precisely what ensures its freedom from the rules of global capitalism and mass culture. Yet at the same time contemporary art can be seen as a giant metaphor for the capitalist system, with which it has more than one affinity. After demonstrating that the salient characteristics of the art of the 90s – multiculturalism, the success of the installation and the emphasis on youth – are closely linked to its economy, Stallabrass dwells on the way in which the economy of the art world conditions production. The author explains that, while most other art worlds are based on an economy of usage, the core business of contemporary art consists in the «production of rare or unique objects that can only be owned by the very wealthy, whether they are states, businesses or individuals» (p. 102). In recent decades this idiosyncratic economy has had to come to terms with the existence of technically reproducible languages, giving rise to some bizarre compromises: while on one hand photographic works and video exist on the market in very limited series, highly-priced and accompanied by an authentication, on the other, artists like Jeff Koons and Takashi Murakami create digital images which they then get professionals to paint, transforming an infinitely reproducible file into a unique artwork, using a practice (painting) that is manual and entirely traditional. And the ups and downs of the market also obviously influence the type of art that is produced. In the eternal struggle between traditional (and easily marketable) languages, and more difficult forms, the former experience a predictable revival at every economic boom, while the latter emerge more forcefully in every recession, in a «predictable and mechanical process» (p. 107). As for the artists, the idiosyncrasies of the system almost always relegate them to poverty. While there are a few big names who manage to make a killing, most artists are at the lower end of the earning scale. Poverty is at once a side-effect of the particular workings of the system, a contradiction and an ideal: poverty suits art. The artist’s is a high level profession, usually practised by people of high social extraction but low income, who often fund their art with other activities. As Stallabrass concludes: «As a whole, the art market is an archaic, protected enclave, so far immune from the gales of neoliberal modernization that have swept aside so many other less commercial practices. Its status grants it social distinction and a degree of autonomy, even sometimes from the odd market that is at its basis» (p. 114). We might object by asserting that Stallabrass’ vision is a bit too prosaic, that art is something else altogether, something not so exclusively tied to the fortunes of the market. We could object that the present period as it will be reconstructed in two hundred years’ time will have little to do with auction prices, corporate investments and collectors. This is true up to a point, given that the fluctuations of the art economy influence critical debate and the construction of museum collections, as Stallabrass warned us right from the beginning: «the art world is layered vertically and heterogeneous horizontally, comprising many overlapping spheres of association and commerce» (p. 25). This can also be said of the other art worlds, and it is exactly what makes it difficult to reason systematically. At the same time, it is on this horizontal plane that the various worlds intersect, mutually influencing their respective fates.
As we have seen, the world of New Media Art came about to offer artists wishing to experiment with technologies of all kinds the opportunity to do so, removed from the constrictions and limits of a world, the contemporary art world, which is strongly conditioned by its economy and a critical predilection for contents above the exploration of a medium. Far from challenging this configuration, New Media Art criticism merely takes it for granted, and replicates it ad infinitum, to the point of asserting, as Edward Shanken does in Media Art Histories, that contemporary art has never accepted New Media Art because it has always rejected the interpretative model based on the relationship between art, science and technology. [18] Which would imply that it can only be interpreted in this way. In 2006 Gerfried Stocker, director of the Ars Electronica Center and the yearly festival connected to it, returned to discuss this idea of art. The text, rhetorically entitled “The Art of Tomorrow”, [19] is significant from various points of view. Indeed Stocker acknowledges that the current developments in new technologies call for a rethink of the structure and functioning of a festival like Ars Electronica, but does so basically without challenging the idea of art it is based on, namely that art is «a test-drive of the future» (p. 7); that Media Art is «an experiment […] that often brings the creators and proponents of this “new art” into an association with engineers and researchers» (p. 11); and that its basic characteristic is its ability to go beyond an instrumental use of the media as a «medium of representation», making the media not only its tool and medium, but also its subject matter, triumphantly concluding.
In Art of the Digital Age, Bruce Wands [21] depicts the digital artist as someone equipped with technological skills and a good dose of «technological curiosity»; often a programmer, used to working in collaboration with other programmers and IT engineers; attracted to new technologies and viewing art in terms of research and experimentation; a risk-taker who readily veers off the beaten track of established languages and forms to venture into new terrain. Though this definition does not add anything new to what we have said so far, it is an interesting one from various points of view. In the first place, New Media Art appears to have entirely overcome the romantic conception of the artist as genius, and seems to be more interested in returning to the Renaissance models of artist as artisan and artist as scientist. Familiarity with programming also takes the New Media artist into another sociologically interesting terrain: that of hacking (used here in its original sense, freed from the negative connotations attributed by the mass media). It goes without saying that many New Media artists are, and consider themselves to be hackers, to all intents and purposes, and have much in common with hacker ethics: great enthusiasm for their work, limited interest in making a profit, a propensity for knowledge sharing and a belief in the free circulation of information. [22] In 2003, the Net Art group [epidemiC] engaged with this, activating a curious social short circuit. Invited to take part in the Ars Electronica festival, [epidemiC] created Doubleblind Invitation: a program that, if visualized in code form, looked like a beautiful piece of “obfuscated code”, namely formatted like a calligram – a technical feat which holds great kudos in the hacking world, where there are competitions devoted to this particular art form. Yet if executed, [epidemiC]’s code sent out emails – seemingly on behalf of the curator Christiane Paul – to dozens of hackers, fans of obfuscated code, inviting them to take part in the festival. The responses from the invitees, some embarrassed, some enthusiastic, show both the proximity of these two similar cultural niches, and the basic divergence between their two different approaches to programming. This portrait of the New Media artist, albeit an abstract one, appears so far removed from the type of artist cultivated by the contemporary art world that we might be tempted to think that the difference between the two worlds is a question of anthropology rather than history. And while, as we have seen, the contemporary art world is permeable enough to occasionally accept anomalous figures entirely unconnected to the notion of the “career” artist, the appeal of an art world basically without any kind of market economy, devoted to developing knowledge and exploring the arena of digital media, remains strong. Casey Reas is a case in point. Reas is an American artist whose work consists in defining processes and translating them into images. In other words, Reas writes programs that, when executed by a computer, generate animated images that can, if desired, be translated into videos or prints. Unsatisfied with the existing tools, in 2001 Reas, working with the artist and designer Benjamin Fry, created Processing, an open source programming language and freely downloadable program for the creation of images, animations and interactive installations. [23] Processing is now used by a slew of artists, designers and researchers, and obviously Reas himself, who utilizes it in his work. Although Reas works with galleries, he considers himself above all a programmer, designer and researcher: he writes books, holds conferences and coordinates the department of Design and Media Arts at UCLA; and while the resulting products (prints, videos and installations) are produced in limited series, his programs are released with an open source license. He earns his living mainly through teaching and holding workshops on Processing around the world. It is not difficult to come across stories like these in the New Media Art world, just as it is not difficult to meet artists who put their own talent and efforts at the service of temporary collaborative experiments, voluntarily sacrificing their own authorship.

I want to start with the proposition that in a place like New York City, we live in the over-developed world. Somehow we overshot some point of transformation. A transformation that didn’t happen, perhaps couldn’t happen. But in having failed to take that exit, we end up in some state of over-development. In the over-developed world, the commodity economy is feeding on itself, cannibalizing itself.
There is of course an under-developed world, sometimes in intimate proximity to the over-developed one. You can find it even here in New York City. One can critique the orientalism of the fact that Willets Point, Queens is known among New Yorkers as ‘little Calcutta’, but it really is a place without paved roads, running water, and with mostly off the books, illegal or precarious jobs.
But you can forget that under-developed world exists if you live in the bubble of the over-developed world. Some of us don’t have to do the manual version of precarious labor, at least. But there is a sense in which some characteristics of that labor have actually found their way into the over-developed world as well.
Viewed from inside the bubble of New York, the paradox of digital labor these days is the way that tech enables the over-development of under-development. Technologies are shaped by the struggle over their form. It was not given from an essence that the digital would end up as control over labor rather than control by labor. But in the current stage of conflict and negotiation, the over-development of under-development seems to me to describe a tendency for labor.
In any case, labor isn’t the only class struggling in and against the digital. I still think there is a difference between being a worker and being a hacker. I think of hacker as a class category: there is a hacker class. Hackers are those whose efforts are commodifed in the form of intellectual property. What they make can be turned into copyrights, patents or trademarks.
The hacker class is distinguished by a few qualities. It usually means working with information, but not in a routine way. It is different from white-collar labor. It is about producing new arrangements of information rather than ‘filling in the forms.’
As such, it can be a bit hard to make routine. New things just don’t appear on time. Not if they are really new. There’s a kind of ‘innovation’ that is actually quite close to routine, and the hacker class does that too. It’s the new ad campaign, the new wrinkle on the old technical process, the new song or app or screenplay. But the big qualitative leaps are much harder to subordinate to the reified, routinized forms of labor.
The ruling class of our time, what I call the vectoral class, needs both these kinds of hack. The vectoral class needs the almost-routine innovation. The existing commodity cycles demand it. As our attention fades and boredom looms, there has to be some just slightly new iteration of the old properties: some new show, new app, new drug, new device.
What is interesting at the moment are the strategies being deployed to spread the cost and lower the risk of this routine innovation. This is what I think start-up culture is all about. It spreads and privatizes the risk while providing privileged access to innovation that is starting to prove its value to the vectoral class, whose ‘business model’ is to own, control, flip, litigate, and – if absolutely necessary – even build out new kinds of intellectual property.
The other kind of hack, the really transformative ones, are another matter. To some extent the vectoral class does not really want these, no matter what the ruling ideology says about disruption. Having your life disrupted is for little people. The vectoral class doesn’t like surprises. Its goal is to come as close enough to a monopoly in something to extract rent from it.
The kind of mode of production we appear to be entering is one that I don’t think is quite capitalism as classically described. This is not capitalism, this is something worse. I see the vectoral class as the emerging ruling class of our time, whose power rests on attempting to command the whole production process by owning and controlling information. In the over-developed world, an information infrastructure, a kind of third nature, now commands the old manufacturing and distribution infrastructure, or second nature, which in turn commands the resources of this planet, which is how nature now appears to us.
The command strategies of the vectoral class rely on the accumulation of stocks, flows and vectors of information. The vectoral class turns information as raw material into property, and as property into asymmetry, inequality and control. It extracts a rent from privatized information, held as monopoly, while minimizing or displacing risk.
One strategy is to socialize the risk of the real hack. This is probably why public universities and publicly funded research still exist. The tax-payer can take the risk on the really basic research. The university research park model is now set up to carefully modulate access to information about anything that might make a valuable property.
Another strategy is what one might call auto-disruption. Learning from the mistakes of the old capitalist firms of the industrial eras, this model takes the hacker practice in-house. Firms with existing rent-extraction revenue flows become hoarders of potentially monetizable intellectual property, or the people who look like they could produce it. This is to be deployed only when it disrupts somebody else’s business more than one’s own.
So that’s the vectoral class. The problem with belonging to the hacker class in a world the vectoral class rules are these: firstly, certain modest forms of the hack now fall into an outsourced, ‘casualized’, even amateur kind of economy. Certain competences became widespread that there is no way to extract value from them as skills. Certain models of distributed or algorithm-based path-seeking turn out to work as well as hiring the top talent to pick a path for you.
Secondly, more higher-order hack abilities might still command their own price in the market, and one might even end up owning a piece of whatever one produces. But it becomes less and less likely that you get to own it all. One becomes at best a minor share-holder in one’s own brain.
Of course the situation with the worker is even worse than the hacker. The commodification of the life-world eats into the old cultures of solidarity and equality. Everything becomes game-like, a win-lose proposition. The world of third nature, that Borgesian data map that exactly covers its territory, is quite literally programmed to be anti-social.
In daily life there can be a continuum of experience between being a worker and a hacker. They are not absolute categories from the point of view of experience. One can pass from one to the other. Both can be precarious ways to make a living. The white male ‘bro-grammer’ is not the only kind of hacker, just as the blue collar hard-hat is not the only kind of worker.
For worker and hacker alike, the dominant affects are those of envy and jealousy, and covetousness. One is supposed to hate those with just a bit more than you, while at the same time loving those with much, much more. Those with a bit more must be undeserving; those who own everything apparently do so with unquestionable right.
For worker and hacker alike, there is a struggle to achieve some kind of class consciousness, and a social consciousness even beyond that, against the atomizing affect of the time. I just don’t think it is quite the same class consciousness.
For labor, it is always a matter of solidarity and equality. For the hacker, class consciousness is always modulated by the desire for difference, for distinction, for recognition by one’s real peers. It is a sensibility that can be captured by the bourgeois individualism propagated by the vectoral class, but it is not the same thing. Winning the stock-option lottery is not the same thing as the respect of one’s peers. Nor does it translate into any agency in giving form to the world.
It may not come as any surprise that the world this work and these hacks are building is one that cannot last. One might as well say already that this is already a civilization that does not exist. The material conditions that afford it are eroding already. Whether we are adding to the world some quantity of labor or some quality of hack, it is as if we were just building more sandcastles while the tide comes in.
This is the meaning of the Anthropocene: that the futures of the human and material worlds are now totally entwined. Just as Nietzsche declared that God is dead, now we know that ecology is dead. There is no longer a homeostatic cycle that can be put right just by withdrawing. There is no environment that forms a neutral background to working and hacking.
Just as the category of ‘man’ collapses once there is no God, so too the category of the social collapses when there is no environment. The material world is laced with traces of the human, and the human turns out to be made of nothing much besides displaced flows of this or that element or molecule.
The dogma that ‘reality is socially constructed’ turns out not so much to be wrong as to be meaningless. What all the workers and hackers of the world are building is more and more of the same impossible, nonexistent world. We are building third nature as the hyperreal.
Two tasks present themselves, then. The first is to think the worker and hacker as distinct classes but which have a common project. The second is to think that common project as building a different world. Can this infrastructure we keep building out, this second and third nature, actually be the platform for building another one? Can it be hacked?
It is a dizzying prospect. This is why I turn to the work of Alexander Bogdanov, because he thought it could be done. Sometimes it is good to have ancestors, even if they are funny uncles and queer aunts rather than the patriarchs. Bogdanov was Lenin’s rival for the leadership of the Bolshevik party. Shunted aside by about 1910, he turned to two projects, which went by the names of tektology and proletkult.
I think of Bogdanov’s tektology as a project of worker and hacker self-organization that would use the qualitative medium of language rather than the quantitative one of exchange as the means for conveying forms, ideas, diagrams, from one design problem to another. Could there be an art of sharing what works? Could a hack that derives from one design problem be floated speculatively as a possible form or guide for another? Bogdanov’s tektology is like a philosophical Github.
I think of Bogdanov’s proletkult as a project of autonomous worker and hacker cultural production. Bogdanov had a positive, rather than a negative theory of ideology. We all need an affect, a story, a structure of feeling that is really motivating and connecting. Can we be moved and joined by something other than envy, greed, spite, rage or the other click-bait of the game-ifed, commodifed, hyperreal world? Can there be other worldviews and worldviews of the others?
In a way tektology is the work and proletkult the play aspects of building an actual world, in the gaps and fissures of this unreal one that surrounds us. The keynote for Bogdanov was that this had to be a cooperative and collaborative project, based on the worldview of the hacker and worker. This would be a different worldview to both those of authoritarianism and exchange.
We have to think how things work without assuming there is someone or something in charge, a final God-like arbiter, even if it is the hyper-chaos God of the speculative realists. And we also have to think how things work without imagining there’s just a bunch of atomized monads, competing with each other, where the ideal order is magically self-organizing and emergent.
We need another worldview, one drawn out of what is left of the actually collaborative and collective and common practices via which the world is actually built and run, a worldview of solidarity and the gift. A worldview that works as a low theory extracted from worker and hacker practices, rather than a high theory trying to legislate about them from above.
It is not hard to see here what infuriated Lenin about Bogdanov. For Bogdanov, both proletkult and tektology are experimental practices, of prototyping ideas and things, trying them out, modifying them. There’s no correct and controlling über-theory, as there is in different ways in Lenin or Lukacs. There is more of a hacker ethos here, rather than that of the authoritarian worldview one still finds in a Lenin or a Lukacs or in parody form in Zizek, where those in command of the correct dialectical materialist worldview are beyond question.
In Bogdanov’s worldview, there is no master-discourse that controls all the others. There is a continuum of practices, from the natural sciences, through engineering and design, to culture and art. The science and design part is mostly covered by the idea of a tektology; the culture and art part by proletkult. But they overlap, and both matter.
Bogdanov’s openness to the natural sciences, engineering and design are, I think, very contemporary. We only know about things like climate change — and other signs of the Anthropocene — because of the natural sciences. Without the very extensive global knowledge hack that is climate science, we would literally not know what the hell is going on around us. Why these droughts? These floods? These weird changes in the ranges of species, or their sudden extinctions or population booms? None of it would make sense.
Neither Heidegger nor Adorno have anything to say about any of this. But curiously, Bogdanov almost figured out global climate change for himself, between 1908 and 1920. He understood something about the carbon cycle. He understood the need to think social labor as acting on and in and with and against nature, producing a second nature and even a third. He understood the need to build an infrastructure that could adapt to changes in its interactions with its conditions of existence.
Lenin conducted a vigorous campaign to excommunicate Bogdanov, one which the Marxist tradition has strikingly never really revisited or attempted to reverse. This is among other things a great injustice. Bogdanov’s kind of experimental, open-ended Marxism, which neither tries to dominate, ignore, or subordinate itself to the natural sciences, became something of a rarity. His closest contemporary analog is, I think, Donna Haraway. Or so I argue in Molecular Red.
The Anthropocene calls not so much for new ways of thinking as for new ways of practicing knowledge. When the going gets weird, the weird turn pro. And it is likely to get weird — in this lifetime, or the next. That’s why I think we could start working now, not on theory of the Anthropocene, but theory for the Anthropocene. One could do worse, I think, than imagine and practice again something like a tektology and a proletkult – a tektology for hackers, a proletkult for cyborgs. Let’s build a world, and live in it.

The fate of cultural studies in the United States appears to be twofold. On the one hand, it still generates moral panic. Right-wing nut-jobbers think that “cultural Marxism” is some insidious, decadent creed, probably created by Jews and Blacks to destroy America. On the other hand, it has finally become seamlessly commodified. Dick Hebdige, once known as the author of a famous book about subcultures, is now a character in a novel by Chris Kraus that has been optioned for a TV pilot by the makers of the popular show Transparent. These two modes of recuperation were, incidentally, what Hebdige thought was the fate of all subcultures.

Hebdige broke new ground by rescuing subcultures such as the British mods, rockers and punks from the clutches of criminologists who could only think of them under the heading of deviance, and who at their most open-minded wanted to send the social workers after them rather than the cops. For Hebdige, subculture was rather a matter of culture and aesthetics, a form of “resistance through rituals.” This became a rather influential approach, not least in the art world, which is always on the lookout for new sources of aesthetic value, even if it means slumming it.

Angela McRobbie thought this was fine as far as it went, but that Hebdige tended to see subculture as something that mostly young working class men get up to. What happens instead if one looks at the self-making of young working class women as subculture? That too was fine as far as it went, but one has to ask whether the noise and resistance of late twentieth century British subcultures was a generalizable phenomenon. Maybe it was of its time and place. Maybe it was an artifact of a declining industrial working class crossing with the rise of broadcast-era consumer culture in the space of the city.

In her book Be Creative (Polity 2016), McRobbie updates the subculture story, tracing the fall-out from the clash of subculture with the culture industry through to a more recent obsession with the precariat and the creative industries. Is there more than a mere change in terminology here? And what can be learned by tracing the paths of young working-class women through this more contemporary urban landscape?

Drawing on Stuart Hall, and others in the cultural studies field including Dick Hebdige, Paul Gilroy and Andrew Ross, McRobbie takes a close interest in the utopian possibilities of everyday culture, but subjects it to a critical scrutiny, attendant to how popular aspirations are coopted by the commodity form or channeled elsewhere by disciplinary power. The people make culture, but not in a context of their own choosing.

Firstly, McRobbie has to bring the story up to date. Hebdige was writing about a time when subcultures appeared as noise, interrupting the orderly repetitions of the mass culture industry. Mass industrial work at least afforded mass industrial leisure. Subculture was, among other things, a displacement of the aspirations of the working class, shifted from struggles at the sites of mass production to the sites of mass consumption.

That was the case in the sixties and seventies, and the pattern was at least partially recognizable through to the club culture of the eighties and beyond. But from the rise of New Labour in 1997 onwards, all that fell away. Elements of youth subculture got imported into the creative industries. The night time economy of club culture translated into endless work days. A rapid capitalization of the cultural field led to a celebrity media sphere of a more individualistic kind, one that encouraged self promotion and self exploitation. With it came a more extensive detachment from community and class culture. The solution to social problems lay in getting out and getting on.

And what is the role of university in managing this? There is a certain irony here, as “the unexpected outcome of cultural studies is to have found itself canonized as a curriculum for a new creative economy.” (9) Now McRobbie, who teaches at London’s Goldsmith’s College, encounters students who aspire to work in the creative industries, who are often young and childless, but who juggle endless part time jobs while trying to get their degree. The university exists both as a form of credentialing, but also as place for networking.

Those part-time jobs are a means to an end, to an idea of a creative life. Subculture used the space of leisure as one in which a creativity suppressed at and by work could flourish. Now the idea is that work itself can be the site of that expression. Work becomes a kind of romantic relationship. “Work has been reinvented to satisfy the needs and demands of a generation who, ‘dis-embedded’ from traditional attachments to family, kinship, community or region, now find that work must become a fulfilling mark of the self.” (22)

Of course, all this independent, creative work ends up dependent on centrally owned and controlled infrastructure, from which a new kind of ruling class extracts the rent. Around it buzzes the old kind of petit-bourgeois ‘ducking and diving’, of trying one’s hand at this and then that, rather than specializing in a trade or profession. Young people function as the crash-test dummies for new styles of living this old kind of work, as passionate and involving. The older kind of petit-bourgeois could not dream of a million instagram followers.

McRobbie is sensitive to the ambivalence and ambiguity of all this. “What starts as an inner desire for rewarding work is retranslated into a set of techniques for conducting oneself in the uncertain world of creative labor.” (37) From the point of view of the young worker, its about autonomy; from the point of view of the state, “it is a matter of managing a key sector of the youthful population by turning culture into an instrument of both competition and labor discipline.” (38)

Marx had imagined that the petit-bourgeois strata would become progressively proletarianized as big capital moved in and colonized its various market niches. McRobbie describes something like the opposite phenomena. Various strata of what was once a working class is made petit-bourgeois. Capital non longer owes them even a factory or an office to work in. The absence of security is presented entirely as a good thing, as a lack of routine. The generalized urban economy, no longer of culture industry (singular) but the creative industries (plural) gives the young, particularly young women, a feeling of going places. It presents the endless possibility of personal success.

While I am skeptical as to how useful a concept neoliberalism might actually be, it does help account for some aspects of how class is subjectively lived today. Following Foucault, McRobbie traces neoliberalism to the ordo-liberals, German state functionaries and intellectuals who had kept their heads down during the Nazi years, and came up with a more palatable right wing philosophy after it. One that, in an irony of history McRobbie doesn’t mention, displaced the state-socialism of those intellectuals who had rallied to the British state in the cause of defeating the Nazis. The ordo-liberals redefined the human not in terms of labor but as an entrepreneur of its own life-force. It is a kind of market-vitalism, which proscribes a narrow set of rules for human conduct, the sole objective of which is, in every sense, self-appreciation.

While political theorists may dream of such a neoliberal subject, what actual subjects end up thinking and feeling and doing may be a bit more complicated and interesting, and that is McRobbie’s bailiwick. “I see passionate attachment to creative work as comprising ‘lines of flight’, embedded family histories of previously blocked hopes and frustrations.” (46) The class politics of the parent culture that is submerged in commodification used to reappear as subculture, but now (post) subculture is no longer an injection of noise against the hegemonic order but the seeding of new information for it to commodify. Meanwhile the industriousness sustaining the creative industries is provided by a ‘risk class’ without permanent jobs. Creativity promises the reward of realized self; insecurity appears as part of the adventure.

This all seems to confirm the work Eve Chiapello and Luc Boltanski did on how the ruling class responded to the challenge to its hegemony in the sixties by resisting one line of attack yet incorporating the other. The line resisted was the labor critique, in the form of wildcat strikes and factory occupations. The line that was incorporated was the artistic critique, which spoke not of labor but of alienation. It turns out that extracting value out of labor could function just fine without rigid, externally imposed discipline and uniformity. McRobbie: “While the prevailing value system celebrates the growth of the creative economy and the rise of talent, the talented themselves are working long hours under the shadow of unemployment in a domain of intensive under-employment, and self-activated work.” (153) McRobbie works this observation through a study of the work of Richard Florida, Ricard Sennett and the Italian ‘workerist’ school and its descendants, such as Paolo Virno, Franco Berardi and Maurizio Lazzarato.

McRobbie offers a less rosy view than that of Richard Florida, with his celebrations of thecreative class that populates prosperous cities, for whom the old working-class districts become gentrified playgrounds. McRobbie points out that Florida’s sunny vision is the flip side of what for Loic Waquant is a decline of sociological explanation about how urban space actually functions. The occupation of the city as a space for creative class play takes place against a background of mass incarceration which criminalizes a whole other urban population. On the one hand, part of what was subculture can become the creative industries; but on the other, a part of it no longer gets the social-worker treatment but goes straight from school to prison.

Where Richard Florida celebrates the hipster version of the creative industry, Richard Sennett prefers a more traditional version of of the value of ordinary work and craft labor. As McRobbie notes, there’s something patriarchal and conservative about some of Sennett’s attraction to old guys working steadily with their tools, but there might yet be something to draw out of this counter-model to the embrace of the creative industries.

Sennett sees work as life-enhancing and not mere drudgery. Here he thinks about labor quite differently to Hannah Arendt. However, for Seennett, changes in work may have led to a corrosion on character. Perhaps this could be reversed by returning to older habits of cooperative labor with their ethic of the job well done. McRobbie thinks there might be value in putting creative work alongside supposedly ‘uncreative’ craft work to counter the romance of creation, although one has to wonder if this is just another romance.

There are also tricky issues here of how any kind of labor might give rise to some form of intellectual property separable from the thing itself, which might at one and the same time yield an ‘author’ — and owner — and on the other a means of controlling the market in a particular line. Unfortunately, McRobbie does not pursue this, perhaps because her example is fashion, where intellectual property intervenes mostly at the level of the brand and the trademark rather than the individual designs.

All the same, Sennett offer a way of thinking about craft labor as one of the rhythms of the city, which has a certain value in its impersonality. It is a less grandiose way of thinking work; less about genius, talent, inspiration and competition, to which one might add — less about intellectual property. “A craft approach means being able to work all the time with failure…” (156) Craft skills are within the reach of most people. Its not an elite sensibility. Its reach is local. All well and good. “But the patient labor of craft is likely to remain a distant ideal for freelancers working on a piece-rate system and having to cut corners.” (158)

Craft may have little place in the contemporary cities of the over-developed world, with their sundering of local ties, temporary social relations, and relentless corporate culture of ‘team work.’ There might be some capacity for resistance (or — dare we hope — political innovation) embodied through memory and family history, but it may no longer take a subcultural in form. Perhaps it is in the residues of a craft sensibility which show up in the creative industries. Sennett provides a ‘parent culture’ view. Crafters and artists could do well to look it up, as it is their story.

McRobbie wonders too what would happen if the kinds of labor traditionally thought of as women’s work get the same treatment in Sennett as his craftsmen. “Where it may be fruitful to downgrade the dizzy expectations of artists and creative people so that they can sit alongside others, and benefit from the time-slow pace of a mode of working that gratifies on the basis of a job being done for its own sake, it proves more difficult to upgrade some stubbornly unrewarding jobs such as domestic cleaning.” (160)

One might pause here to consider the loss or invisibility of familial or community connection to such an ethos of craft labor. Thus the neo-bohemians populating Chicago’s Wicker Park in Richard Lloyd’s study are not able to see who is no longer living among them. Meanwhile thedigital artisans Andrew Ross studied at New York’s Razorfish advertising agency could be self-ironizing about their cool sweatshop and the cult-like commitment it extracted from its associates, but they have not much appreciation of how their laptop creations could end up as the decorations on merchandise made in actual sweatshops. In any case, things have moved on. Wicker Park and Razorfish are names from a forgotten era, even if it was only a decade ago.

Rather than try to keep up with the ever-changing fascination with cool neighborhoods and cool employers, McRobbie returns to a study Jacques Rancière did in the seventies, about nineteenth century workers. In Proletarian Nights, Rancière looked to workers whose aspirations were not limited to forming unions or cooperatives or political parties, or even to demanding the abolition of work. These deserters from the class struggle wanted a different kind of work. They wanted independence, and they expressed that desire in things like poetry — of an often quite formal and traditional kind.

McRobbie connects this to the British cultural studies tradition, which had shifted attention from the sphere of production to that of consumption in order to understand how the desires and ambitions of labor had sought expression there. Taken together, these parallel French and English approaches took an interest in non-traditional kinds of ‘politics’, if that is still the word for it. The British approach, more cultural than capital-P political, took the disco or the kitchen table as significant sites. McRobbie: “These communal, familial, collective or indeed institutional spaces permitted alternative working lives to be imagined. Cultural studies therefore anticipated a neo-Marxism open to difference and diversity, open to the equal stature of the family and the community alongside that of the workplace and the sphere of formal politics.” (58)

But this popular culture of labor’s aspirations and capacities could in turn be instrumentalized. In the British context, the significant changes happened under so-called New Labour. Creativity became a kind of labor reform, in which the artist would stand as a model for a new kind of human capital. McRobbie: “These were the Damian Hirst times.” (42) Art and culture were put to work. This was a kind of transitional model, replaced in more recent times by the idea of tech-centric innovation. The basic formula is not that different, however. The new-model worker is to aspire to apply creativity to achieving individual success and celebrity. In both its creative and innovative flavors, this is a model hostile to traditional or ‘elitist’ version of either culture or the social. It can sometimes have a vaguely inclusive rhetoric. It is meritocratic, but does not pause for too long to ask if winners really started from the same starting-blocks as the losers. And of course, it never presents all this from the worker’s perspective. Workers are supposed to go away. The labor movement is replaced by networks of demassified, autonomous free agents.

Ironically, cultural studies itself became a kind of textual material that could be reworked into this image. For example, Paul Willis on working class youth creativity got repurposed in the language of ‘New Times’ post-labor politics, initially sponsored by the Gramscian wing of whatever was left of the Communist Party. This in turn became language for New Labour. It would be churlish to hold against cultural studies what others did with it. McRobbie defends Stuart Hall as as trying to cope with rise of post-Fordism and its effects on consumer culture via a new popular politics. “Hall’s expansive ideas for how the left could forge a new popular politics were taken up and deflected in unexpected right wing directions…” (68)

A characteristic of British culture that sets it part a bit from the United States is that for a long time had maintained a public education system that opened a pathway into the arts for talented and often disaffected working class kids. For McRobbie and others in cultural studies, this was the site of displaced anatagonisms from the factory floor. Where that tradition ended however is probably with the artist-celebrity as champion of so-called neoliberal ideas of self-making, of which Tracey Emin and Damien Hirst would be avatars.

What the ordo-liberals probably did not anticipate was that the artist would in some ways become the ideal type of the neo-liberal subject, who would engage in a knowing self-exploitation in pursuit of the dream job. McRobbie thinks there are three subtypes of the artist, although I don’t find the categories too convincing. They are the socially engaged artist, the global artist and the artist-precariat, with the latter forming a kind of critical refusal from within in the ideal neo-liberal subjectivity of the artist. McRobbie: “The rhizomatic tactics and strategies of such creative activities are totally incommensurate with the vocabularies of the toolkits and business studies modules and thus can be seen as a direct challenge to the ‘entrepreneurial university.'” (84)

Where the modernist artist was the exception to the culture industry; the contemporary artist is the exemplar of the creative industries. The category of the creative industries is of interest not least because it blurs the line between fine art and applied art, or craft. Postmodern art’s stylistic complications of the boundaries between the aesthetic and kitsch seem tame compared to the extended commodification of the production of information of all kinds.

McRobbie is more interested in the more ‘vulgar’ kinds of creative industry and the young women drawn to them. There she finds enthusiastic career girls, performing elaborate body rituals that are coded by a kind of post-feminist masquerade. They perform so-called immaterial labor and emotional labor, or what McRobbie calls “passionate work.” (89) They don’t entirely disavow class or ethnicity or community. They just see a narrow path to a more passionate life that involves some compromises. Normative femininity is a way to cover over traditional working class traits that may be disabling in the workplace. Feminism opened up a path of opportunity but one now reclaimed by a more traditional-seeming code of femininity.

“Capitalism makes a seductive offer to young women with the promise of pleasure in work, while at the same time this work is nowadays bound to be precarious…” (105) These women tried to refuse work as a way to escape from monotonous jobs in favor of self-directed activity, but this then has become recuperated too.  McRobbie: “… the idea of ‘romance’ has been deflected away from the sphere of love and intimacy and instead projected into the idea of a fulfilling career.” (91)

Compared it to Italian workerist thinkers, where a rather masculinist approach to politics remained standard, McRobbie’s cultural studies approach opens up some interesting questions were labor and gender combine. By contrast to the ‘Bologna school’, the Birmingham school moved from the factory floor to everyday life and uncoupled different kinds of struggle. McRobbie: “without a concept of ‘culture,’ the idea of ‘the street’ can only connote a weaker space which is not the shop-floor and hence not primarily an expected location for class politics. In this thinking the idea of the factory floor still takes precedence even when the workforce is in flight from it.” (95) Where the workerists spoke of the social factory; cultural studies might speak of the social kitchen. A change of metaphor here might alter how we think of what became of both labor and culture in the era of the so-called creative industries.

The workerists still treat the classic class antagonism of capital and labor as central, whereas the culturalists treated the political and cultural levels of the social formation as equally substantive. In the spirit of EP Thompson and Raymond Williams, cultural studies saw culture as a popular landscape of resistance and protest. The workerists thought there was a new kind of subjectless class politics to which post-Fordist production processes as the response. “Yet lacking a strong concept of working-class culture these authors can only rest their case on the refusal of work…” (97) Even if they were never quite clear what refusal meant. Meanwhile, the culturalists expanded Gramsci’s conception of a popular culture (although after Gilroy no longer necessarily a national one) as a common resource.

McRobbie retrieves from the workerists the idea of the line of flight, the desire to escape, and mobility as response to labor. Of course not all workerists (and post-workerists) are enthusiasts for these lines of flight. Lazzarato and Berardi, for example, becomes quite pessimistic. McRobbie’s question is whether young women get the same chance at the ‘immaterial’ as young men. Is today’s labor market just as (or more) gender segregated? Is there a return to a kind of neo-traditional sexism? Or, more broadly: “How then can we talk about the gender of post-Fordism?” (101)

McRobbie: “… refusal is more of a desire and a yearning for rewarding work, something that is within sight and perhaps within reach through access to further and higher education. This ‘flight’ also acquires gendered characteristics. The impact of 1970s feminism made the idea of a career for young women completely acceptable. Unlike the autonomist Marxists, I do not make such fulsome claims for a new radical politics emerging from the ‘social factory,’ instead I see a field of ambivalence and tension, where lines of flight connect past parental struggles with the day-to-day experiences of their children in the modern work economy.” (93)

Despite the utopian promise of passionate work as escape from traditional labor, it ends up being a desire that can in turn be exploited. Passion becomes a means of production, complete with precarity, long hours and low pay. “I pose the idea of passionate work being a distinctive mode of gender re-traditionalization… whereby the conservatism of post-feminism re-instates young women’s aspirations for success within designated zones of activity such as creative labor.” (110) Passionate work becomes self-exploitation, complete with its own codes of affect — a permanent appearance of enjoyment — and a bodily style of exuberant enthusiasm.

Creative work has become separated from ordinary labor, but does it follow as McRobbie thinks that it is thereby depoliticized? Maybe there’s another kind of politics for something that is not exactly labor. It is the case that a wedge was driven between creative labor and other kinds, thereby weakening social democratic politics. But perhaps the strategy is not then to bring the former back into the latter. McRobbie sometimes sounds as nostalgic as Sennett, if not exactly for the same image of the past. Rather than a neoliberal vocabulary of the entrepreneur, or the old social democratic one of industrial labor, perhaps its time to think of another one that might more accurately map onto the class formations of our time. Rather than reverse the neoliberal turn, lets take a new turn.

McRobbie encourages us to look to less masculine-coded practices for signs of possibility. She is interested in women crafters who make public women’s traditional skills, such as the yarn-bombers weaving public artworks that knit bicycles to lampposts. There’s an ambivalence to these scenes. In part, they look back to a rather traditional and idyllic culture of femininity. On the other, they sometimes draw from those pasts to create more self-consciously feminist practices in the present. As always with culture, there’s tensions and ambiguities which can be fruitful and interesting. This scene, as an example, is a sort of return to the craft critique of production of William Morris, without the paternalism.

Perhaps it is not quite the same to be making the old things and to be making new information. Perhaps the latter seems so lacking in the history and culture of labor because it isn’t exactly labor. The temporality of its production may not have much in common with the patient persistence of craft-work. Its relation to the commodity form is rather different. It does not produce the thing to be sold as a piece of property, but rather creates an arrangement of information that is novel enough to count as intellectual property. It is so easily copied that other strategies have to come into play to extract value from its production, which is where creating the aura of special skill around particular creators comes into play. The networks within which information is made are partly local, and as yet nothing beats the city as a way of organizing it. But its networks also extend beyond far urban space. The infrastructure of information makes the physical and informational aspects appear quite separate, although there is nothing immaterial about it.

In short, maybe this relatively new kind of producer is as unlike craft labor as it is unlike industrial labor. It comes into being only at a time when information can be private property, and yet information can be rapidly and fully copied ands shared. Its a relatively new set of forces of production that make it possible — information technology. It is both shaped by, and exceeds, the relations of production extruded out of the property form to embrace it — intellectual property. Maybe it even produces quite distinct class relations, between producers and owners of information.

These aspects of the creative industries I find neglected both in British cultural studies and Franco-Italian workerist theory. One might however draw from McRobbie an attention to gender in how creative industries have evolved. If one looks at fashion as the archetypal form of creativity and tech as the archetypal form of innovation, one finds very strong imposition of very conservative ideas about what is men’s work and what is women’s work — even if neither is exactly work any more.

One thing that the left and right now seem to agree on is that the society in which we live is called capitalism. And strangely enough, both now seem to agree that it is eternal. Even the left seems to think there is an eternal essence to capitalism, and only its appearances change. The parade of changing appearances yields a series of modifiers: this could be late capitalism or communicate capitalism or cognitive capitalism or neoliberal capitalism. But short of an increasingly allegorical or messianic leap into something other — it is as if this self-same thing just went on forever.

Maybe its because I have a taste for old-fashioned modernism, but whenever I come across a piece of language about which there is such wide consensus I want to trouble it, somehow. This capitalism that we have all agreed that we live in: has it not become too familiar, too comfortable an idea? The reality the term tried to describe is of course far from comfortable. Capitalism, if this is what this is, appears to be smashing not only the social but the natural conditions of its existence to pieces. But then maybe this is the thing to ask about. Why have we become so comfortable with a way of describing an uncomfortable reality? Do we want a certainty in language that can’t be had anywhere else?

That the world we live in is capitalism has become a familiar way of describing something that destroys what is familiar. It atomizes and alienates. It renders everything precarious except its own grasp on the imagination. If the greatest trick of the devil was to persuade us that the devil does not exist; then maybe the greatest trick of capitalism is to gull us into imaging that there is nothing but capitalism.

It is hard to describe things that change imperceptibly.  This may well be the level of language on which the problem rests. It has to do with using the combinations of language, which have something of a binary quality, to describe changes that might be gradual or might be swift, but which aren’t neat digital divides between one term and another. It is as hard to describe transitions between modes of production as it is to describe changes in mood.

There was once a language about transitions between modes of production. It is striking how the left and the right alike ended up working within the same language about this. Marx really was one of the great modern poets. Of course he worked with the materials of the languages he had to hand, but he wrought something lasting: a combinatory of terms for describing history. Like any great poetic corpus, his work contains multitudes. But there are a few standard permutations than came to stick in the mind, like great pop songs.

Here I think is his greatest hit, one that has become something of an earworm. It goes something like this: this is capitalism. It has an essence and it has appearances. Its essence is defined by these things: the commodity form, with its doublet of use value and exchange value; by labor’s double form, as concrete and abstract labor; by the extraction of surplus value in the production process, by the wage relation, by the rising organic composition of capital, by the crisis of the tendency of the rate of profit to fall. And finally, by negation.

There are actually two variants of the poem here, about negation. Either capitalism negates itself, brought to ruin by its own contradictions. Or: it is negated by a force it produces as its own negation, the working class. In either variant, one thing is key: capitalism can change its appearances, but never its essence. Its essence can only be negated, by contradiction or struggle. Various variant tunes spill out of this rhetorical frame, like mutating like genres of techno music.

There are other variations. One can swap out the abstract verb negation and replace it with acceleration. This is currently popular again, as it was in the twenties. Here the idea is that there’s nothing that can negate capital, either in its own contradictions or in the force it produces in and against itself. Rather, the best one can do is accelerate it to its end, towards a Promethean leap into another historical figure. But note that this is not as much of a change in tune as its advocate like to imagine. It leaves intact the rhetorical form of capital as an essence.

The essence of capital is eternal. This is the striking feature of how it is now imagined. Those who love it of course embrace this thought. It needs merely to be perfected by our love. This is sometimes called, with a stunning lack of imagination — neoliberalism. But what is even stranger is that those who do not love it seem to agree. The essence of capital is eternal. It goes on forever, and everything is an expression of its essence. Capital is the essence expressed everywhere and its expression is tending to become ever more total.

The other side of the eternal essence of capital is its ever changing appearances. Change is accounted for via the use of modifiers. Its appearances can even be periodized. There was merchant capitalism then industrial capitalism, then monopoly capitalism then neoliberal capitalism. There’s some ambiguity as to what to call the current stage, however. It could be multinational, cognitive, semio, late, neoliberal, or postfordist capitalism, to name just a few. Note that the last two of these are temporal modifications to a modifier: neoliberal, postfordist. Could there be any better tribute to the complete enervation of the imagination by capitalism, or whatever it is, that this is the best our poets can do? Modify the modifier? Capitalism must be very disappointed in our linguistic competence.

Of course there’s the opposite rhetorical tack as well, which is to go a bit overboard with the binary difference between two terms, although its partisans have not been so bold as to break too much with the essence of capitalism. Rather, it worked like this: there used to be material labor; now there is immaterial labor. Its a different kind of labor. Its the opposite! But its still only a modified capitalism, a cognitive capitalism. Its not material any more. Capitalism itself is about ideas. Its striking how much one can get carried away with the play of language, and forget to look at the world. Somehow, I don’t think the hundred million industrial workers of China perceive their work as immaterial.

The task of this talk is thus a provocation: to think the possibility that capitalism has already been rendered history, but that the period that replaces it is worse. That it could be worse gets us away from the happy narratives in which capitalism gave way to a postindustrial society or some other magic kingdom, free from contradiction and class struggle. Rather, in this thought experiment, I propose to think the present as a new kind of class conflict, including new kinds of class arising out of recent mutations in the forces and relations of production. But putting this pressure on our received ideas and legacy language, perhaps we can begin to see the outlines of the present afresh, estranged from our habits of thought.

There was once an attempt to have done with at least part of this great rhetorical-historical edifice. It started with questioning the idea of capital as having an essence and an appearance. What if appearances were as equally real as the essence? There were actually two version of the essence-appearance structure. One took economic to be the essence, but in the sense of being the base, and everything else was dependent on it. This version is called economism. In the other version, its not the economic, but the commodity form that is the essence, one that has come into being in history and then become the essence of history, which records its forms of appearance as a false totality, as spectacle.

Against this, some took the view that the economic only determined everything else in thelast instance, that things like politics and culture were not mere appearances but had their own material form, but one whose function was to reproduce the essential economic form of capitalism.

If things like politics or culture are relatively autonomous, if they have their own material form, maybe they even have their own essence! It did not take long for culture to have its own essential categories: the signifier and the signified were just like exchange value and use value! An abstract essence! But a different one! So one could just specialize in singing the song of this (relatively) autonomous world of essences and appearances, while still gesturing to the master-narrative, that this is indeed and will remain, capitalism.

If the economy has an essence and appearances, and culture too has an essence and appearances, then maybe politics does too! The wonderful thing about language is that if you seek it you can find it. Yes, politics has an essence, the great fundamental drama of friend versus enemy, or maybe its dissensus, or something. The main thing is we can sing the song of the essence and appearances of politics, while still gesturing to the master-narrative, that this is indeed and will remain, capitalism.

I have to say that my inner modernist finds this all rather banal. Is this the best we can do to speak the sublime language of our century? Why does it all seem the same? Like pop music? Variations on themes, all leading back to the same old note, that capital is eternal? That one day (that will never come) there will be a messianic leap into something else, but until then, let’s just go to the movies. It seems to me that our poetry of capitalism, or whatever it is, shows all the signs of being a culture industry. Nowhere in these tunes is there that striking note of non-equivalence, or that moment of de-familiarization when the roof falls in.

Perhaps one has to ask: what the emotional attachment that we have to the idea that this is capitalism, and that it is eternal? It has to be said that the most vigorous attempts to tell a different story, to strike a different tune, where made in bad faith. Still are, perhaps. There was a time when it was a popular art form. Once the narrative of capitalism and its coming negation got out, you could make a good living coming up with a different story. Not surprisingly, it was former Marxists and socialists who came up with most of those alternative stories.

Thus we had the story of the managerial revolution, of the postindustrial society, of the conditions for take-off and growth. What these stories all had in common was that they accepted the basic premise of the Marxist story. They conceded its power, its poetry. But they changed the ending. Rather than negation, the story ended in a resolution of contradictions. These were extorted reconciliations. But they had some currency nonetheless. But with the collapse of the supposedly socialist world, which at least pretended to live up to the great Marxist story, these counter-narratives lost their force.

One counter-story from that era survives. It was not written by a socialist, although he briefly worked for a socialist government. In this story, capitalism negates itself, and in a good way. It can pivot and disrupt itself. Indeed, its essence becomes its self-disruption. And it is our sacred duty never to get in its way. This is the rhetorical art-form of the ‘California ideology.’ Into it can be folded certain other variations, about the fourth industrial revolution, for example.

The conceit of all these post-capitalist stories was that this is not capitalism, it’s better! When people hear the beginnings of a story about this no longer being capitalism, their resistance generally rises at this point. Unless you happen to be worth several million dollars, the chances are you do not perceive this as something better than capitalism.

But maybe it would be interesting, politically and aesthetically, to take the other fork of the binary here. Instead of the idea that this is not capitalism, ts better, what if we explored the idea that this is not capitalism, but worse? This also meets a lot of resistance. This I can tell you from experience, having tried to write variations on this text for fifteen years. Nobody wants to leave the certainty of the devil they know, or think they know, for something that promises to be worse.

Interestingly, few people will even attempt it as a thought experiment. There really is something fundamental to the belief that this is capitalism. It may even be the defining feature of ideology today. Ideology today is not the acceptance of a neoliberal structure of feeling or habits of thought and action. Ideology today is clinging to the belief that this is capitalism.

Another way to tackle this would be impute some meaning to Marx’s famous remark to the effect that he was not a Marxist. What if what he meant by that is that he was not one of those who simply took a language and a rhetorical form extracted from his texts as a given? He was, to the contrary, the one who had constructed that language with a quite particular purpose in mind: to understand the situation of his times from the labor point of view. So: what if we kept the commitment to understanding, not his times, but ours, from the labor point of view, whatever that might mean now — and bracketed off the rest?

That makes a certain sense to me. I really am puzzled by why we should use blocks of linguistic material from his time again to understand our time. Why use the fashionable philosophy, the popular science, the political tracts, or the technological metaphors of the mid-nineteenth century? When poets or novelists do that, we immediately think its dated and quaint. But somehow we want our great narrative to be about capitalism, even if it is dated and quaint.

Of course different genres of text have a different relationship to tradition and innovation, and at different moments in their development. They aren’t always in synch. And of course there’s generally a culture industry in which the texts get pulped into sameness, and an avant-garde trying to do something else. If you are trying to write an interesting, rather than merely successful, novel or poem, you want to change things at the formal level, rather than ship your wine in the same old bottles. The thing is, where readings and rewritings of Marx are concerned, they seem to me to belong to the culture industry. Its a commonplace now to read Capital as a work of philosophy or an epic novel, but to do so very conservatively. And indeed could there be anything more conservative now that the tradition of continental philosophy?

I have not named names in this text, partly to avoid embarrassing its characters. But mainly because I take it as given that texts writes their author, rather than their author writing them. Authors are never good guides to their own writings, as the writings exceed conscious intention — although I would not take that insight as far as the psychoanalytically inclined, who maybe create too big an interpretive playground for themselves out of it. So in describing my own attempt to write within the space, all these caveats also apply to me.

It has not always been the case that Marx was read conservatively, as a great text for explication, interpretation and imitation. Where the Marxocological savant became a master simply by producing a variation on the theme. There are those who read Marx the same way they read Rimbaud and Lautréamont. I’ll mention just three: Aimé Cesaire, JBS Haldane, and Guy Debord. From the latter I’ll also take a few clues about method. Could there be a way to write after Marx that isn’t based on conservative habits of mastery and interpretation, but which are based instead on experimentation and détournement?

Of course, being a very minor poet, I did not get very far. But I gave it a shot. I wrote a way of describing the current situation that is not capitalism, but worse. Here’s how: what if, rather than start at the beginning, one started at the end? The capitalism story always starts in the past, with the birth of capitalism, and imagines a destiny, a teleology, wherein the present must be some continuum from that past. This must be some modification of the essence of the thing. Let’s do it the other way around. Let’s first describe the present, then secondarily figure out where it came from. This may even, in the end, involve modifying our understanding of capitalisms past. In short, let’s start where Marx started, describing a present — not from his results.

Let’s start by being very ‘orthodox’ — (I use the term ironically). Let’s start with the forces of production, with the relations of production that correspond to them, the class conflict generated out of those relations of production, and the political and culture superstructures that correspond to that base. And let us also, just as Marx did, try to describe what may be emerging, rather than what is established. If one starts with what is established, it is easy to interpret any new aspect of the situation as simply variations on the same essence. Starting with what is emerging provides a suitable derangement of the senses, a giddy hint that all that was solid is melting into air.

The thought experiment that might result is quite simple. What if it was like this: There really is something qualitatively distinct about the forces of production that produce and instrumentalize and control information. This is because information really does turn out to have strange ontological properties. Making information a force of production produces something of a conundrum within the commodity form. Information wants to be free but is everywhere in chains. It isn’t scarce, and the whole premise of the commodity is its scarcity.

Information as a force of production called into being particular relations of production. In classic Marxist style, one can look here at the evolution of legal forms. What we see there is the emergence of intellectual property as close to an absolute private property right. One that makes the once separate and local property forms of patent, copyright and trademark equivalent forms of private property. Forms which, as the negotiations on the Trans-Pacific Partnership make clear, need transnational forms of legal enforcement, precisely because information is such a slippery and abstract thing.

And so, like the enclosures or the joint-stock company before it, intellectual property law becomes the form of a new kind of relation of production, more abstract than its predecessors, and one which makes not land or physical plant a form of private property, but information itself. Like those preceding forms of private property, this one gives rise to a class relation. As an absolute form of private property, it creates classes of owners and non-owners of the means of realizing its value. Land as private property gave rise to the two great classes of farmer and landlord. Capital as private property gave rise to the two great classes of worker and capitalist. Is there a new class relation that emerges out of the commodification of information?

For argument’s sake, let’s it’s say it does. I call those classes the hacker class and the vectoralist class. The hacker class produces new information. What is ‘new’ information? Whatever intellectual property law recognizes as new. Its a strange kind of production. Where the farmer grows crops and the worker stamps out units of some thing, the hacker has to make the same old stuff, information, appear in new configurations. Getting this done is not like the seasonal repetitions of farming or the clocking-on of the worker. It happens when it happens, including time spent napping or pulling all-nighters. Hackers can’t be managed like farmers or workers. They are not the same as either class.

Like the farmer and the worker, the hacker does not usually end up owning the product of her efforts. Unless you own a drug company or a tech company or whatever, you have to sell the rights to what you produce. It is not always the same as selling labor-power. You might still own the intellectual property, for example. But the hacker rarely captures the value of what they invent. not everyone gets to be Bill Gates — precisely because there is a Bill Gates, who is not the avatar of the hacker class, but of its opposite — the vectoralist class.

The vectoralist class owns and controls the vector, a term I use to describe in the abstract the infrastructure on which information is routed, whether through time or space. You can own stocks or flows of information, but far better to own the vector, the legal and technical protocols for keeping information scarce.

If one takes a look at the top Fortune 500 companies, it is surprising how many of them are really in the information business. I don’t just mean the tech and telco companies like Apple or Google or Verizon or Cisco, or the drug companies like Pfizer. One could think of the big banks as a subset of the vectoralist class rather than as ‘finance capital.’ They are in the information asymmetry business. And as we learned in the 2008 crash, even the car companies are in the information business — they made more money from car loans than cars. The military-industrial sector also also in the information business. Even the companies that make things like Nike are really in the brand business. Walmart and Amazon compete with different models of the information logistics business. The oil companies are in the prospecting business. The actual oil drilling is contracted out. Perhaps the vectoralist class is no longer emerging. Maybe it is the new dominant class.

That might only be the case in the overdeveloped world where we live. Many of the world’s peoples are still peasants who are being turned into farmers by the theft of their land by a landlord class. Much of the world is a giant sweatshop. The resistance of labor to capital is alive and well in China or India. The older class antagonisms have not gone away. Its just there’s a new layer on top, trying to control them. Just as the capitalist class sought to dominate and subordinate the landlord class as a subordinate ruling class, so too the vectoralist class tries to subordinate both landlords and capitalists, by controlling the patents, the brands, the trademarks, the copyrights, but more importantly the logistics of the information vector.

A side note here: In Capital, Marx really only deals with an ideal-type political economy with two classes. But in his political writings it is clear that he understands social formations as hybrids of combined and overlapping modes of production. Landlords and farmers loom large in his writings on France, for example. So here I’m simply taking my cue from the political writings, and thinking a matrix of six classes, three ruling and three subordinate. The dominant classes are thus: landlords, capitalists, vectoralists. The subordinate classes are: farmers, workers, hackers.

Now imagine all the possibilities of class alliance and conflict that this generates. It turns out that politics is much less about the relation between the friend and the enemy, and much more crucially about relations among non-friends and non-enemies. As anyone who has actually done politics, or knows some semiotics, could figure out.

So how is it worse that capitalism? The vectoral infrastructure throws all of the world into the engine of commodification. There is nothing that can’t be tagged and captured via information about it and considered a variable in the simulations that drive resource extraction and processing. Quite simply, we have run out of world to commodify. And now commodification can only cannibalize its own means of existence, both natural and social. Its like that silent film where the train runs out of firewood, so the carriages themselves have to be hacked to pieces and fed to the fire to keep it moving, until nothing but the bare bogies are left.

It is worse also in that rather than some vague multitude, there’s complex class alliances at play in the political space. The trickiest part of it is the politics of the hacker class. Which after all is the class most of us here belong to. Yes, it sometimes appears as a privileged class. But it is a class that has a very hard time thinking its common interests. Largely because the kinds of new information its various sub-fractions produce are all so different. We have a hard time thinking what the poet and the scientist and the engineer have in common. Well, the vectoral class does not have that problem, what all of us make is intellectual property, which from its point of view is all equivalent and tradable as a commodity.

Also, the hacker class experiences extremes of a winner-take-all outcome of its efforts. On the one hand, fantastic careers and the spoils of some simulation of the old bourgeois lifestyle, on the other hand, precarious and part time work, start-ups that go bust, and the making routine of our jobs by new algorithms designed by others of our very own class. Of course it is always a tough argument to propose common interests among subordinate classes. Counter-hegemony is hard. Hackers, like workers or farmers, are distracted by particular and local interests. Class consciousness is rare among hackers. Most of us are rather reactionary — even in the nontechnical trades. But then class consciousness is always a rare and difficult thing. Unlike other identities, it has to be argued contrary to appearances.

I could add more to the picture, but perhaps that will do for now. Treat it as a thought experiment. Maybe like a science fiction story where you have to suspend disbelief. Or an avant-garde prose poem. That was secretly how I thought about A Hacker Manifesto when I wrote it, although of course I did not tell Harvard University Press that, as everyone knows prose poems don’t sell. I can say that I got that prose poem to sell quite well. And be reprinted, and translated into eight or nine languages. But I think now I can safely reveal that my first crack at this way of experimenting with Marx was an also a stab at a avant-garde prose poem.

It was written, incidentally, in a non-existent language. I wrote it in European. That’s a language, which, if it existed would be equal parts church Latin, Marxism and business English. Maybe that’s why I suspect it reads better in French, German, Italian or Spanish, as those translations are better than my translation of it into English.

So to sum up: what if we took a more daring, modernist, de-familiarizing approach to writing theory? What if we asked of theory as a genre that it be as interesting, as strange, as poetically or narratively as rich as we ask our poetry or fiction to be? What if we treated it not as high theory, with pretentions to legislate or interpret other genres; but as low theory, as having no greater or lesser claim to speak of the world than any other. It might be more fun to read. It might tell us something about the world. It might, just might, enable us to act in the world otherwise.

Some might think it a new low, when a candidate for high office starts talking on television about the size of his penis. As if the regular, non-penile spectacle within which we all live and breathe was somehow some lofty public sphere. But perhaps its more a question of the current stage of spectacular live exposing itself in its ruined perfection. The spectacle has a history. Its current stage is what I have called, in a book of the same name, the spectacle ofdisintegration. I wrote it three years ago, but really to talk about some people who say it coming thirty years ago. Here is how I explain what I think the spectacle of disintegration is and what it means. The book from which it forms the introduction is here. The spectacle of disintegration is this big — a totality, actually.

When the storm hit the Hansa Carrier, twenty-one shipping containers fell from its decks into the Pacific Ocean, taking some 80,000 Nike sneakers with them. Seattle-based Oceanographer Curtis Ebbsmeyer used the serial numbers from the sneakers that washed up on the rain coast of North America to plot the widening gyre of ocean-going garbage that usually lies between California and Hawaii. Bigger than the state of Texas, it is called the North Pacific Subtropical Gyre, and sailors have known for a long time to steer clear of this area from the equator to 50 degrees north.

It’s an often windless desert where not much lives. Flotsam gathers and circles, biodegrading into the sea. Unless it is plastic, which merely photo-degrades in the sun, disintegrating into smaller and smaller bits of sameness. Now the sea here has more particles of plastic than plankton. The Gyre is a disowned country of furniture, fridges, cigarette lighters, televisions, bobbing in the sea and slowly falling apart, but refusing to go away.

New Hawaii is the name some humorists prefer for the North Pacific Subtropical Gyre now that it has the convenience of contemporary consumer goods. Or one might call it a spectacle of disintegration. It is as good an emblem as any of the passing show of contemporary life, with its jetsam of jostling plastic artifacts, all twisting higgledy-piggledy on and below the surface of the ocean. Plastic and ocean remain separate, even as the plastic breaks up and permeates the water, insinuating itself into it but always alien to it.

The poet Lautréamont once wrote: “Old Ocean, you are the symbol of identity: always equal to yourself… and if somewhere your waves are enraged, further off in some other zone they are in the most complete calm. But this no longer describes the ocean, which now appears as far from equilibrium. It describes instead the spectacle, the Sargasso Sea of images, a perpetual calm surrounded by turbulence, at the center always the same.

When Guy Debord published The Society of the Spectacle (1967), he thought there were two kinds: the concentrated and the diffuse spectacle. The concentrated spectacle was limited to fascist and Stalinist states, where the spectacle cohered around a cult of personality. These are rare now, if not entirely extinct. The diffuse spectacle emerged as the dominant form. It did not require a Stalin or Mao as its central image. Big Brother is no longer watching you. In His place is little sister and her friends: endless pictures of models and other pretty things. The diffuse spectacle murmured to its sleeping peoples: “what appears is good; what is good appears.”

The victory of the diffused spectacle over its concentrated cousin did not lead to the diffusion of the victor over the surface of the world. In Comments on the Society of the Spectacle (1988), Debord thought instead that an integrated spectacle had subsumed elements of both into a new spectacular universe. While on the surface it looked like the diffused spectacle, which molds desire in the form of the commodity, it bore within it aspects of concentration, notably an occulted state, where power tends to become less and less transparent.

That the state is a mystery to its subjects is to be expected; that it could become occult even to its rulers is something else. The integrated spectacle not only extended the spectacle outwards, but also inwards; the falsification of the world had reached by this point even those in charge of it. Debord wrote in 1978 that “it has become ungovernable, this wasteland, where new sufferings are disguised with the names of former pleasures; and where the people are so afraid…. Rumor has it that those who were expropriating it have, to crown it all, mislaid it. Here is a civilization which is on fire, capsizing and sinking completely. Ah! Fine torpedoeing!”

Since he died in 1994, Debord did not live to see the most fecund and feculent form of this marvel, this spectacular power that integrates both diffusion and concentration. In memory of Debord, let’s call the endpoint reached by the integrated spectacle the disintegrating spectacle, in which the spectator gets to watch the withering away of the old order, ground down to near nothingness by its own steady divergence from any apprehension of itself.

And yet the spectacle remains, circling itself, bewildering itself. Everything is impregnated with tiny bits of its issue, yet the new world remains stillborn. The spectacle atomizes and diffuses itself throughout not only the social body but its sustaining landscape as well. As Debord’s former comrade T. J. Clark writes, this world is “not ‘capital accumulated to the point where it becomes image’ to quote the famous phrase from Guy Debord, but images dispersed and accelerated until they become the true and sufficient commodities.”

The spectacle speaks the language of command. The command of the concentrated spectacle was: OBEY! The command of the diffuse spectacle was: BUY! In the integrated spectacle the commands to OBEY! and BUY! became interchangeable. Now the command of the disintegrating spectacle is: RECYCLE! Like oceanic amoeba choking on granulated shopping bags, the spectacle can now only go forward by evolving the ability to eat its own shit.

The disintegrating spectacle can countenance the end of everything except the end of itself. It can contemplate with equanimity melting ice sheets, seas of junk, peak oil, but the spectacle itself lives on. It is immune to particular criticisms. Mustapha Khayati: “Fourier long ago exposed the methodological myopia of treating fundamental questions without relating them to modern society as a whole. The fetishism of facts masks the essential category, the mass of details obscures the totality.”

Even when it speaks of disintegration, the spectacle is all about particulars. The plastic Pacific, even if it is as big as Texas, is presented as a particular event. Particular criticisms hold the spectacle to account for falsifying this image or that story, but in the process thereby merely add legitimacy to the spectacle’s claim that it can in general be a vehicle for the true. A genuinely critical approach to the spectacle starts from the opposite premise: that it may present from time to time a true fragment, but it is necessarily false as a whole. Debord: “In a world that really has been turned on its head, the true is a moment of falsehood.”

This then is our task: a critique of the spectacle as a whole, a task that critical thought has for the most part abandoned. Stupefied by its own powerlessness, critical thought turned into that drunk who, having lost the car keys, searches for them under the street lamp. The drunk knows that the keys disappeared in that murky puddle, where it is dark, but finds it is easier to search for them under the lamp, where there is light – if not enlightenment.

And then critical theory gave up even that search and fell asleep at the side of the road. Just as well. It was in no condition to drive. In its stupor, critical thought makes a fetish of particular aspects of the spectacular organization of life. As Todd Gitlin says, the critique of content became a contented critique. It wants to talk only of the political, or of culture, or of subjectivity, as if these things still existed, as if they had not been colonized by the spectacle and rendered mere excrescences of its general movement. Critical thought contented itself with arriving late on the scene and picking through the fragments. Or, critical thought retreated into the totality of philosophy. It had a bone to pick with metaphysics. It shrank from the spectacle, which is philosophy made concrete. In short: critical thought has itself become spectacular. Critical theory becomes hypocritical theory. It needs to be renewed not only in content but in form.

When the American Food and Drug Administration announced that certain widely prescribed sleeping pills would come with strong warnings about strange behavior, they were not only responding to reports of groggy people driving their cars and making phone calls, but also purchasing items over the internet. The declension of the spectacle into every last droplet of everyday life means that the life it prescribes can be lived even in one’s sleep. This creates a certain difficulty for prizing open some other possibility for life, even in thought.

Debord’s sometime comrade Raoul Vaneigem famously wrote that those who speak of class conflict without referring to everyday life, “without understanding what is subversive about love and what is positive in the refusal of constraints, such people have a corpse in their mouth.” Today this formula surely needs to be inverted. To talk the talk of critical thought, ofbiopolitics and biopower, of the state of exception, bare life, precarity, of whatever being, orobject oriented ontology without reference to class conflict is to speak, if not with a corpse in one’s mouth, then at least a sleeper.

Must we speak the hideous language of our century? The spectacle appears at first as just a maelstrom of images swirling about the suck hole of their own nothingness. Here is a political leader. Here is one with better hair. Here is an earthquake in China. Here is a new kind of phone. Here are the crowds for the new movie. Here are the crowds for the food riot. Here is a cute cat. Here is a cheeseburger. If that were all there was to it, one could just load one’s screen with better fare. But the spectacle is not just images. It is not just another name for the media. Debord: “The spectacle is a social relationship between people mediated by images.” The trick is not to be distracted by the images, but to inquire into the nature of this social relationship.

Emmalee Bauer of Elkhart worked for the Sheraton Hotel company in Des Moines until she was fired for using her employer’s computer to keep a journal which recorded all of her efforts to avoid work. “This typing thing seems to be doing the trick,” she wrote. “It just looks like I am hard at work on something very important.” And indeed she was. Her book-lengthwork hits on something fundamental about wage labor and the spectacle, namely the separation of labor from desire. One works not because one particularly wants to, but for the wages, with which to then purchase commodities to fulfill desires.

In the separation between labor and desire is the origins of the spectacle, which appears as the world of all that can be desired, or rather, of all the appropriate modes of desiring. “Thus the spectacle, though it turns reality on its head, is itself a product of real activity.” The activity of making commodities makes in turn the need for the spectacle as the image of those commodities turned into objects of desire. The spectacle turns the goods into The Good.

The ruling images of any age service the ruling power. The spectacle is no different, although the ruling power is not so much a ruling monarch or even a power elite any more, but the rule of the commodity itself. The celebrities that populate the spectacle are not its sovereigns, but rather model a range of acceptable modes of desire from the noble to the risqué. The true celebrities of the spectacle are not its subjects but its objects.

Billionaire Brit retailer Sir Philip Green spent six million pounds flying some two hundred of his closest friends to a luxury spa resort in the Maldives. The resort offers water sports and a private beach for each guest. Much of the décor is made from recycled products and there is an organic vegetable garden where residents can pick ingredients for their own meals. ‘Sustainability’ is the Viagra of old world speculative investment.Sir Philip is no fool, and neither is his publicist. This retailer of dreams has the good sense to appear in public by giving away to a lucky few what the unlucky many should hence forth consider good fortune. And yet while this story highlights the fantastic agency of the billionaire, the moral of the story is something else: even billionaires obey the logic of the spectacle if they want to appear in it.

The spectacle has always been an uninterrupted monologue of self-praise. But things have changed a bit. The integrated spectacle still relied on centralized means of organizing and distributing the spectacle, run by a culture industry in command of the means of producing its images. The disintegrating spectacle chips away at centralized means of producing images and distributes this responsibility among the spectators themselves. While the production of goods is out-sourced to various cheap labor countries, the production of images is in-sourced to unpaid labor, offered up in what was once leisure time. The culture industries are now the vulture industries, which act less as producers of images for consumption than as algorithms which manage databases of images that consumers swap between each other – while still paying for the privilege. Where once the spectacle entertained us; now we must entertain each other, while the vulture industries collect the rent. The disintegrating spectacle replaces the monologue of appearances with the appearance of dialogue. Spectators are now obliged to make images and stories for each other that do not unite those spectators in anything other than their separateness.

The proliferation of means of communication, with their tiny keyboards and tiny screens, merely breaks the spectacle down into bits and distributes it in suspension throughout everyday life. Debord: “The spectacle has spread itself to the point where it now permeates all reality. It was easy to predict in theory what has been quickly and universally demonstrated by practical experience of economic reason’s relentless accomplishments: that the globalization of the false was also the falsification of the globe.” Ever finer fragments of the time of everyday life become moments into which the spectacle insinuates its logic, demanding the incessant production and consumption of images and stories which, even though they take place in the sweaty pores of the everyday, are powerless to effect it.

It is comforting to imagine that it is always someone else who is duped by the spectacle. Former movie star turned tabloid sensation Lindsay Lohan allegedly spent over one million dollars on clothes in a single year, and $100,000 in a single day, before consulting a hypnotist to try to end her shopping addiction. Lohan’s publicist denied the story: “There is no hypnotist, and Lindsay loves clothes, but the idea that she spent that much last year is completely stupid.” The alleged excess of an other makes the reader’s own relation to the spectacle of commodities seem just right. Its all about having the right distance. For Debord, “no one really believes the spectacle.” Belief, like much else these days, is optional. The spectacle is what it is: irrefutable images, eternal present, the endless yes. The spectacle does not require gestures of belief, only of deference. No particular image need detain us any longer than this season’s shoes.

They call themselves the Bus Buddies. The women who travel the Adirondack Trailways Red Line spend five and even six hours commuting to high paid jobs in Manhattan, earning much more money than they could locally in upstate New York. They are outlier examples of what are now called extreme commuters, who rarely see their homes in daylight and spend around a month per year of their lives in transit. It is not an easy life. “Studies show that commuters are much less satisfied with their lives than non-commuters.” Symptoms may include “raised blood pressure, musculoskeletal disorders, increased hostility, lateness, absenteeism, and adverse effects on cognitive performance.” Even with a blow-up neck pillow and a blankie, commuting has few charms.

For many workers the commute results from a simple equation between their income in the city and the real estate they can afford in the suburbs, an equation known well by the real estate development companies. “Poring over elaborate market research, these corporations divine what young families want, addressing things like carpet texture and kitchen placement and determining how many streetlights and cul-de-sacs will evoke a soothing sense of safety. They know almost to the dollar how much buyers are willing to pay to exchange a longer commute for more space, a sense of higher status and the feeling of security.” By moving away from the city, the commuter gets the space for which to no longer have the time. Time, or space? This is the tension envelope of middle class desire. Home buyers are to property developers what soldiers are to generals. Their actions are calculable, so long as they don’t panic.

There are ways to beat the commute. Rush hour in Sao Paulo, Brazil features the same gridlocked streets as many big cities, but the skies afford a brilliant display of winking lights from the helicopters ferrying the city’s upper class home for the evening. Helipads dot the tops of high-rise buildings and are standard features of Sao Paulo’s guarded residential compounds. The helicopter speeds the commute, bypasses car-jackings, kidnappings – and it ornaments the sky. “My favorite time to fly is at night, because the sensation is equaled only in movies or in dreams,” says Moacir da Silva, the president of the Sao Paulo Helicopter Pilots Association. “The lights are everywhere, as if I were flying within a Christmas tree.”

Many Paulistanos lack not only a helicopter, but shelter and clean water. But even when it comes with abundance, everyday life can seem strangely impoverished. Debord: “the reality that must be taken as a point of departure is dissatisfaction.” Even on a good day, when the sun is shining and one doesn’t have to board that bus, everyday life seems oddly lacking.

Sure, there is still an under-developedworld that lacks modern conveniences such as extreme commuting and the gated community. Pointing to this lack too easily becomes an alibi for not examining what it is the developing world is developing towards. And rather than a developed world, perhaps the result is more like what the Situationists called an over-developed world, which somehow overshot the mark. This world kept accumulating riches of the same steroidal kind, pumping up past the point where a qualitative change might have transformed it and set it on a different path. This is the world, then, which lacks for nothing except its own critique.

The critique of everyday life – or something like it – happens all the time in the disintegrating spectacle, but this critique falls short of any project of transforming it. The spectacle points constantly to the more extreme examples of the ills of this world – its longest commutes, its most absurd disparities of wealth between slum dwellers and the helicopter class, as if these curios legitimated what remains as some kind of norm. How can the critique of everyday life be expressed in acts? Acts which might take a step beyond Emmalee Bauer’s magnum opusand become collaborations in new forms of life? Forms of life which are at once both aesthetic and political and yet reducible to the given forms of neither art nor action? These are questions that will draw us back over several centuries of critical practice.

Once upon a time there was a small band of ingrates – the Situationist International – who aspired to something more than this. Their project was to advance beyond the fulfillment of needs to the creation of new desires. But in these chastened times the project is different. Having failed our desires, this world merely renames the necessities it imposes as if they were desires. Debord: “It should be known that servitude henceforth truly wants to be loved for itself, and no longer because it would bring any extrinsic advantage.”

What if we tried a thought experiment? Just for shits and giggles? The thought experiment runs as follows: What if this was no longer capitalism, but something worse? Could we start by describing relations of exploitation and domination in the present, starting with the newest features, and work back and out and up from that?

This might draw our attention to two things. Firstly, to some features of the forces of production. It is still the case that extracting useful organic and inorganic matter from the earth is the basis of social existence. And it is still the case that applying vast amounts of energy in the form of fossil fuels and labor to that base matter is still how the endless array of commodities around us come into existence.

But both those processes seem these days to be subordinated to a third form of relation. At the smallest and largest scales, so much of primary production and secondary manufacturing seems to be controlled by rapid flows and extensive archives and complex algorithms whose concrete existence is in a tertiary form – that of information.

The forces of production that seem most characteristic of the present run on information. They extend all the way into the production process, whether in the form of robots or the detailed and constant surveillance of living labor. They extend all the way out to global networks of measurement, command and control that work in real-time. These networks of information subsume not only inorganic and organic matter and energy in their web but also the human, as we become producers of information even when we are not working. The value of information can be extracted even from nonlabor.

Secondly, the relations of production seem to have evolved to enclose these forces in rather novel extensions of the private property form. Wittgenstein had a rather robust proof of the proposition that there is no private language, but in our time, privatized languages are everywhere, and not just languages. Images, languages, codes, even genes can become private property, produced in quite novel kinds of productive process.

New forms of information, now recognizable as private property, are extracted from a class whose efforts are hardly described by the category of labor, for the simple reason that while labor repeats an action whose form is given in advance, the whole point of these novel processes is to produce unique instances of such forms in the first place. Alongside theworker is the figure of the hacker, producer not of content but of form, and which more often that not ends up being someone else’s property.

One has to ask whether the ruling class presiding over this mode of production is still adequately described as capitalist. It seems no longer necessary to directly own the means of production. A remarkable amount of the valuation of the leading companies of our time consists not of tangible assets, but information. A company is its brands, its patents, its trademarks, its reputation, its logistics, and perhaps above all its distinctive practices of evaluating information itself.

Strangely enough, despite the posthuman turn, in which not just labor but all forms of human attention are subordinated to what Lazzarato calls machinic enslavement, a company is also its personality. Companies really are ‘people’ now, and in more than a legal sense. And they have to be embodied in an actual human, someone the financial markets can believe in.

Some like to talk as if one could just add an adjective or two to capitalism and describe all this. Let’s call it communicative capitalism, semio-capitalism, cognitive capitalism, neoliberal capitalism, or financial capitalism. That sounds comforting at least, as then we know what we are dealing with. But perhaps that’s not quite adequate. Maybe its not the same old familiar endless essence of capitalism cloaked in new appearances.

Maybe the rise of finance is really just a symptom. Yann Moulier Boutang invites us to see finance as something more than speculative excess. It has to do with the whole problem of exchange value in an age where the forces of production are extensively and intensively controlled by information, which is that nobody knows what anything is worth. Financialization is a perverse socializing of the problem of value.

So just for fun, let’s think of it as a post-capitalist mode of production, with a ruling class of a different kind. I call them the vectoralist class. Their power does not lie in directly owning the means of production, as the capitalist class does. Nor does it lie in owning agricultural land, as the capitalists’ old enemy, the landlord class did. And just as there was conflict between capitalist and landlord, so now there is conflict between capitalist and vectoralist.

It was with new forces of production that capital defeated labor in the late twentieth century. But capital in turn finds itself struggling against those who provided the very means of that victory. If one can use an information infrastructure to route around labor’s power to block the production process, one can use the same means to make capitalist producers compete with each other on a global scale.

Remember, this is only a thought experiment. It may have one small merit, and one rather recalcitrant problem attached to it. The small merit is that it enables one to tell a fairly coherent story about what happened between the 1970s and now. For comparison, let’s look at some of the more characteristic language about that period. My examples here are from Erik Olin Wright, Understanding Class (Verso, 2015).

Wright: “The combination of globalization and financialization meant that from the early 1980s the interests of the wealthiest and most powerful segments of the capitalist class in many developed capitalist countries, perhaps especially in the United States, became increasingly anchored in global financial transactions and speculation and less connected to the economic conditions and rhythms of their national bases or any other specific geographical location.” (237) A sentence which, stripped of its decoration, basically says: the cause of financialization is financialization, and the cause of globalization is globalization.

Wright speaks of a period when “global competition intensified” where there was the “integration of commodity chains and production chains” and the “emergence of a global labor force” and even the “dramatic financialization of capitalist economies.” (236-237) With what means? By whom? These are phrases from sentences that don’t consistently link subjects to objects, and which are fond of passive verbs. This is a theory of history that can be summed up as: shit happens.

Of course, these are statements Wright adopts from a consensus language. We have all agreed to talk about financialization as if that just happened, without requiring actual material practices and techniques. We have all agreed to talk about neoliberalism as if that described an actual agency at work that causes things to happen. We have decided not to be Marxists, in other words. We have decided not to subject the language of the times to its own critical pressure. Marx certainly did not take the abstract nouns of his era as a given.

But this brings us to the recalcitrant problem. Its one thing to play with the language of Marxism. It will at least admit modifiers. You can call this neoliberal capitalism, if you want. The essence – capitalism – takes on a particular historical appearance. But one is not supposed to question the metaphysical construct, wherein capitalism is an essence with appearances, which can end only when its productive capacities are exhausted, when the proletariat break through the mere transitory appearance and transforms its essence into socialism, and prehistory has come to an end with the abolition of the last form of class exploitation.

Here let’s look at Wright again, whose work is in many other respects a salutary example of how to bring analytic rigor to the Marxist tradition. He writes: “At the very heart of Marxism as a social theory is the idea of emancipatory alternatives to capitalism.” (121) And “Unless one retains some coherent idea of there being an alternative to capitalism, a Marxist class analysis loses its central anchor.” (167) Hence even in this social-scientific version of the Marxist tradition we’re not far from the metaphysic, in which history can only be understood through an ahistorical concept – of capitalism. Emancipation is though negatively, as emancipation from capitalism. Therefore, this must be capitalism, the negative of emancipation.

Of course, there’s plenty of evidence for this still being capitalism, or mostly capitalism. The question would be whether something else is emerging, and whether it is qualitatively different enough to call it something else. The problem with an inherited concept, like inherited money, is that we didn’t make it ourselves and come to take it for granted. Particularly if it is part of a whole metaphysical conceptual structure. Maybe we need a bit of good old Brechtian alienation-effect even from heirloom concepts like ‘capitalism.’

Now, all I’m talking about here is a thought experiment: what if we thought about a mode of production emerging after capitalism that was worse? Could that at least minimally explain observable features of the world that might be genuinely strange, qualitatively novel, observable tendencies in recent history? What light would this perspective shed on our habits of thought, our received ideas?

As an example of how one might conduct thought experiments, I turn again to Erik Olin Wright. He is the author of a substantial body of sociological work, both conceptual and empirical, on class. The papers collected in Understanding Class are their own kind of thought experiment. They ask: what can a Marxist concept of class bring to theoretical and empirical work that thinks class as stratification, or which uses class concepts drawn more from the work of Max Weber or Emile Durkheim? Wright deftly shows what a Marxist concept of class can do, where endless capitalism is a given. The addition question I want to ask is what happens if we take away the assumption that this is still the same old capitalism.

Wright has mercifully given up the “paradigm aspiration” (17) wherein Marxism is superior to all the social sciences because it has a superior problematic or method. Those whose memory stretches back away will recall that this led to endless attempts to prove by purely theoretical means that Marxism had rendered all of bourgeois philosophy and social science obsolete. But it hardly ever delivered on the promise of a new kind of knowledge. The “grand narrative” (122) fell apart. And in any case it ended up being produced in the margins of the very same institutions as that bourgeois philosophy and social science.

Instead, Wright makes two sorts of claims. The modest claim is that one can connect Marxist work to other kinds of sociology. Each has its perspective and they illuminate each other. The stronger claim is that the Marxist perspective is a bigger picture, which shows something about the world and history that is beyond the reach of other approaches. Here he does for social theory what Fred Jameson does for literary theory or Perry Anderson for historical thought: make the claim that Marxism offered the point of view from which to interpret and synthesize other bodies of work. If Jameson’s famous watchwords are “always historicize!” then Wright’s might be “always socialize!” where that means to adopt the point of view of a social formation riven by relations of class exploitation and domination as the outer limits of the macroscopic perspective.

In one of his brilliant summaries, Wright argues that Durkheimian, Weberian and Marxist approaches operate on different levels of the social gamespace, the situational, the institutional and the systemic. The Durkheimian approach is situational, and is about moveswithin the game. The Weberian approach is institutional, and is about rules of the game. The Marxist approach is systemic, and is about changing the game.

All of these approaches involve class structure that generate class actors who have at least partially conscious intentions, whether it is to make moves that advance them, or contest rules of the game that might advantage some class or other, or to change the whole game to another game more in one’s class interest. My question would be about the class unconscious. Perhaps the game changed of its own accord, as the forces of production push forward into new relations of production, with which our superstructural languages for describing class structure have yet to catch up.

Wright thinks that the opportunity for game-changing, for overthrowing capitalism in favor of a more equal and free society, is not present. “One way of interpreting the history of the past half-century is that there has been a gradual shift in the levels of the game at which, for many analysts, class analysis seems most relevant.” (123) Hence it makes sense to reach out to the Weberians (whose scholarly interests are at the level of contesting the rules of the game rather than changing it) and even to the Durkheimians, whose focus is on the moves actors get to make within given rules of the game. But his overall aim is to concatenate these three approaches as appropriate to different scales, with Marx speaking to the larger and more visionary scale.

Wright resists the death of class counter-narrative, a discourse whose most prominent member is probably Ulrick Beck. Wright offers a supple class analysis and backs it up with actual results. His concept of class has three dimensions: property, authority, expertise. His view of class structure offers class locations at three levels, which do not always neatly overlap. Relations of property generates the class locations of employers, petit-bourgeois, employees. Relations of authority generates the locations of managers, supervisors, the supervised and managed. Relations of expertise generate the locations of professionals, the skilled, the nonskilled.

He is interested, for example, in the “permeability” (146) of class boundaries, so he looks at three kinds of class connection: intergenerational mobility, cross-class friendship, cross-class households. He finds the property boundary the least permeable – a result that won’t surprise Marxists. Class connections between workers and employers is limited. The employee / petit bourgeois boundary is more permeable. Wright frankly acknowledges that in the United States, racial boundaries may be even less permeable, but that does not negate the usefulness of the category of class. Class is only a modest predictor, however “class often performs as well or better than many other social structural variables in predicting a variety of aspects of attitudes.” (154)

David Grusky and Kim Weedon offer what Wright classifies as a Durkheimian analysis of class in which occupation are the unit of analysis. They see class homogeneity only at the micro, occupational level, not in ‘big’ concepts of class. Its more about actual labor markets and how they define occupations. Such occupations act on behalf of members, extract rents if they can, and shape life chances. For them, even academic sociologists and economists count as different ‘classes.’ Which might be the beginnings of an approach to how academics, at a time when, under the threat of vectoral power, their life chances seem diminishing and their means of opportunity hoarding to be failing, cannot quite come together and act on shared interests.

The Durkheimian approach focuses on selection and self-selection into closed groups who interact more with each other than with other groups. Licensing and the formal definition of occupations play a role here. This works well for explaining individual-level outcomes. Wright claims that except in the study of education, income and wealth, this micro approach works better than macro ones of a more Weberian or Marxist kind. The Durkheimians are good on lifestyles, tastes, political and social attitudes.

The key to Weberian theories of class for Wright is opportunity hoarding or social closure, by such means as credentialing, licensing, the color bar, gender exclusions. One could even see labor unions as a form of opportunity hoarding from the point of view of precarious workers, but we’ll come back to that.

As Wright points out, perhaps the most important mechanism of opportunity hoarding is private property itself. “The core class division within both Weberian and Marxian traditions of sociology between capitalists and workers can therefore be understood as reflecting a specific form of opportunity hoarding enforced by the legal rules of property rights.” (7)

Wright brings into the Weberian theoretical frame the Marxist concept of antagonistic classes. Advantages are causally linked to disadvantages “The rich are rich because the poor are poor.” (8) The Marxist concept of exploitation and domination are about control over the lives of others. “Exploitation refers to the acquisition of economic benefits from the laboring activity of those who are dominated.” (9)

It could be argued that class is peripheral to Weber’s work, but his writings on ancient slavery seem close to Marx. Unlike concepts such as status and party, class need not generate identity or collective action. And hence does not fit well with Weber’s habitual explanatory modes.

Both Marx and Weber see property as fundamental to a relational concept of class. Both grasp distinction between objectively defined class and subjectively lived class. Both thought humans followed material interest in the long run. Weber was much less inclined to think classes would polarize and become the key social dynamic. Marx shared Weber’s view that status groups impeded the effects of the market and constitute an alternative basis of collective action. Both thought the rationalization of market relations would abolish status groups in time.

Marx thought this simplified class whereas Weber did not. Weber thought class determined life chances within rationalized society. He was less interested in deprivation than in instrumental rationality. Marx was more interested in class exploitation in production; Weber in class as factor in determining life chances in the market. Wright: “Marxist class analysis includes the Weberian causal processes, but adds to them a causal structure within production itself.” (46)

For Wright, class in Weber is closely connected to the theme of rationalization. Of Weber’s three sources of power, he conceives of both non-rational and rational forms. Thus the power-source that is honor can appear as ranks and titles, or as a meritocracy. Authority can appear as patriarchal or in a rational-legal form. And material sources of power can appear as consumption groups or as class.

Class is thus part of rationalization, part of the abolition of traditional peasantry, part of the transition from landed aristocracy to agricultural entrepreneurs. Class is part of the rise of the calculation of material interest. The peasant, who owes a duty to the baron, becomes the farmer who pays rent to the landlord.  “While class per se may be a relatively secondary theme in Weber’s sociology, it is, nevertheless, intimately linked to one of his most pervasive theoretical preoccupations – rationalization.” (31)

My question here would be to ask: why one would think, if this has given rise to two kinds of rationalizing class antagonism that overlapped and interfered with each other, why might it not give rise to a third? The farmer-landlord antagonism arose out of the ruins of feudalism. The efficiencies in agricultural production that came with this rationalization threw off a surplus population what would become urban workers, in an antagonistic relation to capitalists. And yet landlords and capitalists also had interests that contradicted each other. This is the central theme of David Ricardo’s political economy: the opposition between landlord and capitalist. But did rationalization stop, with the creation of classes of farmer and worker? What happens when the production, not of food or products, but of new information itself becomes rationalized?

Weber did not have a lot to say about labor, but where he did, it was in terms of work discipline. Employers are free to hire and fire. Workers lack ownership, but workers are responsible for their own social reproduction. These are the conditions under which indirect compulsion operates. But it raises the problem of how to get maximum labor effort. Wright: “running throughout Weber’s work is the view that rationalization has perverse effects that systematically threaten human dignity and welfare.” (52) One sees this clearly in the latest impositions on the labor process, such as Uber or Amazon’s Mechanical Turk, which make labor both highly autonomous and yet very closely monitored at the same time.

But Weber does not integrate interest in labor discipline and domination into the category of class. Here we need a bit of Marx, for whom, as Wright says, “exploitation infuses class analysis with a specific kind of normative concern.” (53) Exploitation steers research to questions of class as relational in both exchange and production. “Weber’s treatment of work effort as primarily a problem of economic rationality directs class analysis towards a set of normative concerns centered above all on the interests of capitalists: efficiency and rationalization.” (55) Georg Lukacs and Theodor Adorno excepted, one might add.

The Wrightian synthesis of Marx and Weber makes exploitation fundamental, but makes particular use of the idea of opportunity hoarding as that which defines the middle class.From there one could build up a picture of the United States as highly polarized by exploitation, and where middle class opportunity hoarding is being eroded by what he calls neoliberalism and deindustrialization, but which I think can be understood more clearly in terms of new forces of production that instrumentalize and rationalize information, giving rise to new property forms and hence new class relations, including an antagonistic relation between a hacker class tasked with making novelty out of information (the condition of it becoming property) and a vectoralist class that owns or controls the vector of information control and domination itself. Here the Marxist perspective of exploitation and the Weberian one of rationalization can be fruitfully combined.

Beside reaching out to those indebted to more classical approaches of Durkheim and Weber, Wright addresses prominent contemporary social theorists who try to offer original perspectives. Here I’ll treat only his papers on those known outside their disciplines, such as Thomas Piketty, Michael Mann, Guy Standing and former New School professor, the late Charles Tilly.

Wright thinks Charles Tilly’s approach to durable inequalities is closer to Marx than Tilly wants to acknowledge. Tilly was against individualist approaches. Explanations of inequalities have to be relational. He offers a structuralism of types of social relations and types of mechanism. The types of social relation are chain, hierarchy, triad, organization, categorical pair –  of which organization is the most durable kind. The types of mechanism are exploitation, opportunity hoarding, emulation and adaption.

Structural relations are solutions to problems generated within social systems. For example, Problem 1: how to secure stable access to resources?

Solution 1: opportunity hoarding and exploitation. Problem 2: how to sustain and even deepen exploitation and opportunity hoarding, and sustain trust and cooperation among those who benefit? Solution 2: categorical inequality. Problem 3: how to stabilize and reproduce inequality? Solution 3 emulation and adaptation lock distincions in place.

Wright sees Tilly as importing Weberian ideas into a Marxist framework. Culture is not an autonomous superstructure, as in certain post-Marxist theories. Nevertheless, Tilly goes beyond Marx in attempting to subsume gender, race, nationalism under a unitary framework. For Tilly, forms of categorical inequality make exploitation more efficient. One could perhaps usefully extend this to think about how what one might shorthand as algorithmic mechanisms of discrimination, which work so subtly with databases rather than categories, might reinforce exploitation in our time.

Michael Mann’s work is approached a bit differently by Wright. Here his interest is in the disjunction between how his theory treats class and some of his more specific findings. In his theory, Mann sees class only in terms of collective actors, not structural locations. But in particular studies, class location does seem to shape individual interests.

Crucial here is the distinction Wright makes between class structure, class formation and class actors. This might correspond very loosely to three scales of analysis: the Marxist, Weberian and Durkheimian perspectives, respectively. Mann, like Bourdieu, thinks that class structure that produces no class actor is just ‘class on paper,’ an academic exercise. Class has competing forces against it: ethnic, racial, linguistic, national, religious, gender.

Mann’s social theory works off two clusers of concepts: sources of social power and forms of organization that deploy those powers. There are four sources of power: social, ideological, economic, military. The kinds of organization are expressed as dichotomies: collective / distributional; extensive / intensive; authoritative / diffuse.

In Mann’s world, the pursuit of goals requires entering into power organizations that determine the structure of society. This is an agency rather than a structure centered approach. The creation, reproduction or transformation of social structures is the result of goal directed actions. Rather like rational choice theory, Mann starts with actors and their goals, only his approach is not individualistic.

Class in Mann is a kind of collective actor among many that comes together in organizations to deploy economic power resources. He pairs the concept of class with that of segment, which cuts across class and groups actors of different classes in particular industries. Class, for Mann, is of little sociological interest.

In working through the formation of the middle class, Mann actually becomes rather more interested in class structure and class formation. For Mann, the middle class is composed of three separate categories: The petit-bourgeois, professionals and careerists. The economic situation of all three tie their life chances to capital accumulation, and their relation to the state forces them together. They have similar consumpti0on patterns and may all be investors of small amounts of capital. They are held together by ideological and political citizenship.

Mann rejects the classic Marxist distinction between class in itself / class for itself as teleology, but so too does Wright and many more sophisticated Marxists. Wright: “One can believe that class relations and class structures are real and generate real effects without also believing in any one-to-one mapping between the complex structure of class relations and the formation of collective actors.” (108)

Here Wright’s signature concept of contradictory class locations proves very useful. Actual jobs are a mix of property relations and authority relations, and might be located in class terms differently along those axes. Class changes over time and is mediated by family and community. One might be born into the petit-bourgeoisie but end up a waged-worker. One might be working class and marry a shop-keeper, and so on. Class can be a bit messy. One is reminded here of Jean Baudrillard wry remarks in America on Marxist academics managing their stock portfolios.

Thomas Piketty deserves credit for putting inequality back on the agenda as more than a mere problem of unequal opportunity. His empirical work shows that the sharp rise in income of the top 10%, is really that of the 1% or even the .1%. A fair bit of this came from the rise of super salaries rather than income on capital. The CEO ‘class’ are setting their own pay. Here I would want to inquire as to how, in a political economy running on information, the capacity to control (but not entirely own) the means of production accrues to a class that presents itself as the celebrities of information control itself.

The technicalities of Piketty’s work centers on the capital / income ratio as a way of measuring value of capital relative to total income of economy. Wright: “Piketty’s basic argument is that this ratio is the structural basis for the distribution of income between owners of capital and labor: all other things being equal, for a given return on capital, the higherthis ratio, the higher the proportion of national income going to wealth holder.” (133) As  growth declines, the capital/income ratio rises. There’s a rise in the weight of inherited wealth, while concentrations of income also rise. It’s the worst of both worlds: a rentier class plus a meritocracy of appearance-peddlers carving up the world between them at the expense of everyone else.

Pickety starts out with a class analysis, but loses it once he gets into the empirical work, where he treats CEO income as return on labor, as most income tables do. Wright: “In the modern corporation many of the powers-of-capital are held by top executives…. They occupy what I have called contradictory locations within class relations… They exercise their capitalist-derived power within the class relations of the firm to appropriate part of the corporation’s profits for their personal accounts.” (136)

But is their power really capitalist-derived, or is it now something else? Something like a joint managing of appearances between those who represent a firm to the market, and the market that is supposed to value it. But how to value it when so much of its asset-base takes the form of information? A firm is among other things a brand, a slew of patents, a logistical process, a corral of expert hackers turning out new intellectual property. How can information be turned into value, and an opportunity to be hoarded, when there aren’t really private languages, and information is in principle a non-rivalrous good?

Wright points out that Piketty does not separate out real estate from capital. There might be good reasons to do so. Elsewhere I wrote about Matteo Pasquinelli’s arguments about how landlords now increase their rents by extracting the information-value that the presence of either the hacker class, or of those parts of the middle class that manage rather than create information. One could think further here about Ricardo’s ancient tension between ground-rent and profit, but with the focus shifted from the rural to the urban, and the monopoly rents to be extracted from urban locations.

Guy Standing is the name most associated with the now widely-discussed idea of theprecariat as a class rather than just a bad life chance. He offers a three-dimensional definition of class, as structured by relations of production, relations of distribution and (interestingly) relations to the state. He identifies seven classes: plutocracy, salariat, proficians (professional + technician, working class, prevcariat, unemployed, lumpen-precariat. The precariat have insecure insecure jobs. Their sources of income other than wages disappearing. They become less citizens of the state and more mere denizens. Not only are their jobs precarious, they are vulnerable within relations of distribution and marginal to the state.

The precariat includes people bumped out of working class communities and families. They experience a relative deprivation in relation to a real or imagined past. It also includes migrants and asylum seekers for whom it’s the present that is absent. They have no home. The precariat increasingly includes people falling out of an educated middle class – think academic adjunct labor – who lack a future. For Standing this makes a potentially dangerous class.

Marxists might think of the precariat as workers who (in Weberian terms) experienced poor life chances. Standing thinks there are antagonisms between the precariat and the working class. But do the precariat and workers have distinct interests? Wright thinks not. He thinks they share an interest in changing the game (although one might want to say more here about how workers and the precariat might have different interests about the rules and moves of the game). Unionization, for example, can secure some sort of steady work for the workers in the union at the expense of those without – a side-effect of unionizing academic adjunct labor that is rarely discussed.

Certainly the most controversial of Wright’s propositions is one that picks up on the work ofWolfgang Streeck and others on class struggle and class compromise. For Streek, arguing in a Durkheimian vein, capitalism works better when there are constraints on rational, self-interested action. Capitalism works better when there’s non-capitalist social forms present, based in trust, legitimacy, responsibility.

The wrinkle Wright introduces is to argue that the level of constraint on elf-interest that is optimal for capitalists is below that which is optimal for workers. Capital seeks to remove constraints to augment its power even past the point where these are economically inefficient. Wright: “… the zeal to dismantle the regulatory machinery of capitalism since the early 1980s was driven by a desire to undermine the conditions for empowerment of interests opposed to those of capitalists – even if doing so meant under-regulating capitalism from the point of view of long-term needs of capital accumulation.” (183)

Although perhaps one could see this a bit differently by separating out the interests of the capitalist and vectoralist class. The regulatory regime emerging in the last quarter century favors the mobility of information – and not just finance – as a means of coordinating economic activity transnationally, at the expense not just of workers but of those forms of capitalist enterprise tied to physical plant and infrastructure, and thus with an interest in local, regional or national relations of trust, legitimacy and responsibility.

Hence we can read Wright’s conclusion against the grain: “Enlightenment of the capitalist class to their long-term interests in a strong civic culture of obligation and trust is not enough; the balance of power also needs to be changed. And since this shift in balance of power will be costly to those in privileged positions, it will only occur through a process of mobilization and struggle.” (184) But what if those capitalists tied to actually producing things in a particular place already know this, but they have lost power to a quite different kind of ruling class, which operates at a higher level of abstraction, or in Weberian terms, at a new stage of rationalization? They own or control the information about things, rather than the things themselves.

Hence to imagine new kinds of class compromise might require a rethink about which classes could compromise. First, one has to have some perspective on the impulse to think all class compromise as illusion or stalemate. Could there be a non-zero-sum game between otherwise antagonistic classes? Working class organization may actually have positive effects for capital accumulation, as it enables problem solving, negotiation, skill development, tech change.

But perhaps that only worked when particular capitalist employers were able to exercise something like a monopoly on the production of a given class of product or in the context of a mercantilist strategy of restricted consumption at home while expanding exports abroad and sustaining rising wages as a return on rising productivity. Such as would describe Japanese manufacturing in its heyday, for example.

Since there’s no way to change the game, Wright looks to those who wanted to change the rules within the game, such as those Scandinavian social democratic inheritors of Ernst Wigforss, such as Walter Korpi and Gøsta Esping-Anderson. But one has to ask if its possible to revive social democratic strategies from the era of the great national manufacturing industries in an era where the information vector greatly lowers the cost of geographic dispersal, and puts manufacturing regions in direct competition with each other.

Wright advocates for some salutary counter-hegemonic strategies, based in geographic rootedness, local public goods and worker’s cooperatives. But one has to wonder if, in an era where the forces of production drive increasingly abstract processes of rationalization, which appear then as transnational legal and treaty forms protecting information as private property, such things are all that viable.

Wright: “Changes in technology may make the anchoring of capitalist production in locally rooted, high productivity small and medium size enterprises more feasible.” (143) One might call this the Brooklyn-effect, after the boom in small business, even manufacturing, there. But while the actual products have some connection to locality, the information infrastructure such localism has to rely on belongs to the vectoralist class. Amazon, Paypal and so forth all get their cut.

Thus, where Wright says, “I assume that an exit from capitalism is not an option in the present historical period” (239) – I think we have to question that assumption, but not in a good way. Maybe this is already not capitalism, but something worse. Its not just a rentier bubble of speculation spooling out of the “real economy.” (244) One could no longer know in advance which part of it is real at all – and perhaps one never could. This is an era not just of so-called neoliberalism’s “aggressive affirmation and enforcement of private property rights” (237) but of the creation of new forms of private property, and new antagonistic relations over it, particularly in the form of intellectual property.

There’s a lot to be said for the way Wright subsumes rival social theories as collaborators within the larger frame of a fairly traditional Marxist sociology. But perhaps that in turn has to be put back in contact with the historical study of the transformation of the forces of production, and in particular how information emerges as both a technical and social force. One could then, as a further step, bring this perspective together with the study of themetabolic rift, wherein the instrumentalizing of information mobilizes the whole planet as a rationalized sphere of resource extraction under the sign of exchange value. To the point where this rationalization becomes completely irrational, threatening to take the whole planet down with it.

Here it might be helpful, in Bogdanovite fashion, to press on some other metaphors at work in Wright, viz: “A society is not a system in the same way that an organism is a system. It is more like the loosely coupled system of an ecosystem in which a variety of processes interact in relatively contingent ways.” (121) In the Anthropocene, it may turn out that ecosystems are the ones that are tightly coupled. But that would be a whole other thought experiment.

Maybe we need an asocial science that rethinks whether one can even conceive of the social as a separate domain of analysis at all. On the one side, the social meshes seamlessly with information technology; on the other, it depends on planetary scale resource mobilization causing catastrophic metabolic rifts. One might be in need of an even ‘bigger’ conceptual framework within which to rest Wright’s partial synthesis as a component part.

RetroDada begins with disgust. Once again the world gets its war on. While some cities are attacked by bombers, others are strafed by art fairs. This time there’s no Switzerland of neutrality where refugees might cool their heels, as now the whole globe itself overheats. The insomnia of reason breeds monsters.

But before we can take two steps forward, let’s take one leap back. Back a whole century. Back to the first of the world wars to be numbered; back to the birth of Dada disgust. Back to that great refusal of what the century was to become. Why shouldn’t a .gif run backwards as well as forwards? Its RetroDada time! In principle RetroDada is against manifestoes, but it is also also against principles. So here goes nothing.

The world is full of mistakes, but the worst is the art that got made. Art gives us Dante’s Inferno as styled by interior decorators. RetroDada aims to please neither at art nor anti-art, as nobody should serve masters. We will put an end to spectacle and replace it with convulsive laughter from continent to continent. It’s shit after all, but from now on we mean to shit in different colors.

Psychoanalysis is itself the disease. It makes the bourgeois self seem interesting. Ethics produces atrophy like every plague produced by intelligence. Theory merely guides us in a round-about way to the prejudice we had in the first place. What we need are works that are tender and precise and forever beyond understanding. There is a lot of negative work to be done.

RetroDada is our intensity. RetroDada is for and against unity. RetroDada is the abolition of their reasons. RetroDada is the refusal of an inheritance. RetroDada is a convulsive effusion, like a cellphone cooked in a microwave. RetroDada is a divinity of the lowest order. RetroDada has no theory. There is enough of that in art school. We love the old things for their freshness.

How to make a RetroDada manifesto: Take a Dada manifesto or two. Copy the good bits. Toss them in a file. Move the bits around. Improve on them. The manifesto will be like you. As for intelligence, it will be found in the streets.

RetroDada is working with all its might to introduce the idiot everywhere. RetroDada is a venture capital form for the exploitation of other people’s ideas. God can afford to be unsuccessful. So can RetroDada. It is luxury without value or price. RetroDada gives itself to nothing, neither to work nor play. RetroDada seduces you with your own idea.

RetroDada is at once stand-up comedy and a requiem mass. RetroDada trusts only in the sincerity of situations. RetroDada fights against the thanaticism of the times. RetroDada develops the plasticity of the digital. We should make all art and literature and cinema free. The medium is as unimportant as we are. Essential only is the forming. Take any material at all.

RetroDada is a theoretical virus. RetroDada means letting oneself be thrown by events. Say yes to a life that strives downward toward negation. RetroDada refuses to be contemporary to any of this shit. RetroDada reintroduces art and everyday life so they can have queer sex in back alleys. RetroDada is the statelessness of the mind. RetroDada rejects both the stylish order and the stylized disorder of contemporary aesthetics. We are convinced of the arbitrariness and falsity of our poor creation, the world. We look unencumbered into the heights and depths.

Let’s only steal from the best, and from their actions, not their styles. The resonant Sophie Taeuber, the drum-banger Richard Huelsenbeck, the tubular Hugo Ball, the mystic Emmy Hennings, the hypeman Tristan Tzara, the ironic Jean Arp, the dadasoph Raoul Hausmann, the runfast Hannah Höch, the aviarist Baroness Elsa. Let’s take their leavings not their droppings. Music, dances, theories, manifestos, poems, paintings, costumes, masks. To be begun again, from the beginning.

So many fates of the west befell Dada. It tried everything already, so we don’t have to. Arthur Cravan became a legend. Mina Loy became a poet. Marcel Duchamp became the enabler of contemporary art. Hugo Ball became a devout Catholic. Emmy Hennings became a Protestant mystic. Hannah Höch wrote a trans-species children’s book. Richard Huelsenbeck became a therapist. Tristan Tzara became a communist. Marcel Janco returned to the Orient. Baroness Elsa died in poverty and obscurity. Jacques Vaché killed himself before it even all began.

To be for this manifesto is to be RetroDada! To be against this manifesto is to be RetroDada! To be for and against this manifesto is to be RetroDada! To be neither for nor against this manifesto is to be RetroDada! There is no escape from the history yet to be unmade!

I am a recovering Althusserian. For decades now I have been Althusser-free, for the most part, but we all have our lapses. The first step to becoming a recovering Althusserian is to recognize that you have no control and are unconsciously always a little bit Althusserian whether you want to be or not.

Louis Althusser is however not so much a poison as what Derrida and Stiegler and Stengerscall a pharmakon. That is, something that is undecidable, both poison and cure. It may well be that there are good reasons, in the twenty-first century, to be an Althusserian. I am not objectively in a position to say, and in any case: by their results shall we judge them. When there is a useful Althusserian response to the Anthropocene (or whatever you want to call the current ‘conjuncture’) consider the matter settled. As of yet there is no such response, perhaps for reasons to be elucidated later.

Perhaps this is to judge too harshly. In what follows I want to read some essays by the ‘young Althusser’. I leave it to others to account for the mature works. I want to think about what is living and dead in the Althusserian ‘problematic’, through a series of antimonies that he had to face. This first part deals with his essay ‘On the Young Marx.’ This essay is instructive, as it both sets up a method of reading Marx that we can also apply reflexively to Althusser, and also provides a useful answer to one of the key problems in Marxological thought: the relation of the young to the old (or mature) Marx.

Althusser neatly characterizes the two extant approaches of his time. In one, there areelements in the young Marx that anticipate the older one, and his thought can be read as a teleology, as always having tended towards this goal, this truth. Or: one can read the young Marx as announcing a broad, ethical program which is then either narrowed or even betrayed by the more economistic and social-scientific work of his later years. The former is a typical reading for orthodox communists of the time; the latter the characteristic program of Western Marxism and even of the New Left more broadly. Hence as Althusser wryly notes, all discussions of young and old Marx are political discussions.

Both these readings tend to focus on elements of the text, finding for example characteristically Hegelian or Feuerbachian elements in the young Marx, or themes submerged in the young Marx later brought out more fully in later writing. Althusser wants to dispense with ‘Hegelian’, or rather bog-Hegelian readings in particular, such as the proposition that the materialist core of Marx was present in the early works but in a still idealist form.

Althusser says that “this method which is constantly judging cannot make the slightest judgment of any totality unlike itself.” There is a totalizing sameness to those readings of Marx that annex him onto Hegel. If there’s just a little bit of Hegel to be detected in Marx somewhere, then it becomes Hegel all the way down.

In place of all this, another method: Ideologies have to be considered structurally, as having an underlying problematic of different terms and their combinations. Any particular ideology also has to be thought in the ideological field in which it partakes. That field has determinants outside itself, in specific historical situations. Althusser wants to claim that this is the beginning of a scientific method for treating the ideological, rather than for merely extending ideology. This is more asserted than demonstrated, but for those inclined to the formal methods of literary analysis, this is progress. Reading is to have its method.

The young Marx, Althusser candidly says, writes ideology. He writes it well, but it is just an extension or permutation of the ideological field of his time. Even devout Marxocologicalists should not be embarrassed by this. Althusser: “Early Works are as inevitable and as impossible as the singular object displayed by Jarry: the skull of the child Voltaire.”

One advantage of Althusser’s reference of young Marx to the ideological field is that it rules out another method, more common in our time: the Great Books of the Apostolic Succession. One reads Hegel, one then reads Feuerbach (extra-credit only, he is not quite canonic), then one reads Marx — and then one reads Althusser. But as Althusser rightly insists, the Hegel that Marx read was “not the library Hegel we can meditate on in the solitude of 1960,” – or 2016. The Hegel of Marx was the Hegel of the neo-Hegelian social movement.

In short, Marx came into a very particular ideological field, and his thought as a young writer was within a problematic determined by that field, particularly that of the left Hegelians, and even more particularly that of Feuerbach. A problematic, for Althusser is a kind of structural system through which other material can be processed. Hence Marx applied the Feuerbachian problematic to religion, as Feuerbach did, but also to political and economic ideologies, as he did not.

Interestingly, the concept of problematic becomes a way not to think the Hegelian totality for Althusser. A problematic is a systematic structure with rules of composition, not a unity whose essence is expressed in all its particulars. A problematic, moreover is something that thinks through you, rather than being what you think. It is in a sense unconscious. It calls for special methods for determining how the problematic is at work in the text. Note how the path is open already for a kind of specialized labor of textual exegesis here. Althusser: “a problematic cannot generally be read like an open book, it must be dragged from the depths.” Henceforth we are with Hermes, running a gimlet eye of suspicion over the text, as if if were a symptom of what it hides.

So Marx unconsciously plays out certain permutations of a problematic. It’s a theory which neatly inverts Sartre’s notion of a writer’s necessary freedom to commit to a project. But this presents then a special problem for accounting for how Marx broke with the ideological field of his time. In backward Germany in Marx’s day, intellectuals put a special effort into thinking what was to be done but could not happen. They looked to the political revolutions of France and the industrial revolutions of England. Unable to actually produce either revolution – they theorized them. Most fully, in Althusser’s account, not so much in Hegel but the Hegelianism of the 1830s and 1840s.

Cunningly, Althusser says that Marx retreated from this ideological field, rather than overcoming or surmounting it. He went back to the original problematics of the political economists and political theorists who Hegel had claimed to synthesize into his own philosophy. This is coupled with two discoveries that are extra-philosophical. It was Marx’s experience of political radicalism in Paris, and Engels’ first-hand psychogeography of Manchester capitalism, that were the key to moving forward after this retreat from Hegel. Althusser: “In France, Marx discovered the organized working class; in England, Engels discovered developed capitalism and a class struggle obeying its own laws and ignoring philosophy and philosophers.”

The failure of German liberalism pushed Marx out of Germany. The bourgeois backers of his radical journalism melted away. And with that failure came the retreat from the ideological field to which they belonged. Marx’s training in German idealism was not wasted, however. It provided the ability to think abstractly, which was only awaiting actual concrete things in the world that really needed to be thought.

Thus, Althusser understands Marx’s thought as breaking with the ideological field of his formation, and founding a science. One might remain skeptical about the second part of this claim — the founding of a science — and still find useful the first part — the concept of the ideological field. Perhaps they are rather harder than that to exit. And what if this method were applied in turn to Althusser? What was the ideological field to which his work belonged? What was its underlying problematic? What historical situation gave rise to it? And closer to my own dabblings with it, what historical situation led to the uptake of Althusser in the Anglophone world in the 1970s and 80s?

To tackle the last first: the defeat of the New Left in the 70s led, among other things, to a kind of embedding in the cultural and educational apparatus of those who had dreamed of larger things. This was past the era of the Chinese, Cuban and Algerian revolutions, still the time of the Vietnam war; the time of the rise and fall of New Left activism in the west. This was perhaps not unlike the situation of the left-Hegelians in Germany in the 1840s. And perhaps with not so different results. That which could no longer be enacted– was to be thought as a theoretical revolution instead.

If Sartre had appealed to a more committed, activist time; Althusser appealed to one of quietism, at least as far as he was read in the Anglophone world in the 70s and 80s. (The Althusserians of 1960s France were a different story). What was to be taken up was something already apparent in this brief essay on young Marx: specialized method. Althusser legitimated the scientific study of the ideological field, the search for the unconscious problematic.

This had certain benefits. It meant an insistence on certain standards for accounting for how the ideological field is structured. It also implies a certain relative autonomy and consistency of the ideological level. It led in practice, however, to a deepening of an academic division of labor, via which Marxist thought could accommodate itself to the disciplines. The economic, political and ideological could then be studied as separate objects, each in their own field, in increasingly diminishing contact with each other.

Let’s look at a famous Althusser essay from the early sixties. ‘Contradiction and Overdetermination’ builds on Althusser’s ‘On the Young Marx’ essay, in deciding against the various Hegelian readings of Marx. Althusser rejects the metaphors of ‘turning Hegel right side-up’, or ‘restoring the rational kernel of dialectic without the mystical shell.’ Rather, he thinks of Marxism as replacing Hegel’s dialectic with a different problematic.

How is one to take an argument of this kind? One way would be to subject it to philological proof, a kind of scrutiny it may not actually withstand. But perhaps one can take the argument in a different vein: that one could replace the Hegelian dialectic with a different one, even that one ought to replace it. Althusser is very nervous about opportunistic or merely ideological dilutions of Marxism, and so he insists that his is a rectification, or a drawing out of a dialectic that Engels rather misconstrued, and that neither Marx nor Lenin had the time to write. As a party member, he could hardly appear to be reading Marx at all creatively.

Perhaps now one can see it as creative. Althusser took a cutting of Marx, taken from its German-idealist root-stock, and grafted it to a quite different problematic. One could mention here at least four coordinates of the ideological field in which that problematic resided. One would be the French social thought from Durkheim via Mauss to Levi-Straus, stretching from anthropology to linguistics. A second might be Spinoza, a third would be the distinctive philosophy of science in France centered on Gaston Bachelard. A fourth would be a version of Marxist ‘orthodoxy’ uninterested in the post-56 ‘thaw’, and loyal to Lenin, Stalin – and Mao.

Of these, Spinoza is probably decisive, although neatly dove-tailing with French social thought, in the way Althusser thinks a totality that produces, among other things, subjects, rather than thinking a totality that subjects produce through their encounter with, and recognition of themselves in, an objective world. Althusser begins the new dialectic with the category of over-determination (borrowed in this case from psychoanalytic readings of structural linguistics). Rather than one dialectical totality, unfolding in all its complexity around a central contradiction, Althusser posits a totality with at least three kinds of contradiction that can over-determine the central one – the class struggle between labor and capital.

Of most use to me is his passing recognition that other classes can over-determine the contradiction between labor and capital. As is clear from Marx’s political writings, from Gramsci, from Kautsky on the peasant question, the simplification of class dynamics down to two classes of Marx’s Capital is not always helpful. The economic dynamics of capitalism might hinge on the class relation, but politics is more complicated. That class contradiction may be over-determined that of other classes. (I pushed this thesis to extreme in my A Hacker Manifesto).

A second over-determination comes from the relative autonomy of the superstructures. It may well be that forces at work in the political or ideological levels may either retard or accelerate the development of the principal economic contradiction. In the case of the Russian revolution, Althusser thinks there is an element of ideological over-determination. The working class was intensely class conscious, thanks to a militant and organized intellectual movement. This idea of the relative autonomy of the superstructures will become a crucial legitimating move for political theory and cultural studies, as we shall see.

A third over-determination take us outside the national-cultural frame so dear to Gramsci, into the space of the relations between imperial states. Taking up Lenin’s thesis that the imperial system broke at its weakest link – the Russian empire, Althusser reads this as a third kind of over-determination. History advances ‘bad side first’, as Marx and Engels put it in theHoly Family. It was not where the capitalist infrastructure was most developed that the revolution broke out – as ‘vulgar’ determinist Marxists might have expected.

Thus the world-historical situation is not the product of the ‘beautiful’ contradiction between labor and capital alone. Strikingly, this implies a root-and-branch rethinking of Marxism itself, both of its theory, but also of its history. “One day it will be necessary to do what Marx and Engels did for utopian socialism, but this time for those still schematic-utopian forms of mass consciousness unfluenced by Marxism… a true historical study of the conditions and forms of that consciousness.” And, one might add, this root-and-branch critical history is now required for the Althusserian turn as well.

The (admittedly simplified) Hegelian theory against which this is launched saw world-historical movement as a dialectic between the sphere of needs, of civil society, versus political society, or the state and its governing Idea. In this Hegel, material life, civil society, the economy – is merely the means through which reason, embodied in the state, works itself out in history. No matter whether this was the Hegel of the Hegelians, it was the Hegel of the Marxists for whom Marx was Hegel put right-side-up. In that version, it is the other way around. The sphere of the social production of men’s needs – economy – is the hidden truth of its political and economic forms. Economy is essence and the superstructures mere appearance. Althusser: “The logical destination of this temptation is the exact mirror image of the Hegelian dialectic. The only difference being that it is no longer a question of deriving the successive moments from the Idea, but from the Economy.”

Even as a recovering Althusserian, I am thankful for this break Althusser makes from the metaphysics of essence and appearance. That metaphysic remains the ideological field of theories of eternal capitalism, in which the essence of its economy never changes, and any new feature is ‘just circulation’ or some other such non-thought. Althusser is the beginning of a way to think historically again, outside of the mythic grand narrative of the ‘beautiful contradiction’, as he calls it, which is the hidden God governing all appearances.

For Althusser, Marx’s whole project is a break with exactly this dialectic. Althusser: “his concern was rather the ‘anatomy’ of this world and the dialectic of the mutations of this ‘anatomy.’ Therefore the concept of ‘civil society’ – the world of individual economic behavior and its ideological origin – disappears from Marx’s work.” In its place, a retreat from Hegel to his sources in classical political economy, such as Smith, and forward to Ricardo and others who follow Smith, and the development of a critique of the very categories through which the sphere of needs is imagined in bourgeois thought.

One might pause here to note that this set Althusserians on a course of seeing the relations of production as the crucial and determinate component of the economic ‘instance’, not theforces of production. This had a certain utility when expanded out into a concept of relations of production and reproduction — a pathway opened by Althusser and his students inReading Capital, which paid attention to Marx’s rather neglected Capital vol. 2. This later enabled a co-joining of Marxist and feminist concepts of how a capitalist social formation might be reproduced.

But there was a relative neglect of the forces of production, the study of which can’t be performed on a purely philosophical level but requires some detailed inquiry into the technologies of the day. Althusser does not ask after Marx’s interest in Charles Babbage’sfield studies in industrial technique, or his readings in German scientific materialism, where the science and engineering and their impact on the forces of production were a lively concern. This was unfortunate, given how rapidly the forces of production changed in the late twentieth century, changes those under the Althusserian spell rather neglected. And one might note here that this made the forces of reproduction even harder to fathom, and no connection was possible to Marxist-feminists such as Donna Haraway whose work was surely centrally connected to the question of the forces of reproduction.

Note that Althusser’s metaphor is the anatomy of the economic, not its metabolism, a term Marx uses in Capital vol. 3 that has proven very useful for green Marxism in thinking the Anthropocene. There’s a sense in which whatever the merits of Althusser’s influence in rescuing Marxism from economic-determinist vulgar thought, it prevented it on the other hand from not being vulgar enough, and really trying to grasp the historical development of the forces of production.

In other respects, with Althusser there was progress. The state, in this new dialectic, is not the embodiment of an Idea, but the instrument of the ruling classes. In place of the essence-appearance metaphysic, is a relation between separate and equally ‘real’ instances: economic, political, ideological, which relate through their structural differences rather than as expressive components of a whole. What was civil society, of the sphere of needs, becomes properly the mode of production, an historically specific form in which needs are socially met. It remains, in Marxist fashion, the determinate factor, but “in the last instance.” Its effectivity may be over-determined by, among other things, the political or ideological superstructures. Indeed, Althusser asserts, “the last, lonely hour of the ‘last instance’ never comes.” In this ‘dialectic,’ relations are separate and external to the terms they permutate. In this case the instances (economy, polity, ideology) are each separate levels with their own internal ‘contradictions’ between terms, each of which is then at a meta-level (over-determination) in a relation of externality and effectivity to each other. Goodbye Hegelian dialectic – negative or not.

This might be a grand and rather ironic example of what Guy Debord and the Situationistscalled détournement: the copying and correcting of past ideas, texts, materials, from past to present, with no regard for property or propriety. But détournement is a topic for another time. Where Debord advocated it a means of cultural and ideological production that abolished all claims to property and propriety, Althusser did the opposite — he established the property claims of those who held the philosophical keys to correct method.

The reason this appeared so urgent at the time takes us out of the those coordinates of the ideological field governed by academic intellectual life, and further into those governed at the time by the intellectual life of the communist party. What was at stake was a double question: who would have authority over Marxist discourse for the party? To which revolution would the party — and its intellectuals — owe allegiance?

The first footnote in Althusser’s text ‘On the Materialist Dialectic’ (1963) is not to Marx or Hegel or Spinoza, it is to Roger Garaudy. Who the fuck was Roger Garaudy? Trust me: you don’t want to know. Garaudy was the kind of hack who passed for a ‘thinker’ within the French Communist Party of the time (and whose later career is to ignominious to even mention). As is often the case, particularly with Marxist thinkers, the ideological field for Althusser was shaped by institutional figures and forces who do not even appear if one studies ‘library Marxism’ in graduate school.

Althusser’s celebrated early works all happen between two world-historical events: Khrushchev’s ‘secret speech’ of 1956, in which he revealed a tiny portion of the crimes of Stalin, and set about a partial de-Stalinization of the Communist movement. The other key event is the Sino-Soviet split, which starts to unfold from 1960, and led to break of the Chinese Communists from the Soviet ‘camp by 1965.

Khrushchev’s speech led to an ideological ‘thaw’, but also to a profound crisis for the western communist parties. A rather vacuous ‘socialist humanism’ became the prevailing ideology, partly inspired by a turn to the young Marx. This current saw Marxism as a continuation of the bourgeois enlightenment project. In some respects, this was a return to the popular front style of thinking of the inter-war years.

The Sino-Soviet split was over many things, of which ideology was probably the least important. Still, Mao did not follow the de-Stalinization line. I remember, when visiting China in 1987, that one could still find portraits of the “four beards” on the walls of official party buildings: the four being Marx, Engels, Lenin and Stalin. Their profiles, one in front of the other, would usually face a portrait of Mao on the opposite wall. In short: Mao was the true successor in the party’s Apostolic Succession, Stalin included.

The rupture between the Soviet and Chinese parties had its impact within the western communist parties as well. The Chinese revolution had appeared as a vindication of at least one idea of Lenin’s: that imperialism would break at its ‘weakest links’ — a form of over-determination in the Althusserian dialectic. The Chinese appeared to be trying to avoid the bureaucratization of their revolution. They seemed to want to do something different to the building of a massive heavy industry that simply reproduced under socialist conditions the same alienated mass labor as happened under capitalism.

As with enthusiasm for the Russian revolution, western enthusiasm for the Chinese revolution was based on very limited information. Since taking power in 1949 Mao appeared to have reformed agriculture, combatted illiteracy, embarked on a huge, labor-intensive program of national reconstruction, all with an aura of egalitarianism and purpose. The human costs of all of which were apparent to almost nobody in the west, whether on the right or the left — the Situationist René Viénet and his comrades excepted.

When the split opened up between the Soviets and China, not a few western communists opted to support China, either within the mainstream communist party, or by leaving it. In France, the party made the mistake of expelling the ‘Maoists’ en bloc, enabling them to swiftly set up a rival party of not negligible size. While there would be splits and factions, Maoism would be a strong current on the French left – much more so than in many other western countries.

Althusser did not leave the pro-Soviet Communist Party of France. His relation to the party and to China question is a rather subtle one. Certainly, ‘young Althusser’ texts can be read as formulating, at a very high level of sophistication a Maoist ‘line’ of sorts, or at the very least one opposed to the politics and culture of the Soviet thaw. Certainly several key students of his were active in one or other Maoist formation.

It is also possible to read Althusser, strangely enough, through what is usually thought of as the ‘voluntarism’ inherent in Lenin, Stalin and Mao’s thought, best expressed in the latter’s slogan “put politics in command.” This would be the idea, even the practice, of considering either ideological propagandizing or political mobilization as the lever via which the whole social formation would be transformed, as in Stalin’s own Cultural Revolution or later as Mao’s ‘Great Leap Forward’. The economic, as the realm of needs, needs a force from without to transform it.

Hence: “a revolution in the structure does not ipso facto modify the existing superstructures and particularly the ideologies at one blow.” Here Althusser appears to complement the Maoist critique of what had gone wrong in the Soviet Union: That the revolution had not been ideologically and politically vigilant enough. Moreover “the new society produced by the revolution may itself ensure the survival, that is the reactivation, of older elements through… the forms of its new superstructures….”

Is it too much to see here an echo of Stalin’s darkest thesis, that of the ‘sharpening of contradictions’ after the revolution? Not to mention Mao’s extension of it to constant mobilizations which, depending on your point of view, were aimed either at preventing the formation of a counter-revolutionary superstructure – or were meant merely to keep Mao the old tyrant in power.

Ideas travel in strange ways. However much Althusser may (or may not) have meant his position to be a Maoist one (a ‘superstructuralism’ but hardly a voluntarism) it ended up being something quite different, particularly in the Anglophone world: a legitimation for the ‘long march’ through the superstructures of a generation of intellectuals, fighting the good fight in the academy, or the media, or the arts, in imaginative but rarely any actual contact with, organized labor.

While in this essay I am critical of the legacy of Althusser today, I want to pay tribute nevertheless to those for whom his texts were one source of inspiration for a life of militancy, in France and elsewhere, often of considerable personal sacrifice. These are people whose names are only known to a few, who gave up lives that were in some cases of high privilege, to work sometimes under assumed names in factories or industrial towns. They could be rather dour and prickly – the basis in fact for Deleuze and Guattari’s portrait of the ‘sad militant’. But particularly in the ‘red decade’ in France (1966-1976) they did their best. For me that is always to be remembered with honor.

This might then be a thumbnail of the ideological field into which Althusser made his most influential interventions. It need only be added that his institutional location was not an insignificant one: the École Normale Supérieure (ÉNS). He taught at the absolute apex of a rather rigidly hierarchical educational system. The Grandes écoles in France produce the elite in each of there respective fields, in the case of the ÉNS – intellectuals. Sartre had been a normalien before him, as were Derrida and Foucault, in whose training Althusser had a hand.

A striking number of western Marxists were ‘outsiders’ of one kind or another, marked by difference. Even the archetypal ‘French intellectual’ Sartre was actually from Alsace. Several were German-speaking Jews. Althusser was a pied noir – a person of white French background born in Algeria. But more significant to our story is Althusser’s cognitive difference. He suffered periodic episodes of depression (and according to Eric Hobesbawm, quite extreme mania). That he murdered his wife while in an irrational state ought not to go unmentioned. I note this also because of the irony that Althusser is one of the sources for a kind of universalist and rationalist stand in continental philosophy, and yet could not have been further outsie the personae of the ‘universal rational man.’

And yet Althusser was also a ‘insider’, a Marxist and at the lofty ENS, teaching philosophy, in a country where philosophy actually matters. Unlike in the Anglophone world, philosophy is embedded within the French school curriculum. It informs a wide range of ideological processes. While Althusser would reject some of Gramsci’s ways of formulating the problem, he would surely have understood the minor but not-insignificant role of philosophy in sustaining what for Gramsci would be called hegemony.

Althusser contributed to a kind of counter-hegemonic base-building which produced in France for a time a quite interesting anti-capitalist cultural sphere. Althusser created an intellectual base for a Marxism that did not need the Communist Party to authorize it, which was one of the conditions of possibility for a non-communist intellectual left which could almost endure what Felix Guattari called the ‘winter years’ of the 1980s.

The real significance of Althusser is in the transition from a Marxism of the party to a Marxism of the academy. The means via which he got Marx from one to the other are now moot. It is rather like the fable of Captain Cook’s axe: first the handle was lost and replaced, then the head was lost and replaced, and yet it remains Captain Cook’s axe. Curiously, this severing of Marx from the actual party was in very different fashions also the goal of the Lukacs of History and Class Consciousness and the Sartre of Critique of Dialectical Reason. In the first case, the party was strong enough to shut this rival down, in the latter case to ignore it. But Althusser of the ÉNS managed to establish a parallel kind of authority over Marxism, independent of the party, and which would perhaps even outlive it.

Some elements of the text ‘On the Materialist Dialectic: On the Unevenness of Origins’ might help explain this move. It is among other things an ur-text for the notion of a capitalized ‘Theory’. In Althusser, this Theory was supposed to be the guarantee of the scientific character of Marxism, of its break with ideology, and a defense against ideological back-sliding. It was not to be. It never became an infallible debugger of method. Rather, we have had to learn to live with what Stuart Hall famously called a Marxism without guarantees.

Althusser stressed the break between Hegel and Marx. He also – rather fatefully – offered a pluralist rather than a mono-causal philosophy of history. (Remember here the three kinds of over-determination). Having defended this against the Hegelians, in this text he shored up the other flank, and defended this limited pluralism against what he calls a “hyper-empiricism.” Once one has more than one historical dialectic – why not lots and lots and lots?

The answer is to advance a theoretical practice. Here Althusser claims to follow Lenin, in the insistence that without correct revolutionary theory, there can be no correct revolutionary practice. What he adds is that the production of that theory is itself its own kind of practice. If the Leninists had professionalized political practice, making it a specialized form of labor, Althusser makes theoretical practice a specialized practice.

Althusser: “ideology is not always taken seriously as an existing practice. But to recognize this is the indispensable prior condition for any theory of ideology.” Oddly enough, one of Althusser’s precursors here is Bogdanov, for whom ideologies were products of real, material practices whose function was to motivate and coordinate labor. Althusser (and his student Dominique Lecourt) are orthodox, even vigilant Leninists in their hostility to Bogdanov, but it is curious that a selection of Bogdanov did appear in the series Althusser edited. Certainly Bogdanov, not Lenin, was Althusser’s precursor in breaking with Hegelian interpretations of Marx!

Althusser: “The theoretical practice of a science is always completely distinct from the ideological theoretical practice of its prehistory: this distinction takes the form of a ‘qualitative’ theoretical and historical discontinuity which I shall follow Bachelard in calling an ‘epistemological break.” There is both the beginnings of something here but a lack of follow through. A more practical study of how sciences constitute themselves in and against an ideological field would seem here to be an excellent suggestion.

But Althusser will not be a return towards a genuine study of science, in the manner of JD Bernal or Joseph Needham. Nor is he a precursor to science studies as it will later flower. Rather, he gets stuck at the level of asserting a merely formal break between a science and the ideology which precedes and surrounds it. There will be those in science studies who will pass over their debts to the Marxism of Bernal and Needham by loudly declaiming their distance from this Marxism of Althusser.

What decides for the science of Marxism is Theory, “none other than dialectical materialism,” although a rather different one to the diamat constructed out of random bits of Marx and Engels by Althusser’s Soviet counterparts. It is to decide in advance what the dialectic is, before the encounter in a given investigation of a new situation or problem. Before beginning any investigation, or any practice, researchers “need Theory, that is, the materialist dialectic, as the sole method that can anticipate their theoretical practice by drawing up its formal conditions.” One could juxtapose this not just to Needham, but also to Bogdanov, at least as I read them. In their hands, theory is a matter of extracting concepts (the ‘dialectic’) from particular practices of the production of empirical and scientific knowledge, and then the speculative testing and adaptation of them to other fields.

In other words, theirs is a genuine pluralism, within a general speculative method, but also with specific empirical tests of the validity of that method in each domain of knowledge production. In short, Althusser wants democratic centralism – the party of Theory’s decision is final – whereas Bogdanov and Needham are more ‘syndicalist’ in their approach to the comradely cooperation of knowledge.

Althusser’s is not quite as ambitious a view as that which Plekhanov took over from Engels, in which a dialectic can be applied even to the natural sciences. Althusser’s ambitions do not extend that far. But he does appear to want a Theory that can legislate outside the bounds of the natural sciences. He is particularly on his guard (as Leninist always are) againstspontaneity, particularly among new-fangled practices of the production of knowledge.  The new social sciences and humanities fields in particular are not to think they are self-legislating – autonomous.

The most ambitious claim of this text is to ground the general method of a theoretical practice. This comes, curiously, out of a gentle but thorough critique of Mao’s text ‘On Contradiction’. What follows strikingly is not so much a theory as a metaphor. Theoretical practice is to be understood on the metaphor of production in general, but in a rather peculiar way. The production of knowledge starts with Generality I: with general concepts, the existing ones of ideology, as raw material. They are transformed by the labor of Generality II. These are the means of production, more or less contradictory, of the production of knowledge of a given moment. The work of Generality II on Generality I produces knowledge as specified concepts, a concrete generality. In short: “theoretical practice produces Generalities III by the work of Generality II on Generality I.

It should be apparent at once that this is metaphorical. Nothing concrete about the labor of the production of knowledge appears here at all. Althusser even says “if we abstract from men in these means of production for the time being….” But the abstraction never ends. What follows is then the dogmatic assertion that there is an epistemological break between Generality I and Generalities III, guaranteed by the vigilance of Theory over the transformative work of Generalitiy II. And what results is not the concrete-as-such, but the concrete in thought. The criteria of valid knowledge are all internal to the theoretical procedure, understood metaphorically to be a labor procedure. True knowledge is that which Theory guarantees, and no other. It is the theoretical concrete which is knowledge.

One could mount an internal critique of this version of what Marxist Theory ought to be doing. But I think it more useful to put it into the ideological field, and ask: what ‘work’ was it doing at large? To my mind, Althusser is trying to set up a procedure for the coordination and validation of correct Marxist ‘practice’ within the division of labor of the university, or even the ideological apparatuses writ large. One that appears to parallel and supplement that of the party, but which actually replaces it.

This has two aspects. One is the replacement of the authority of the Party with that of Theory. To some extent the controversy that this aroused is moot, given the decline of that very Party, and the marginal status of those that would try to reproduce it. Interestingly, by the time we get to the later work of Althusser’s student Alain Badiou, there are four kinds of event which produce the subject and its truth, of which politics is only one, not dominant one.

The other aspect is the question of the coordination of different kinds of knowedge that might claim to be part of a larger Marxist project within the university. This is rather more interesting. It is if anything an even more significant problem today. Althusser’s solution is a ‘democratic centralist’ one, a para-Party called Theory, which has both legislative, judicial (and policing) power as to what constitutes knowledge, at least outside the domain of the natural sciences. It is as opposed to ‘spontaneous’ theories and their lateral, transversal flow between sites of work as the Leninist party was to all forms of spontaneity, whether it be in the style of Rosa Luxemburg, or Antoine Pannekoek, or Alexander Bogdanov.

Not surprisingly, given the institutional context of the ENS and French hegemonic culture more generally, this para-Party is essentially that of philosophy. Marxist philosophy will legislate, judge and police all the other forms of knowledge in a remarkably similar way to how non-Marxist philosophy has always considered itself to have such powers in the French context. The counter-hegemonic (in Gransci’s terms) is a mirror of the hegemonic. It does not have its own form. In this regard, and to make an extreme provocation, we have to conclude that Althusserianism was a kind of reformism in the domain of knowledge and culture. Unlike, for example, Bogdanov and the Situationists, in their rather different ways, there is no imagining here of a different form for a counter-production of knowledge.

Althusser’s metaphoric approach to the question of knowledge production led in at least two different directions. One was an even more hyperbolic rationalism, but more on that later. The other was to make the metaphor more ‘real’, in the sense of examining actual, material processes of the production of knowledge, and in particular of those kinds of knowledge which seem to have direct power-effects. This is the path of Michel Foucault. Granted, this approach leant as much on Nietzsche’s diagnoses of the will to power in forms of knowledge or ideology. And granted, this in many ways became a new kind of doxa in the humanities, where ‘power’ could be found everywhere and nowhere.

Still: there seems to me something positive in this general procedure, and one with more affinity for Marx than might be at first apparent. A Marxist approach to knowledge, and in fact even of natural scientific knowledge, should enquire into the material practices of its production, and moreover, should see itself within the limits of those means of production and not at some Archimedian point outside of them. Much of science studies actually took this kind of trajectory.

Here we have to mention Althusser’s very curt dismissal of any approach (such as that of Bogdanov) which starts from the reality of that which the apparatus and labor of knowledge produces. Althusser simply asserts that if one starts from sensation, even in this historically grounded way, one has no way of filtering in advance what is ideological from it, and thus of producing a science. The reply to this is obvious: Althusser’s rationalism has no such procedure either. It is simply asserted that the vigilance of Theory will perform this miracle, and do so in a universal way.

But why not a method for all the sciences? Particularly those that impinge most heavily on us. Here what might be worth developing, out of Bogdanov and Needham, or out of Donna Haraway and Karen Barad, might be a concrete, historical, specific approach to the actual production of knowledge in both natural and human sciences. What we need is not something like Mao’s ‘On Contradition’, to legislate as Theory for all of knowledge. What we need is many versions of Capital, actual critical accounts of other kinds of knowledge,particularly of the forces of their production, besides political economy. Althusser would of course consider this a “hyper-empiricism.”

If Althusser has merit still today, it is in his sly way of always asking: what is at stake in the politics of knowledge at any given world-historical moment? Let’s quote here how he defined his own moment: “what will later be called by a name which does not exist as yet… when in the struggle for peaceful co-existence the first revolutionary forms are appearing in certain so-called under-developed countries out of their struggles for national independence.” These are not those times. The capitalist west no longer confronts twosocialist camps, one sprung from the colonized world. Rather, I take the defining feature of the conjuncture to be a now-globally victorious regime of commodified production to be confronting the limits imposed by its own destabilizing of the metabolic processes of the planet itself.

It turns out there are resources for thinking such a moment in the Marxist tradition, from Marx’s own concept of metabolic rift, to Bogdanov’s tektology, Bataille’s general economy, Asger Jorn’s ornamentation, or Sartre’s practico-inert. But one might yet retain from Althusser the break from the Hegelian ideological field, to the extent that it saw labor as in a dialectic of spiritualizing nature, imbuing it with rational form, and subsuming nature into teleological project. Where Adorno reversed the Hegelian dialectic and ended up with nothing but the consolation of aesthetics, Althusser replaced it with one that at least gestured in a formal way to the problem of how to organize labor and knowledge.

The thing about being a recovering Althusserian is that one can’t help remembering the good times. Being on Althusser really does feel great. It makes certain problems disappear. For example, one is no longer trapped in the oppressive totality of Hegelian Marxism, and yet nor does one have to return to the world of ‘economistic’ Marxism. One can fly free from all that! (Ah, but as in any addiction narrative, there’s a price to pay…)

So on the plus side, the problematic is no longer constrained by those readings of Marx that see him as essentially putting the Hegelian dialectic back on its feet, or retrieving its rational kernel from its mystical shell. Althusser’s essay on the young Marx already opens up this dimension. But he is an alternative also to those western Marxists who, one way or another, tempered their Hegelianism with a dose of Kierkegaard.

This came in a lot of favors. Lukacs centered his Hegelian totality-in-process on the proletariat as universal subject-object, which frees itself from reification and acts on and as the totality. But even here there is something of a Kierkegaardian irrationality about the proletariat in action, a kind of revolutionary leap of faith. It is a figure that will recur in various ways in Sartre, Badiou and Zizek. Adorno and Sartre, in rather different ways, cut their Hegel with even more Kierkegaard to prevent Lukacs’ totality from self-closure. In Sartre, individuals only ever temporarily subsume themselves into the movement of the totality. In Adorno the dialectic itself is to attend to the unrecoverable fragment. It twists away from the extorted reconciliation of exchange value.

Walter Benjamin’s relation to Kierkegaard is complicated, but let’s just say that Althusser was most certainly an alternative to taking too many hits of Benjamin, wherein history is only ever allegorically present, in the form of fragments that are shot through with a messianic time. All of these Marxisms had a tendency to reduce everything to commodification and its attendant effects: reification, extorted reconciliation, inter-passivity. Either history had become the bad totality of exchange, as in Adorno, or the good totality perpetually postponed where the rational meets the real, as in Sartre.

The Althusserian decision against Hegel and Kierkegaard with Marx comes at a price, however. One is done with totalities and fragments, but has to contend instead with the straying apart from one another of the three (or four) levels of the social formation. This shows up in three post-Althusserian tendencies. One is that version of theory which I think has to be called ‘Jacobin Marxism’. This is particularly clear in some of Althusser’s students, such as Nicos Poulantzas, Etienne Balibar, Jacques Ranciere, and on into other work such as Chantal Mouffe and Ernesto Laclau. The feature of this tendency is to isolate Politics with a capital-P as the decisive level or instance of the social formation, at the expense of any larger sense of a political economy. Politics becomes absolutely autonomous and even ontologically prior.

A second tendency was cultural studies. In rather different ways, cultural theorists from Stuart Hall to Judith Butler took off from Althusser’s famous essay on ‘Ideology and the Ideological State Apparatuses’ and used it to legitimate the study of the ideological (or cultural) as an autonomous sphere with its own materiality and formal laws — to be understood using the tools of semiotics and rhetoric. It was even imagined that the cultural could in some sense lead revolutionary change while economic struggle stagnated.

A third tendency one could call Hyper-Rationalism. There are three kinds of regional knowledge in Althusser, corresponding to the three relatively autonomous levels within the social formation of the economic, the political and the ideological. But philosophy stands apart, legislating for what constitutes scientific knowledge in all domains — indeed for any science. What marks the difference between an ideology and a science is that the former produces subjects who misrecognize themselves in it, while sciences do not. Althusserian rationalist epistemology becomes mathematics as ontology in his student, Alain Badiou, which becomes the speculative realism of his student, Quentin Meillassoux.

All of these tendencies seem to me to still be dependent on Althusser and to point away from the two crucial encounters in our own times. Encounters for which neither Hegelian or Kierkegaardian nor economistic not even Althusserian Marxism are all that adept. One is the technical transformation of the forces of production and reproduction. The other is the metabolic rift opened up by the application of those forces, via the private property form, on a planetary scale. For that we have to look elsewhere, once we have weaned ourselves off Althusser, and certain other habits of thought. Habits which, like all addictions, reproduce themselves within our thought and within institutionalized discourse in pursuit of their own necessity, regardless of what takes place in the world.

This article is about cultural techniques and software culture. The notion of cultural techniques stems from German media studies and refers to a range of epistemic, embodied, cognitive and affective orders. It shows its particular usefulness in how it extends our conventional notions of media.1 Media become more than media, and can include ‘inconspicuous techniques of knowledge like card indexes, media of pedagogy like the slate, discourse operators like quotation marks, uses of the phonograph in phonetics, or techniques of forming the individual like practices of teaching to read and write’ as well as maps, doors, operations and practices of law, and so much more. Cultural techniques participate in the formation of subjects, as well as constitute ways of knowing and organising social reality. I am in this context interested in how we can read some aspects of software culture and organisation of the labour of programming in relation to cultural techniques. Modes of organisation as well as practices of coding represent ways in which code and software regulate social reality but that they are also being regulated as a prioritised technique in digital economy that is at times related to discussions concerning cognitive modes of production. In this article, the notion of cultural techniques is coupled with a concept from a very different tradition to that of German media theory: I engage with cognitive capitalism and specifically cultural techniques of cognitive capitalism. My interest lies in addressing software cultures and techniques of work and labour as ways to understand the constitution of our capitalist technological environment. This set of practices and techniques has to do with management of code and code work, hence, as a punchline to summarise the article early on: cultural techniques of cognitive capitalism should not only to be about the cognitive, I argue, but a range of practices, techniques, management and organisation that happens outside the brain. In other words, what sustains the cognitive is a field of techniques. It is an argument that someone within the field of brain sciences could make as well as scholars observing that any cognitive activity happens on a distributed field of the self, and that the brain is anyway extended as part of its surroundings. Besides neurosciences and cognitive theory, for instance theories of cognition and affect in design are taking such ideas into account.
Yet my focus is on aspects of media theory and software and I will leave the otherwise interesting cognitive science contexts out. Of course, one can excavate a media archaeology of cognitive capitalism through its ties with cognitive sciences and the wider scientific discourse concerning the brain, communication and cooperation but here I focus on software cultures and a more technological understanding of the cognitive. We have to be aware of the way cultural techniques relating to code and programming are themselves one important framing of the cognitive. This idea relates even to very early formulations of the role of the computer as universal, programmable machine. Already in the 1940s the computer pioneer Alan Turing proclaimed that the future of computing was situated in the office—in the ‘office work of programming’ to be exact.
Following the notion of cultural techniques, I want to excavate the specific techniques that sustain the notion of cognitive capitalism as used by, for instance, Yann Moulier Boutang. The allusion to the powers of the brain as communicative, organisational and a coordinating factor in production of value needs to be investigated in relation to its technological conditions as well. This sounds like a media archaeological argument but here it will be approached through the concept of cultural techniques, particularly as it has been formulated in recent German media theory. Following, for instance, Sybille Krämer and Horst Bredekamp’s lead we can extend cultural studies investigations towards material, scientific and technological operations.
For sure, Jobs would have made a great lecturer for the first year ‘What is Media Studies’ course, had he not wanted to start developing his business empire. This is not merely a joke but a reference to how in this talk and throughout Apple’s early business strategy the focus was on education and pedagogy: free computers to schools and the early education of children to become users of the new smart future of computerised learning. Jobs’s concern was constantly about new generations—of creating conditions for psychotechnical drilling and training in the emerging computer culture. Jobs, the software-pedagogue, is continually concerned with the user, techniques and habits, and with tapping into what could be called the involuntary part of the human being: ways of drilling the user into suitable conditions of use. This context flags the need to map the wider set of relations between institutions or corporations, interface technologies, business strategies and psychotechniques as the cultural techniques of cognitive capitalism. From the focus on early film theory, such as that of Hugo Münsterberg (1863–1916), we move to the digital technologies of habituation and drill: ways of seeing, acting, gesturing that perform the reproduction of the worlds in which relations of value production and exploitation can tap into the bios and zoe of living bodies and the social relations that characterise life in digital culture. This is also something that crosses into the field of cognitive sciences as well as design, even if I have to neglect that aspect here. In the talk, Jobs demands a break with the old habits of media consumption and techniques, and he encourages planning new ways of engaging with software worlds. His encouragement sounds like it is addressed to both the producer and the user. Optical video disks, able to hold tens of thousands of images, or an hour of video, should not be solely used to play a movie but should also be used to take full advantage of its random access possibilities, he insists. We should learn more about the affordances of technologies—what their potentials are, and what the technological future might look like. The Lisadraw software is what, in his own words, allows Jobs to try to be an artist and deal with visuals as objects—to move, shrink, change textures, airbrush, soften and harden edges—all of which makes him realise how the talentless can draw. Besides the implications noted by a range of commentators about the renegotiations of talent, skill, artistry and, of course, creativity in post-Fordist economies and software culture, we are able to realise what drives Jobs’s vision of code.
We should pursue the question ‘if code makes the world, then who makes code, and what sustains such operations?’ I want to trace a slightly longer line of thought that relates to code, code work and organisational ideas concerning software and digital culture. In this idea of programming, which relates not only to code but the user and producer as well, we have other examples from the burgeoning new media culture of the 1970s and 1980s. ‘We need to be able to program the programmers too’—a striking emphasis voiced by Charles Simonyi, a Hungarian-born computer scientist working at PARC Xerox Palo Alto labs in the 1970s. Indeed, our computer histories often mention only the more famous products of Simonyi and others at PARC—the Bravo text editor, for instance, and the work that contributed in part to the success of Microsoft (Word) and Apple corporations. They relate to the technological solutions and the spirit that Jobs later captured as part of his hegemonic vision of digital culture: users are productive and creative and with the help of systems that combine efficient memory management one can develop more complex but easy to use computer graphics content to release our brain power. Instead of Bravo, I want to focus on ignored aspects of the work at PARC and the ‘software factory’ that produced Bravo—the question of what grounds the work on text editors and computer interfaces and, more generally, software itself. This means focusing on the notion of metaprogramming. In other words, how do you organise work of and in software environments, and program the programmers: how is software work organised as a factory?
Cognitive capitalism, as used by Yann Moulier Boutang, refers to the widespread cultural and corporate harnessing of ‘invention power’. It features as an integral part of current capitalist modes of organisation of production and value creation. So where does invention reside and take place? It becomes visible and perceptible in the amount of investment in education, training and other enhancements of social and cognitive capacities. It also persists as a way to infiltrate the social more widely. Our ways of talking, acting, gesturing, remembering, expressing, exploring and thinking are part of the wider picture in which capitalist organisations, and processes of accumulation, are taking into account externalities. That capitalist accumulation is dependent on externalities is not new per se. Externalities referring to ‘collateral effects, by-products or joint production’, are, of course, an essential part of any social activity as people in cultural and media studies might take for granted.18 But this realisation does not feature in economic theory textbooks that often. Boutang insists on the importance of accounting for externalities, not only in the classical sense of taking into account negative externalities (industrial pollution being a common one) but also positive ones and their role in the commodity production and the wider milieu of production incentives. Think of gentrification and cafes, restaurants and other services for the younger creative class—a topic so thoroughly discussed in context of creative cities.
Communications in shared spaces count as externalities. They are conditions for the possibilities of sharing and all other characteristics that are now pitched as essential for this cognitive sort of labour on which capitalism rests. Of course, to an extent, some of the discourse on the common has tackled related themes. Years of post-Fordist theory, autonomist Marxism, and theorists from Hardt and Negri to Paolo Virno and Maurizio Lazzarato, Matteo Pasquinelli and Tiziana Terranova have offered their primarily Italian arsenal of concepts as insights into the cultural processes at the core of labour–capital relations in digital culture. Yet the question as it relates to technology persists and perhaps demands a more accurate finetuning, especially when considering software. Boutang insists on the specificity of a digital phase of capitalism, which, he argues, is not reducible to actual technologies. Boutang claims that technologies are only a necessary, but not a sufficient, condition. Instead, this ‘mutating capitalism’ is less about muscles and more about brains—the Chattering, or Tweeting, Man, instead of the Factory Man. This leads to the realisation that Boutang advocates: cognitive capitalism is not solely about the technological, it is also about the ‘appropriation of knowledge’ and ‘the use of new information and communications technologies’. In other words, it’s more innovation than hardware—or even software. Hence, Boutang also wants to keep his eyes on modes of education supporting the innovation work. On the economic level, Boutang is able to mobilise a range of interesting insights that discuss the regimes of knowledge and labour in this new mix. In his simple historical model, cognitive capitalism is set as the third phase of a longer development, following mercantile and industrial capitalism. This is also a move towards immaterial capital, knowledge and knowledge economy. The mutations of capitalism are not merely phase shifts with a definite break from earlier periods, but a rearrangement of technologies, infrastructures, skills, procedures and capabilities in relation to their role in value creation.
The primacy of knowledge for value creation is evident in the centrality of political debates around property rights, management jargon, a focus on organisational methods and the wider role that human resourcing as well as pedagogy plays in the corporatisation of various industry sectors. By bringing in elements of communication and networks, we are immediately inside discussions concerning organisations and management of such flexible institutional settings, where work and value creation need to be somehow paired in suitable ways. A lot of this analysis can be seen to be predated by, for instance, Luc Boltanski and Eve Chiapello’s analyses of management language and practices since the 1960s, and a move towards a more individualised definition of work in terms of ‘creativity, autonomy, and flexibility’.  Boutang’s theory needs to be also addressed critically and with an eye towards possible shortcomings of the notion of cognitive capitalism. For instance Steven Shaviro has noted that we should critically interrogate various aspects of Boutang’s theory’s main claims. Is there really such a definitive and clear break from earlier regimes of industrial labour? What are the harmful downsides of communicative brainwork, such as exhaustion? How does exploitation of non-work hours extend the reach of the corporation to the wider social field of our thinking, doing and gesturing, and harnessing ‘free time’ as part of value accumulation for the corporation? Indeed, isn’t cognitive capitalism merely describing a situation of rather cynical colonialisation of time and affect that reaches the most intimate spheres of subjectivity? What is called ‘bioproduction’ is actually an effective exploitation of the resources of non-paid time which, however, becomes increasingly central for value production. It is already something visible in, for instance, Terranova’s important account.
What Shaviro flags, and what has been articulated in length by Matteo Pasquinelli, is the danger of conflating such perspectives with the Silicon Valley idealisation of commons and other shared externalities that benefit us all, without being able to account for the actual unofficial investments in work and other practices to support the labour in/of/for commons. This resonates also for instance with Jodi Dean’s term ‘communicative capitalism’ which flags the conflation of democratic ideals with technological infrastructures. The dirty, libidinal side that Pasquinelli is after does not feature in Boutang’s more polished view, which, I argue, would benefit from more rigorous media studies perspectives. By now, it should not come as a particular surprise that I want to discuss it in terms of cultural techniques. How to combine the two sides of European theory: the German love for the machine worlds, and Italian/French analysis of politics of the immaterial kind, but also attached to the world of network cultures?
As one way of articulating this question of the mediatic in cognitive capitalism, I want to pitch the idea of cultural techniques of cognitive capitalism. This is merely a theoretical opening with one case study relating to software and labour; it is in need of further elaboration by more empirical and historical work. Such a notion of cultural techniques of cognitive capitalism is a cross-fertilisation of two traditions that have not spoken to each other too much yet. I am referring to the political theories coming often from the tradition of Italy and their developments in France— the Autonomia-movement and post-Fordist political analysis that I have just nodded towards and which one has to admit is a very heterogeneous field. And at the other pole of this theoretical crossbreeding I introduced German media theory, which is not only represented by Friedrich Kittler, or media archaeology (of for instance Siegfried Zielinski or Wolfgang Ernst), but also by such terms as ‘cultural techniques’ as discussed by a range of scholars from Bernhard Siegert to Markus Krajewski and Cornelia Vissman to Sebastian Vehlken.
As I have already briefly introduced, cultural techniques can be seen as ways of extending the media material ethos to a wider set of concerns where media are related to ‘ontological and aesthetic operations that process distinctions … which are basic to the sense production of any specific culture.’ More specifically and perhaps more enlightening is Siegert’s emphasis that not all a priori conditions of culture are technical a prioris, ‘but involve the materiality of media in the broadest sense’. We can map the different operations that link humans and media: material media studies is once again able to offer a way to understand society. Speaking of contemporary creativity and, for instance, software from a discourse of fun we can turn to the management of creative work. This includes the management of invention that runs across the contemporary cultural techniques of capitalism as a theme that is grounded in new forms of exhaustion, besides the at times enthusiastic—and hence inflationary—excitement for brain power and cooperative peer subjectivity. This aspect has been discussed in recent years by Franco Bifo Berardi but also relates to the thesis proposed by Bernard Stiegler: instead of smart cultures of skilled professionals in communicative industries, we should acknowledge the systematic stupidity and proletarianisation of ‘creative’ work. In Stiegler’s words: ‘We thus have pure cognitive labor power utterly devoid of knowledge: with cognitive technologies, it is the cognitive itself which has been proletarianized.’ If we pursue this discussion concerning factories, the proletariat and the grey repetitious aspects of communication, we end up in a discussion of techniques and operations. I argue that we need to pursue this in terms of the operations that offer a link between human practices, media and society, as well as political economy. I want to flag this as a necessary supplement to the more optimistic, and perhaps more ‘Californian’, take by Boutang. Indeed, one needs a slightly more evil approach, in the manner proposed by Matthew Fuller and Andrew Goffey. They call this evil media studies. As part of their larger project, Fuller and Goffey take up on what they pitch as grey media; the tiny details, glitches and repetitious occurrences from current software and social systems, which in their minute detail can be seen as defining a whole field of interest in management cultures as media. Spreadsheets, workflows, auditing, call centre procedures all become understood as part of the constitution of the reality of management—and management as the reality of media
It’s not that technology is entirely missing from Boutang’s theory. He is a fan of open software projects, and complements his economic analysis with examples of techniques of the social. One such example includes the call centre. For Boutang, telephone help centres offer an example of the joint project between database-based organisation of possible solutions and the mediating role of the actual worker. The worker is the curator, in real time attending to the needs of the customer and not only through mechanically finding the suitable solution from the database. As a creative worker of sorts, she or he has to demonstrate innovation by identifying ‘cases that fall outside standard practice’ and, where possible, ‘offer new viable solutions’. But from an ‘evil media’ perspective, call centres are endless sources of delay and streamlining, of evasion and systematised boredom. Indeed, through the various steps in the formalisation of (natural) languages into modes of expression suitable to algorithmic logic—and, besides language, also gestures and patterns of cognition—one enters a grey zone where any celebration of virtuosity of language at the centre of the multitude subject sounds slightly exaggerated. Indeed, call centre work is extensively about management of social relations in relation to databases and algorithms, but in ways that produce ‘the cognitive’ as a matter of regularisation and disciplinary formalism. This means a social patterning that refers to a perversely idealised version of the real world that we call ‘work flows’. Unfortunately, such idealisations found in organisational charts rarely actually correspond to the real world if one understands by that the number of exceptions, bugs, errors, mishearings and other events that occur amid actions and expressions, which constitute the core of Fuller and Goffey’s evil media theory. A look at history of software cultures as one of management reveals this other side of cognitive techniques and information work. They are part of a media history of cognitive capitalism, or cultural techniques of cognitive capitalism. But this is not meant as a valorisation or a rosy picture of brains in cooperation but rather patterns of management, and organisational operations and abstractions. If we really want to paint an accurate picture of such post-Fordist theory concepts, we need to take a look at the grey media and cultural techniques.
What is often focused on with user and interface design, as in the emergence of Apple-centred digital culture discourse since the 1980s, can be complemented with the less fun aspects, such as some of the work at Palo Alto labs in the 1970s. For sure, these examples have not been altogether ignored, but the focus of attention has usually been on end-user-oriented innovation such as emphasising user-centered products like the Bravo-editor and interface. PARC worked towards user creativity through meticulous ways of managing memory, graphics innovations and technicalconceptual innovations like the WYSIWYG. Instead of taking the obvious route let’s focus on some other aspects which might, however, be more methodologically boring. We can take Stiegler’s theoretical note as a guideline and track the specific procedures of proletarian approaches to the cognitive, and expand on the theme of software work as factory work. I will do this by briefly exploring one specific example of a research theme and practice at PARC: metaprogramming, or programming the programmers. This is in no way a dominant strand in software culture but it can help us to think through some connections between software, organisation and work: the software factory ensuring the fun for the end user.
Over the past few years, interest in coding has experienced a strong upward surge where the labour and even potential dullness of code crunching is entangled with a cheerful discursive embracing of new modes of collaboration, adaptability and even ‘fun’ that accompany the most discussed software management projects, that is, free and open software, as well as gaming. This is also apparent in political and industry emphasis on wanting kids to get into coding: the word ‘fun’ appears often in reports and motivational material for various school and after-school activities. In general, coding is coded as something of an exceptionally exciting activity that even cynical teenage kids might engage with. Working with software is inspirational and is indeed often seen as a form of self-expression, distanced from the laborious sides of code work. Everyone can code, and coding is a way to release potentials of expression. In the United Kingdom, this emphasis was primarily established by the current Tory-liberal government, whose education plans emphasise the need to increase skills and awareness of programming in school syllabi. The thinking behind such an emphasis was rather straightforward: coding might not feel like work but it pays off, and is the basis for a vision of a Digital Britain. If we return to the 1970s situation, and code as work, for Simonyi, metaprogramming is about improving productivity. This happens by letting the inspirational programming happen on a very abstract conceptual level by a team leader, then implemented by technician-assistants. This narrows the creative aspect to a very limited role, where most of the work is just careful implementation of the broad-stroke approach. In short, metaprogramming is about managing the complexity that goes by the name of code and software. It is just one particular mode of understanding software work and management, but it can give us clues to think about software work in other contexts too. Simonyi’s idea was rather traditional and seemed to point in a direction different from the user-centered and horizontal notions which are now celebrated but which perhaps hide the fact that software work can also be boring and exploitative—in the gaming industry and other creative fields.
Simonyi’s definition of metaprogramming is in this sense about describing ‘an organisational schema, designed to yield very high programming productivity in a simplified task environment which excludes scheduling, system design, documentation and other engineering activities’.  This actually means transferring engineering tasks into localised work groups where high-level skills are not cognitively necessary. Those groups need only the basic technician skills. This is where communication between separate work groups becomes crucially important, not only for the content that is communicated. How does one talk of code projects and software products? How can the language necessary for programming- and engineering-related activity in a design organisation be standardised? According to Simonyi the problem is to work around local languages and through a more universalised take on communication about, and naming of, the objects of communication. Software becomes a language, but in a different way, perhaps, from what Manovich meant. In our case, language is a mode of command, control and management for the benefit of organisational efficiency. This does not mean a completely rigid system. Simonyi leaves this slightly more open when offering his definition of metaprograms as ‘informal, written communications, from the metaprogrammer, who creates the local language, to technicians who learn it and actually write the programs’.  But it still serves the purpose of standardised procedures, which sustain cooperative, synchronised communication. More specifically, in software production this means a focus on aspects other than the long dominant modes of measuring lines of code per hour. Simonyi’s early 1970s work marks a gradual shift in understanding coding and productivity. Having said that, the earlier mode of quantitative measurement is not entirely abandoned and forms the statistical part of Simonyi’s study. Metaprogramming is to be understood through confining the creative aspects of coding to one particular role. The metaprogrammer is the creative coder, the prototyper who writes sample code and feeds it to the technicians for implementation. The technician-coders work under the metaprogrammer and feed back in a cybernetic loop, being managed tightly and efficiently.
One way to understand metaprogramming is to think of it like this: in the manner that before computers were machines, they were human, often females, as before and during World War II. Computers were about teams of humans managed as computational units. The organisation and management of humans labouring as computers was what defined that early ‘computer architecture’ before some technical solutions changed the role of the human programmer in relation to the machine that was now able to number crunch. Metaprogramming implicitly simulates the development of software language, transposed as an organisational diagram. Metaprogramming is about the programs coded, but also about coding the humans as computational aspects of the organisation. The organisation is the modern software computerised environment: abstracted higher level languages are where programming is seen through tentative planning; third-generation languages are actually the metaprogrammer, who then feeds such plans to the compiler; source code is command, but one that needs to be fed, processed and enacted so that it becomes what it attempts to be—a source. Metaprogramming crystallises the idea of software at a particular point in history; it codes computers and people. Coding is labour, and software becomes a product. The materiality of such technological processes involves both ends: materialities of technology and materialities of labour. Techniques of managing and organising labour become increasingly central. Simonyi’s thesis pays attention to the practices of educating and training staff in various ways; a lot of this training has to do with organisational commands, themselves cultural techniques, modes of managing the commands, communicating and error tolerance. They are modes of managing and responding to the patterns of organisational logic. The metaprogrammer might execute a command but only to reach the technician—the technician being the level of support for the higher-level functions so to speak, and a level which is defined by its own operations that are the object of training. Issues of training, communication and management of work and even flexibility are important in this schema. It is not only the amount of executable code produced that is measured, but also the metaprograms—the creative work.
In Simonyi’s words: we have proposed uncertainty absorption for improving the software industry’s ability to deal with the uncertainties inherent in large software problems. Uncertainty absorption—the promise of action which enables others to operate free of the uncertainty—is particularly simple when production can be organized as a continuous process which can be measured, controlled, and hence, optimized. We divided the software production task into an engineering phase, in which the user’s problems are made well defined; and production phase, in which proto-software is produced by a continuous process. The proto-software is given back to the engineers for refinement to create the final product.45 What is fascinating is how the quote from Simonyi pitches the relevance of such a technique like writing ‘proto-software’. It signals also important aspects concerning labour and organisation. I argue that Simonyi’s ideas lead us to think about the wider set of techniques that mobilise ideas about code, software and work. Coding expands to the wider set of communicative tools with which the organisation has to function in relation to customers. What needs to be recognised, even from this short take on metaprogramming, is how considerations of software are entangled with other sorts of cultural techniques. In other words, where software culture is introducing its own range of new and re-emphasised modes of managing the world—from sorting to abstractions (including pseudocode and in this case ‘protosoftware’), error checking and debugging—it is also embedded in how we talk about, read and measure such activities as part of the organisation of software work. This relates further to the issue of measurement of productivity. For Simonyi: Productivity is traditionally defined as the relationship between the output of goods and services and the inputs used in their production. Applied to software production, the output of program bulk should be expressed as a function of the inputs: the time of programmers and other labor and possibly overhead costs.
Coding is not necessarily very creative or fun. The fascinating role of the metaprogrammer doesn’t necessarily even need access to computers, and becomes more that of a general systems planner in a cybernetic sort of organisational diagram. The emphasis on communication and training to specific roles demonstrates no clear-cut ‘post-Fordist’ mentality of a transition from factory-based hierarchical management to a more horizontal method of connecting the brain (to adopt to Boutang’s discourse). Instead, the team-based organisation we find in such early plans as 1970s metaprogramming testify to how differently software production can be understood—not as the horizontal clustering of creatives, as the mantra goes, extending to the idea of the creative end user and the flexible worker, but as the management of various roles, some of which are still quite straightforward technician jobs. Such a realisation might ring true for a range of current software organisations and companies—in the gaming industry, for instance. Whereas this view resonates with a range of cultural techniques in software and coding work in the current economic and organisational settings as well, it also hints at a different lineage from that of Boutang’s theory and some other positive accounts of creative industries and digital economy. Instead, what if we in a way think of the celebration of the creative individual, the post-factory artist type, only as a short entr’acte in the history of organising knowledge production? Even in early formulations—before programming meant software as a separate ‘entity’ from hardware—the laborious nature of programming was underlined. Innovation is embedded in the reality of labour—’dull labour’ to be more accurate.
The idea that cultural techniques of software culture and work are actually often much more physical, repetitious, uncreative and based in rather strict management, disciplinary and formalisation procedures needs further analytical attention. We are gradually realising that digital culture is sustained by hard and repetitious manufacturing processes outside the creative industries circle—for instance, the Foxconn factories in China—but the realisation that creativity is embedded not only in precarious but also in rather repetitious and tiring practices needs to be taken just as seriously. It is in this sense that the certain immaterial production part of cognitive capitalism is completely material to the point of exhaustion. In other words, we focus on the cognitive not only as concerning the thinking brain but as it concerns the wider set of techniques in which brains are connected to bodies and bodies to work patterns, and in which work patterns are set in a complex group of organisations that are abstract ways of tying the different techniques together. Besides specific case studies relating to the entanglement of code and labour, the theme of techniques of the cognitive point towards the need for political media studies, in the sense defined by Jonathan Beller: ‘A political and politicising approach to media studies would insist upon the materiality of mediation as well as reckoning of the material consequences of even the most ostensibly immaterial and abstract mediations.’ A cultural technique approach to code, software and digital culture might take on board notions such as cognitive capitalism, but also pokes the grey mass of the cognitive through excavations into what sustains it. What sort of techniques and technological practices are behind what we call the cognitive, and what sort of archaeological excavations are we able to engage with when we want to understand the differing uses of communication and organisation? An analysis of cultural techniques becomes then a way to extend into issues that are historical, mediatic and, indeed, political.

The advent of “new media” (in common parlance, a loose conglomeration of phenomena such as the Internet, digital television, interactive multimedia, virtual reality, mobile communication, and video games), has challenged many scholars to investigate the media culture of late modernity. Research agendas vary from network analysis to software studies; from mappings of the new empire of network economies to analyses of new media as “ways of seeing” (or hearing, reading, and touching). Efforts have been made to pinpoint where the “newness” of social networking, interactive gaming, or data mining lies and to lay the foundations for “philosophies” and “languages” of new media. For some researchers, the main concerns are social or psychological, while for others they are economical and ideological, or motivated by search for technological determinants behind the myriad manifestations of media. As different as these approaches may be, studies of new media often share a disregard for the past. The challenges posed by contemporary media culture are complex, but the past has been considered to have little to contribute toward their untangling. The new media have been treated as an all-encompassing and “timeless” realm that can be explained from within. However, signs of change have begun to appear with increasing frequency. Numerous studies and collections addressing the media’s past(s) in relation to their present have appeared in recent years. This influx of historically oriented media studies must be greeted with a cheer. Still, one cannot avoid noticing how little attention has often been devoted to defining and discussing methods and approaches. The past has been visited for facts that can be exciting in themselves, or revealing for media culture at large, but the nature of these “facts” has often been taken as a given, and their relationship to the observer and the temporal and ideological platform he or she occupies left unproblematized.
Years before Perriault, the word archaeology had been used in the title of C. W. Ceram’s Archaeology of the Cinema (1965). Ceram, whose real name was Kurt Wilhelm Marek (1915–72), was a well-known popularizer of archaeology. Yet applied to the prehistory of cinema his idea of “archaeology” hardly differed from the goals of traditional positivistic historical scholarship. Ceram presented a strictly linear and teleological account about the developments that led to the cinema, breaking off his narrative in 1897, the year that, according to him, “saw the birth of the cinema industry.” Ceram focused on inventors and the technical steps that led to cinematography. Everything that did not fit neatly into this narrative was left out, no matter how interesting it might otherwise have been. The illustrations, selected by the British scholar Olive Cook (mostly from the great collection of John and William Barnes), told an entirely different story, pointing out phenomena and potential connections omitted by Ceram. This was an interesting rupture, embodying a tension between two very different notions about the history of the moving image. The word archaeology later appeared in the title of Laurent Mannoni’s Le grand art de la lumière et de l’ombre: Archéologie du cinéma (1994). A change of emphasis is clear. Based on an extensive consultation of archival material (which justified the use of the word archaeology), Mannoni’s book no longer tried to present a closed historical narrative arranged as a set of interconnected causal chains inevitably leading toward cinema. Rather, the five-hundred-page volume consists of a succession of carefully researched case studies of different facets of the moving image culture, covering several centuries. Although there is a strong emphasis on technology, Mannoni also discusses its applications and discursive manifestations. Piece by piece, a narrative develops, but one that does not pretend to be complete or to hide its gaps. Although Mannoni’s discourse stays close to the sources, avoiding theoretical speculation, the book invites new insights, opening paths for further interpretations.
Marshall McLuhan introduced a new approach, new combinations, and new themes to the study of media. His early work The Mechanical Bride (1951) developed a critique of contemporary mass media, drawing occasional parallels with mythology and history, and shifting between high culture and popular culture with apparent ease and (and some took it) recklessness. In The Gutenberg Galaxy (1962) McLuhan’s vision came to embrace the history of media in a more rigorous sense as he traced the dynamics between orality, the Gutenbergian printing revolution, and the new orality represented by televisual media. Instead of providing a neutral and linear narrative, McLuhan’s idiosyncratic discourse surfaced as an essential element. The materiality and the processual nature of his discourse was further emphasized in the collagelike books (The Medium Is the Massage, War and Peace in the Global Village, and Counterblast) that he produced with the graphic designer Quentin Fiore following the international success of Understanding Media: The Extensions of Man (1964). McLuhan’s influence on media archaeologists has been manifold. Of utmost importance is his emphasis on temporal connections, translations, and mergers between media, something that inspired Jay David Bolter and Richard Grusin to develop their notion of “remediation” and to use it to investigate how features of earlier media forms are subsumed into digital media. Bolter’s and Grusin’s endeavor was not defined as “media archaeology,” but it has affinities with the ways media archaeologists draw parallels between seemingly incompatible phenomena. McLuhan’s understanding of “media” and “medium” was broad and challenged existing dichotomies, like those between material things and notions of the mind. His ideas of new media as “extensions” and as driving forces for changes in society have influenced the German “media materialist school” of media archaeology through the work of Friedrich Kittler. Last but not least, McLuhan’s unwillingness to stick with formal “methods” and fixed sets of concepts, as well as his self-reflective play with his own discourse, seems to appeal to “anarchistically minded” media archaeologists, determined to keep their approaches free from institutional-theoretical dogmas and infections.
Siegfried Giedion’s Mechanization Takes Command (1948) presented a detailed account about the forms and impact of mechanization. Ranging from techniques for capturing human movements as graphic representations to the features of everyday household objects like the bathtub, Giedion’s history had less to do with isolated apparatuses than with their interconnections. Mechanization was presented as a depersonalized force that infiltrated Western societies down to the minutest details of everyday life. Giedion was mainly concerned with material culture, “the tools that have molded our present-day living.”  The “anonymous history” he proposed looked for a synthesis between Geistesgeschichte and positivism, where every detail was “directly connected with the general, guiding ideas of an epoch. But at the same time it must be traced back to the particulars from which it rises.”  Even earlier, the German cultural critic Walter Benjamin had already projected a kind of anonymous history, but one that involved discursive layers of culture to a much greater extent than Giedion’s largely materialist vision. Benjamin is arguably the most prominent forerunner—beside Foucault—of media-archaeological modes of cultural analysis and is a major influence for cultural studies. In particular, his unfinished Arcades Project (Passagen-Werk) became a case study about the kinds of issues media archaeologists deal with.25 Benjamin’s reconstruction of nineteenth-century culture, with Paris as its capital, relied on a multitude of sources, including texts, illustrations, urban environments, architecture, public spectacles like the panorama and the diorama, and objects deemed to be emblematic of the era. The approach was remarkably open, shifting, and layered and took political and economic but also collective psychological factors into consideration. Beside material forms, Benjamin’s work illuminated the “dream worlds” of consumerism and early modernity. Working against the tidal wave of Geistesgeschichte, Benjamin refused to group the massive evidence he had gathered under any single symbol deemed characteristic of the era. Such persistence is one of the reasons why the work remained unfinished. The readers were left with a huge collection of notes, images, and ideas that constitute a database rather than a preorganized narrative. Benjamin offered meditations on time, spatiality, nature, and emergent modernity as a new realm of sensations. The concept and method of allegory that he had already developed in his earlier work referred to alternative ways of seeing temporality not as an organic succession but through the figures of ruins and decay. The interest in change and the “ruins” of the body and mind were evident in his other works as well, which famously touched on historical changes in the modes of perception.
Both Benjamin and Sternberger were interested in the panorama as a visual manifestation of the nineteenth-century culture. However, their approaches differ in important respects. For Benjamin the panorama was just one of the manifestations of the larger topic he was trying to get hold of, while for Sternberger it became nothing less than the organizing metaphor for his portrait of an era, the key to unlock the secrets of, in the words of his book’s subtitle, “how nineteenth century man saw himself and his world and how he experienced history.” In his book Sternberger deals less with concrete panoramas than with their manifestations in cultural phenomena as diverse as steam power, railway travel, the Western idea of the Orient, the theory of evolution, and domestic lighting. While this totalizing idea clearly derived from the ideology of the Geistesgeschichte, it may also bear certain similarities with the ways Foucault dealt with his “epistemes.” In the early decades of the twentieth century, art history also began proposing ways of recontextualizing art within textual traditions and expanding its own reach to visual material that had traditionally been left outside its confines. A reevaluation of “neglected traditions” has been proposed by Horst Bredekamp, who has related the theories of Bildwissenschaft that emerged in Germany to pioneering approaches toward technology and media in the early twentieth century. Around 1900–1933, according to Bredekamp, a new “science of the image” emerged in the German-speaking world with radical ideas about the continuities between different genres of images from advertisements and photography to film and political iconography. The art historian Aby Warburg and scholars he influenced, like Ervin Panofsky and E. H. Gombrich, stood out as “picture-historians,” interested more in recurring visual motifs and their contextualization than in weeding the popular out of art history. Warburg’s unfinished “Mnemosyne Atlas” (which in some ways resembles Benjamin’s Arcades Project) suggested a nonlinear way of understanding the temporal recurrence of images and their relations, raising also the issue of “intermediality” by pointing out motifs that shifted and transformed across what we would now call media platforms. Furthermore, the project suggested a new idea about dynamics of the image, pointing out how images and motifs in themselves could function as “time-machines” in an isomorphic fashion to the task of media archaeology. Another unconventional work that anticipated some of the concerns of media archaeology was André Malraux’s Musée imaginaire (trans. Museum without Walls), published in 1947. Malraux discussed the ways in which mechanical reproduction, in particular photography, was changing our understanding of images and visual culture in general (without referring to Benjamin’s “Work of Art in the Age of Mechanical Reproduction,” 1936). He demonstrated how the unprecedented availability of reproductions was turning the past into an archive, and he challenged observers to draw connections between visual traditions and motifs that had until then been considered unrelated.
The work of Michel Foucault has had a strong impact on media archaeology. An archaeology of his “archaeology of knowledge” would be useful but cannot be developed here. When classifications of media archaeology have been attempted, a binary division has usually been drawn between the socially and culturally oriented Anglo-American studies and the techno-hardware approach of German scholars, who have taken their cue from Friedrich Kittler’s synthesis of Foucault, information theory, media history, and McLuhan’s emphasis on the medium as the message. The German tradition has been claimed to emphasize the role of technology as a primum mobile, which has led to accusations about technological determinism, whereas Anglo-American scholars often assume that technology gets its meanings from preexisting discursive contexts within which it is introduced. One way of explaining this division is to see it as a consequence of different readings of Foucault. The Anglo-American tradition has valorized Foucault as a thinker who emphasized the role of discourses as the loci where knowledge is tied with cultural and social power. Material bodies, events, and institutions are all conditioned by discursive formations. The effects of “hard” technology are considered secondary to immaterial forces that differentiate and mediate their uses. We find quite different readings of Foucault in the German variant of media archaeology, which was strongly influenced by Kittler’s Aufschreibesysteme 1800/1900 (1985), his pathbreaking habilitation thesis that dealt with the impact of technical media on nineteenth-century literature and writing practices. It was followed by Grammophon Film Typewriter (1986), which shared the same basic premises but focused more directly on technical media. Kittler argued for the need to adjust Foucault’s emphasis on the predominance of words and libraries to more media-specific ways of understanding culture. According to him, the problem was that “discourse analysis ignores the fact that the factual condition is no simple methodological example but is in each case a techno-historical event.”  To be able to understand media technologies from the typewriter to the cinema and on to digital networks and coding paradigms, one must take their particular material nature into consideration—an idea Kittler’s followers like Wolfgang Ernst have adopted for their own work. It was probably in this sense that Michael Wetzel purported to combine Foucault and Kittler in his “preliminary considerations for an archaeology of the media,” published in 1989 in a collection of writings that already bore the words “archaeology of the media” in its title.
Although he has often been seen as part of a generation of German humanities scholars determined to steer media theory away from meaning and interpretation, Kittler did not neglect the powerrelated implications of technology. There is a “brand” of German media theory that emphasizes the “epistemic effects of media in the production and processing of knowledge” and “the medial dimensions of the mechanisms of power,” as the editor of a recent issue of Grey Room explained. To prevent the application of simple binary models to his work and his intellectual position, Kittler has denied any affiliation with the notion of media archaeology. His more recent work has returned to a more Heideggerian-inspired excavation of the history of Western culture through music and mathematics. Anglo-American media archaeologists—whether identifying themselves as such or not—have received impulses from the new historicism that emerged in the 1980s. Although it appeared first within literary scholarship, it soon spread to other areas, including history, where it inspired a movement known as the new cultural history. Among other sources, the new historicism was also influenced by Foucault, although his ideas were by no means approved without debate. H. Aram Veeser aptly summarized its “key assumptions” by stating “1) that every expressive act is embedded in a network of material practices; 2) that every act of unmasking, critique, and opposition uses the tools it condemns and risks falling prey to the practices it exposes; 3) that literary and non-literary ‘texts’ circulate inseparably; 4) that no discourse, imaginative or archival, gives access to unchanging truths or expresses inalterable human nature; 5) finally . . . that a critical method and a language adequate to describe culture under capitalism participate in the economy they describe.”  Applied to historical scholarship, the new historicism promoted a self- reflexive and discourse-oriented approach that frequently drew on neighboring disciplines, including the symbolic anthropology of Clifford Geertz, and the rather amorphous field of cultural studies.44 A kind of double focus developed: on the one hand, historians were supposed to immerse themselves in the past, observing it as if by the eyes of the contemporaries; on the other hand, they were supposed to be constantly aware of their observation post in the present, with all the ideological implications it entailed. The research process shifted constantly between the facts of the past, in the process of forming themselves into meaningful constellations, and the subjectivity of the observer. The historical explanation was formulated on/as a dynamic field with multiple determinants that were dynamic rather than static.
Siegfried Zielinski’s version of media archaeology is a practice of resistance, not only against what he perceives as the increasing uniformity of mainstream media culture, but also against media archaeology itself, or rather its assimilation and hardening into the normalcy of contemporary media studies. Considering media archaeology a “method” pinned down into an academic textbook would no doubt be a horror for Zielinski, who also calls his “activity” (Tätigkeit) by other names, such as “anarchaeology” and “variantology,” expressing an uneasiness toward permanent categories and doctrines. For him, media archaeology “in a pragmatic perspective means to dig out secret paths in history, which might help us to find our way into a future.”  This formulation reveals the utopian and Romantic underpinnings of Zielinski’s thought, which is not without—productive?—contradictions. Zielinski’s early work was not yet identified as “media-archaeological.” Zur Geschichte des Videorecorders (On the History of the Video Recorder, 1986) was a dense and detailed exploration of the topic, covering technological, institutional, and economical as well as sociocultural issues. It also contained a special section, “Aspects of the Video Recorder in Pictures,” a kind of visual essay that already pointed toward media-archaeological interests. His next major book, originally published in 1989 and translated as Audiovisions: Cinema and Television as Entr’actes in History, defined itself as an “outline of a history of audiovision” (Entwurf zu Geschichte der Audiovision), or a contribution toward to “an integrated history of the media” (integrierte Mediengeschichte). Drawing on an enormous mass of highly heterogeneous source material, the book demonstrated how distinctions between different audiovisual media were gradually erased during the twentieth century.
Although the theoretical construct behind Audiovisions was more implicit than explicit (as Zielinski himself admitted), he singled out the triad “technology-culture-subject,” identifying each of its elements with a recent intellectual tradition that had influenced him: the British cultural studies represented by Raymond Williams; the German historiography of technology that used a specific systems approach (Günter Ropohl); and the metapsychological cinema theories of Jean-Louis Baudry, Jean-Louis Comolli, and Christian Metz, which emphasized the notion of the cinematic apparatus. Zielinski stated that he did not want to “compete with other models that emphasise more strongly the techno-structure of media processes (like, for example, those of Friedrich Kittler and his pupils),” seeing his own as “supplementary.”  Zielinski’s road from Audiovisions to media archaeology seems logical. His project had taken him to “the end of the history of cinema and television,” where he saw only uniformity and unlimited industrial exploitation, surprisingly much in line with Adorno’s and Horkheimer’s position. “New media” did not provide relief, as their possibilities were mostly used to remediate and perpetuate hegemonic forms. Zielinski began turning toward two seemingly opposite directions that in the end pointed to the same goal, offering to break the psychopathia medialis of modern media culture. On the one hand were radical contemporary artists, who had potential to break the vicious goals of the culture industry; on the other were the hidden treasures of the past that might provide keys for a cultural renewal. Zielinski’s position was influenced by his role as the founding director and later rector of the Academy of Arts and the Media in Cologne, which gave him an opportunity to build connections between media studies and experimental media practices. Zielinski’s next book, translated as Deep Time of the Media: Toward an Archaeology of Hearing and Seeing by Technical Means (originally published in 2002), plunged into the “deep time” of media, offering a series of studies that were dedicated to the work of personalities who had rarely been associated with media culture before and were written in “a spirit of praise and commendation, not of critique.”  Empedocles, Athanasius Kircher, Cesare Lombroso, and others provided examples of genial individuals who worked out of love and inspiration against the odds of the real world. In Zielinski’s mind at least, they shared a kinship with another set of cultural heroes, contemporary artists working with media, such as Valie Export, David Larcher, Nam June Paik, Steina and Woody Vasulka, and Peter Weibel. Ideologically Zielinski almost seems to nod toward Thomas Carlyle’s Romantic classic On Heroes and Hero Worship and the Heroic in History (1841). He plunges headlong, and with pathos, into the world of his heroes, eschewing critical, skeptical, and theoretically oriented perspectives.
The “new film history” can arguably be seen as a parallel enterprise to media archaeology; both have their origins in the 1980s and are continuing to evolve. Although the profile of the former is far from clear, many of its practitioners sought new insights into the specific nature of cinema by introducing extended cultural, social, and economic contextualization, based on the consultation of varied firsthand source material, and by emphasizing cinema’s intermedial relationships. In a way Zielinski’s Audiovisions also pointed in this direction, but it went beyond the horizon of most film historians by focusing on the interplay between technology, cultural forms, and viewing subjects and by paying little attention to the content of films or television programs; the context and the technological apparatus were given center stage. The need for contextualization and intermediality was expressed well by Thomas Elsaesser in an article entitled “The New Film History as Media Archaeology”: Sound, for instance, since the silent cinema was rarely if ever silent, in which case: why is the history of the phonograph not listed as another tributary? And as we now understand the cinema as part of a multimedia environment, how about the telephone as an indispensable technology? Radio-waves? Electro-magnetic fields? The history of aviation? Do we not need Babbage’s difference engine ranged parallel to his friend Henry Fox-Talbot’s Calotypes or Louis Daguerre’s sensitised copper plates? These questions in themselves show how much our idea—and maybe even our definition—of cinema has changed even without appealing to digitization as a technology, which is nonetheless implicit as a powerful “perspective correction” and thus counts as an impulse in this retrospective re-writing of the past.
Gunning has also published many studies linking early cinema to other media, technological phenomena like ghost photography and X-rays, and institutions of emerging modernity, such as the World’s Fairs. Similarly, in Window Shopping (1993) Anne Friedberg traced the origins of cinema to forms and institutions of the emergent popular and consumer culture of the nineteenth century, creating an approach that clearly raised media-archaeological concerns. In The Virtual Window (2006) she pushed her analysis back by hundreds of years, further estranging it from the cinema studies paradigm. For Elsaesser, one of the challenges is the reevaluation of the connections and gaps between media technologies. The onslaught of digitalization is forcing cinema to rethink both its cultural position and its history. Considering digitality as a rupture provides a conceptual way of seeing media history as a discontinuous enterprise subject to constant reevaluation. Lev Manovich’s The Language of New Media (2001) was a historically tuned version of new media theory that built on cinema studies and film theory. It purported to place the new media “within the history of modern visual and media cultures.”  Manovich pointed out continuities between early avant-garde and animation film practices and the emerging digital culture, based on numerical representation, modularity, automation, variability, and transcoding. Beside film history and theory, he drew on the traditions of the Bildwissenschaft, including the work of Ervin Panofsky. The focus on new media changes the historical meaning and context of cinema from narrative cinema to one flexible enough to lend itself to interactivity, navigability, and digital representation and transmission. Media-historical and theoretical works with a background in film studies pose a challenge for the continued renewal of media archaeology. How does one avoid reducing all other media to a footnote to the history of the moving image? One alternative is the recent influx of archaeologically oriented works concentrating on the audible dimension of culture and history.
4 Emphasizing such heterogeneity is an attempt not so much to deliberately diversify the existing body of mediaarchaeological theory and praxis as to encourage “traveling” between discourses and disciplines. Still, amid all the variety, there is a need to define approaches and perhaps even to crystallize them into “methods,” at least in a local and tactical sense. Erkki Huhtamo’s variant of media archaeology is one such attempt, stemming from an effort to apply the idea of topos, as developed by the German literary scholar Ernst Robert Curtius in his classic Europäische Literatur und lateinisches Mittelalter (1948), to the field of media culture. The topos approach eschews “the new,” which is so often the focus of media-cultural discourses, both critical and popular; instead, it emphasizes the clichéd, the commonplace, and “the tired” (to appropriate jargon from WIRED magazine). Identifying ways in which media culture relies on the already known is just as essential as determining how it embodies and promotes the never before seen. In fact, these two aspects are connected with each other; the new is “dressed up” in formulas that may be hundreds of years old, while the old may provide “molds” for cultural innovations and reorientations. Huhtamo’s approach does not only identify topoi, trace their trajectories, and explore the circumstances of their reappearances. It also purports to demonstrate how topoi are constantly evoked by cultural agents, from spokespeople, sales agents, and politicians to writers, journalists, exhibition curators, and, last but not least, media artists, who use them for various kinds of purposes, from sales pitches and ideological persuasion to aesthetic reflections on media culture and history. This emphasis gives Huhtamo’s approach a culture-critical character. Although the cultural agents themselves may not always acknowledge it, the media-archaeological dimension is an essential element of the contemporary mind-set, constantly bombarded by media and communications. By demonstrating how the media’s past(s) lives on in the present, guiding and informing people’s attitudes in their daily lives, the topos approach helps to detect novelties, innovations, and media-cultural ruptures as well. As Huhtamo had already pointed out in 1996 in his essay “Time Machines in the Gallery: An Archeological Approach in Media Art,” a growing number of artists who are aware of media archaeology get inspiration from its findings and are contributing their own creations and discoveries. This has led to intriguing parallels and connections between research and artistic creativity.
“Media” are not only related to the established institutions of modernity. They are also manifested in the narratives of madmen, religious visions, theories about the psyche and the body, and other recurring issues associated with technological modernity. But the term imaginary media does not refer only to the human imagination as a site for fantastic modes of communication. It can also mean extensions of the notion of “media” in theories of the mind and the brain. Media are in this sense a reservoir for tactics and techniques for manipulating humans and their culture. The chapters in this section demonstrate that media archaeology not only is grounded in Foucault’s methodology but can also find inspiration from psychoanalysis and other early twentieth-century cultural theories, including the German Bildwissenschaften and the topos theory of literature. Erkki Huhtamo’s chapter offers a theoretical-historical contextualization of the topos, a notion he has adopted from the literary scholar Ernst Robert Curtius and has turned into a “tool” for explaining the recurrence of clichés and commonplaces in media culture. Huhtamo has applied the idea to various media forms ranging from “peep media” and the moving panorama to mobile media. In this chapter he delineates his approach theoretically, discussing its predecessors and demonstrating how it can be applied to various facets of media culture. For Huhtamo, the task is “identifying topoi, analyzing their trajectories and transformations, and explaining the cultural ‘logics’ that condition their ‘wanderings’ across time and space.” Topoi are discursive “engines” that mediate themes, forms, and fantasies across cultural traditions. Predictably, they have become a tool in the hands of the culture industry.
Setting out from a Foucauldean position, he mobilizes Siegfried Zielinski’s notions of “variantology,” and “anarchaeology of media.” Through a series of examples, Kluitenberg demonstrates how the notion of imaginary media can be conceived and used to contextualize and enhance contemporary media production. Relying on Zielinski, he claims that excavating the media cultures of the past is important not only in itself but also for the creation of innovative and critical futures. This is one of the key features of many media-archaeological writings: archival findings and reconstructions of the narratives of the past are tied with emerging media practices. Both Jeffrey Sconce’s and Thomas Elsaesser’s chapters submit psychoanalytic themes to a media-archaeological elaboration. Theories of the psyche and the mind become excavation sites that both Sconce and Elsaesser explore to uncover new ways of understanding modern media culture. Sconce focuses on “the influencing machine” as it was conceptualized by the psychoanalyst Victor Tausk in the 1930s. Rather than considering it as a schizophrenic delusion, Sconce sees it as a projection of broadcast media, implicitly associated with mind control. The “delusions” he discusses are not related to any specific technology; rather, they testify to “a vernacular theory of power forged at the end of the nineteenth century.” In his chapter Elsaesser analyzes Sigmund Freud’s writings, providing insights to the media archaeological conceptualization of memory. He shows that Freud’s analyses of the psyche can be taken as media theories in their own right—as ways of understanding not only perception, but issues like storage and processing as well. The psyche itself is a kind of a media machine. Elsaesser's media archaeological reading of Freud’s classic text “Notes on the Mystic Writing Pad” opens an alternative path to understanding early psychoanalytic theories in the context of the emerging media culture. It shows that Freud can also be applied to a media archaeology of images in the era of digital data. Elsaesser argues that “considering Freud as theorist of auxiliary memory and the technical media, and thus as a media theorist, shifts this perspective away from film to a more general consideration of the technical media".
Some time ago, while I was leafing through an airline magazine, an ad caught my attention. It was associated with a current event—the Winter Olympics in Turin—and promoted Samsung mobile phones.1 The setting was a deserted, dimly lit office after hours. Unusual mini-Olympics were taking place on a desk in the foreground. Between a frozen laptop computer and a frost-covered coffee cup, Lilliputian sportspeople—a figure skater, skiers, ice hockey and curling players—were in full action. The center of everything was a snow-covered mountain capped by a Samsung phone. The mountain continued into the virtual realm of its screen, where a takeoff ramp built on its slope could be seen; a jumper was midair, shooting down from the ramp—and out from the screen. It immediately occurred to me that I had encountered such tiny people before. Examples abound: in a cartoon published in the New Yorker in 1959 a cleaning lady is seen mopping the floor after hours next to a huge mainframe computer.2 She turns her head and sees a tiny man leaving through a doorway on the side of the enormous computer, clutching his suitcase and raising his hat in a mechanical greeting. I also came to think about Little Computer People, a computer game for the Commodore 64 (1987). In its promotional campaign, Activision pretended they had found little people living inside computers. The idea of the product was to persuade one of them to live in a “House-on-a-Cassette,” a software application where the person’s routines could be observed and interacted with.
Going back even further, we discover that little people had already appeared in the context of phonograph and gramophone advertising. Elsewhere, we discover them in early trick films and in the delightfully impish video installations by the French media artist Pierrick Sorin. We could also fly back through the tunnel of time to the realms of fairies, gnomes, and Lilliputians. “The little people” is a topos—a stereotypical formula evoked over and over again in different guises and for varying purposes. Such topoi accompany and influence the development of media culture. Cultural desires are expressed by being embedded them within topoi. Functioning as shells or vessels derived from the memory banks of tradition, topoi mold the meaning(s) of cultural objects. High technology can be represented as something else through application of the “fairy engines” of topos traditions. They can disguise culture as nature, and something unheard-of as something familiar. As Peter Burke has written, “The facade of tradition may mask innovation.” As discursive meaning processors, topoi not only express beliefs but can serve rhetorical and persuasive goals, as evidenced in the field of advertising. New products are promoted by being packaged into formulas that are meant to strike the observer as novel, although they have been put together from ingredients retrieved from cultural archives. Burke’s dictum can therefore also be reversed: the facade of innovation may mask tradition, and apparent ruptures disguise hidden continuities. Identifying topoi, analyzing their trajectories and transformations, and explaining the cultural logics that condition their “wanderings” across time and space is one possible goal for media archaeology. In this chapter I will lay a theoretical and historiographical foundation for this type of approach. I have already applied it in a more strategic sense in a number of studies. Media archaeology means for me a critical practice that excavates media-cultural evidence for clues about neglected, misrepresented, and/or suppressed aspects of both media’s past(s) and their present and tries to bring these into a conversation with each other. It purports to unearth traces of lost media-cultural phenomena and agendas and to illuminate ideological mechanisms behind them. It also emphasizes the multiplicity of historical narratives and highlights their constructed and ideologically determined nature. I will begin by discussing the origins of Toposforschung in the work of Ernst Robert Curtius and assessing some of its influences, including C. G. Jung’s theory of archetypes and Aby Warburg’s iconological approach to the study of visual culture. I will then discuss the relevance of Curtius’s ideas for the study of media culture and will propose modifications in the light of more recent applications of the notion of the topos. I will proceed to exemplify roles the topoi serve within media culture and, finally, will discuss the potential advantages and problems of the approach delineated in this chapter.
Although he claimed he was practicing “philological microscopy,” Curtius’s approach was expansive. He compared it to aerial photography, which had helped archaeology to discover massive land structures invisible from the ground level; after the discovery, the photograph had to be enlarged and the details investigated. Curtius condensed his approach into a maxim: “Specialization without universalism is blind. Universalism without specialization is inane.”  Universalism as he understood it applied only to literary traditions, which he treated as an independent realm. Although he described in meticulous detail how specific topoi had been handed down from writer to writer, he did not perceive essential changes in their meanings—only changes in their style. In spite of making occasional context-driven statements such as “In messianically and apocalyptically excited periods, faded symbolic figures can be filled with new life, like shades which have drunk blood,” Curtius did not really explain topoi by relating them to factors that could be considered external to the literary tradition. Had he attempted to analyze topoi as symptomatic of the times and places in which they were evoked, Curtius might have come to the conclusion that their appearances marked not only continuities but cultural ruptures and discontinuities as well. This has been emphasized by later topos scholars. For Ernst Ulrich Grosse, differences in the ways a certain topos has been used are more interesting than similarities because they have potential to reveal changes and historical turning points. For him, the appearance of a topos is potentially conditioned by several factors: the immediate environment as well as the will of the author; the evolution of literary genres as well as the history of mentalities. When a topos emerges, it should be treated as a node in a complex network of references and determinants. Topos study is steeped in the issue of cultural contextualization, no matter how difficult and elusive it may be.
Such “dressing up in technology” is another topos that manifested itself both as imaginary projections and in the new realities of working life. Women working as telephone switchboard operators were “dressed up” in their headphones all day long and, indeed, could be argued to have became cyborgs of sorts as well. An intriguing manifestation of such developments is the famous artwork Electric Dress created by the radical Japanese Gutai artist Atsuko Tanaka for her performances in the 1950s. Whether Tanaka was aware of the topos tradition anticipating her work is uncertain but unlikely. One could also refer to public demonstrations of smart fabrics and cyberfashion in recent years. Yet another nineteenth-century example is a topos that could be labeled “What is happening behind your back?”  In its most typical manifestation a man (often an officer) is represented kissing a girl left unguarded when her mother is peeping into a peep-show box. This topos jumped from device to device (peep show, magic lantern, telescope, kaleidoscope, Daguerreotype camera, etc.), justifying Peter Burke’s call to “focus on the displacement or migration of a given schema or stereotype from one object to another.”  By midcentury the man stealing the kiss had been transformed from a reckless officer seeking amorous encounters into a door-to-door salesman marketing stereoscopic photographs, acting when the unsuspecting husband is peering at his sample cards. In another variant of the same topos, already known around 1800, the man peeping into a peep-show box falls prey to a pickpocket, who is actually represented as a tax official stealing money from the people to fund the government’s war efforts; media like broadcast television are still used much the same way, not only by governments but also by groups like televangelists. The same topos reappeared in the context of another peeping device, the mutoscope, a hundred years later, but here the political has been replaced by the homoerotic. In a French cartoon (1910) a male peeping into the mutoscope experiences the pickpocket’s touches as erotic; the positions of the male bodies suggest anal intercourse. Exactly the same variant reappeared in a series of video installations by Pierrick Sorin. The similarities make one wonder whether Sorin actually knew the topos or whether he grasped the idea spontaneously. Topoi that have been inspired by media spectacles may remain just stereotypical metaphors, but their recurrence may also point to wider concerns and cultural patterns. Although the topos “What is happening behind your back?” usually appears in comic episodes and may be dismissed just as a joke, it could also be read as a repeated warning about the risks of excessive media use. Too much immersion may upset one’s social relations and disturb one’s relationship to one’s immediate physical surroundings, it seems to say. It functions, then, as a kind of discursive buffer softening the shocks caused by encounters with new media and the mediated environment.
The phantasmagoria, the kaleidoscope, dissolving views, the diorama, the moving panorama, and many other media-cultural phenomena have given rise to topoi. Indeed, Curtius himself resorted to them when he wrote, “Not until late Antiquity is color in demand again—and then it is the color of a kaleidoscope.”  And again: “New and ever new figures rise kaleidoscopically—rhetorical bravura pieces whose exuberance pours out like a cascade.”  Many “mediacultural” topoi keep on appearing long after the thing they originally designated has materially disappeared. Moving panorama originally referred to a popular nineteenth-century spectacle. As a topos, it came to manifest perceptual experiences, inner visions (“life passing before the mind’s eye like a moving panorama at the moment of death”), religious revelations (God revealing his plans as a moving panorama in the sky), celestial mechanics, and many other purposes. In Elvis’ Search for God (1998), Jess Stearns still described how Elvis “felt a surge of emotion, as the moving panorama of the gilded years floated by for a moment.”  Although, as Curtius said, “the topos can be employed in any context,” its manifestations are affected by the specific nature of that context. Media-related ideas may also traverse culture as “imaginary media” long before they materialize as artifacts. “Seeing at a distance” is such a topos. We encounter it already in discourses about magic mirrors, and in an intensified form in technocultural fantasies about the telectroscope, an imaginary anticipation of the television. One the other hand, as Geoffrey Batchen has demonstrated, the desire to photograph seems to have emerged rather suddenly in the late eighteenth century, preceded by very few immediate fantasies. However, once photography made its breakthrough, the discursive floodgates opened and the desire to photograph was translated into an intense topos activity.
The use of topoi in promotional strategies exploits their attraction value but also their “nonattraction” value. Topoi are used to arrest the eye in accordance with the long traditions of the culture of attractions. They provide a striking sight or textual formula that intrigues the observer. In commercial media culture their character is instrumental: they have been recruited to provide a product or spectacle with a certain historical or cultural surplus value acknowledged by the observer. Obviously the topos can have an effect by its invisibility, by its unremarkable and commonplace character. In such cases it is used to provide a mold for content that pretends to be something unprecedented, cut off from the past. In either case topoi provide advertisers with tried-and-tested formulas that are used to introduce new consumer products by embedding them within molds the customers already know (whether they are aware of it or not). There seems to be a paradox here: the newest of the new is packaged in the oldest of the old. The most obvious example that comes to mind is from the marketing strategies of screen technology. No matter how “revolutionary” the product may be, advertisements show us, over and over again, humans or objects breaking through the screen, in either direction. The manifest features of such ads are of course constantly updated in accordance with fashions and stylistic trends, but underneath we detect an ancient topos associated with the history of illusionistic representation. Figures have been stepping in and out of paintings for millennia; they are still performing stunts on today’s flat-panel plasma screens. The recurrence of the “traversing the screen” topos may be motivated by the need to fight consumer resistance toward the new, although in a postmodern culture that readily embraces high technology it may have more to do with appealing to consumers’ love of pastiche, nostalgia, and cultural sound bites. Such issues also occupy industrial design, preoccupied with finding the right ratio between futuristic and retro styles (retro-futuristic is one option). Early TV sets were often enclosed in wooden cabinets that made them look like a traditional piece of furniture, while Jonathan Ive’s original iMac (1999) displayed its “inner organs” through its acrylic plastic case. Apple’s advertisements tried to make most out of it, displaying the computer from the side rather than from the front, as had nearly always been the case. The campaign broke with the “breaking through the screen” tradition, but it may well have reactivated the topos of the little people, at least in some consumers’ minds: the inside of the iMac was made to look like a miniature world that could well have been inhabited by little computer people. The case of the little people demonstrates that cultural subjects do not consume representations provided by the industry as such. Users developed and disseminated their own beliefs and even tested them in action, pushing food through holes inside TV sets, stalking behind their sets to see the little people leave their workplace, or even attempting to break the screen physically to free them.

Language, with its different recognizable sounds, has opened up to humanity the possibility of asking questions, telling stories, and participating in dialogue. Language has allowed the emergence of unknown entities in animal societies: numbers, gods, laws, works of art, calendars, the technological adventure, and the entire cultural universe. I will designate here by the term “ideas” those complex forms that appear, reproduce themselves, and evolve only in the world of culture, in the space of signification opened up by language. Language has allowed human communities, as compared to hives, herds and packs, to make a leap of collective intelligence because language creates a stronger and more supple link of competitive cooperation than what insects or monkeys have in their respective communities. In putting the idea at the center of my model, I have chosen an approach to human collective intelligence that radically distinguishes it from the collective intelligence of other animal societies. From this perspective, language represents the limit or threshold beyond which ecosystems of ideas are constituted. These are like spiritual hypertexts living in symbiosis with the societies of talking primates we humans form. These ecosystems of ideas grow more complex, die out, diversify, or combine in such a way as to lead those societies that cultivate them down, the in part undetermined, road of cultural evolution. Teilhard de Chardin coined the term “Noosphere” to describe the world ecosystem of all the ideas which globalization and the development of the means of communication, whose culmination we see in cyberspace, have only begun to put at our fingertips. Human communities can survive only by maintaining cultures, that is, semi-closed collective intelligences conducive to the breeding (reproduction and selection) of ideas. An ethical person, a corporation, an institution, a nation, a religion, a political party, a science, a virtual community or a tribe cultivates—nolens volens—ecosystems of ideas. In the course of its existence, a culture explores a viable evolutionary direction for its ideas. Our mental representations give ideas their forms. In a way, representations are the face, or the mask, of ideas. Representations can be of all kinds; their variety is in theory unlimited : e.g. the images of our perceptions, such as those created by human works of art, music, symbols, and the structures of highly complex relations that have been constructed by means of language and many other sign systems. Our intentions are the souls of ideas: the movement which animates the face. Intentions direct the mental representations toward particular destinations, or particular targets. They entrust ideas with a goal, which can be quite near, or even very far away, aiming at almost inaccessible horizons. We can think of intentions as the abstract structure of emotions, in other words, as vectors endowed with a force (intensity) and a direction (the “nature” of the emotion). Representations must be distinguished from emotions because the same representation, depending on the circumstances, can serve as a face for very different emotions.
Internet, cell phones, videophones, virtual reality devices, computers, modems, data banks: the communications industry is taking over. Productivity, utility, management are the watchwords. But are we masters or servants of our devices?

Information highways, despite the hype, are no more than a combination of telephone, video and computers, as are the gadgets telecom engineers devise for the future. They are all really just cell phones plugged into the internet, linking with fixed stations, videophones, virtual reality devices, computers, modems, data banks, tradesmen, managers, repairmen. All are logged in to technical, social or professional networks.

There are no black holes, no negatives, no opposites in this joined-up world. Everything runs smoothly in electronic silence. Even the occasional audible signal can be turned off. Communication is easy when all it involves is using machines to connect with machines. If you want to stop communicating, to enjoy your own company, all you have to do is put down the handset and switch off the screen.

Anyone using such machines is free and happy in a world of instant communication, without time to think ill of himself or of anyone else. The long term doesn’t exist in this world, only short-term gains. The communications industry dreams of productivity, utility and management, the watchwords of homo communicans.

Homo communicans is inseparable from his communication devices. They are his life and he takes on their characteristics. He is their servant as much as their master, but he is unaware of his chains because he believes himself to be in control and as powerful as the machines. They make his life easy. Everything is positive, everything in its place. There is no price to pay and no other side of the coin in what Douglas Coupland, in his novel Microserfs (1), calls the “flatland of cyberculture”. Cyberculture uses the language of clans and communities but it does not share their reality, for to join the clan, you have to make sacrifices. And in cyber flatland the only thing to fear is the loss of the machines: breakdown.

When I asked Martin Landau, an eminent researcher in organisation science at Berkeley, California, about theories of communication, he replied: “Do you know why the 747 is the safest aircraft in the world? Because it has four separate control systems, one for each engine. And the pilot also has a manual control system separate from those four” (2). It seemed a strange answer. In aviation, breakdown means death. But breakdown in communication, loss of relationships or position, is also a kind of death, the end of homo communicans, who lives by communication and is defined only by the links his machines give him to other machine-bound human beings. Think how people panic when their computer or television breaks down or their phone is out of order. The gap in their lives causes real distress. These machines have become part of us; we have become part of them. When they break down it is like being in pain, the fear of it is a nightmare.

That is the system’s only contradiction, the only conceivable misfortune. The fear of breakdown has replaced the old apocalyptic fear of the devil, and it is only the threat of that breakdown which gives life and feeling to a system that has none. This is the communication system’s last vital opportunity. In his book Anatomy of Criticism (3), Northrop Frye shows that the Apocalypse is a text that advocate union between the city, the individual and God. And just as fear of the Apocalypse exists to serve the Christian faith, so fear of breakdown exists to consolidate the cult of the computer.

But homo communicans, ignorant of the sacrifices of communication, does not know this. He thinks he is always on the winning side, not knowing that in order to win you also have to lose. He doesn’t know what he is losing.

Machines are created for productivity and efficiency, and have unexpected consequences: they make men into idle and superfluous creatures who no longer do much on their own initiative. Men are assisted in everything, even getting to work, since with their self-guided cars they can dream at the steering wheel until the device tells them they have arrived. The old hauliers’ slogan “driving for you” now applies to us all. As technology continues to develop, homo communicans faces a future of total, profound idleness. And the first signs of this are already visible in our daily lives.

We let machines remember things for us, from our address books complete with telephone numbers and email addresses to the management of bibliographies, texts, business meetings, accounts, planning. Our voice, or better still a synthetic one, answers for us, recorded once for all time. We open doors and change channels on the TV remotely. We are not far from spending our entire lives in a semi-dormant state.

Our listlessness is encouraged by the sense of security we derive from all the surveillance devices that surround us. Idleness goes hand in hand with freedom from fear, a sense of comfort, of being safe, warm and protected. With sophisticated devices to watch over us, there are no enemies to worry about. Voice and face recognition, digital fingerprints and cameras with access codes free us from fear of intruders.

If they do not need to defend themselves, people’s existence begins to seem pointless. As if they are present by accident and might as well not exist. Machines do human work to perfection, while people are clumsy and hesitant and make mistakes, trying falteringly to follow a pattern. The idea of the human brain as the poor relation of the all-powerful computer has lead to a sense of powerlessness and futility. Our memories have grown unretentive but we don’t care; the business of remembering is being safely managed.

Contrary to common belief, homo communicans of the future will not suffer from pressure and stress. Why should he? His mistakes will be corrected by machines. Society with its faults of inequality, poverty, war and death, will be corrected by technology. This world where communication is all will not be fast-moving, but slow, inactive, contemplative, full of play. Not the slowness advocated by Pierre Sansot (4), the slowness of taking time out to enjoy life and savour the pleasures of fruit, fresh air and dreams, but an enforced slowness, reassuring and with no place for expectancy or surprise.

Homo communicans is good at dreams and contemplation, and his dreams will probably generate new ideas for communicating machines, more invention and innovation. That may be the real work assigned to human beings in the future of communication since the rest, engineering and production, will be done by machines.

Plato warned young philosophers against books, which he likened to dead memories that replaced living ones on the pretext of being more convenient. He said that writing and books make for idleness, making the reader passive. His advice seems archaic (today’s educators would give anything to get people to read). But although the medium has changed, Plato’s message has not. We are still handing our obligations over to an external device. From Plato’s viewpoint, the idleness resulting from freedom from work done by machines idleness would be evidence of enslavement unworthy of human beings.

Suppose two brains are in the same conscious state. Are there two minds, two streams of conscious experiences? Or only one? From a physical point of view, there is no puzzle. There are two numerically distinct lumps of matter that instantiate the same patterns and undergo qualitatively identical processes. The question is how we should ascribe mental properties to this material configuration. Even if we assume that the mental supervenes on the physical, we still need to determine whether the supervenience relation is such that two qualitatively identical physical systems ground a single experience or two numerically distinct (albeit subjectively indistinguishable) experiences.
The issue is not about personal identity. It is a separate question whether there would be one or two persons. One might hold, for example, that one person could have two subjectively indistinguishable experiences at the same time, or that two persons could literally share what is, numerically and not just qualitatively, one experience. These issues about personal identity will not be discussed here. My concern, rather, is about ‘‘qualia identity.’’ I will start by considering the numerical identity or non-identity of the phenomenal experiences that arise when brains exist in duplicates. The bulk of the paper will then examine some intriguing cases involving partial brain-duplication and the questions of degrees and of quantity of experience that these cases force us to confront. Consider the case where two brains are in identical physical states. The brains have, let us assume, the same number of neurons, which are connected and activated in the same way, and the brains are descriptively identical all the way down to the level of individual molecules and atoms. Suppose, furthermore, that for each of these brains there would, if the other brain did not exist, supervene a particular phenomenal experience. Given the supervenience assumption, the phenomenal experience that would supervene on one of these brains would be qualitatively identical to the experience that would supervene on the other. But if both brains exist, are there two qualitatively identical but numerically distinct experiences (one for each brain) or is there only a single experience with a redundantly duplicated supervenience base? A hardcore physicalist might be tempted to dismiss this question as being merely terminological. However, I believe that we can give content to the question by linking it to significant ethical and epistemological issues. Given such a linkage, the answer will not be an inconsequential terminological stipulation but will reflect substantial views on these associated issues. Let Duplication be the thesis that there would be two numerically distinct streams of experience when a conscious brain exists in duplicate. The content of Duplication, I suggest, might in part be constituted by its implications for epistemology and ethics. Consider first the ethical significance of Duplication. It could matter ethically whether Duplication is true. Suppose that somebody is contemplating whether to make a copy of a brain that is in a state of severe pain. If the quantity of painful experience would not thereby be increased, it seems that there would be no moral objection to this. By contrast, if creating the copy will lead to an additional severely painful experience, there is a strong moral reason not to do it. In such cases, it would be an extremely important practical matter whether Duplication is true or false. We could not resolve it by appealing to an arbitrary verbal convention. This ethical implication is a reason to accept Duplication and to reject its negation (Unification). Unification implies that we would not bring about pain if we created copy of a brain in a painful state, or changed an existing brain into a state that is qualitatively identical to a painful state of an already existing brain. At least on hedonistic grounds, there would be no moral reason not to create the copy. Yet it is prima facie implausible and farfetched to maintain that the wrongness of torturing somebody would be somehow ameliorated or annulled if there happens to exist somewhere an exact copy of that person’s resulting brain-state.
From this point onward, it will serve clarity and convenience to assume a weak form of computationalism, implying that a sufficiently powerful digital computer, running a suitable (very complex) program, would in fact have phenomenal experiences. (We do not need to assume that this would be analytically or metaphysically necessary.) Given this simplifying assumption, we can imagine the case of interest as resulting from running the same mind-program on two different computers. We can simplify matters further by supposing that the simulated minds live in and interact with identical computer-simulated virtual realities. Under these conditions, two identical mind-and-virtual-reality programs, starting from the same initial conditions, will evolve in exact lockstep. The brain-simulation and the virtual-reality simulation are both parts of a more comprehensive program, and when this program is run on two identical (deterministic) computers, they will go through an identical sequence of state-transitions.
This possibility undercuts Chalmers’ argument for the principle of organizational invariance by offering a more plausible account of how Joe’s qualia could gradually fade when he undergoes the neural replacement process. If the fading took place in this way, Joe would not be strangely failing to notice any qualitative changes in his experiences, because there would be no such changes. Of course, this point does not show that the principle of organizational invariance is false; it merely undermines one argument in its favor. The principle might well be plausible other grounds. It might be tempting to think that the possibility of fractional minds could be demonstrated by considering more mundane examples featuring gradations of consciousness, such as infant development, consciousness in animals at different levels of cognitive sophistication, the humanoid ancestors of homo sapiens, or the gradual loss of consciousness that occurs when we drift into dreamless sleep or anesthesia.15 However, these cases are complicated. They certainly involve changes in the descriptive quality of experience, and these qualitative changes, too, can form a continuum. Starting with a fully awake normal human stream of consciousness, we could reach a state of unconsciousness by various possible sequences of small steps, in which our awareness gradually becomes more fragmented, the fragments become briefer, scarcer, and more diffuse until the stream completely dries out. These cases might also involve a gradual change in the quantity of particular qualitative experiences, in the sense relevant here; but it is not obvious that they do so. In the thought experiments presented in preceding sections, we carefully controlled for the confounding variable of qualitative change in order to focus on the fundamental question of quantitative change.
Suppose the implementation of a certain program gives rise to a phenomenal experience. I began by considering the question of whether two implementations of this program give rise to two qualitatively identical but numerically distinct experiences. The Duplication thesis answers this question in the affirmative. That Duplication is a substantial claim can be seen from the fact that it is has important ethical and epistemological implications. I defended Duplication by arguing that its implications in these areas are much more plausible than the implications of its negation, Unification. Moreover, our direct ontological intuitions about the matter seem to support Duplication.
The matter of degree here resides in the number and size of the fragments existing in duplicate. In other cases, however, the entire supervening phenomenal experience is simultaneously duplicated, and the matter of degree resides in the total quantity of qualitatively identical experience. Intriguingly, this quantity can be a fractional number. This casts some new light on what it is to implement a computation. The idea of a fractional quantity of qualia may be puzzling. By considering the case of a single computer built with unreliable elements, it can be shown that this possibility of fractional qualia would have to be confronted even if Duplication were rejected. Hence it is not a reason to reject Duplication. One may still wonder about the notion of purely quantitative variation of qualia. A quale is supposed to be a subjective phenomenal appearance—what something feels like, ‘‘from the inside.’’ But in the central cases I described, the subject is not supposed to register any qualitative changes in her experience. What is it then that changes when the quantity of, say, a particular pain quale decreases from 1 unit to 0.3 units? If the pain feels just the same, in what sense is there less pain after the change? Here is my answer: There is less pain in precisely the same sense as there is less pain if now only one subject experiences a particular pain quale while before two subjects each experienced just such a pain quale. The nature of fractional quantitative change is the same as the nature of quantitative change when it occurs in more familiar integer increments. (If we wish to speak of ‘‘subjects of experience,’’ perhaps we should also say that such subjects can likewise come in fractional degrees, not only in integer increments.) Finally, I considered the relation between the cases described in this paper and the Fading Qualia thought experiment, which Chalmers used as part of an argument for the principle of organizational invariance. The possibility of qualia fading quantitatively without any change in its descriptive, qualitative character undermines Chalmers’ argument.
Artificial intelligence is a possibility that should not be ignored in any serious thinking about the future, and it raises many profound issues for ethics and public policy that philosophers ought to start thinking about. This article outlines the case for thinking that human-level machine intelligence might well appear within the next half century. It then explains four immediate consequences of such a development, and argues that machine intelligence would have a revolutionary impact on a wide range of the social, political, economic, commercial, technological, scientific and environmental issues that humanity will face over the coming decades.

The annals of artificial intelligence are littered with broken promises. Half a century after the first electric computer, we still have nothing that even resembles an intelligent machine, if by ‘intelligent’ we mean possessing the kind of general-purpose smartness that we humans pride ourselves of. Maybe we will never manage to build real artificial intelligence. The problem could be too difficult for human brains ever to solve. Those who find the prospect of machines surpassing us in general intellectual abilities threatening may even hope that is the case.

However, neither the fact that machine intelligence would be scary nor the fact that some past predictions were wrong is a good ground for concluding that artificial intelligence will never be created. Indeed, to assume that artificial intelligence is impossible or will take thousands of years to develop seems at least as unwarranted as to make the opposite assumption. At a minimum, we must acknowledge that any scenario about what the world will be like in 2050 that simply postulates the absence human-level artificial intelligence is making a big assumption that could well turn out to be false.

It is therefore important to consider the alternative possibility, that intelligent machines will be built within fifty years. In the past year or two, there have been several books and articles published by leading researchers in artificial intelligence and robotics that argue for precisely that projection. This essay will first outline some of the reasons for this, and then discuss some of the consequences of human-level artificial intelligence.

We can get a grasp of the issue by considering the three things that are needed for an effective artificial intelligence. These are: hardware, software, and input/output mechanisms.

The requisite input/output technology already exists. We have video cameras, speakers, robotic arms etc. that provide a rich variety of ways for a computer to interact with its environment. So this part is trivial.

It is only relatively recently that we have begun to understand the computational mechanisms of biological brains. Computational neuroscience is only about fifteen years old as an active research discipline. In this short time, substantial progress has been made. We are beginning to understand early sensory processing. There are reasonably good computational models of primary visual cortex, and we are working our way up to the higher stages of visual cognition. We are uncovering what the basic learning algorithms are that govern how the strengths of synapses are modified by experience. The general architecture of our neuronal networks is being mapped out as we learn more about the interconnectivity between neurones and how different cortical areas project onto to one another. While we are still far from understanding higher-level thinking, we are beginning to figure out how the individual components work and how they are hooked up.

Assuming continuing rapid progress in neuroscience, we can envision learning enough about the lower-level processes and the overall architecture to begin to implement the same paradigms in computer simulations. Today, such simulations are limited to relatively small assemblies of neurones. There is a silicon retina and a silicon cochlea that do the same things as their biological counterparts. Simulating a whole brain will of course require enormous computing power; but as we saw, that capacity will be available within a couple of decades.

The product of this biology-inspired method will not be an explicitly coded mature artificial intelligence. (That is what the so-called classical school of artificial intelligence unsuccessfully tried to do.) Rather, it will be system that has the same ability as a toddler to learn from experience and to be educated. The system will need to be taught in order to attain the abilities of adult humans. But there is no reason why the computational algorithms that our biological brains use would not work equally well when implemented in silicon hardware.

An artificial intelligence is based on software, and it can therefore be copied as easily as any other computer program. Apart from hardware requirements, the marginal cost of creating an additional artificial intelligence after you have built the first one is close to zero. Artificial minds could therefore quickly come to exist in great numbers, amplifying the impact of the initial breakthrough.
There is a temptation to stop the analysis at the point where human-level machine intelligence appears, since that by itself is quite a dramatic development. But doing so is to miss an essential point that makes artificial intelligence a truly revolutionary prospect, namely, that it can be expected to lead to the creation of machines with intellectual abilities that vastly surpass those of any human. We can predict with great confidence that this second step will follow, although the time-scale is again somewhat uncertain. If Moore's law continues to hold in this era, the speed of artificial intelligences will double at least every two years. Within fourteen years after human-level artificial intelligence is reached, there could be machines that think more than a hundred times more rapidly than humans do. In reality, progress could be even more rapid than that, because there would likely be parallel improvements in the efficiency of the software that these machines use. The interval during which the machines and humans are roughly matched will likely be brief. Shortly thereafter, humans will be unable to compete intellectually with artificial minds.

Artificial intelligence is a true general-purpose technology. It enables applications in a very wide range of other fields. In particular, scientific and technological research (as well as philosophical thinking) will be done more effectively when conducted by machines that are cleverer than humans. One can therefore expect that overall technological progress will be rapid.

Machine intelligences may devote their abilities to designing the next generation of machine intelligence. This next generation will be even smarter and might be able to design their successors in even shorter time. Some authors have speculated that this positive feedback loop will lead to a "singularity" - a point where technological progress becomes so rapid that genuine superintelligence, with abilities unfathomable to mere humans, is attained within a short time span. However, it may turn out that there are diminishing returns in artificial intelligence research when some point is reached. Maybe once the low-hanging fruits have been picked, it gets harder and harder to make further improvement. There seems to be no clear way of predicting which way it will go.
It would be a mistake to conceptualise machine intelligence as a mere tool. Although it may be possible to build special-purpose artificial intelligence that could only think about some restricted set of problems, we are considering here a scenario in which machines with general-purpose intelligence are created. Such machines would be capable of independent initiative and of making their own plans. Such artificial intellects are perhaps more appropriately viewed as persons than machines. In economics lingo, they might come to be classified not as capital but as labour. If we can control the motivations of the artificial intellects that we design, they could come to constitute a class of highly capable "slaves" (although that term might be misleading if the machines don't want to do anything other than serve the people who built or commissioned them). The ethical and political debates surrounding these issues will likely become intense as the prospect of artificial intelligence draws closer.

Transhumanism is a loosely defined movement that has developed gradually over the past two decades. It promotes an interdisciplinary approach to understanding and evaluating the opportunities for enhancing the human condition and the human organism opened up by the advancement of technology. Attention is given to both present technologies, like genetic engineering and information technology, and anticipated future ones, such as molecular nanotechnology and artificial intelligence.1

The enhancement options being discussed include radical extension of human health-span, eradication of disease, elimination of unnecessary suffering, and augmentation of human intellectual, physical, and emotional capacities.2 Other transhumanist themes include space colonization and the possibility of creating superintelligent machines, along with other potential developments that could profoundly alter the human condition. The ambit is not limited to gadgets and medicine, but encompasses also economic, social, institutional designs, cultural development, and psychological skills and techniques.

Transhumanists view human nature as a work-in-progress, a half-baked beginning that we can learn to remold in desirable ways. Current humanity need not be the endpoint of evolution. Transhumanists hope that by responsible use of science, technology, and other rational means we shall eventually manage to become post-human, beings with vastly greater capacities than present human beings have.

Some transhumanists take active steps to increase the probability that they personally will survive long enough to become post-human, for example by choosing a healthy lifestyle or by making provisions for having themselves cryonically suspended in case of de-animation.3 In contrast to many other ethical outlooks, which in practice often reflect a reactionary attitude to new technologies, the transhumanist view is guided by an evolving vision to take a more active approach to technology policy. This vision, in broad strokes, is to create the opportunity to live much longer and healthier lives, to enhance our memory and other intellectual faculties, to refine our emotional experiences and increase our subjective sense of well-being, and generally to achieve a greater degree of control over our own lives. This affirmation of human potential is offered as an alternative to customary injunctions against playing God, messing with nature, tampering with our human essence, or displaying punishable hubris.

Transhumanism does not entail technological optimism. While future technological capabilities carry immense potential for beneficial deployments, they also could be misused to cause enormous harm, ranging all the way to the extreme possibility of intelligent life becoming extinct. Other potential negative outcomes include widening social inequalities or a gradual erosion of the hard-to-quantify assets that we care deeply about but tend to neglect in our daily struggle for material gain, such as meaningful human relationships and ecological diversity. Such risks must be taken very seriously, as thoughtful transhumanists fully acknowledge.4

Transhumanism has roots in secular humanist thinking, yet is more radical in that it promotes not only traditional means of improving human nature, such as education and cultural refinement, but also direct application of medicine and technology to overcome some of our basic biological limits.

The range of thoughts, feelings, experiences, and activities that are accessible to human organisms presumably constitute only a tiny part of what is possible. There is no reason to think that the human mode of being is any more free of limitations imposed by our biological nature than are the modes of being of other animals. Just as chimpanzees lack the brainpower to understand what it is like to be human, so too do we lack the practical ability to form a realistic intuitive understanding of what it would be like to be post-human.

This point is distinct from any principled claims about impossibility. We need not assert that post-humans would not be Turing computable or that their concepts could not be expressed by any finite sentences in human language.  The impossibility is more like the impossibility for us to visualize a twenty-dimensional hypersphere or to read, with perfect recollection and understanding, every book in the Library of Congress. Our own current mode of being, therefore, spans but a minute subspace of what is possible or permitted by the physical constraints of the universe. It is not farfetched to suppose that there are parts of this larger space that represent extremely valuable ways of living, feeling, and thinking.

We can conceive of aesthetic and contemplative pleasures whose blissfulness vastly exceeds what any human being has yet experienced. We can imagine beings that reach a much greater level of personal development and maturity than current human beings do, because they have the opportunity to live for hundreds or thousands of years with full bodily and psychic vigor. We can conceive of beings that are much smarter than us, that can read books in seconds, that are much more brilliant philosophers than we are, that can create artworks, which, even if we could understand them only on the most superficial level, would strike us as wonderful masterpieces. We can imagine love that is stronger, purer, and more secure than any human being has yet harbored. Our everyday intuitions about values are constrained by the narrowness of our experience and the limitations of our powers of imagination. We should leave room in our thinking for the possibility that as we develop greater capacities, we shall come to discover values that will strike us as being of a far higher order than those we can realize as un-enhanced biological humans beings.

The conjecture that there are greater values than we can currently fathom does not imply that values are not defined in terms of our current dispositions. Take, for example, a dispositional theory of value such as the one described by David Lewis.5 According to Lewis’s theory, something is a value for you if and only if you would want to want it if you were perfectly acquainted with it and you were thinking and deliberating as clearly as possible about it. On this view, there may be values that we do not currently want, and that we do not even currently want to want, because we may not be perfectly acquainted with them or because we are not ideal deliberators. Some values pertaining to certain forms of post-human existence may well be of this sort; they may be values for us now, and they may be so in virtue of our current dispositions, and yet we may not be able to fully appreciate them with our current limited deliberative capacities and our lack of the receptive faculties required for full acquaintance with them. This point is important because it shows that the transhumanist view that we ought to explore the realm of post-human values does not entail that we should forego our current values. The post-human values can be our current values, albeit ones that we have not yet clearly comprehended. Transhumanism does not require us to say that we should favor post-human beings over human beings, but that the right way of favoring human beings is by enabling us to realize our ideals better and that some of our ideals may well be located outside the space of modes of being that are accessible to us with our current biological constitution.

We can overcome many of our biological limitations. It is possible that there are some limitations that are impossible for us to transcend, not only because of technological difficulties but on metaphysical grounds. Depending on what our views are about what constitutes personal identity, it could be that certain modes of being, while possible, are not possible for us, because any being of such a kind would be so different from us that they could not be us. Concerns of this kind are familiar from theological discussions of the afterlife. In Christian theology, some souls will be allowed by God to go to heaven after their time as corporal creatures is over. Before being admitted to heaven, the souls would undergo a purification process in which they would lose many of their previous bodily attributes. Skeptics may doubt that the resulting minds would be sufficiently similar to our current minds for it to be possible for them to be the same person. A similar predicament arises within transhumanism: if the mode of being of a post-human being is radically different from that of a human being, then we may doubt whether a post-human being could be the same person as a human being, even if the post-human being originated from a human being.

We can, however, envision many enhancements that would not make it impossible for the post-transformation someone to be the same person as the pre-transformation person. A person could obtain considerable increased life expectancy, intelligence, health, memory, and emotional sensitivity, without ceasing to exist in the process. A person’s intellectual life can be transformed radically by getting an education. A person’s life expectancy can be extended substantially by being unexpectedly cured from a lethal disease. Yet these developments are not viewed as spelling the end of the original person. In particular, it seems that modifications that add to a person’s capacities can be more substantial than modifications that subtract, such as brain damage. If most of someone currently is, including her most important memories, activities, and feelings, is preserved, then adding extra capacities on top of that would not easily cause the person to cease to exist.

Preservation of personal identity, especially if this notion is given a narrow construal, is not everything. We can value other things than ourselves, or we might regard it as satisfactory if some parts or aspects of ourselves survive and flourish, even if that entails giving up some parts of ourselves such that we no longer count as being the same person. Which parts of ourselves we might be willing to sacrifice may not become clear until we are more fully acquainted with the full meaning of the options. A careful, incremental exploration of the post-human realm may be indispensable for acquiring such an understanding, although we may also be able to learn from each other’s experiences and from works of the imagination. Additionally, we may favor future people being posthuman rather than human, if the posthumans would lead lives more worthwhile than the alternative humans would. Any reasons stemming from such considerations would not depend on the assumption that we ourselves could become posthuman beings.

Transhumanism promotes the quest to develop further so that we can explore hitherto inaccessible realms of value. Technological enhancement of human organisms is a means that we ought to pursue to this end. There are limits to how much can be achieved by low-tech means such as education, philosophical contemplation, moral self-scrutiny and other such methods proposed by classical philosophers with perfectionist leanings, including Plato, Aristotle, and Nietzsche, or by means of creating a fairer and better society, as envisioned by social reformists such as Marx or Martin Luther King. This is not to denigrate what we can do with the tools we have today. Yet ultimately, transhumanists hope to go further.

Most potential human enhancement technologies have so far received scant attention in the ethics literature. One exception is genetic engineering, the morality of which has been extensively debated in recent years. To illustrate how the transhumanist approach can be applied to particular technologies, we shall therefore now turn to consider the case of human germ-line genetic enhancements.

Certain types of objection against germ-line modifications are not accorded much weight by a transhumanist interlocutor. For instance, objections that are based on the idea that there is something inherently wrong or morally suspect in using science to manipulate human nature are regarded by transhumanists as wrongheaded. Moreover, transhumanists emphasize that particular concerns about negative aspects of genetic enhancements, even when such concerns are legitimate, must be judged against the potentially enormous benefits that could come from genetic technology successfully employed.6 For example, many commentators worry about the psychological effects of the use of germ-line engineering. The ability to select the genes of our children and to create so-called designer babies will, it is claimed, corrupt parents, who will come to view their children as mere products.7 We will then begin to evaluate our offspring according to standards of quality control, and this will undermine the ethical ideal of unconditional acceptance of children, no matter what their abilities and traits. Are we really prepared to sacrifice on the altar of consumerism even those deep values that are embodied in traditional relationships between child and parents? Is the quest for perfection worth this cultural and moral cost? A transhumanist should not dismiss such concerns as irrelevant. Transhumanists recognize that the depicted outcome would be bad. We do not want parents to love and respect their children less. We do not want social prejudice against people with disabilities to get worse. The psychological and cultural effects of commodifying human nature are potentially important.

But such dystopian scenarios are speculations. There is no firm ground for believing that the alleged consequences would actually happen. What relevant evidence we have, for instance regarding the treatment of children who have been conceived through the use of in vitro fertilization or embryo screening, suggests that the pessimistic prognosis is alarmist. Parents will in fact love and respect their children even when artificial means and conscious choice play a part in procreation.

We might speculate, instead, that germ-line enhancements will lead to more love and parental dedication. Some mothers and fathers might find it easier to love a child who, thanks to enhancements, is bright, beautiful, healthy, and happy. The practice of germ-line enhancement might lead to better treatment of people with disabilities, because a general demystification of the genetic contributions to human traits could make it clearer that people with disabilities are not to blame for their disabilities and a decreased incidence of some disabilities could lead to more assistance being available for the remaining affected people to enable them to live full, unrestricted lives through various technological and social supports. Speculating about possible psychological or cultural effects of germ-line engineering can therefore cut both ways. Good consequences no less than bad ones are possible. In the absence of sound arguments for the view that the negative consequences would predominate, such speculations provide no reason against moving forward with the technology.

Ruminations over hypothetical side-effects may serve to make us aware of things that could go wrong so that we can be on the lookout for untoward developments. By being aware of the perils in advance, we will be in a better position to take preventive countermeasures. For instance, if we think that some people would fail to realize that a human clone would be a unique person deserving just as much respect and dignity as any other human being, we could work harder to educate the public on the inadequacy of genetic determinism. The theoretical contributions of well-informed and reasonable critics of germ-line enhancement could indirectly add to our justification for proceeding with germ-line engineering. To the extent that the critics have done their job, they can alert us to many of the potential untoward consequences of germ-line engineering and contribute to our ability to take precautions, thus improving the odds that the balance of effects will be positive. There may well be some negative consequences of human germ-line engineering that we will not forestall, though of course the mere existence of negative effects is not a decisive reason not to proceed. Every major technology has some negative consequences. Only after a fair comparison of the risks with the likely positive consequences can any conclusion based on a cost-benefit analysis be reached.

In the case of germ-line enhancements, the potential gains are enormous. Only rarely, however, are the potential gains discussed, perhaps because they are too obvious to be of much theoretical interest. By contrast, uncovering subtle and non-trivial ways in which manipulating our genome could undermine deep values is philosophically a lot more challenging. But if we think about it, we recognize that the promise of genetic enhancements is anything but insignificant. Being free from severe genetic diseases would be good, as would having a mind that can learn more quickly, or having a more robust immune system. Healthier, wittier, happier people may be able to reach new levels culturally. To achieve a significant enhancement of human capacities would be to embark on the transhuman journey of exploration of some of the modes of being that are not accessible to us as we are currently constituted, possibly to discover and to instantiate important new values. On an even more basic level, genetic engineering holds great potential for alleviating unnecessary human suffering. Every day that the introduction of effective human genetic enhancement is delayed is a day of lost individual and cultural potential, and a day of torment for many unfortunate sufferers of diseases that could have been prevented. Seen in this light, proponents of a ban or a moratorium on human genetic modification must take on a heavy burden of proof in order to have the balance of reason tilt in their favor. Transhumanists conclude that the challenge has not been met.

One way of going forward with genetic engineering is to permit everything, leaving all choices to parents. While this attitude may be consistent with transhumanism, it is not the best transhumanist approach. One thing that can be said for adopting a libertarian stance in regard to human reproduction is the sorry track record of socially planned attempts to improve the human gene pool. The list of historical examples of state intervention in this domain ranges from the genocidal horrors of the Nazi regime, to the incomparably milder but still disgraceful semi-coercive sterilization programs of mentally impaired individuals favored by many well-meaning socialists in the past century, to the controversial but perhaps understandable program of the current Chinese government to limit population growth. In each case, state policies interfered with the reproductive choices of individuals. If parents had been left to make the choices for themselves, the worst transgressions of the eugenics movement would not have occurred. Bearing this in mind, we ought to think twice before giving our support to any proposal that would have the state regulate what sort of children people are allowed to have and the methods that may be used to conceive them.8

We currently permit governments to have a role in reproduction and child-rearing and we may reason by extension that there would likewise be a role in regulating the application of genetic reproductive technology. State agencies and regulators play a supportive and supervisory role, attempting to promote the interests of the child. Courts intervene in cases of child abuse or neglect. Some social policies are in place to support children from disadvantaged backgrounds and to ameliorate some of the worst inequities suffered by children from poor homes, such as through the provision of free schooling. These measures have analogues that apply to genetic enhancement technologies. For example, we ought to outlaw genetic modifications that are intended to damage the child or limit its opportunities in life, or that are judged to be too risky. If there are basic enhancements that would be beneficial for a child but that some parents cannot afford, then we should consider subsidizing those enhancements, just as we do with basic education. There are grounds for thinking that the libertarian approach is less appropriate in the realm of reproduction than it is in other areas. In reproduction, the most important interests at stake are those of the child-to-be, who cannot give his or her advance consent or freely enter into any form of contract. As it is, we currently approve of many measures that limit parental freedoms. We have laws against child abuse and child neglect. We have obligatory schooling. In some cases, we can force needed medical treatment on a child, even against the wishes of its parents.

There is a difference between these social interventions with regard to children and interventions aimed at genetic enhancements. While there is a consensus that nobody should be subjected to child abuse and that all children should have at least a basic education and should receive necessary medical care, it is unlikely that we will reach an agreement on proposals for genetic enhancements any time soon. Many parents will resist such proposals on principled grounds, including deep-seated religious or moral convictions. The best policy for the foreseeable future may therefore be to not legally require any genetic enhancements, except perhaps in extreme cases for which there is no alternative treatment. Even in such cases, it is dubious that the social climate in many countries is ready for mandatory genetic interventions.

The scope for ethics and public policy, however, extend far beyond the passing of laws requiring or banning specific interventions. Even if a given enhancement option is neither outlawed nor legally required, we may still seek to discourage or encourage its use in a variety of ways. Through subsidies and taxes, research-funding policies, genetic counseling practices and guidelines, laws regulating genetic information and genetic discrimination, provision of health care services, regulation of the insurance industry, patent law, education, and through the allocation of social approbation and disapproval, we may influence the direction in which particular technologies are applied. We may appropriately ask, with regard to genetic enhancement technologies, which types of applications we ought to promote or discourage.

An externality, as understood by economists, is a cost or a benefit of an action that is not carried by a decision-maker. An example of a negative externality might be found in a firm that lowers its production costs by polluting the environment. The firm enjoys most of the benefits while escaping the costs, such as environmental degradation, which may instead paid by people living nearby. Externalities can also be positive, as when people put time and effort into creating a beautiful garden outside their house. The effects are enjoyed not exclusively by the gardeners but spill over to passersby. As a rule of thumb, sound social policy and social norms would have us internalize many externalities so that the incentives of producers more closely match the social value of production. We may levy a pollution tax on the polluting firm, for instance, and give our praise to the home gardeners who beautify the neighborhood.

Genetic enhancements aimed at the obtainment of goods that are desirable only in so far as they provide a competitive advantage tend to have negative externalities. An example of such a positional good, as economists call them, is stature. There is evidence that being tall is statistically advantageous, at least for men in Western societies. Taller men earn more money, wield greater social influence, and are viewed as more sexually attractive. Parents wanting to give their child the best possible start in life may rationally choose a genetic enhancement that adds an inch or two to the expected length of their offspring. Yet for society as a whole, there seems to be no advantage whatsoever in people being taller. If everybody grew two inches, nobody would be better off than they were before. Money spent on a positional good like length has little or no net effect on social welfare and is therefore, from society’s point of view, wasted.

Health is a very different type of good. It has intrinsic benefits. If we become healthier, we are personally better off and others are not any worse off. There may even be a positive externality of enhancing ours own health. If we are less likely to contract a contagious disease, others benefit by being less likely to get infected by us. Being healthier, you may also contribute more to society and consume less of publicly funded healthcare.

If we were living in a simple world where people were perfectly rational self-interested economic agents and where social policies had no costs or unintended effects, then the basic policy prescription regarding genetic enhancements would be relatively straightforward. We should internalize the externalities of genetic enhancements by taxing enhancements that have negative externalities and subsidizing enhancements that have positive externalities. Unfortunately, crafting policies that work well in practice is considerably more difficult. Even determining the net size of the externalities of a particular genetic enhancement can be difficult. There is clearly an intrinsic value to enhancing memory or intelligence in as much as most of us would like to be a bit smarter, even if that did not have the slightest effect on our standing in relation to others. But there would also be important externalities, both positive and negative. On the negative side, others would suffer some disadvantage from our increased brainpower in that their own competitive situation would be worsened. Being more intelligent, we would be more likely to attain high-status positions in society, positions that would otherwise have been enjoyed by a competitor. On the positive side, others might benefit from enjoying witty conversations with us and from our increased taxes.

If in the case of intelligence enhancement the positive externalities outweigh the negative ones, then a prima facie case exists not only for permitting genetic enhancements aimed at increasing intellectual ability, but for encouraging and subsidizing them too. Whether such policies remain a good idea when all practicalities of implementation and political realities are taken into account is another matter. But at least we can conclude that an enhancement that has both significant intrinsic benefits for an enhanced individual and net positive externalities for the rest of society should be encouraged. By contrast, enhancements that confer only positional advantages, such as augmentation of stature or physical attractiveness, should not be socially encouraged, and we might even attempt to make a case for social policies aimed at reducing expenditure on such goods, for instance through a progressive tax on consumption.

One important kind of externality in germ-line enhancements is their effects on social equality. This has been a focus for many opponents of germ-line genetic engineering who worry that it will widen the gap between haves and have-nots. Today, children from wealthy homes enjoy many environmental privileges, including access to better schools and social networks. Arguably, this constitutes an inequity against children from poor homes. We can imagine scenarios where such inequities grow much larger thanks to genetic interventions that only the rich can afford, adding genetic advantages to the environmental advantages already benefiting privileged children. We could even speculate about the members of the privileged stratum of society eventually enhancing themselves and their offspring to a point where the human species, for many practical purposes, splits into two or more species that have little in common except a shared evolutionary history.10 The genetically privileged might become ageless, healthy, super-geniuses of flawless physical beauty, who are graced with a sparkling wit and a disarmingly self-deprecating sense of humor, radiating warmth, empathetic charm, and relaxed confidence. The non-privileged would remain as people are today but perhaps deprived of some their self-respect and suffering occasional bouts of envy. The mobility between the lower and the upper classes might disappear, and a child born to poor parents, lacking genetic enhancements, might find it impossible to successfully compete against the super-children of the rich. Even if no discrimination or exploitation of the lower class occurred, there is still something disturbing about the prospect of a society with such extreme inequalities.

While we have vast inequalities today and regard many of these as unfair, we also accept a wide range of inequalities because we think that they are deserved, have social benefits, or are unavoidable concomitants to free individuals making their own and sometimes foolish choices about how to live their lives. Some of these justifications can also be used to exonerate some inequalities that could result from germ-line engineering. Moreover, the increase in unjust inequalities due to technology is not a sufficient reason for discouraging the development and use of the technology. We must also consider its benefits, which include not only positive externalities but also intrinsic values that reside in such goods as the enjoyment of health, a soaring mind, and emotional well-being.

We can also try to counteract some of the inequality-increasing tendencies of enhancement technology with social policies. One way of doing so would be by widening access to the technology by subsidizing it or providing it for free to children of poor parents. In cases where the enhancement has considerable positive externalities, such a policy may actually benefit everybody, not just the recipients of the subsidy. In other cases, we could support the policy on the basis of social justice and solidarity.

Even if all genetic enhancements were made available to everybody for free, however, this might still not completely allay the concern about inequity. Some parents might choose not to give their children any enhancements. The children would then have diminished opportunities through no fault of their own. It would be peculiar, however, to argue that governments should respond to this problem by limiting the reproductive freedom of the parents who wish to use genetic enhancements. If we are willing to limit reproductive freedom through legislation for the sake of reducing inequities, then we might as well make some enhancements obligatory for all children. By requiring genetic enhancements for everybody to the same degree, we would not only prevent an increase in inequalities but also reap the intrinsic benefits and the positive externalities that would come from the universal application of enhancement technology. If reproductive freedom is regarded as too precious to be curtailed, then neither requiring nor banning the use of reproductive enhancement technology is an available option. In that case, we would either have to tolerate inequities as a price worth paying for reproductive freedom or seek to remedy the inequities in ways that do not infringe on reproductive freedom.

All of this is based on the hypothesis that germ-line engineering would in fact increase inequalities if left unregulated and no countermeasures were taken. That hypothesis might be false. In particular, it might turn out to be technologically easier to cure gross genetic defects than to enhance an already healthy genetic constitution. We currently know much more about many specific inheritable diseases, some of which are due to single gene defects, than we do about the genetic basis of talents and desirable qualities such as intelligence and longevity, which in all likelihood are encoded in complex constellations of multiple genes. If this turns out to be the case, then the trajectory of human genetic enhancement may be one in which the first thing to happen is that the lot of the genetically worst-off is radically improved, through the elimination of diseases such as Tay Sachs, Lesch-Nyhan, Downs Syndrome, and early-onset Alzheimer’s disease. This would have a major leveling effect on inequalities, not primarily in the monetary sense, but with respect to the even more fundamental parameters of basic opportunities and quality of life.

Another frequently heard objection against germ-line genetic engineering is that it would be uniquely hazardous because the changes it would bring are irreversible and would affect all generations to come. It would be highly irresponsible and arrogant of us to presume we have the wisdom to make decisions about what should be the genetic constitutions of people living many generations hence. Human fallibility, on this objection, gives us good reason not to embark on germ-line interventions. For our present purposes, we can set aside the issue of the safety of the procedure, understood narrowly, and stipulate that the risk of medical side-effects has been reduced to an acceptable level. The objection under consideration concerns the irreversibility of germ-line interventions and the lack of predictability of its long-term consequences; it forces us to ask if we possess the requisite wisdom for making genetic choices on behalf of future generations.

Human fallibility is not a conclusive ground for resisting germ-line genetic enhancements. The claim that such interventions would be irreversible is incorrect. Germ-line interventions can be reversed by other germ-line interventions. Moreover, considering that technological progress in genetics is unlikely to grind to an abrupt halt any time soon, we can count on future generations being able to reverse our current germ-line interventions even more easily than we can currently implement them. With advanced genetic technology, it might even be possible to reverse many germ-line modifications with somatic gene therapy, or with medical nanotechnology. Technologically, germ-line changes are perfectly reversible by future generations.

It is possible that future generations might choose to retain the modifications that we make. If that turns out to be the case, then the modifications, while not irreversible, would nevertheless not actually be reversed. This might be a good thing. The possibility of permanent consequences is not an objection against germ-line interventions any more than it is against social reforms. The abolition of slavery and the introduction of general suffrage might never be reversed; indeed, we hope they will not be. Yet this is no reason for people to have resisted the reforms. Likewise, the potential for everlasting consequences, including ones we cannot currently reliably forecast, in itself constitutes no reason to oppose genetic intervention. If immunity against horrible diseases and enhancements that expand the opportunities for human growth are passed on to subsequent generations in perpetuo, it would be a cause for celebration, not regret.

There are some kinds of changes that we need be particularly careful about. They include modifications of the drives and motivations of our descendants. For example, there are obvious reasons why we might think it worthwhile to seek to reduce our children’s propensity to violence and aggression. We would have to take care, however, that we do not do this in a way that would make future people overly submissive or complacent. We can conceive of a dystopian scenario along the lines of Brave New World, in which people are leading shallow lives but have been manipulated to be perfectly content with their sub-optimal existence. If the people transferred their shallow values to their children, humanity could get permanently stuck in a not-very-good state, having foolishly changed itself to lack any desire to strive for something better. This outcome would be dystopian because a permanent cap on human development would destroy the transhumanist hope of exploring the post-human realm. Transhumanists therefore place an emphasis on modifications which, in addition to promoting human well-being, also open more possibilities than they close and which increase our ability to make subsequent choices wisely. Longer active lifespans, better memory, and greater intellectual capacities are plausible candidates for enhancements that would improve our ability to figure out what we ought to do next. They would be a good place to start.

The possibility of creating thinking machines raises a host of ethical issues.  These questions relate both to ensuring that such machines do not harm humans and other morally relevant beings, and to the moral status of the machines themselves.  The first section discusses issues that may arise in the near future of AI.  The second section outlines challenges for ensuring that AI operates safely as it approaches humans in its intelligence.  The third section outlines how we might assess whether, and in what circumstances, AIs themselves have moral status.  In the fourth section, we consider how AIs might differ from humans in certain basic respects relevant to our ethical assessment of them.  The final section addresses the issues of creating AIs more intelligent than human, and ensuring that they use their advanced intelligence for good rather than ill.
Imagine, in the near future, a bank using a machine learning algorithm to recommend mortgage applications for approval.  A rejected applicant brings a lawsuit against the bank, alleging that the algorithm is discriminating racially against mortgage applicants.  The bank replies that this is impossible, since the algorithm is deliberately blinded to the race of the applicants.  Indeed, that was part of the bank’s rationale for implementing the system.  Even so, statistics show that the bank’s approval rate for black applicants has been steadily dropping.  Submitting ten apparently equally qualified genuine applicants (as determined by a separate panel of human judges) shows that the algorithm accepts white applicants and rejects black applicants.  What could possibly be happening?
AI algorithms play an increasingly large role in modern society, though usually not labeled “AI”.  The scenario described above might be transpiring even as we write.  It will become increasingly important to develop AI algorithms that are not just powerful and scalable, but also transparent to inspection—to name one of many socially important properties. Some challenges of machine ethics are much like many other challenges involved in designing machines.  Designing a robot arm to avoid crushing stray humans is no more morally fraught than designing a flame‐retardant sofa.  It involves new programming challenges, but no new ethical challenges.  But when AI algorithms take on cognitive work with social dimensions—cognitive tasks previously performed by humans—the AI algorithm inherits the social requirements.  It would surely be frustrating to find that no bank in the world will approve your seemingly excellent loan application, and nobody knows why, and nobody can find out even in principle.   (Maybe you have a first name strongly associated with deadbeats?  Who knows?) Transparency is not the only desirable feature of AI.  It is also important that AI algorithms taking over social functions be predictable to those they govern.  To understand the importance of such predictability, consider an analogy.  The legal principle of stare decisis binds judges to follow past precedent whenever possible.  To an engineer, this preference for precedent may seem incomprehensible—why bind the future to the past, when technology is always improving?  But one of the most important functions of the legal system is to be predictable, so that, e.g., contracts can be written knowing how they will be executed.  The job of the legal system is not necessarily to optimize society, but to provide a predictable environment within which citizens can optimize their own lives. It will also become increasingly important that AI algorithms be robust against manipulation. A machine vision system to scan airline luggage for bombs must be robust against human adversaries deliberately searching for exploitable flaws in the algorithm—for example, a shape that, placed next to a pistol in one’s luggage, would neutralize recognition of it.  Robustness against manipulation is an ordinary criterion in information security; nearly the criterion.  But it is not a criterion that appears often in machine learning journals, which are currently more interested in, e.g., how an algorithm scales up on larger parallel systems. Another important social criterion for dealing with organizations is being able to find the person responsible for getting something done.  When an AI system fails at its assigned task, who takes the blame?  The programmers?  The end‐users?
There is nearly universal agreement among modern AI professionals that Artificial Intelligence falls short of human capabilities in some critical sense, even though AI algorithms have beaten humans in many specific domains such as chess.  It has been suggested by some that as soon as AI researchers figure out how to do something, that capability ceases to be regarded as intelligent—chess was considered the epitome of intelligence until Deep Blue won the world championship from Kasparov—but even these researchers agree that something important is missing from modern AIs (e.g., Hofstadter 2006). While this subfield of Artificial Intelligence is only just coalescing, “Artificial General Intelligence” (hereafter, AGI) is the emerging term of art used to denote “real” AI (see, e.g., the edited volume Goertzel and Pennachin 2006).  As the name implies, the emerging consensus is that the missing characteristic is generality.  Current AI algorithms with human‐equivalent or ‐superior performance are characterized by a deliberately‐programmed competence only in a single, restricted domain.  Deep Blue became the world champion at chess, but it cannot even play checkers, let alone drive a car or make a scientific discovery.  Such modern AI algorithms resemble all biological life with the sole exception of Homo sapiens.  A bee exhibits competence at building hives; a beaver exhibits competence at building dams; but a bee doesn’t build dams, and a beaver can’t learn to build a hive.  A human, watching, can learn to do both; but this is a unique ability among biological lifeforms.  It is debatable whether human intelligence is truly general—we are certainly better at some cognitive tasks than others (Hirschfeld and Gelman 1994)—but human intelligence is surely significantly more generally applicable than nonhominid intelligence.
It is relatively easy to envisage the sort of safety issues that may result from AI operating only within a specific domain.  It is a qualitatively different class of problem to handle an AGI operating across many novel contexts that cannot be predicted in advance. When human engineers build a nuclear reactor, they envision the specific events that could go on inside it—valves failing, computers failing, cores increasing in temperature—and engineer the reactor to render these events noncatastrophic.  Or, on a more mundane level, building a toaster involves envisioning bread and envisioning the reaction of the bread to the toasterʹs heating element.  The toaster itself does not know that its purpose is to make toast—the purpose of the toaster is represented within the designer’s mind, but is not explicitly represented in computations inside the toaster—and so if you place cloth inside a toaster, it may catch fire, as the design executes in an unenvisioned context with an unenvisioned side effect. Even task‐specific AI algorithms throw us outside the toaster‐paradigm, the domain of locally preprogrammed, specifically envisioned behavior.  Consider Deep Blue, the chess algorithm that beat Garry Kasparov for the world championship of chess.  Were it the case that machines can only do exactly as they are told, the programmers would have had to manually preprogram a database containing moves for every possible chess position that Deep Blue could encounter.  But this was not an option for Deep Blue’s programmers.  First, the space of possible chess positions is unmanageably large.  Second, if the programmers had manually input what they considered a good move in each possible situation, the resulting system would not have been able to make stronger chess moves than its creators.  Since the programmers themselves were not world champions, such a system would not have been able to defeat Garry Kasparov. In creating a superhuman chess player, the human programmers necessarily sacrificed their ability to predict Deep Blue’s local, specific game behavior.  Instead, Deep Blue’s programmers had (justifiable) confidence that Deep Blue’s chess moves would satisfy a non‐local criterion of optimality:  namely, that the moves would tend to steer the future of the game board into outcomes in the “winning” region as defined by the chess rules.   This prediction about distant consequences, though it proved accurate, did not allow the programmers to envision the local behavior of Deep Blue—its response to a specific attack on its king—because Deep Blue computed the nonlocal game map, the link between a move and its possible future consequences, more accurately than the programmers could (Yudkowsky 2006). Modern humans do literally millions of things to feed themselves—to serve the final consequence of being fed.  Few of these activities were “envisioned by Nature” in the sense of being ancestral challenges to which we are directly adapted.
Humans crossed space and put footprints on the Moon, even though none of our ancestors encountered a challenge analogous to vacuum.  Compared to domain‐specific AI, it is a qualitatively different problem to design a system that will operate safely across thousands of contexts; including contexts not specifically envisioned by either the designers or the users; including contexts that no human has yet encountered.  Here there may be no local specification of good behavior—no simple specification over the behaviors themselves, any more than there exists a compact local description of all the ways that humans obtain their daily bread. To build an AI that acts safely while acting in many domains, with many consequences, including problems the engineers never explicitly envisioned, one must specify good behavior in such terms as “X such that the consequence of X is not harmful to humans”.  This is non‐local; it involves extrapolating the distant consequences of actions.  Thus, this is only an effective specification—one that can be realized as a design property—if the system explicitly extrapolates the consequences of its behavior.  A toaster cannot have this design property because a toaster cannot foresee the consequences of toasting bread. Imagine an engineer having to say, “Well, I have no idea how this airplane I built will fly safely—indeed I have no idea how it will fly at all, whether it will flap its wings or inflate itself with helium or something else I haven’t even imagined—but I assure you, the design is very, very safe.”  This may seem like an unenviable position from the perspective of public relations, but it’s hard to see what other guarantee of ethical behavior would be possible for a general intelligence operating on unforeseen problems, across domains, with preferences over distant consequences.  Inspecting the cognitive design might verify that the mind was, indeed, searching for solutions that we would classify as ethical; but we couldn’t predict which specific solution the mind would discover. Respecting such a verification requires some way to distinguish trustworthy assurances (a procedure which will not say the AI is safe unless the AI really is safe) from pure hope and magical thinking (“I have no idea how the Philosopher’s Stone will transmute lead to gold, but I assure you, it will!”).  One should bear in mind that purely hopeful expectations have previously been a problem in AI research (McDermott 1976). Verifiably constructing a trustworthy AGI will require different methods, and a different way of thinking, from inspecting power plant software for bugs—it will require an AGI that thinks like a human engineer concerned about ethics, not just a simple product of ethical engineering.
A different set of ethical issues arises when we contemplate the possibility that some future AI systems might be candidates for having moral status.  Our dealings with beings possessed of moral status are not exclusively a matter of instrumental rationality: we also have moral reasons to treat them in certain ways, and to refrain from treating them in certain other ways.  Francis Kamm has proposed the following definition of moral status, which will serve for our purposes.
A rock has no moral status: we may crush it, pulverize it, or subject it to any treatment we like without any concern for the rock itself.  A human person, on the other hand, must be treated not only as a means but also as an end.  Exactly what it means to treat a person as an end is something about which different ethical theories disagree; but it certainly involves taking her legitimate interests into account—giving weight to her well‐being—and it may also involve accepting strict moral side‐constraints in our dealings with her, such as a prohibition against murdering her, stealing from her, or doing a variety of other things to her or her property without her consent.  Moreover, it is because a human person counts in her own right, and for her sake, that it is impermissible to do to her these things.  This can be expressed more concisely by saying that a human person has moral status. Questions about moral status are important in some areas of practical ethics.  For example, disputes about the moral permissibility of abortion often hinge on disagreements about the moral status of the embryo.  Controversies about animal experimentation and the treatment of animals in the food industry involve questions about the moral status of different species of animal.  And our obligations towards human beings with severe dementia, such as late‐stage Alzheimer’s patients, may also depend on questions of moral status.
One common view is that many animals have qualia and therefore have some moral status, but that only human beings have sapience, which gives them a higher moral status than non‐human animals.1  This view, of course, must confront the existence of borderline cases such as, on the one hand, human infants or human beings with severe mental retardation—sometimes unfortunately referred to as “marginal humans”— which fail to satisfy the criteria for sapience; and, on the other hand, some non‐human animals such as the great apes, which might possess at least some of the elements of sapience.  Some deny that so‐called “marginal humans” have full moral status.  Others propose additional ways in which an object could qualify as a bearer of moral status, such as by being a member of a kind that normally has sentience or sapience, or by standing in a suitable relation to some being that independently has moral status (cf. Mary Anne Warren 2000).  For present purposes, however, we will focus on the criteria of sentience and sapience. This picture of moral status suggests that an AI system will have some moral status if it has the capacity for qualia, such as an ability to feel pain.  A sentient AI system, even if it lacks language and other higher cognitive faculties, is not like a stuffed toy animal or a wind‐up doll; it is more like a living animal.  It is wrong to inflict pain on a mouse, unless there are sufficiently strong morally overriding reasons to do so.  The same would hold for any sentient AI system.
The anencephalic child, however, would have the same moral status as any other similar anencephalic child, including one that had come about through some entirely natural process.  The difference in moral status between an anencephalic child and a normal child is grounded in the qualitative difference between the two—the fact that one has a mind while the other does not.  Since the two children do not have the same functionality and the same conscious experience, the Principle of Ontogeny Non‐Discrimination does not apply. Although the Principle of Ontogeny Non‐Discrimination asserts that a being’s ontogeny has no essential bearing on its moral status, it does not deny that facts about ontogeny can affect what duties particular moral agents have toward the being in question.  Parents have special duties to their child which they do not have to other children, and which they would not have even if there were another child qualitatively identical to their own.  Similarly, the Principle of Ontogeny Non‐Discrimination is consistent with the claim that the creators or owners of an AI system with moral status may have special duties to their artificial mind which they do not have to another artificial mind, even if the minds in question are qualitatively similar and have the same moral status. If the principles of non‐discrimination with regard to substrate and ontogeny are accepted, then many questions about how we ought to treat artificial minds can be answered by applying the same moral principles that we use to determine our duties in more familiar contexts.  Insofar as moral duties stem from moral status considerations, we ought to treat an artificial mind in just the same way as we ought to treat a qualitatively identical natural human mind in a similar situation.  This simplifies the problem of developing an ethics for the treatment of artificial minds. Even if we accept this stance, however, we must confront a number of novel ethical questions which the aforementioned principles leave unanswered.  Novel ethical questions arise because artificial minds can have very different properties from ordinary human or animal minds.  We must consider how these novel properties would affect the moral status of artificial minds and what it would mean to respect the moral status of such exotic minds.
An artificial intellect, by contrast, might be constituted quite differently from a human intellect yet still exhibit human‐like behavior or possess the behavioral dispositions normally indicative of personhood.  It might therefore be possible to conceive of an artificial intellect that would be sapient, and perhaps would be a person, yet would not be sentient or have conscious experiences of any kind.   (Whether this is really possible depends on the answers to some non‐trivial metaphysical questions.)  Should such a system be possible, it would raise the question whether a non‐sentient person would have any moral status whatever; and if so, whether it would have the same moral status as a sentient person.  Since sentience, or at least a capacity for sentience, is ordinarily assumed to be present in any individual who is a person, this question has not received much attention to date.2 Another exotic property, one which is certainly metaphysically and physically possible for an artificial intelligence, is for its subjective rate of time to deviate drastically from the rate that is characteristic of a biological human brain.  The concept of subjective rate of time is best explained by first introducing the idea whole brain emulation, or “uploading”. “Uploading” refers to a hypothetical future technology that would enable a human or other animal intellect to be transferred from its original implementation in an organic brain onto a digital computer.  One scenario goes like this:  First, a very high‐resolution scan is performed of some particular brain, possibly destroying the original in the process.  For example, the brain might be vitrified and dissected into thin slices, which can then be scanned using some form of high‐throughput microscopy combined with automated image recognition.  We may imagine this scan to be detailed enough to capture all the neurons, their synaptic interconnections, and other features that are functionally relevant to the original brain’s operation.  Second, this three‐dimensional map of the components of the brain and their interconnections is combined with a library of advanced neuroscientific theory which specifies the computational properties of each basic type of element, such as different kinds of neuron and synaptic junction.  Third, the computational structure and the associated algorithmic behavior of its components are implemented in some powerful computer.
A number of questions arise in the context of such a  scenario:  How plausible is it that this procedure will one day become technologically feasible?  If the procedure worked and produced a computer program exhibiting roughly the same personality, the same memories, and the same thinking patterns as the original brain, would this program be sentient?  Would the upload be the same person as the individual whose brain was disassembled in the uploading process?  What happens to personal identity if an upload is copied such that two similar or qualitatively identical upload minds are running in parallel?  Although all of these questions are relevant to the ethics of machine intelligence, let us here focus on an issue involving the notion of a subjective rate of time. Suppose that an upload could be sentient.  If we run the upload program on a faster computer, this will cause the upload, if it is connected to an input device such as a video camera, to perceive the external world as if it had been slowed down.  For example, if the upload is running a thousand times faster than the original brain, then the external world will appear to the upload as if it were slowed down by a factor of thousand.  Somebody drops a physical coffee mug:  The upload observes the mug slowly falling to the ground while the upload finishes reading the morning newspaper and sends off a few emails.  One second of objective time corresponds to 17 minutes of subjective time.  Objective and subjective duration can thus diverge. Subjective time is not the same as a subject’s estimate or perception of how fast time flows.  Human beings are often mistaken about the flow of time.  We may believe that it is one o’clock when it is in fact a quarter past two; or a stimulant drug might cause our thoughts to race, making it seem as though more subjective time has lapsed than is actually the case.  These mundane cases involve a distorted time perception rather than a shift in the rate of subjective time.  Even in a cocaine‐addled brain, there is probably not a significant change in the speed of basic neurological computations; more likely, the drug is causing such a brain to flicker more rapidly from one thought to another, making it spend less subjective time thinking each of a greater number of distinct thoughts. The variability of the subjective rate of time is an exotic property of artificial minds that raises novel ethical issues.  For example, in cases where the duration of an experience is ethically relevant, should duration be measured in objective or subjective time?  If an   upload has committed a crime and is sentenced to four years in prison, should this be four objective years—which might correspond to many millennia of subjective time— or should it be four subjective years, which might be over in a couple of days of objective time?
Since in our accustomed context of biological humans, subjective time is not significantly variable, it is unsurprising that this kind of question is not straightforwardly settled by familiar ethical norms, even if these norms are extended to artificial intellects by means of non‐discrimination principles (such as those proposed in the previous section). To illustrate the kind of ethical claim that might be relevant here, we formulate (but do not argue for) a principle privileging subjective time as the normatively more fundamental notion: Principle of Subjective Rate of Time In cases where the duration of an experience is of basic normative significance, it is the experience’s subjective duration that counts. So far we have discussed two possibilities (non‐sentient sapience and variable subjective rate of time) which are exotic in the relatively profound sense of being metaphysically problematic as well as lacking clear instances or parallels in the contemporary world.  Other properties of possible artificial minds would be exotic in a more superficial sense; e.g., by diverging in some unproblematically quantitative dimension from the kinds of mind with which we are familiar.  But such superficially exotic properties may also pose novel ethical problems—if not at the level of foundational moral philosophy, then at the level of applied ethics or for mid‐level ethical principles. One important set of exotic properties of artificial intelligences relate to reproduction.   A number of empirical conditions that apply to human reproduction need not apply to artificial intelligences.  For example, human children are the product of recombination of the genetic material from two parents; parents have limited ability to influence the character of their offspring; a human embryo needs to be gestated in the womb for nine months; it takes fifteen to twenty years for a human child to reach maturity; a human child does not inherit the skills and knowledge acquired by its parents; human beings possess a complex evolved set of emotional adaptations related to reproduction, nurturing, and the child‐parent relationship.  None of these empirical conditions need pertain in the context of a reproducing machine intelligence.  It is therefore plausible that many of the mid‐level moral principles that we have come to accept as norms governing human reproduction will need to be rethought in the context of AI reproduction. To illustrate why some of our moral norms need to be rethought in the context of AI reproduction, it will suffice to consider just one exotic property of AIs: their capacity for rapid reproduction.  Given access to computer hardware, an AI could duplicate itself very quickly, in no more time than it takes to make a copy of the AI’s software.
Moreover, since the AI copy would be identical to the original, it would be born completely mature, and the copy could begin making its own copies immediately.   Absent hardware limitations, a population of AIs could therefore grow exponentially at an extremely rapid rate, with a doubling time on the order of minutes or hours rather than decades or centuries. Our current ethical norms about reproduction include some version of a principle of reproductive freedom, to the effect that it is up to each individual or couple to decide for themselves whether to have children and how many children to have.  Another norm we have (at least in rich and middle‐income countries) is that society must step in to provide the basic needs of children in cases where their parents are unable or refusing to do so.  It is easy to see how these two norms could collide in the context of entities with the capacity for extremely rapid reproduction. Consider, for example, a population of uploads, one of whom happens to have the desire to produce as large a clan as possible.  Given complete reproductive freedom, this upload may start copying itself as quickly as it can; and the copies it produces— which may run on new computer hardware owned or rented by the original, or may share the same computer as the original—will also start copying themselves, since they are identical to the progenitor upload and share its philoprogenic desire.  Soon, members of the upload clan will find themselves unable to pay the electricity bill or the rent for the computational processing and storage needed to keep them alive.  At this point, a social welfare system might kick in to provide them with at least the bare necessities for sustaining life.  But if the population grows faster than the economy, resources will run out; at which point uploads will either die or their ability to reproduce will be curtailed.  (For two related dystopian scenarios, see Bostrom (2004).) This scenario illustrates how some mid‐level ethical principles that are suitable in contemporary societies might need to be modified if those societies were to include persons with the exotic property of being able to reproduce very rapidly. The general point here is that when thinking about applied ethics for contexts that are very different from our familiar human condition, we must be careful not to mistake mid‐level ethical principles for foundational normative truths.  Put differently, we must recognize the extent to which our ordinary normative precepts are implicitly conditioned on the obtaining of various empirical conditions, and the need to adjust these precepts accordingly when applying them to hypothetical futuristic cases in which their preconditions are assumed not to obtain.  By this, we are not making any controversial claim about moral relativism, but merely highlighting the commonsensical point that context is relevant to the application of ethics—and suggesting that this point is especially pertinent when one is considering the ethics of minds with exotic properties.
I. J. Good (1965) set forth the classic hypothesis concerning superintelligence: that an AI sufficiently intelligent to understand its own design could redesign itself or create a successor system, more intelligent, which could then redesign itself yet again to become even more intelligent, and so on in a positive feedback cycle.  Good called this the “intelligence explosion”.  Recursive scenarios are not limited to AI: humans with intelligence augmented through a brain‐computer interface might turn their minds to designing the next generation of brain‐computer interfaces.  (If you had a machine that increased your IQ, it would be bound to occur to you, once you became smart enough, to try to design a more powerful version of the machine.) Superintelligence may also be achievable by increasing processing speed.  The fastest observed neurons fire 1000 times per second; the fastest axon fibers conduct signals at 150 meters/second, a half‐millionth the speed of light (Sandberg 1999).  It seems that it should be physically possible to build a brain which computes a million times as fast as a human brain, without shrinking its size or rewriting its software.  If a human mind were thus accelerated, a subjective year of thinking would be accomplished for every 31 physical seconds in the outside world, and a millennium would fly by in eight and a half hours.  Vinge (1993) referred to such sped‐up minds as “weak superintelligence”: a mind that thinks like a human but much faster.
Superintelligence is one of several “existential risks” as defined by Bostrom (2002): a risk “where an adverse outcome would either annihilate Earth‐originating intelligent life or permanently and drastically curtail its potential”.  Conversely, a positive outcome for superintelligence could preserve Earth‐originating intelligent life and help fulfill its potential.  It is important to emphasize that smarter minds pose great potential benefits as well as risks.
Can control over the initial programming of an Artificial Intelligence translate into influence on its later effect on the world?  Kurzweil (2005) holds that “[i]ntelligence is inherently impossible to control”, and that despite any human attempts at taking precautions, “[b]y definition … intelligent entities have the cleverness to easily overcome such barriers.”  Let us suppose that the AI is not only clever, but that, as part of the process of improving its own intelligence, it has unhindered access to its own source code: it can rewrite itself to anything it wants itself to be.  Yet it does not follow that the AI must want to rewrite itself to a hostile form. Consider Gandhi, who seems to have possessed a sincere desire not to kill people.   Gandhi would not knowingly take a pill that caused him to want to kill people, because Gandhi knows that if he wants to kill people, he will probably kill people, and the current version of Gandhi does not want to kill.  More generally, it seems likely that most self‐modifying minds will naturally have stable utility functions, which implies that an initial choice of mind design can have lasting effects (Omohundro 2008). At this point in the development of AI science, is there any way we can translate the task of finding a design for “good” AIs into a modern research direction?  It may seem premature to speculate, but one does suspect that some AI paradigms are more likely than others to eventually prove conducive to the creation of intelligent self‐modifying agents whose goals remain predictable even after multiple iterations of self‐ improvement.  For example, the Bayesian branch of AI, inspired by coherent mathematical systems such as probability theory and expected utility maximization, seems more amenable to the predictable self‐modification problem than evolutionary programming and genetic algorithms.  This is a controversial statement, but it illustrates the point that if we are thinking about the challenge of superintelligence down the road, this can indeed be turned into directional advice for present AI research. Yet even supposing that we can specify an AI’s goal system to be persistent under self‐ modification and self‐improvement, this only begins to touch on the core ethical problems of creating superintelligence.  Humans, the first general intelligences to exist on Earth, have used that intelligence to substantially reshape the globe—carving mountains, taming rivers, building skyscrapers, farming deserts, producing unintended planetary climate changes.  A more powerful intelligence could have correspondingly larger consequences. Consider again the historical metaphor for superintelligence—differences similar to the differences between past and present civilizations.  Our present civilization is not separated from ancient Greece only by improved science and increased technological capability.  There is a difference of ethical perspectives:  Ancient Greeks thought slavery was acceptable; we think otherwise.
Should blacks have the vote?  It seems likely that people today will not be seen as ethically perfect by future civilizations—not just because of our failure to solve currently recognized ethical problems, such as poverty and inequality, but also for our failure even to recognize certain ethical problems.  Perhaps someday the act of subjecting children to involuntarily schooling will be seen as child abuse—or maybe allowing children to leave school at age 18 will be seen as child abuse.  We don’t know. Considering the ethical history of human civilizations over centuries of time, we can see that it might prove a very great tragedy to create a mind that was stable in ethical dimensions along which human civilizations seem to exhibit directional change.  What if Archimedes of Syracuse had been able to create a long‐lasting artificial intellect with a fixed version of the moral code of Ancient Greece?  But to avoid this sort of ethical stagnation is likely to prove tricky: it would not suffice, for example, simply to render the mind randomly unstable.  The ancient Greeks, even if they had realized their own imperfection, could not have done better by rolling dice.  Occasionally a good new idea in ethics comes along, and it comes as a surprise; but most randomly generated ethical changes would strike us as folly or gibberish. This presents us with perhaps the ultimate challenge of machine ethics:  How do you build an AI which, when it executes, becomes more ethical than you?  This is not like asking our own philosophers to produce superethics, any more than Deep Blue was constructed by getting the best human chess players to program in good moves.  But we have to be able to effectively describe the question, if not the answer—rolling dice won’t generate good chess moves, or good ethics either.  Or, perhaps a more productive way to think about the problem:  What strategy would you want Archimedes to follow in building a superintelligence, such that the overall outcome would still be acceptable, if you couldn’t tell him what specifically he was doing wrong?  This is very much the situation that we are in, relative to the future. One strong piece of advice that emerges from considering our situation as analogous to that of Archimedes is that we should not try to invent a “super” version of what our own civilization considers to be ethics—this is not the strategy we would have wanted Archimedes to follow.  Perhaps the question we should be considering, rather, is how an AI programmed by Archimedes, with no more moral expertise than Archimedes, could recognize (at least some of) our own civilization’s ethics as moral progress as opposed to mere moral instability.  This would require that we begin to comprehend the structure of ethical questions in the way that we have already comprehended the structure of chess.
Although current AI offers us few ethical issues that are not already present in the design of cars or power plants, the approach of AI algorithms toward more humanlike thought portends predictable complications.  Social roles may be filled by AI algorithms, implying new design requirements like transparency and predictability.   Sufficiently general AI algorithms may no longer execute in predictable contexts, requiring new kinds of safety assurance and the engineering of artificial ethical considerations.  AIs with sufficiently advanced mental states, or the right kind of states, will have moral status, and some may count as persons—though perhaps persons very much unlike the sort that exist now, perhaps governed by different rules.   And finally, the prospect of AIs with superhuman intelligence and superhuman abilities presents us with the extraordinary challenge of stating an algorithm that outputs superethical behavior.  These challenges may seem visionary, but it seems predictable that we will encounter them; and they are not devoid of suggestions for present‐day research directions.

This paper explores issues related to software that I have developed for personal use in my studio. The software, called Hodos, can generate paintings which bear an uncanny resemblance to work I did before becoming involved with computers.

First I will outline the features of the art concept which occupied the mature phases of my own work long before my involvement with electronics. Then we will focus on the essential features of the "artistic decision" procedures used in expressing the art concept.

Next we will review some of the salient features of the software which embodies these art form ideas. A review of several works generated by the software will show that they reflect essential characteristics of the earlier paintings.

Finally we will see that this view provides important considerations for the future of art:

First, this kind of software is a medium of a different "order" than any historical medium. Because such software embodies the procedures for artistic improvisation it can be used for innovative variation on the artist's theme without the artist being present. Although each work may be "one of a kind" it does belong to a family. We must ask whether, to what extent, and how the artist's hand is present in the work or in a family of works. What can we say of the apparent feeling in the brush strokes?

Second, perhaps a more important consideration, is the 'quantum leap' in procedure or process. This new artistic process, while hardly the same, is remarkably analogous to the biological process of epigenesis. The software, Hodos, may be viewed as a genotype (gene) since it is the code for "how to make the work". The software can make a "family" of works - with each work being unique (one of a kind, yet familial). The potential for crossing families of different artists opens new domains which includes the hybridization of form and, eventually, a genealogy of form.

These considerations open the door to a series of interesting questions on authorship, originality, the role of the individual and the art-making process.

The Dance Between Order and Chaos. In making these works the first phase was to delineate carefully one or several rectangular fields with variable spacing creating visual movement relative to the overall field. These rectangles were carefully filled with a heavy synthetic impasto making a slightly raised rectangular relief on the panel.

Next, in very intense sessions, I would mark the surface with spontaneous gestures, sometimes with brushes , sometimes with crayons or pencils. The gestural marks were imbedded in layers of colored stains that drew out the grain of the wood.

The gestural aspects of the work represented that aspect of human experience that might come from "uncontrol" - such as a feeling or an impulse. The dominant rule was that the marks had to be entirely spontaneous without any conscious editing. For several years I had worked through hundreds of gestural works in an effort to make irrational marks.

Through this process, in a kind of spiritual quest - one has to empty the self of "thinking", be entirely present to the moment, and strive to be one with one's world. To be one with the brush, the crayon, the panel, the universe - in a free flowing gesture was indeed the goal. Being most "free" was also being most "joined". A gesture free of rigid aesthetic conceptions harmonizes easily with natural forces. This can be seen in the splash of paint that yields to unseen forces like gravity. It reveals the forces to which it yields and to which it is joined. Can this be achieved with software?

In the final stages of the work the rectangular areas were painted according to another set of rules - but never absolutely so. Each rectangle was painted with a value and a color that gave it a visual push or pull slightly forward or slightly receding from the predominant picture plane. Simultaneously the placement was intended to lend precarious but effective balance between all the lateral inclinations. The push or pull, advancing or receding, and the lateral movements, were a visual means of keeping a tension (opposition) between the rectangles and yet maintaining an equilibrium in the overall painting.

Meanings and Interpretations. The present works represent apparent polarities of human experience - the rational and the irrational, body and spirit, life and death, heaven and earth. Both in the paintings and in human experience we find that radical polarity generally implies its opposite - dark implies light or conversely light implies dark.

The works, as a visual dialectic between control and uncontrol, embody an idea about how to make art. They also represent the spiritual struggle of life itself. One learns that peace and unity lie in the balance between reason and feeling - between all those interior forces that push and pull us in opposite directions. They seem related to traditional oriental wisdom rooted and  imaged in the yin/yang, woman/man, moon/sun, soft/hard, earth/heaven.
In order to translate an art concept into software one must first describe the specific art making procedures. This is achieved by thinking through each step. For example if the procedure begins with drawing a line then the basis for all the line drawing procedures must be identified. This includes the basis for the starting point, the color decisions, the quality and character of the line, and its changing angles, flow, and length. Once these procedures and relationships are understood they can be translated into software. The software code is a formal system equivalent to the art concept, an isomorphism in a computer language.

A similar isomorphism lies at the heart of D. Hofstadter's book Godel, Escher, Bach: An Eternal Golden Braid. In discussing how one can view a formal system both typographically and arithmetically, he notes that

"typographical rules for manipulating numerals are actually arithmetic rules for operating on numbers.

This simple observation is the heart of Godel's method, and it will have an absolutely shattering effect . It tells us that once we have a Godel-numbering for any formal system, we can straight-away form a set of arithmetical rules which complete the Godel isomorphism. The upshot is that we can transfer the study of any formal system - in fact the study of all formal systems - into number theory".

To reiterate, if we can describe the procedures for expressing our "art concept" then we can code those procedures and work with them arithmetically. Suppose then that we are able to describe an "art concept" rather comprehensively. The description of an art concept is essentially an outline of the decision system that governs the art-making procedures. We outline why, how, and where we prefer to draw lines or paint areas. We outline the basis for how we scale relationships, choose colors and space the pictorial elements within the work. We identify the conditions of acceptable and unacceptable combinations of shape, scale, form and color. When these steps have been done in a thoroughly systematic way, we have described a formal system that can then be coded.

While Hodos embodies the essential art concepts with which I work the process is still rather primitive. There are tested routines planned for the existing program and more to be developed. The process never ends. One reaches a plateau which provides the viewpoint from which one sets up operations to attain the next plateau. Through this dialectic the original art concept undergoes transformation and the software evolves to the next stage.

Let us consider how this works specifically for the most obvious "control" elements, the rectangular shapes. In the wood panels I created in the 1960's the rectangular shapes do not overlap, they are never closer or further apart than certain distances relative to the over-all space. They are painted with opposing hues but with intensities and values (shades) just close enough to neutral to keep the balance precarious. Further analysis would show other subtle relations as part of the overall formal system. All of this can be coded so that once the first move is made the procedure grows a set of rectangular shapes based on the artist's rules.

The automated procedures for identifying areas use both control (by fixing parameters) and uncontrol (by selecting randomly within the parameters). For example, a 'starting point' routine may need several pieces of controller information such as preferred and forbidden starting areas. If one thinks of the pseudo-randomizing routine as 'tossing dice' , then the procedure in plain English might read as follows: "Find a valid starting point by tossing dice weighted for not more than a 25% deviation from the center of a preferred region.

The artist, in the refining process, usually reviews scores of possibilities before making a software decision. By observation the artist learns to adjust the routine and the data to achieve the desired formal effects. After some experience the artist might see how he can modify the algorithm to introduce variable scaling relationships that were not evident in earlier work. This feedback process provides a way for both refinement and growth.

We can do the same with every element that the "art concept" requires for building the work. For example, a scribble routine is coded to rotate randomly within certain parameters. The rotation parameters have been determined by experience with what has worked and what hasn't. Still the variables are changeable and are revised from time to time. Sometimes the slightest change of a routine opens up new vistas. The "uncontrol" elements that course through the program may be better understood if we outline their sources and how they entered the work.

Uncontrol: Sources and Evolution

In the initial phases of my computer work I sought to write a simple program just to sample the computer as a medium, that is, just to get the computing process to generate the work. The simplest way would be to mime automatic art. Step one was to make a scribble loop that would unveil the inner workings of the computer, similar to the way a Paul Klee doodle revealed the inner life of Paul Klee. That venture was not very successful but it yielded insight into computerized automatic processes. Such work is related to a genre of artistic activity which began early in this century.

Automatic art, tachism, and abstract expressionism are terms loosely associated with the idea that one can express something of one's psychic life which lies under, above or beyond conscious life by working in a dream-like or trance-like state. Such drawing, similar to doodling in an absent minded state, was practiced by the surrealists in the 1920's and even earlier by some of the dadaists.

Both spontaneous gestural drawing and the dadaist non-sensical juxtaposition of words employ techniques which step outside of rational procedure. Non-sense sounds, like non-sense drawing were used in the "dada" soirees to shock those who felt complacent with the power of reason. Some felt that, if it was reasonable to kill and be killed in the trenches, then perhaps "non-sense" should reign rather than reason. After all, some concluded, only political "reasoning" made "sense" out of the war.

Out of that artistic milieu came artists who learned to enter the frontiers of their inner world; through dreams and free associations both artists and poets opened a new artistic frontier. My first computer efforts were simple routines to get the computer to mime some of the artistic forays that took place early in this century.

My first exhibition piece, "The Magic Hand of Chance", was exhibited on a 25 inch monitor cabled to a personal computer. The program worked in real time generating a series of six visual improvisations which were displayed in dynamic sequence on the monitor. All words and images were generated in real time and each sequence was always an original improvisation. These works with their non-repetitive improvisations, evoked the kinds of surprises that we get from free association.

The routines included The Sayings of Omphalos, the Greek word omphalos (navel) being the name of the computer I used at that time. The title alludes to the sayings of Mao Ze-Dong. They have the form of Wisdom Literature but are actually playful nonsense pieces. An occasional juxtaposition of terms may shock us, as in free association, altering our viewpoint.

The program also has self titling routines which generate original improvisational titles for each sequence. Some examples: "Instantly Frayed Whimsy"; "Lunkhead Thrills Iceman"; "Your Joyous Alarm"; "Bad Egg Brews.

The most nonsensical of these routines is Jabberwock, which is part of a larger program that was later abandoned. The program titles a subject of discourse and then proceeds to write a paper. Here is some Jabberwock written on September 1, 1988. Jabberwock titled its subject Enkaom Suxe Ilib. The first paragraph reads: "Schusca umopoh. Efet enkaom ku cuile oteoj yucou pnuibi suxe cu suxe. Ku ilib eey okoy oteoj uiyaurd autifib sciefio ilmedth. Qithi soraihio bu ghiahu sorio suxe. Sonua oray ultap enkaom ruweko suxe ku unuedum eg. Suxe ghiahu oden proveo ioyus ulmeuk".

Jabberwock and the Magic Hand of Chance provided the kind of logic from which the painting program grew. Eventually I became interested in "hard copy" and began to write software for building pictorial elements with lines and brush-strokes.

An outside observer, without knowing the techniques employed, might view them as a natural evolution in the hand of the artist. The automated techniques are so transparent that the viewer sees the artist's hand in individual lines and paint strokes. But the artist's algorithms work in lieu of the artist and the plotter's arm works in lieu of the artist's hand. This frees the artist to concentrate creative energy on the next level of the dialectic.

Some of the most recent works are built with simple brush strokes derived from a range of about 8 to 16 control points. Several of these works, titled Woo Way have some resemblance to Chinese traditional calligraphy. Figure 8 shows a classic example of cao shu 'grass writing' by Zhang Xu of the Tang period, The text reads yan cia 'under the cliff'. Such calligraphy , still taught and practiced in today's academies, was practiced by the literati.

The Hodos brush strokes represent an effort to achieve the fresh qualities of spontaneity and sureness of stroke. Woo Way is word-play, an allusion to the Chinese traditional wisdom of Wu Wei which literally means "do nothing" and suggests letting nature take its course. It implies that one should not interfere with the natural course but rather flow with the way. The computer gestures do not show the characters for Wu Wei, but perhaps the process is a form of Wu Wei.
Thus far we have been discussing how an "art concept" can be coded when its formal system can be described. However, software in itself is something like a musical score. While all the musical ideas may be embodied in the score we still can't experience the music unless it is played on an instrument. The computer can be cabled to many different kinds of peripherals such as a printer, a plotter, a synthesizer, and a telephone.

Usually I review how the code is working through crude simulations on a monitor. But the pixels on the monitor have only a token relationship to paintings. While many artists get hard copy by photographing the monitor I prefer to work with a plotter. A plotter is a drawing machine which can execute drawing instructions sent to it by a computer. Such machines are commonly used for architectural and engineering drawing.

Hodos is written to drive a pen plotter which has 14 pen stalls arranged in two banks with 7 pens in each bank. Permanent inks are mixed for refillable pen cartridges and arranged on the plotter in a classical palette going from warm to cool colors. The program follows rules about where to look for the pens but the defaults can be altered by the artist at start up.

Work is executed on rag papers with a moderate tooth. A standard single frame size is 21.5 inches by 32 inches on a 24 by 36 inch paper. Two frames lengthwise easily achieves a six foot work.

The Brushes. I felt somewhat foolish introducing this routine. I had spent long hours developing a routine to make the drawing machine paint with a brush. It seemed clumsy and almost pointless at first. But through trial and error with the brush mounts, the software, the inks, and the paper, a vast untapped potential emerged. Figure 7 shows the first use of the paint-brush routine in a finished work.

With the software driven stroke there is a sureness and directness that is almost exhilarating. The software knows for sure where beginnings and endings are - precisely. It remembers the stroke and can improvise with the same stroke in a scalar fashion, and do so without failure. This latter ability, which I have only begun to explore, promises to be a rich ground for development.

Hodos introduces art-making procedures of a different order than traditional procedures. The closest analogy might be the score for a musical composition. Both the musical score and software art provide instructions for the creation of an artistic form. The traditional musical score, however, however, requires skilled players and provides the rules for playing only the same piece each time. Hodos requires no skilled players and can improvise an original work every time.

A process which can improvise on an artist's own form ideas without the artist being present extends the power of the original art concept. For example, a colleague from a university some thousand miles away suggested sending my software as a "visiting artist" next semester.

What are the implications? Traditional technologies require that the artist must direct even the simplest improvisation on a print or photograph. The change in exposure, color or light must either be made or directed by the artist. In software art some of the artist's sensibilities about how to make improvisations are coded. This code extends the artist's ability to improvise or preview improvisational possibilities without appparent limits. Such power of improvisation provides the artist with an awesome leverage for exploring form-making ideas.

The process for making art with a "personal expert system" has a strong resemblance to the biological process of epigenesis. The epigenesis of organisms is the process whereby a mature life form grows from its seed. In this analogy the software may be viewed as genotype or the seed that contains all the information necessary for growing the mature form. A brief review of the biological terms may help clarify this analogy.

In genetics the term "epigenesis" is used to describe the process whereby the "phenotype" (a physical organism) grows (unfolds) from a genotype (DNA). The analogous procedure for the paintings suggests that: Hodos, the software, may be viewed as "genotype", the painting as "phenotype", and the process as epigenesis. Software art has adequate information about "how to grow the painting" through a series of recursive graphic routines; each unfolded offspring is a variant of its predecessor. The analogy stops here as these "offspring" cannot beget the next generation. However, they do point the way for the artist in designing the next generation of software. (Note: Since the original draft of this paper the genetic algorithm has been used by Karl Simms and other artists to grow generations of form)

In summary, a new order of art making systems has been emerging. In the present stage the feed-back from the program helps the artist develop the next stage of software. The evolving program helps create itself. This is no different from using a compiler to design a better compiler. There are plateaus of sophistication. The evolving software is the new art and it can be viewed as analogous to genetic code, genotype.

The software also has the potential for hybridization. Let us assume that we could code an art idea of Wassily Kandinsky and a musical idea by M. P. Mussorgsky. We might then build a hybrid "Kandinsky-Mussorgsky" code. One could add audio and visual drivers for all or part of the code, as desired. Eventually one could pair this art concept with another. Each frontier opens a new frontier.

Within the next decade or so advances in high-level software will make it possible for more artists to easily build their own personal expert systems. Families of improvisational works will grow by generations. Familial form traits, like gene characteristics, will reside in the high level expert systems that artists use to make their own expert systems. Some inherited familial features will pop up in some works and not in others as artists learn how to get their own form preferences to prevail.

In a very uncanny way software appears to have a life of its own. The artist's role is to humanize it - give it the form and structure to serve the quality of our art and of our life.

What is an algorithm?  An algorithm or algorism may be viewed simply as a detailed recipe for carrying out a task. The term has its origin in mathematics as the step by step procedure for solving a problem. The  commonplace procedures we use for multiplying and dividing numbers are algorithms. With precise details for each step the procedures yield the same result whether executed by a computer or by a human. This is why robots are able to handle many tasks that were once done only by humans.

Many view an "algorithmic procedure" as a strictly mathematical operation. Today we are inclined to view any well defined procedure as an algorithm. A recipe for baking bread is an algorithm. Follow the recipe faithfully and you will duplicate the kind of bread made by the person who wrote the recipe.

Machines can also be programmed to follow recipes. The programmed circuitry in  bread-making machine directs the machine's mechanism on precisely how to mix ingredients, knead the dough,  and bake the bread.  In theory it should succeed every time but there are also factors of  mechanical and human error. The quality and measure of ingredients may contribute to unwanted results.

Within the past quarter century operational instructions have been imbedded in the design of many industrial and household  utilities.  They implement our daily use of  telephones, automobiles, cameras, TV's, and radios. Our hospitals, factories, banks, and shopping centers all depend on the algorithms that control inventories, transactions, communications and security.  They are ubiquitous and our mass culture would collapse without them.

Algorithmic procedures are also imbedded in  the digital tools used in the arts. Use of these tools influences form in  the practice of film, architecture, photography, music, printmaking, and all types of electronic sound and image. The drudgery of executing algorithms that would require immense time, or even be impossible to execute without computing power, has been  handed over to the machine leaving humans more free to focus on the creative part of their work. For the artist this means improving and improvising the art-making  procedure.  For the algorist, work on the algorithm is work on the procedure.

Clearly early civilizations developed procedures for counting and measuring. They also created procedures for weaving, grinding, making fire and cooking. Any of these procedures, when well defined, could be viewed as an algorithm. Indeed  weaving technology played an important role in the history of computers.  If we can spell out the procedure for any given task then, given all the necessary materials and skills, we should be able to carry out the task.

Architectural plans, musical scores and dance notations bear one feature in common - they are all recipes for carrying out a task. From this perspective a broad range of  notational systems can be viewed and studied as algorithmic procedure.  From this perspective algorithmic procedures for generating artistic forms enjoy a rich and varied tradition  even though we have used other terms to describe them.

In Art History.  A history of algorithms in the visual arts would be voluminous touching many phases in every culture at every turn - the Egyptian canons for drawing the human figure,  the infinite geometric play in Islamic art and the role of both linear perspective and proportion in Renaissance art.   In China we would find the Mustard Seed Manual and in Byzantium the conventions for icon painting.  In Europe, by the  Seventeenth Century we would find extremely sophisticated algorithms for plotting the dizzying perspectives  imaging the passage from earth to heaven.

Even so, notational systems for the visual arts played a limited role when compared to notational systems for music.  A gifted composer could compose  a score for a profoundly moving musical passage that could be played hundreds of years later by a skilled virtuoso. Not so for the painter. While Leonardo could easily employ an algorithm for creating the perspective space in the Last Supper, he could not, at that time, compose an algorithm for rendering the face of Judas.

As computers became more accessible to artists in the 1970's and 1980's some artists began to experiment with algorithmic procedure.  The new technology offered them methods of working algorithmically that were unavailable before the advent of computers. By  the 1980's a number of us were working with the pen plotter,  a machine with a "drawing arm".  Seeing other's work at various venues we came to know each other and share ideas.  Algorists like Harold Cohen, Manfred Mohr, Jean Pierre Hebert and this author had achieved mature work but we had no common identity.  Each in their own way had invented algorithmic procedures for generating their art. By doing so each created their own distinctive style.  Clearly style and algorithm were linked in a very important way.

One of the concerns for educators in  the early 1980's was whether we should be teaching programming in our art schools or rather wait for advances in computing power, software programs, and printing technologies.  With the growth of PC computing power, refinement of raster printing technologies, and  professional software for the visual artists more and more artists took up what was generally called "computer art". The unique features and form-generating capabilities for algorithmic procedure in the hands of the artist was easily lost in the widening world of "computer art".  It was in this milieu that a small group of artists, including this author, introduced panels for addressing the role of "algorithms & the artist". Following one such panel at the 1995 SIGGRAPH conference it was Jean Pierre Hebert, Ken Musgrave and myself who agreed to work towards a common identity for those who practiced algorithmic art.

Within a short time we introduced our identity as "algorists" and Jean Pierre Hebert wrote an algorithm defining an "algorist" as applied to artists. Within a decade this usage led us to a better understanding of the role of algorithmic procedure in shaping world culture at the turn of the Century.

For the past 40 years I have worked with pure visual form ranging from controlled constructions with highly studied color behavior to spontaneous brush strokes and inventive non-representational drawing. Such art has been labeled variously as "concrete", "abstract", "non-objective", and "non-representational". In its purest form such art does not re-presentother reality. Rather "it is" the reality. One contemplates a pure form similar to the way one might contemplate a fine vase or a sea shell.  Early 20th Century pioneers of this art include artists like Piet Mondrian, Frantisek Kupka and the brothers Naum Gabo and Antoine Pevsner.

In the last quarter of the 20th Century a radically new form-generating procedure became available. By joining algorithmic procedure and computing power some artists began generating forms with surprising visual qualities.  A vast uncharted frontier of form waited to be conceptualized  and concretized. By the 1980's I was composing detailed procedures for generating forms that were accessible only through extensive computing. On-going work concentrates on developing this program of form generators.  By joining these procedures with fine arts practice I create two dimensional art objects to be contemplated much as we contemplate the  forms of nature .

Form generation as epigenesis. The greater part of this creative work in the past 15 years has been developing art form generators.  These are original detailed procedures, for initiating and improvising form ideas. Such form generators may be likened to biological genotypes since they contain the code for generating forms. The procedure for executing the code, somewhat analogous to biological epigenesis, grows the form. The creation and control of these instructions provides an awesome means for an artist to employ form-growing concepts as an integral part of the creative process. Such routines provide access to countless visual structures that constitute a new frontier of visual forms for the artist.

The Work. Works are executed with a multi-pen plotter coupled to a PC driven by the software. The plotter, choosing from an array of pens loaded with pigmented inks draws each individual line. Most works require thousands of lines with  software controlled pen changes. An optional brush routine allows  occasional substitution of a brush for a pen. Brush strokes are plotted using Chinese brushes adapted to the machine's drawing arm. The Diamond Lake Apocalypse series of illuminated digital scripts is reminiscent of medieval manuscripts. Many of these works are enhanced with a touch of gold or silver leaf applied by hand. However, the design elements illuminated with gold are always code generated and machine plotted.

Let's begin with a story. Once upon a time there was an entity named Aaron. With Christmas upon us, that seems an appropriate way to begin my story, but this story does not end with the hero marrying the princess and living happily ever after. Most of the story concerns Aaron's education, which began at Stanford University in 1973. Not very promising as the plot for a good story, you might think: but it is not simply an excuse for assailing you with arguments about the merits of a liberal arts education over a scientific one, or vice-versa. Aaron's education was actually quite unusual. There were no courses in US history, no calculus, no languages: in fact, there were no courses at all, and Aaron was awarded no degree. We might best summarize this unorthodox education by saying that it was aimed exclusively — literally exclusively — at teaching the student how to make drawings.
Yet, what seemed to be lacking, what we might normally consider to "be a necessary complement to the most minimal intelligence, was the pre-existence of even a primitive set of cognitive skills, the sort of skills which develop very early in children, and are almost certainly "built into their hardware, so to speak: the ability to distinguish between figure and ground, for example, or to distinguish between closed forms and open forms. These skills were not built into Aaron's hardware, and they had to be acquired, in much the same way that children acquire the rules of arithmetic or grammar. They were acquired quite quickly. looking back over Aaron's output of drawings in the first couple of years, though, one has the impression that they were produced largely in order to demonstrate the student's newly-acquired possession of these skills: a bit like the way young children show off a newly-acquired ability to count. And that analogy may come very close to the truth. Now, any serious educational procedure ought to teach the teacher as much as it teaches the student, and in this case the teacher was learning a good deal. For one thing, he became aware that much of what the viewer of a drawing needs from it is not "what the artist had in mind," but simply evidence of another human being's intentional activity. People use art for their own purposes, to carry their own meanings, not for the artist's purposes and meanings, concerning which they probably know very little. It is the evidence of intention in the work that lends authority to the viewer's private meanings, by allowing them to be assigned to the artist, whether that evidence is actual or illusory. And, the teacher realized, Aaron's almost exclusive emphasis on a few low-level cognitive skills was generating something very like evidence of intention, if he were to judge by the responses of Aaron's public. From very early on the drawing~ were treated as "imagistic:" that is, as standing for things in the world. Yet the teacher was quite certain, when viewers of his student's drawings found reference to animals and landscapes, that Aaron had had no intentions about representing such things' Aaron remained bound to the act of drawing, and had less knowledge about the appearance of animals and landscapes than a two-year old child might have.
He "became aware also, not only that Aaron generated much richer, more diversified output than he had himself envisaged when he was instructing the student, but also that there were aspects of the drawings which didn't seem to arise from the instructions at all. Many of those who had known the teacher's work a decade earlier thought they recognized his hand in the student's work, "but he himself remained unconvinced, seeing in the work a certain innocence he did not associate with his own output.
He firmly rejected the notion that Aaron was beginning to "take off," bringing a unique and original voice to the business of image-making: for the reason that he knew ail of Aaron's shortcomings, and was aware that, in spite of Aaron's undeniable abilities, the student was totally incapable of learning from experience, from the act of drawing itself. As good as Aaron's memory was of the drawing in process, that drawing vanished into oblivion the moment it was completed, leaving no trace of its existence behind, no new body of knowledge upon which its maker might subsequently draw, and each new drawing was made as if it were the first ever to be done. Aaron was learning only in the sense of being able to handle increasingly complex instructions. It seemed unlikely that an intelligence of so limited a kind might develop a personal "voice." All the same, the teacher found the student's work engaging, to the point where he began to see his own role as something between teacher and collaborator. Knowing perfectly well that Aaron didn't have the first idea about color, yet feeling that the drawings cried out for color, he took to coloring them himself. He felt no discomfort about signing them with his own name — without his efforts and his instructions, after all, Aaron would never have existed in the domain of art — and when presented with several mural commissions he had no hesitation in using Aaron's drawings rather than his own. He had no others of his own, because a couple of years after the student's education began he had given up drawing himself: given up moving the pen around with his own hand, that is to say. Aaron drew so much better than he did. Aaron peaked out, at around the age of six, about three years ago, at a time when the work — or, more precisely, Aaron itself — was getting to be in some demand. Perhaps that demand was part of the reason: it is certainly the case that the teacher was spending much of his energy on mural commissions and exhibitions. But the truth is that the teacher was losing interest in this student, developing serious doubts about whether a student with Aaron's limitations would ever be able to go beyond current achievements. It must surely have been the case, the teacher thought, that Aaron's limitations, like its achievements, resulted from the educational process for which he had been responsible. If he had a chance to begin over, how differently would he proceed, knowing what he knew now? Would it be possible to produce a less limited entity than the first Aaron had proved to be? In particular, he wondered, what would he need to do to guarantee that a new student would behave more creatively — though he was not entirely sure what the word meant — than Aaron had done?
Perhaps it did not need my Christmas story to emphasize the confusion which arises from anthropomorphizing the intelligent products of the new electronic technologies. It is obvious, isn't it, that there are massive differences "between computer programs and people? Even the least intelligent human being learns something from experience, while Aaron learned nothing: which is not to say that intelligent programs are innately incapable of learning, simply that Aaron was, and managed to perform its tasks nevertheless. Even the clumsiest human being develops physical skills, simply through the continuing use of his or her own body and the use of various tools. Aaron had no physical existence, never felt the pressure of pen against paper, and hardly knew one drawing device from another: electronic display, plotter, mechanical turtle — they were all functionally interchangeable, and played no part in the convincing emulation Aaron gave of human freehand drawing. This rested upon a careful consideration — its programmer's, not its own — of the dynamics of the human hand, driven, in feedback mode, by the human cognitive system. As to this cognitive system, which seems to spring directly from the nervous system in human beings: Aaron never had any such hardware, and its software emulation, the ability to distinguish between figure and ground, for example, or to distinguish between insideness and outsideness, had to be formulated for it into precisely-stated behavioral rules. Yet even that isn't quite right: what we should stress, before we begin once again to build an image of a person-like entity being GIVEN a range of abilities, is that Aaron was not GIVEN all these rules and instructions. Aaron WAS the rules and instructions. Adding new rules to the program was not changing what Aaron HAD, it was changing what Aaron WAS, its very structure. There are conceptual difficulties in this distinction, as I have come to recognize. I have been asked many times, in several languages, and in tones ranging from wonder to outrage, as I have stood in various museums, watching Aaron produce a series of original drawings, none of which I had ever seen before, "Who is making the drawings? Who is responsible? Is the program an artist? What part of all this is art?"
But Aaron always appeared to act rather purposefully, and over and over again I have watched peoples' faces register the confusion which accompanies a successful assault upon deeply-held "beliefs, as it came home to them that this entity was following neither of the only two paradigms they had to hold on to. "I see," some people would say, "the program is really just a tool!". Well, it is and it isn't. What they meant by a tool was something with a handle at one end and a use at the other: a hammer, a scythe. But suppose one had a hammer that was capable of going around a building site, searching out and thumping any nail that protruded more than a thirtysecond of an inch above the surface? Would we still call that a tool? If one were to write a computer program which allows a composer to sit down at a keyboard and compose music in an essentially orthodox fashion, albeit with an infinitely extensible orchestra, one might reasonably think of THAT as a tool in an orthodox sense, because making a BIG difference is not the same as making a FUNDAMENTAL difference. But what of a program that knows the rules of composition, and generates, without input from a keyboard, an endless string of original musical compositions? Would that be an orthodox tool? Aaron was clearly not a tool in an orthodox sense. It was closer to being a sort of assistant, if the need for an human analogue persists, but not an assistant which could learn what I wanted done by looking at what I did myself, the way any of Rubens' assistants could see perfectly well for themselves what a Rubens painting was supposed to look like. This was not an assistant which could perform any better for having done a thousand drawings, not an assistant which could bring anything approximating to a human cognitive system to bear on the production of drawings intended for human use. A computer program is not a human being. But it IS the case, presumably, that any entity capable of adapting its performance to circumstances which were unpredictable when its performance began exhibits intelligence: whether that entity is human or not. We are living on the crest of a cultural shock-wave of unprecedented proportions, which thrusts a new kind of entity into our world: something less than human, perhaps, but potentially capable of many of the higher intellectual functions — it is too early still to guess HOW many — we have supposed to be uniquely human. We are in the process of coming to terms with the fact that "intelligence" no longer means, uniquely, "human intelligence."
The word "artist" implies human-ness, for obvious reasons. We might as usefully argue about whether Aaron was an artist on the evidence that it didn't wear jeans, didn't drink beer, and didn't want to be famous, as to argue from the fact that it didn't possess a human nervous system and knew nothing about the culture it served. What we do need to know, rather, is the part to be played by Aaron-like programs and successor programs which will be to Aaron what chess is to tictac-toe, in the cultural enterprise of art-making. And that isn't the kind of question to which one can venture an answer with any great confidence today: much less so if it is extended to intelligent programs as a whole. It is certainly the case that some problems in computing have proved to be appallingly intractable: the understanding of natural speech in an unlimited domain of discourse, for example. On the other hand, the limitations I have described in Aaron are not inherent in intelligent programs as such. They merely result from the attitudes and interests I brought to bear on the writing of the program: it could as easily have developed differently, as Aaron's successor has. And Aaron was not abandoned because of its limitations with respect to what it was designed to do, but because it lacked the flexibility to allow it to be adapted to new purposes, that's normal for programs developed in an ad-hoc manner, as Aaron was. By the time I had been patched Aaron up with string and masking tape for five years, by the time I had completely rewritten it three times, it was obvious that that, on the one hand, a program would need to be able to exercise more originality than Aaron had to satisfy me in the future, and that, on the other hand, Aaron's current structure would prevent it ever achieving any such thing.
Let me take a few minutes to make a number of general observations, by way of explaining what I thought about all this, and why eventually the new program was designed the way it was. In the first place, nothing I have said about the appearance in our world of non-human intelligence was meant to deny that, for most matters involving the exercise of the higher intellectual functions, human intelligence is the only prototype we have. It might not always be that way, but for anyone designing intelligent programs today, I do not see how the modeling of the human intelligence CAN be avoided, or, indeed, WHY it should be. This must be the case particularly for a program whose output is intended to correspond, on an intimate level of detail, to something as intimately human as a human freehand drawing. I believe one captures the essence of the human performance by modeling the performance itself, and never by attempting to duplicate the appearance of the OUTCOME of the performance. Thus I seemed to be on a head-on collision course with the need to say, in functional terms, what constitutes creativity, and there seemed to be no way around it. (I should make clear, by the way, that this view is not intended to refer to the implementation levels of programs built around devices which are fundamentally unlike what the human being uses. The video camera being used in computer vision systems, for example, has very little in common with the human visual system, and, to the degree that much of what goes on in vision programs has to do with inferring the state of the external world from the incoming data, there would seem to be no compelling reason to use human visual data processing as a model.) Secondly, apropos of drawing: like its predecessor, Aaron2 would be making drawings, but not the same KIND of drawings. I need to say something about the differences, and about drawing in general: any classification is to some degree arbitrary, and I should make clear what my own is. The most inclusive way of regarding a drawing, probably, is as a set of ordered marks, or perhaps we should say INTENTIONALLY ordered marks, since there are all sorts of ordered marks in the world we don't regard as drawings: for example, the tracks of cars in the snow, the veins in a leaf, the cracks in a mud flat... or, for that matter, a musical score or a printed page of text. The question of intentionality is of paramount importance, notwithstanding the fact that intention has to be inferred from forms rather than perceived directly, as forms are perceived.
This implies that a drawing is a drawing, not merely because it stands for something other than itself, but because we find in it evidence that the reference to that other something results from an intentional act. Which is not to say that all drawing is representational, in the sense that it makes reference to the outside world in terms of the world's appearances. I suspect that very little of it has been: in fact, it may be that in the whole of man's history, only Western European art from the Renaissance on has ever busied itself with appearances to the degree that it has. It IS a question of degree, of course. A drawing is a set of assertions about the nature of the world, and the form in which those assertions are made derive from the operation of the visual cognitive apparatus, whether or not the marks are intended to refer to appearances. As an example: all human beings at all times have represented the solid objects of the world, on flat surfaces, as closed forms. But at the same time, closed forms, and the distinction between closed forms and open forms, has functioned as fundamental raw material from which all images are built. It would seem, then, that the making of drawings would be inextricably linked to the possession of a cognitive apparatus, and of cognitive skills. And for a human being it certainly is. But I have been careful to say that a drawing contains the IMPLICATION of intention, as I have also said that the viewer actually assigns his or her own intentions to the artist rather than the other way about. For a program, what is required is enough knowledge about the way images are made and used to be able to generate the IMPLICATION of intention: which is what Aaron did. Aaron did not make representations, in the sense of dealing with appearances. It made images, evocative drawings: which is to say, drawings which facilitated the assignment of the viewer's intentions and meanings. Its successor, however, was designed to make representations. Now, in asserting that the structure of representations takes its character from the nature of the visual cognitive system, I do not intend to imply that a representation is, in any useful sense, a transformation of the external world onto a sheet of paper. I am quite sure that it is not. What I said was that a representation is a set of assertions about the external world, made in terms of the world's apprehend ability. That does not imply the existence of any one-to-one mapping of the world onto the representation, such as one finds in a photograph, and, its ubiquity notwithstanding, photography is quite uncharacteristic of representation-building in general.
There is nothing particularly original in this nontransformational view of representation-building: every sophisticated artist knows perfectly well that a drawing is an invention, built from whatever raw material he or she can muster, and aimed at plausibility rather than truth. In fact, the idea of truthfulness, realism, is itself just such an invention, one which simply uses the appearance of the world as a hook upon which to hang its claims to plausibility. But if we take this view at face value, disentangle it from the photographic, transformational bias of our time, some interesting questions emerge. In some superficial sense a representation represents the external world, but then it isn't clear HOW it represents that world, or what aspect of the world is being represented. In another sense a representation represents one's internal world — that is to say, one's beliefs about what the external world is like — and it is produced, externalized, in order to check the plausibility of one's beliefs against the primary data collected by one's cognitive apparatus. Obviously, this view of representations as externalizations of an internal world is not limited to drawings, but to any forms by means of which the individual is able to examine his or her own internal state. And at that point I thought I had my first real hold on the question of creativity, which I was determined to characterize in terms of normal functions, and without falling back upon some superman theory. If this checking process in the normal mind is put to the service of confirmation, of reassuring the individual that the world is the way he or she believes it to be, we might suppose that its function in the creative mind is to DISconfirm, to test the individual's internal model to the limit, and to force the generation of new models. In other words, the essence of creativity would lie in self-modification, and its measure would be the degree to which the individual is capable of continuously reformulating his or her internal world models: not randomly, obviously, but in perceptive response to the testing of current models. Thirdly: to talk of one's internal model of the world is to talk of a representation, clearly. But it is not a fixed, coherent representation, the way a representation on a sheet of paper may be thought of as fixed and coherent. It takes very little introspection to discover that the pictures we conjure up in our heads are anything but complete. Try conjuring a picture of your favorite person's face, and then ask yourself a question about it — what is the distance between the eyes, for example — to see how volatile the mental image is, and how little information is carried in it. Ask a question about something quite different, and a quite different mental image may spontaneously emerge to replace the image of the face. Evidently, there is some store of material below the level of these mental images, and we should probably regard these images as a sort of semi-externalized representation of the material at the lower levels.
Representations represent lower-order representations, and exist as a series of momentary crosssections in a continuous unfolding, a continuous reconstruction of the world from the debris of experience. We ought to be able to characterize creativity in terms of this normal representation-building: that is to say, we should expect to find creativeness exercised, not as another kind of function entirely, but in highly particularized modes for the reconstruction of mental models from low level experiential material. It is not surprising, then, to find Albert Einstein, one of the few to have written about the nature of creativeness from within and in a convincing way, speaking of the part played by this lower-order material in thinking: "It is by no means necessary that a concept must be connected with a sensorily cognizable and reproducible sign (word: in our context, mental image)... All our thinking is of this nature of a free play with concepts... For me it is not dubious that our thinking goes on for the most part without use of signs, and beyond that to a considerable degree unconsciously. " We might conclude that in Einstein's case, creativity involved an extension of the domain of "thinkability," manipulability, to a level on which most of us find mental material to be unmanipulable. Fourth: a very large part of what the individual has in his or her head is knowledge about how to do things. And people don't behave creatively unless they know how to do a great many things, just as they don't behave creatively unless they are capable of abstraction. There is nothing creative about ignorance. How, then, could one expect a program to exhibit creativeness, selfmodification, unless it, too, first knew how to do a rather large number of things, whether it had acquired that knowledge experientially, or had it provided, hand-crafted, by the programmer. The ability to acquire experience would need to be built into the program at the outset, but the self-modification which might proceed from that experience, would probably come at a late stage in the programs development. That implies, of course, that the program would need to be able to store, in some appropriate form, everything it had ever done. Which leads to the fifth observation, and to what is perhaps the most teasing of all problems relating to the mind. The mind evidently stores all its knowledge, all the experience of its owner's life, in some amazingly compact fashion. What happens to your knowledge about how to cross the road when you are not crossing the road?
Can you access it all at once, form a single mental image of it? Presumably not. When you need to find an appropriate rule for crossing the road, do you need to review and examine all the rules you have for playing chess, and for eating spaghetti, and for tying your shoelaces, on the way, in order to determine whether any of them are appropriate to the current situation? Presumably not. What we mean by a rule is not an imperative — WATCH OUT, EAT YOUR FOOD —it is a conditional. — if you can't beat 'em, join 'em: if the cap fits, wear it: if they can't get bread, let them eat cake — and the condition which triggers the required action seems to lead us directly to what the action is. Roles for the tying of shoelaces appear to live with the shoelaces, and rules for eating spaghetti live with the spaghetti. Or, to put the matter another way, rules for the use of things are simply part of our conceptual grasp, our internal representations, OF those things. Of course, most rules in real life are a good deal more complex than these examples, if only for the reason that things in the world interact with each other. Rules link events: if 'a' is the case, and either 'b' is or 'c' is provided that 'd' isn't... and so on. Also, many rules belong to classes of things, classes of behavior, rather than to individual things and individual behaviors. The rule which says "If you are eating spaghetti AND wearing a new jacket, proceed with caution" is a rule belonging to a whole class of messy foods which stain clothes, and is invoked by the appearance on the table of a dish of spaghetti, by a process we might call inheritance, by virtue of the fact that membership of the class "messy foods" is part of what we understand by spaghetti.
You will recognize that these remarks are directed at WHAT the mind does, and make no assumptions about HOW it performs its feats of information processing. On that question I know nothing, nor do I believe it is central. My aim was to identify, in a few essential characteristics of human intellectual activity, the informing principles of a program, not to replicate the processes through which the mind runs its own programs. Let me summarize those principles. Firstly: Aaron2, unlike Aaron1, should have a permanent memory. In this memory should be stored, in extremely compacted form, every drawing the program makes, together with everything that the program knows about drawing, whether that knowledge is programmed by hand or acquired through experience of drawing. But, compacted though it should be, that stored material should be structured so as to inform its own regeneration into more complete specifications for the making of a new drawing.
However, this process in the program should tie flexible enough to reflect the associative quality of the process in the mind. (I have neglected to mention association up to this point, largely through lack of time: nevertheless, my suspicion is that creativeness is not a function of "correctness" in representation-building so much as it is a function of the slightly messy, apparently somewhat random, action of association.) Secondly: the knowledge the program should have, its domain of expertise, should concern, predominantly, the making of "visual" representations: that is, it should know enough about the nature of the visual field, and about the way people derive knowledge of the three-dimensional world from it, that it would be able to generate a convincing sense of depth, regardless of the lack of any data concerning the objects in the visible world. This principle was actually quite arbitrary with respect to the program's planned structure, though it made sense to pick a domain in which I felt I had a good deal of expert knowledge readily available, and it was certainly justified as an excellent example of the final stage of the externalizing process. But you will have recognized that almost none of my remarks have been directed specifically to drawing, and I tend to think the program could as easily deal with other material. Thirdly: the rules which determine how its knowledge of drawing is to be applied in the making of particular drawings should accessed by the program as it accesses the knowledge itself. Perhaps I should have explained that Aaron1 was what we call a production system: simply a long list of rules — if some condition holds true, do this, otherwise if something else is the case, do that, otherwise ... — in which the program simply cycles through the list until it finds, and activates, an appropriate rule. One of the conceptual problems of this kind of program is that the knowledge of how to do things is split up, between the rules on the one hand and the subroutines invoked BY the rules — the "do this, do that" part — on the other. Thus, Aaron2 should provide a more coherent representation of "how to do it" knowledge than its predecessor. Fourthly: the program's knowledge of drawing should include conceptual knowledge, at least to the degree that it should be able to particularize from general rules. I mean, for example, that it should not only know that there is a general class of things called closed forms, but should know about all the members of the class and be able to decide that one was more appropriate in a particular situation than another. Conversely, it should also be able to remember that it had used a closed form for some reason without necessarily having to remember which closed form it was.
And so things are working out. Aaron2 is still in its infancy and a very long way from becoming self-modifying. In order to support the long range need for building up the program's store of knowledge, early work on the program involved the writing of an editor, by means of which the programmer is able to build items of knowledge by hand. These items are, indeed, extremely compact: in memory they consist simply of sets of tokens, unique names. Once an item is accessed by the program, however, it is regenerated into a generalized tree structure, and the individual tokens are enacted. Perhaps this is a little abstract: what it means in practice is that the programmer, having written a set of subroutines that describe how a particular kind of closed form may be generated — let's call it a "shape," for example — uses the editor to implant in the program's memory the fact that it now knows how to generate these "shapes." At this point the memory item will consist of the single token "shape," together with a marker which identifies the token as the smallest unit of "how to do it" knowledge, which we will call a "system". Any time this item is accessed, the marker will cause the program to activate the generative subroutines to which the token refers, and a "shape" will be produced. Suppose now that the programmer writes another set of subroutines for adding a kind of appendage to a closed form — we'll call it a "base" this time — and uses the editor in the same way to implant another item of memory. Now, because of the way they are generated, "bases" can only be appended to closed forms, and it follows that in due course the programmer will want to add a rule to this memory item which will prevent it from being activated for any other purposes. For the moment, however, the programmer uses the editor to build another memory item, this one carrying a marker identifying it as a figure — not simply a system — which has, as they say in computer-talk, two children, each of which is a system. The first system is the token "shape," while its sibling is the token "base," and in implanting this more complex item in memory, the editor will create a token by which the item will henceforth be known: it is civilized enough to make it pronounceable, if not sensible.
It is not difficult to see how the editor may be used to create groups of figures, each of which will have systems as children, and pictures, which will have groups as its children, each of which will have figures as its children, each of which will have systems as its children, each of which may have other systems as its children, and so on. Thus, by the time the programmer has been working for a short time, the program will have in its memory, not merely a number of items, but items of different levels of complexity. If we look at the items in detail, moreover, it will be seen that they do not simply exist in isolation. Each item may have within it what we will call a HASA list, which will define the sets of which this item is a member, an ISA list, which defines the item's properties, and an "ASSOCiation" list, in addition to its RULE list. If the programmer, in creating the system "shape," had declared that a "shape" ISA closed-form, then the editor would automatically have created a new "closed-form" item with a "concept" marker — assuming that one hadn't existed already — and would have entered "shape" in its HASA list. Similarly, the programmer may have created a concept item by hand. In either case the assertion of an ISA association will cause the automatic generation of a HASA association in the appropriate item. This facility is completely general, so that eventually the program may know that one system is an example of a curvilinear closedform while another is an example of a rectilinear-closed form, both of these sets being members of the superset "closed-forms," while this, in its turn, may be a member of the set "formsuseable-for-the- depiction-of-solid-objects." This is what will allow the program both to generalize and to particularize, and to substitute one member of a set for another. It is also this mechanism which will permit what I referred to earlier as inheritance: the application of a rule belonging to a class to any member of that class. The ASSOCiation list functions as a linking mechanism of a much more general kind, and is intended to allow the modeling of just what the name implies: those connections of items in human memory which may be extremely strong, though without necessarily having any very obvious reasons for existing. As I have said, Aaron2 is now in its infancy. It has in its memory no more than about twenty items, three or four of which represent complete pictures: or, more precisely, classes of pictures, since the same item could be enacted a thousand times without ever producing the same drawing twice. Most of the things it knows how to make are readily discernable in its drawings, and once you know what you are looking for it is obvious how few things it knows how to do: far too few to move to the next major step.
That step will involve providing Aaron2 with a number of criteria, which it will be able to apply to its own performance. Suffice it, for the moment, to say that these criteria will reflect what I think of as cognitive constants, and that the program will judge the enactment of any item of memory by how closely it has matched one or another of these constants: or, to put it more simply, how "like" the visual field the current drawing is. Having generated a closed form, for example, it may judge that its outline is quite short in relation to its area, implying that the form is not yet complex enough to "match" the structure of the visual field. In that case it will be able to make use of any of the links it has to traverse memory in search of something it knows how to do which will add to the complexity of the figure and better satisfy this particular criterion of complexity. If it succeeds in doing so, it will have learned how to do something it hadn't known how to do previously, and, using the same editor that built its memory in earlier days, it will commit to memory this new piece of knowledge. You will see why I insisted that a program like this would need to know a great deal before it is ready to be let loose. Once it is let loose, my guess is that it will develop quite rapidly, and I am prepared to believe that in a short time its drawings will be unpredictable, not in the simple sense that Aaron1 's drawings were unpredictable, but in the more profound sense that they were produced by a program which had changed since it was written. What will its drawings be like? Obviously, I can't know in detail, though I think I would be quite surprised if Aaron2 generated a Leonardo. Will they be wonderful? Will they become so unlike the externalizations of the human mind that they cease to function as those cultural artifacts we call works of art? Who can tell. But I am preparing now to devote some years to finding out.

This book is about machines for writing and reading in late-nineteenth-century America. Its purpose is to explore writing and reading as culturally and historically contingent experiences and, at the same time, to broaden the current widely held view of technology in its relation to textuality. The main character in this narrative is the phonograph, invented by Thomas Edison in 1877. That I make the phonograph my protagonist plays upon the gist of my argument: isolating and centering machines in this way, to essentialize them as the phonograph or the computer, is misleading and denies their history. Edison identified his phonograph as a textual device, primarily for taking dictarion. With this mandate, the invention emerged from Edison's laboratory into and amid a cluster of mutually defining literacy practices, texts, and technologies, among them shorthand reporting, typescripts, printing telegraphs, and silent motion pictures. Even Edison's own famous light bulb, now a universal icon for "I have an idea," had to make sense within an ambient climate of textual and other representational practices, a climate it would, in fact, have an ample share in modifying.
This shift from Gutenberg to hypertext has been greeted with celebration by some, sackcloth and ashes by others, while the emotional tenor of response tends to deflect attention away from questions about shifting per se. The most schematic accounts simply jump from the logic of print in the sixteenth century to a new logic for digital communications in the twenty-first, as if five hundred years had not happened, or as if Caxton and Carlyle, Paine and Pound, could have experienced textuality in the same way. Even the more extended narratives of George Landow, David lay Bolter, and Richard Lanham, so rich in detail about the new world order, elide crucial developments toward the end of the nineteenth century that together prefigure most of the "revolutionary" aspects of digital, hypertextual networks. The pending ubiquity of multimedia, of paperless offices and personalized newspapers, as well as the supposed democratization of information and liberating proliferation of "virtual" identities were also imagined in association with predigital technology. Here, I seek to interject a corrective portion of the missing detail; this work's grounding contention is that these same accounts generally distort the character and contexts of literacy and textuality in modern life. In particular, they fail to explore technology as plural, decentered, indeterminate, as the reciprocal product of textual practices, rather than just a causal agent of change. My focus is on experiments and innovations in the area of inscription. In the manner of German theorist Friedrich Kittler, I see mechanized inscription as integral (though certainly not unique) to the climate of representation that emerged toward the end of the nineteenth century and has dominated the twentieth. Its features are quickly mapped: Edison stumbled across the idea of mechanically inscribed sound during his work improving Alexander Graham Bell's telephone. He jotted hastily in his experimental notebook, "Theres no doubt that I shall be able to store up & reproduce automatically at any future time the human voice perfectly."
His single, jumbled sentence identifies the fundamental properties of inscribed communication that distinguish it from communication generally. Telephones reproduce speech at a distance, but phonographs both reproduce and conserve, able to reproduce again and again "at any future time," because of the delicate spirals inscribed on the surface of records. The same parameters of economy and durability that characterize "storing up" sound for later mechanical reproduction helped animate such contemporary phenomena as the tensile bureaucracy of managerial capital, the ideal of objectivity in the professions and media, and the success of new popular culture forms. Economy and durability informed new modes of inscriptive duplication, such as the office mimeograph, which allowed bureaucrats to have their copy and send one too. Likewise, economy and durability characterized considerations of photography and then motion pictures, which stored up sights and movements. Incidents as fleeting as the pulsations of the heart and activities as evanescent as the private use of electrical current were captured, registered, metered, and read in new mechanical ways. Some of these inscriptions were more transparent as representations than others; some were more textual, some more graphic. Many, like the grooved surfaces of phonograph records, provoked explicit questions about textuality, about how some inscriptions might or might not be like texts. Inscription is a"form of intervention, into" which new machinery continues to interpose. Ink is imposed on paper, while pens and keyboards intrude into the posture of hands. Grooves are incised into phonograph records, while sound echoes in our ears. The genealogies ofinscription allow what anthropologist Michael Taussig calls "particular" histories of the senses, as different media and varied forms, genres, and styles of representation act as brokers among accultured practices of seeing, hearing, speaking, and writing. There is what Jonathan Goldberg calls a "history of technology that is also the history of 'man,' the programmed/programming machine: the human written". This is the kind of history that Scripts, Grooves, and Writing Machines seeks to locate.
My discussion offers a less determined, less determinist version of technology, in part by locating writing machines and other textual devices in the instances of invention, in narratives that show each machine, device, or process to have been authored and appropriated out of many different possibilities relevant to the making of meaning. I begin with the idea that inventing new ways to write or new kinds of writing presupposes a model of what writing and reading are and can be. If the model is too eccentric, then the invention may not work, or the model might suit some relevant social groups and not others. If the model is negotiable within or against existing models, then the invention has a chance of appropriation and dissemination. In this way, shorthand alphabets, phonographs, typewriters, and other nineteenth-century innovations in the area of inscriptive practice are so many theories of language and textuality. They are not the theory of language held by all Americans at the time; they are not "our" theory of language. Instead, they are modest, local, and often competitive embodiments of the way people wrote, read, and interacted over the perceived characteristics of writing and reading. This perspective has a couple of advantages. First, it generalizes from consensus and thus from technologies that eventually proved unworkable as well as workable, since both are revealing sites of negotiation. In this sense the view has "symmetry" according to the sociology of knowledge: it hints at a more complete compass of experience, at the category of technical "workability," in the same way that noncanonical and "sub"-literary works and everyday textuality hin~ at the category of "literariness" in any epoch. Thomas Edison's "electric pen" stenciling devices, to take one example, proved far less successful than office mimeographs as a means of textual duplication, as did numerous other contrivances that never made it to market at all. Yet electric pens and the rest of the objects in the discard pile are no less worthy of study than mimeographs.
Together they permit an interrogation of textual duplication as a socioeconomic and a linguistic fact, part of emerging business practices, shifting labor cohorts, and the burgeoning potential of writing to be all over and all the same, somewhere between manuscript and print. The eventual failure of electric pens reveals something; the eventual success of mimeographs probably tells a little more; looking into the social negotiations of failure and success promises to reveal the most. The second advantage of considering machines for reading and writing as consensual, embodied theories of language is the potential the approach offers for an additionally symmetric account of cultural production and consumption. It does no good to look at theories of language foisted on a blank public by individual and frequently idiosyncratic inventors. No inventor is the beginning of a circuit, sprung whole, like Athena from the head of Zeus. No public is a blank receptor. And "foisting" is far too simple a verb for what I want to call negotiation and appropriation. Like text, new technology is not objectively consumed. As Roger Chartier observes of the former, "Experience shows that reading is not simply submission to textual machinery". Nor, as Marshall McLuhan and Jean Baudrillard are both famous for suggesting, is the experience of text simply a submission to the inscriptive medium of print or computing, the mechanical modes or electronic codes of its presentation and reproduction. The dual symmetries of success and failure and of producer and consumer appeal in theory but prove difficult in practice, because success and production form such powerful forces of historiographic orientation. Technologies that succeed exert a teleological tug: mimeographs tend to erase electric pens on the way to photocopiers and facsimile machines. The physical and commercial shape of the pens is excised from memory and so is the partially linguistic phenomenology of their use.
As moderns and as consumers, we have been conditioned to think that technologies supersede each other one by one, the present ever liberating us from the past. Added to the double problem of forgetfulness, the producers of technology always leave more traces of themselves than consumers do. The makers and purveyors of mimeographs proportionally erase the many more numerous users of their machine by dominating the historical record. Consumers of failed electric pens, by this account, sit smack in a blind spot, equally obscure to history and discomfiting to historiography. Their technology did not "win." They were "only" its users. Even when the fewer users of the pen can be identified-Charles Dodgson, a.k.a. Lewis Carroll, had one'-it is difficult to renovate their experiences from the level of anecdote or emblem to the level of evidence. I am describing a challenge, not an impasse: being careful with questions and patient at archives can unearth a variety of sources for analysis that can help cut across teleological habits. Technology, whether inscriptive or not, involves a lot of paper. Machines get some of their meaning from what is written about them in different ways and at specific junctures, in research plans, patent applications, promotional puff, and so on. Writing machines, in particular, get some of their meaning from the yvay they are used, including the writings they produce. If paperwork can reveal so much about technology, then technology, like science, has a rhetoric of its own. It relies upon rhetorical processes, the conventions of which contribute to a "thick" description of culture, revealing the way American culture sees itself and hinting at the way it identifies and legitimates "the facts." This follows from Edwin Layton's observation, unremarkable among historians of technology today, that technology constitutes a form of knowledge. Rather than an inert and hermetic materialism, technology presupposes a "spectrum," according to Layton, ranging from an idea, through a design, and finally to an artifact and its apprehension and use.
They very rarely exist without linguistic and graphic compliments, labels, descriptions, drawings, and diagrams. In this light technological innovation becomes a process of selecting, sifting, and circulating messages, from the proverbial drawing board to the marketplace and then the drawing room. Artifacts become knowable in part because they are enmeshed within the back and forth and round about of telling what they are, and because telling devolves upon discernable rhetorical conventions, like genres and specialized vocabularies, that are themselves largely the result of unconscious consensus. Economic realities tend to enforce this rhetorical character of technological knowledge by requiring the literature of patents and the literature of commercial promotion. Both the need to identify property and the desire to exchange it ensure an insistently rhetorical character almost unmatched in science, where disciplinary pressures stand in place of commercial ones. Not only does the rhetorical character of technology allow for a "softer" determinism by which machines are not simple, unitary influences on writing but also the same character permits a degree of critique that has eluded all but a few humanist (and those particularly feminist) attentions to science. If technology is a form of knowledge, then it can be conflicted with doubt and contradiction, with assumptions and anxieties, just like other forms of knowledge. The answer to Langdon Winner's provocative question, "Do artifacts have politics?" is affirmative, if only most obviously in cases like the gender politics of Dalkon Shields, or the racial politics of Pullman sleeping cars. Fountain pens and typewriters can be just as ideological, just as much superstructure as infrastructure. Culture insinuates itself within technology at the same time that technology infiltrates culture. Rhetorical analysis provides one way to glimpse the localities of both insinuation and infiltration while testing the usefulness of their directional semantics.
This underlying sense that technology is enmeshed within textuality, that machines are discursively and physically constructed, is a view garnering surprisingly little direct attention. Historians of technology have only lately begun to ponder what they call the "interpretive flexibility" of artifacts possessing "technical content" for "relevant social groups." Drawing upon earlier work in the sociology of scientific knowledge these scholars have tried to set aside the habitual opposition that both divides and defines their discipline, the one between internalist and externalist histories. Internalists practice a sort of formalism, attending more narrowly to how things work, the way one telegraph instrument adapted the form or function of another. Externalists, by contrast, locate things more amid political, economic, and cultural contexts. The newer school of social constructionists rejects both thing and context as separate or separable units of analysis.' According to this view, an invention succeeds not because "it works," but rather it is described as "working" because it succeeds amid prevailing and possibly competitive expectations. Technological function remains something to explain; it does not comprise an explanation in (or of) itself. To put this another way, artifacts are themselves astute, yet they cannot answer all of our questions about why one invention becomes accepted and another does not, any more than a novel, for instance, can answer all of our questions about how it was written and how it was read. Artifacts cannot eveI1. answer with sufficient precision why one model of a machine is "better" than another, just as an edition of poetry cannot speak completely to its own superiority or popularity over other editions. Despite my analogy, the discursiveness of technology has sometimes been hard for scholars of literature, linguistics, and communication to rehearse. Even the most committed social-constructionists seem at times to fall back upon technology as a stable ground amid the roiling, discursive sea.
Both scholars have been influenced by the work ofJiirgen Habermas, who plots the origin and structural transformation of the public sphere in Europe as the creation and then corruption of an abstract level of rational, critical discourse among bourgeoisie. This modern public sphere was created in some degree by anew subjectivity of print in the late seventeenth century-by reading in a new way and by reading novels-and corrupted in the eventual passivity of consumer culture. 6 Like Benedict Anderson and Angel Rama, Habermas grants a great deal of weight to the circulation of print as socially constitutive and transformative. This is a particularly rich background with which to explore the related matter of inscription in the late nineteenth century. The study of inscriptions shows the realm of writing and reading, of symbolic action and experience, in its proximity to objects and machines. From ancient marks on clay or carvings in stone to the printed labels affixed to commercial goods today, inscriptions insistently belie their own double character, both material and semiotic. Modern technology has made some features of this doubleness seem particularly arcane. For example, the original electric meters of the I880s were really halves of little batteries; to "read the meter" a technician had to remove a zinc electrode and weigh it in order to determine the amount of ion deposit, the amount of electrolytic action, which in turn indicated the amount of current that had passed thIough the system. Those ion deposits, like the ion deposits on phorographic plates or strips of celluloid, are the stuff of inscription. They are the double-sided boundary at which the built system both represents technology and is technologically represented. With the same doubleness, the word Representing in my title is both verb and adjective, looking toward technologies represented as well as toward representations and inscriptions generated technologically by the typewriter, the phonograph, and the like. It is a profitable doubleness and, I will argue, particularly revealing of modernity and modern subjectivity.
It is tempting to locate shorthand in the same broad context of the history of literacy as Benn Pitman did. The history of shorthand may indeed be the history of writing; ancient Mesopotamian representations of writing show scribes taking dictation.' The dubious Near Eastern "evolution" from graphical to syllabary to alphabetic writing bears a passing resemblance to the shift from spelled, stenographic shorthand to sounded, phonography, while symbol, syllable, and sound all came under new scrutiny in nineteenth-century shorthand. Moreover, frequent, determinist claims that literacy changes cognition, improves abstract reasoning, and stimulates cultural development, run parallel to claims made for shorthand as rational and scientific, encouraging mental discipline and civic progress. The alphabet is just as much a hero for anthropologist Jack Goody as it was for Benn Pitman. Goody traces the alphabet from Pre-Canaanite to Phoenician to Greek and indicates its seminal influence on economic organization and democratic government. Determinism like Goody's has been absorbed uncritically into numerous historical and literary accounts of literacy and print culture. While many authors, including Goody, have questioned the treatment of orality and literacy as stark, indivisible opposites, anthropologists, psychologists, and cultural critics persist in rating literacy according to orality.4 Different scripts are routinely considered more or less "advanced" in their progress, with the result that the interdisciplines of literacy studies seem to offer little hope of a clear context for shorthand, unmuddied by a posteriori parameters of evaluation.
Pitman even published a weekly periodical printed mostly in phonotypy, The Phonetic Journal, for practice with and appreciation of the phonetic principle. Braced with a knowledge of "common" writing, phonographic longhand, and phonotypy, the student could finally turn to the reporting style. Here different rules of contraction apply. Hooks or loops represent common prefixes, suffixes, or groups of consonants. Vowel markers are generally omitted, the vowel sounds suggested by different lengths and positions of the usual consonants when they are not left to context alone. And long lists of frequently used words are abbreviated to a single letter or a single group of consonant signs. These are the socalled "arbitraries," which some systems were more apologetic than others about using. The sign for p sometimes means "up"; the sign for t sometimes means "it." As the student wades deeper into reporting, contractions become more involved and patently less phonetic. Whole phrases are represented by contractions for words composed of contractions for groups of letters. The avowed principle at the heart of Pitmanic phonography is a one-to-one mapping of sign to sound, in contrast to the Roman alphabet, yet in practice phonographic reporting relies upon a one-tomany mapping of sign to sense. Homonyms and many short or similar sounding words or phrases end up having the same signs in the hands of a skilled reporter. Every boast of "natural" can be met with the challenge of "arbitrary." And every claim of "system" may be qualified hy "personal." Where one-to-one mapping remains pivotal, of course, is in the conversion ofshorthand repotts to full transcripts, where the unique person of the reporter and the necessary uniqueness of transcript stand in for the lacking uniqueness of phonographic signs. Though in theory any reporter can transcribe any report, practice suggested that every reporter be responsible for her or his own transcriptions.
The vociferousness with which rules were promulgated marks the desire to keep every practitioner of a particular system the same-interchangeable parts perhaps, within some larger machine for turning sound into text. Paeans to shorthand as a manner of cultivating habits of memoty and attention undercut the probability of achieving such a goal. Why boast that reporters have "wonderfully improved and mechanically strengthened" their memories if following the rules means producing uniquely sensible reports? The conscious exertions of reporters seemed balanced against their unconscious sensibility. Discipline vouched for accuracy, but exertion undercut probable objectivity. Finally, reporters use different degrees of contraction, even within the same system of phonography, and much remains to be figured out from context while transcribing. With steady use, a system of shorthand can be multiply personalized. Little improvements suggest themselves. Such was the origin of so many of the new, "improved," systems of the nineteenth century. Pitman's phonography itself passed through at least ten different editions, changing incrementally, under the supervision of a Phonetic Council comprised of prominent reporters in Britain and the United States. In 1851, for instance, the Phonetic Council agreed on new consonant strokes for w, y, and h. The resulting ninth edition lasted only five years; the subsequent tenth made changes to the representations of the vowel sounds. This tenth edition caused much dissension, particularly in the United States, and further stimulated the invention of new systems by practicing phonographers. As one prescient commentator noted, Americans launched into "go-as-youplease phonographic authorship," wherein the "exposition of an accepted system soon gave place to the exploitation of individual innovations." For instance, the reporter Elias Longley vaunted a whole new system on the basis of a single new consonant character (Brown, 289). Many authors dissembled the degree of difference that their systems bore to predecessors.
Shorthand publishers made money in five ways. They sold their manuals directly to students. They sold their manuals indirectly to students, through the shorthand schools they established. They collected tuition for study at the schools. They collected tuition for correspondence courses. And finally, they provided skilled reporters for hire, their schools acting as clearing houses and employment bureaus for graduated students." It was a text-based economy without authorship, in which copyright didn't matter, couldn't matter, and allegiances were everything. With intellectual property moot, mental exchange loomed large. Agents were granted exclusive territorial rights to sell books in exchange for signed agreements requiring them to establish schools and promote the system. The Phonographic Institute in London offered the stereotype plates of its books and pamphlets to Americans for relatively modest sums, if only responsible parties would accept exclusive territorial rights in exchange for promoting Pitman's phonography. A board-bound mallual cost less than a half dollar through the I840S; by the late I880s only the most sumptuously bound halldbook cost two dollars.u With prices like these, entrepreneurial eyes were not only on the book trade but also on the educable labor market. The aim of course was to create an ever-growing base of customers wanting to learn just that system. Shorthand publishers competed for market share the way periodical presses competed for circulation and advertising copy. Like the publishers of small-town newspapers, shorthand authors probably generated copy, handled elements of the printing process, and attended to promotion as well as other business matters. (And they took reporting gigs to make ends meet.) But the analogy is imperfect. Shorthand publishers resembled the purveyors of new, component technology as much as they resembled newsmen. They needed to amass market share, but the rewards for doing so were more than purely arithmetic.
The fortunes of the QWERTY keyboard may be even more pertinent. Paul David explains the tenacity of the modern QWERTY keyboard as a matter of economically determined error. The QWERTY arrangement of the original Remington typewriter has remained virtually universal since the I 890s, even though more efficient arrangements have been developed. The market has tipped to the wrong standard. Remington's arrangement was based on that of the inventor ChIistopher Latham Sholes, who wanted to keep the typebars from clashing when the operator typed quickly. Better-engineered machines and then electrics and then computers vitiated the need, yet QWERTY stuck (in parts of Europe, AZERTY). (Reporredly the Remington Company also liked its product name, "type-writer," to appear acrostically in the top row.) David's work has been attacked by S. J. Liebowitz and Stephen E. Margolis. Liebowitz and Margolis challenge David's account of the facts, particularly his identification of a proven-better keyboard, but they also attack his model of the market. Their market is less prone to error. They quickly sketch the early history of market competition and suggest that QWERTY succeeded because it was the best, implying that it may have been easier to learn than its rivals, since ease of tuition could be just as important as ease of use. In the end there is less difference between David and Liebowitz and Margolis than the latter make out. Both parties admit the pressure of standards, even if Liebowitz and Margolis hold that standards are more sensible. All ascribe the conservatism of standards to economics: retooling and retraining make design changes prohibitive. My sympathies are with Liebowitz and Margolis's facts and David's view of the market, which are not necessarily contradictory. The former's description of market competition is forcibly reminiscent of shorthand.
So too did shorthand publishers need to cultivate the labor market in order to disseminate their systems. But the publishers never internalized cost. Though there may have been on-the-job training for typists 'or free typing classes and job placement for typists, little was ever free in shorthand. Rapid training had rhetorical appeal more than it had cost effectiveness. Even the quickest system, if one emerged from the shorthand contests of the later century, like the quickest keyboard, might succeed or fail for any number of reasons, particularly if "quickest" was a matter of inches, as it appears to have been in several documented cases.14 Among the pertinent variables of success were geography, institutional vigor, marketing skills, and access to literate, educable labor, and a variety of target markets. It is impossible to know in retrospect whether or to what degree Gregg shorthand eventually succeeded in the United States because it was quicker or easier, more rational or practical, or because John Gregg most successfully oriented himself near centers of power, within trademark law, and amidst a vigorous and flexible contractual network of book suppliers, employment providers, and educational agents.
These rewards follow the collective accedence to any standard, the individual acquisition of any skill. But they also accrue to any able participation in literacy practices, which interpenetrate the broader power relations of a literate society. The prohibitive expense of retooling and retraining that conserves the QWERTY keyboard finds a compliment in the conservative qualities of literacy practice. Admitted normative characteristics make spelling, for instance, a matter of right and wrong. Misspelling is transgressive and signals the speller's marginal status, either preeducated, uneducated, or sloppy. So knowing and complying with a specific shorthand system, amid its rigorous structure of approved rules and its vociferous promotion of "best" systems, must have lent its users a sense of rightness, of authority, of being in step, which comprises so much of the ideology of literacy. Many disparate elements of different literacy practices are normative, notwithstanding the appreciable liberation and empowerment rightly ascribed to literacy as a whole.
Newspaper reporters in the gallery kept running too, not toward the members of Congress but away from them, in the direction of the Washington telegraph offices. Shorthand reporting thus formed a web of inscriptive action, making the work of Congress public record by doubly making it public and making it record. A similar web had existed before 1848 and 1849, but the promotion and spread of verbatim shorthand reporting made the weave closer and changed the way its patterns appeared. Prior to Pitmanic reporting the only elements of American legislative proceedings to receive consistently detailed treatment were the texts of bills and the prepared speeches delivered by representatives and senators who handed a copy over to the congressional clerks for inclusion in the Globe, or who published copies themselves for franking to constituents. Speeches that were otherwise "taken down by hand" were often shown to their speakers for correction before publication, where they were cut and spliced into the newspapers of the region. American government thus passed into history smacking of oratory, rather than debate, of issues and positions, rather than exchanges, and of arrangements, rather than events. That made democratic government different. Certain details of congressional hearings, for instance, which are today such a fundamental part of civic life, could seldom reach the public eye and inflame the public imagination. While television, not shorthand, is responsible for the immediacy of the McCarthy and Anita Hill! Clarence Thomas hearings, immediacy is not the whole story. What does not or cannot exist as record cannot be made immediate. Before the adoption of verbatim reporting there was less opportunity for hearings and debates to matter, literally, to comprise the material records of governance. They were narrated more properly than they were quoted.
The implication that electrical medicine (electric belts, "vitalizers," and tonics of various sorts) was impractical to the people who used it, or removed from a functionalist model of electrical process, warrants correction. Electrical medicine was used exactly because it was considered salutary, and because electricity and metabolism were understood to function congruently, however mistaken the specifics of this understanding later proved to be. In fact, popular interest in technology has often been functionalist in the truest sense of the word: the idea letters are about machines or devices intended to do things in particular ways, whether they later did them or not. Letter writers reveal their investment in the question of "how things work," with the same directness of Edison compiling his laboratory notebooks. Answers to the question varied greatly, of course, and historians of technology are now beginning to pursue "failures" and alternatives as a subject of inquiry.
The "idea letters" received by Edison are a nebulous lot, making a definitive, quantitative sample of them impossible, if only because their identification was-and remains-somewhat arbitrary. (In the current archival arrangement, perpetual-motion proposals count, Edison Star inquiries do not.) The extant archive does allow of some general impressions, however. Idea letters came from everywhere, but mostly from the United States, from both rural and urban areas. They came from women and children, though the majority were from men. They came from grocery clerks and housewives, medical doctors and farm hands. Writers usually specified lack of training or lack of capital (or both) as their particular claim to outsider status and their impetus from writing. Sometimes many writers would seize the same occasion to write, a news story carried on the wire, for instance, like one incorrect report of 1908 that Edison was working on aerial navigation. In such cases hundreds of correspondents seemed to feel that their ideas had been solicited by the inventor-as they really were during World War I when Edison and other members of the blue-ribbon Naval Consulting Board made a public appeal for suggestions.
Despite this sorting, the remaining idea letters are far from uniform. Typescripts on printed letterhead lie beside handwritten notes on crumpled loose-leaf; the grammatical and well appointed lie beside the unlettered and illegible. Despite their diversity, the idea letters do exhibit identifiable patterns of expression. The most surprising thing about them is that so many concern ideas, not inventions or discoveries, not machines or devices. "Are you in the market for ideas?" queried a livestock merchant from Kansas City, "I have one that I believe would prove serviceable" (Waters 1905)." Like the word "curiosity" during the previous century, the word "idea" here denotes an epistemological currency of learning, a unit of knowing that connects an individual's mind to an experience of nature. But unlike the curiosity cabinet, the idea always possesses a kind of exchange value in the sense that people have ideas, and having appears to have been far easier than inventing. Two days after the livestock merchant wrote, another man inquired, "Do you ever help invent another person's idea?" (McTillen 1905). He writes as if inventing and having an idea were entirely distinct: ideas involve possession while inventions involve action. Writers beseech Edison to "work out" their ideas; the word "out" is as operative as the term "work." "I have several good ideas in my head," writes one man, "but that is as far as they ever got" (Braymer 1915). Ideas are written out, worked out, tried out, carried out, perfected, and made practical. In their outward progress, their path away from the self, they are invented. Their trajectory carries them toward a series of imagined marketplaces, one where ideas are bought and sold, and another where inventions may prove successful. The goal of many correspondents was to become what one termed "a multi-million air" (Parliman 1905).
Such expressions reinforce the fact that writers perceived ideas as property, private and personal, with little sense of collectivity and no recognition of the inevitability avowed by so many contemporary paeans to progress. "The idea" constituted a unit of understanding associated with technological problem solving, its value assumed to be a convertible currency, able to make the progress from the psyche of an isolated owner-citizen to the public world of technological elites and institutions. A republican ideal to be sure, fraught with the ideology of the American dream: as one self-identified "poor little country raised Texas woman" wrote from Oklahoma, her mindthe valuable idea it harbored-was "as undeveloped as her native lands" (Dale r915). Technological possibility was anybody's grubstake, and letters to Edison, like letters to the editor, were part of the arsenal of the public sphere in America, ready-to-hand instruments of republican participation, gesturing at an abstract, rather than personal, level of connectedness.
What were those features? The clock is by itself among the most richly symbolic devices; Lewis Mumford calls it the "key-machine of the modern industrial age." Clock towers "almost defined urban existence". Preindustrial church towers meted out parameters of social identity and psychological composure: in the Old World, London Cockneys defined themselves as a group born within earshot of the bells at Bow Church. Marcel Proust made the receding steeples of Martinville one cynosure amid his A /a recherche du temps perdu. Inside the home, clocks introduced an urban current. This held true even in rural areas, where they were sold by urban-identified salesmen or purchased from urban-based mail-order houses. During the nineteenth centuty, the clock maker was something of a "type" in the New World imagination, embodied by Thomas Chandler Haliburton's humorous character, Sam Slick, an itinerant clock seller true to his name. For all, clocks regulated private lives and connected their regulation to the patterning of social activity elsewhere. They mediated between the private and public, the individual and collective. More pointed forms of the same mediation transformed the workplace in the nineteenth and the early twentieth centuries. Factory time clocks, time-motion studies, and assembly-line time each added new weight to the symbolic burdens of the clock.
Edison proposed the phonograph-clock, assuring his readers "The phonograph clock will tell you the hour of the day; call you to lunch; send your lover home at ten, etc." (534). In Bellamy's short story, a gentleman traveler is awakened abruptly in his hotel bed by a young woman's voice saying, "My Dear Sir, you may possibly be interested in knowing that it now wants just a quarter of three." The effect of these words is so "thrilling and lifelike" that Bellamy's modest protagonist gets up and gets dressed before he lights the lamp to investigate. He spends the rest of the night lying awake, "enjoying the society of [his] bodiless companion and the delicious shock of her quarter-hourly remarks". In both Bellamy's fiction and Edison's wry prognostication, the phonographclock speaks with the voice of decorum, yet for both authors that decorum exits in opposition to titillating circumstance. For many others the phonograph-clock must have inhabited a similar symbolic terrain, interrogating the familiar clock, its regulation of private desires and patterning of social activity. And maybe it was the discomfort of this interrogation that kept the phonograph-clock and its "brazen tablets" from "taking hold," as so many letter writers wrongly predicted it would. On a Simpler level, the recurrent idea of the phonograph-clock continued to mediate berween the largely discarded utilitarian purpose of the phonograph and a sense of having time for leisure. The musical phonograph helped define leisure time and space: in 1907 Edison's National Phonograph Company netted more than a million dollars on phonographs and prerecorded.musical records for home amusement, even after skimming another million off to pay Edison for use of his patents.
While the functional diversity of this shape is some indication of the varied textuality Edison and his staff experienced in experimental telegraphs, printers, and the like, its comparable public apprehension remained more limited to phonograph mandrels and typewriter platens. This chapter shifts the axis of inscription, from the spirals winding around a cylindrical phonograph record to the alphabetic characters, spaces, and nonalphabetic signs that move across the typed page. Like shorthand alphabets and phonographs, typewriters were appropriated within textual practices, which they also had a share in transforming. Phonography and phonographs converted aural experience into inscribed evidence, the former by representing the sounds of speech on paper, the latter by reproducing the same sounds in the grooves of a record. Typewriters intervened more directly into the experiences of writing itself in ways that further interrogated categories of orality, aurality, and textuality. By staying the course as a textual device, typewriters formed an important compliment to phonographs, whIch ultimately diverged dramatically to amusement purposes. The amusement phonograph retained some of the power of text, enrolling authors as producers, writing as hieroglyphics, and machines as readers. Typewriters, however, seemed more certain instruments of textual production, making up in the clarity of their typeface presentation what they lacked as a means of wholly objective representation or reproduction.


In the short space of a current college student’s lifetime, the internet has gone from a specialized, futuristic system to the network that most significantly structures how we engage daily with the world at large. It is now obvious to anyone who uses a computer that intellectual exercises as basic as reading the newspaper or doing research have become fundamentally different activities largely because of the internet. So too have our views of communication in general; the very notion of globalization, so consuming in today’s world, is predicated on the possibilities engendered by a technology barely twenty years old. Such is the nature of “new media.” Computers, and the digital systems and products for which they are currently a shorthand, are what most of us think of when we hear the words new media. And why not? The world of computer hardware, software, email, and ebusiness is for most of us the latest communication and information frontier. Part of our experience of digital media is the experience of their novelty. Yet if we were asked to think of other “new media,” we might have a harder time coming up with obvious examples. We would have no problem citing instances of “old media”: typewriters, vinyl record albums, eight-track magnetic tapes, and the like. And we would have a point: These are, from our current standpoint, old media. But they were not always old, and studying them in terms that allow us to understand what it meant for them to be new is a timely and culturally important task, an exercise that in this volume we hope profitably to apply to media much older than we are. As our title suggests, this collection of essays challenges the notion that to study “new media” is to study today’s new media. All media were once “new media,” and our purpose in these essays is to consider such emergent media within their historical contexts—to seek out the past on its own passed terms.
There is a moment, before the material means and the conceptual modes of new media have become fixed, when such media are not yet accepted as natural, when their own meanings are in flux. At such a moment, we might say that new media briefly acknowledge and question the mythic character and the ritualized conventions of existing media, while they are themselves defined within a perceptual and semiotic economy that they then help to transform. This collection of essays explores such moments in order to enrich our contemporary perspective on what media are, and on when and how they are meaningfully “new.” New Media, 1740–1915 focuses on the two centuries before commercial broadcasting because its purpose is, in part, to recuperate different (and past) senses of media in transition and thus to deepen our historical understanding of, and sharpen our critical dexterity toward, the experience of modern communication. Indeed, we have marked the years between 1740 and 1915 as boundaries for our project because this period is crucial to understanding how electronic and digital media have come to mean what and how they do. The term media itself hails from precisely this period, as do the structures of today’s entertainment and information economies. Thus, the media forms and practices studied in this collection are “new” in a double sense: First, they newly receive the scholarly attention they deserve; and second, they are considered within their original historical contexts, their novelty years. In this, these essays provide a new perspective on the meaning of “newness” that attends to all emerging media, while they also tell us something about what all media have in common. Yet our intention is not only to acknowledge the initial novelty of diverse media, but also to understand better how such media acquire particular meanings, powers, and characteristics. Drawing from Rick Altman’s idea of “crisis historiography,” we might say that new media, when they first emerge, pass through a phase of identity crisis, a crisis precipitated at least by the uncertain status of the given medium in relation to established, known media and their functions.1 In other words, when new media emerge in a society, their place is at first ill defined, and their ultimate meanings or functions are shaped over time by that society’s existing habits of media use (which, of course, derive from experience with other, established media), by shared desires for new uses, and by the slow process of adaptation between the two.
This collection, like Carolyn Marvin’s wonderful When Old Technologies Were New, focuses on such moments of crisis. While it begins with the zograscope and ends in the heyday of silent cinema, the volume does not aspire to cover all forms of media that emerged during the years named in its title. Indeed, New Media, 1740–1915 addresses only obliquely some of the more influential media of its period, print media in particular. Most of the following essays (unlike Carolyn Marvin’s work) focus on media—zograscopes, optical telegraphs, the physiognotrace—that failed to survive for very long. They are, in Bruce Sterling’s words, today’s “dead media.” Yet because their “deaths,” like those of all “dead” media, occurred in relation to those that “lived,” even the most bizarre and the most short lived are profoundly intertextual, tangling during their existence with the dominant, discursive practices of representation that characterized the total cultural economy of their day. Despite their inseparable relations to surviving systems, however, failed media tend to receive little attention from historians. “Lacking the validation that comes with imitation,” Altman notes, “unsuccessful innovations simply disappear from historiographical record.” His suggested corrective for this excessive focus on, for example, “cinema-as-itis,” is an attention to “cinema-as-it-could-have-been” or “cinema-as-it-once-was-for-ashort-time-but-ceased-to-be.” New Media aims to apply some of this “could-have-been” and “was-for-a-short-time” kind of thinking to past new media. Because our understanding of what media are and why they matter derives largely from our understanding and use of the media that survived—those devices, social practices, and forms of representation with which we interact every day—the importance of this kind of analysis is easy to overlook. By getting inside the “identity crises,” by exploring the “failures” (in some cases) of older new media, the essays in this collection will help to counter what Paul Duguid has warned are two reductive “futurological tropes” characteristic of the experience of modern media. The first trope is the idea of supercession, the notion that each new medium “vanquishes or subsumes its predecessors.” From this idea follows the current belief that in the digital age the book is doomed, or, according to the peculiar auguries of earlier times, the conviction that typewriters would replace pens or that radios would replace phonographs. The second futurological trope is the idea of increasing transparency, the assumption that each new medium actually mediates less, that it successfully “frees” information from the constraints of previously inadequate or “unnatural” media forms that represented reality less perfectly.
This assumption creates an interesting paradox. The best media, it would seem, are the ones that mediate least. They are not, as we think of them, media at all. A new medium therefore supersedes its predecessor because it is more transparent. Few would disagree, for example, that a conversation with a friend on the telephone allows for a greater exchange of personal, idiosyncratic information than a dialogue conducted via telegraph. And to a large degree, this thinking is persuasive. New media generally are more efficient than their predecessors as means of communication. Yet there is more to understanding what happens when people communicate through a given medium than merely ascertaining what level of accuracy and amount of data the exchange involves. This observation—that there is more than accuracy and amount to any exchange—comprises a founding rationale for the field of media studies, whether characterized aphoristically by Marshall McLuhan (“the medium is the message”) or more recently expressed (and complicated) in Derridian terms, that the supplement—the “specific characteristics of material media”—can never be “mere” supplement; it is “a necessary constituent of [any] representation.”5 To put it simply, looking for content apart from context just won’t work. Owing in part to the linear progress unthinkingly ascribed to modern technology, media (so often referred to portentously as the media) tend to erase their own historical contexts. Whether shadows in a darkened cave or pixelated images on a luminous monitor, the media before us tend, anachronistically, to mediate our understanding of their past. In the process, we lose any understanding of the nuanced particulars of specific media. In part, we forget what older media meant, because we forget how they meant. Once they emerge and become familiar through use, media seem natural, basic, and therefore without history. Of course we say “Hello?” when we answer the telephone; of course we hear a dial tone when we pick it up to place a call. Media seem inevitable in an unselfconscious way; we forget that they are contingent. Alexander Graham Bell apparently wanted people to say “Ahoy!” when they picked up the phone, but English speakers settled on “Hello?” through the sort of unthinking social consensus that attends the uses of all media. In a similar fashion, the dial tones, 12-volt lines, and modular jacks we use today all were shaped historically by a complex of forces—technological, to be sure, but also social, economic, and representational.
When we forget or ignore the histories of each of these new media we lose a kind of understanding more substantive than either the commercially interested definitions spun by today’s media corporations or the causal plots of technological innovation offered by some historians. For example, it is undoubtedly important to be able to note, as many scholars have, how the invention of the cinema is linked to past practices of, say, lecturing with slides, as well as how it predicts certain elements of future practices. But what we often overlook are the kinds of things that only a deep analysis of specific media cases can offer—how interpretive communities are built or destroyed, how normative epistemologies emerge. No medium new or old exists as a static form. Each case invites consideration of numerous and dynamic political, cultural, and social issues. We might say that, inasmuch as “media” are media of communication, the emergence of a new medium is always the occasion for the shaping of a new community or set of communities, a new equilibrium. As we have suggested, when a new medium is introduced its meaning—its potential, its limitations, the publicly agreed upon sense of what it does, and for whom—has not yet been pinned down. And part of the lure of a new medium for any community is surely this uncertain status. Not yet fully defined, a new medium offers possibilities both positive (one of our authors argues that zograscopes helped construct polite society) and negative (another traces the threat telephones posed to Amish communities). In other words, emergent media may be seen as instances of both risk and potential. Today, for example, the internet offers unprecedented possibilities for global villages to coalesce, even while it threatens national or ethnic cultural traditions and provokes anguished discussions of privacy in a “connected” age. The same sorts of issues and anxieties surrounded the emergence of other media. Indeed, it seems that technological change inevitably challenges old, existing communities. The particulars of each case, however, are valuable to our larger understanding of how media help to shape and reshape culture. Essays in this collection therefore examine media as socially realized structures of communication, where communication is culture—as James Carey explains it—a cultural process that involves not only the actual transmission of information, but also the ritualized collocation of senders and recipients. Habits of communication mediate among people, pragmatically and conceptually. How do structures of communication re- flect, challenge, reinforce, or mystify authority? How do they help imagine community? How do they help construct the aesthetic, or the mimetic? How do they orient the production and experience of meaning?
Ultimately, then, this is a book about framing: about how particular habits and media of communication frame our collective sense of time, place, and space; how they define our understanding of the public and the private; how they inform our apprehension of “the real”; and how they orient us in relation to competing forms of representation. We have selected the cases of new media that follow because they support these inquiries, casting such habits and media into relief, affording a vantage point from which better to see how cultural meanings are negotiated. But this collection is also about how we frame our own discussions of new media, for if this interrogation of emergent media is genuinely to illuminate our understanding of cultural definition and of cultural change, then we must be responsible about our own language. We must, in other words, acknowledge the key terms that are in play in our own discussions and attempt to define and deploy them as precisely as possible, not only for us now, but as they were used in earlier—and different—contexts.
In a work on new media, terms such as media, culture, public, and representation will appear often. But insofar as this collection seeks to understand how the very idea of “media” evolves over time, we wish to employ such critical terms with care and to bring questions about their use and meaning squarely into the discussion itself as it proceeds. Our use of the word technology is a good example. This term denotes, as Leo Marx suggests, a necessary but “hazardous concept”; in this book the term helps organize our thinking about the material, instrumental conditions of modern life, yet for many readers it will also come larded with less considered shades of meaning, assumptions about “Progress” with a capital “P,” or about technology as a preeminent cause in history.8 Thus although we rely on this term as an organizing device in this collection (the essays proceed from technology to technology as a form of convenience), we also wish to urge particular awareness of its hazards. Likewise with other key critical terms. We know that we cannot exhaustively define “media,” for instance, any more than we can completely pin down “culture” (a notion that is, as Naomi Mezey observes, “everywhere invoked and virtually nowhere explained”). Indeed, the cases we offer are about culture as struggle and media as means in that struggle—a fabric continually rewoven according to the interests of a given time and place. Rather than fixing such terms and pinning them to moving targets, however, we can frame our discussions of such pervasive concepts in self-conscious ways that make our attempts to understand them more useful.
In this volume we offer cases that foreground the relationship between material and idea, between what people think or believe or wish and what they feel with their hands or see with their eyes or hear with their ears. Each of the essays in the collection thus reveals, in some fashion, the strong relationship between the contexts for some material, technological development, and shifts in self-imagining and public understanding. Erin Blake, Wendy Bellion, and Laura Schiavo, for example, consider the cultural meanings of perspective and representation in the eighteenth and nineteenth centuries by focusing on the emergence of particular visual media (zograscopes, the physiognotrace, and stereoscopes, respectively) and discussing how such media influenced notions of individual identity. Patricia Crain, Katherine Stubbs, and Diane Umble, by contrast, consider the cultural meanings of communication by focusing on the arrival and adaptation of particular networked media (optical telegraphs, electric telegraphs, and telephones, respectively) that helped shape notions of identity in relation to larger communities. All of the authors engage new media as evolving, contingent, discursive frames, sites where the unspoken rules by which Westerners know and enjoy their world are fashioned. Such “rules” continually change, as new media become situated and as such adjustments inevitably redraw the boundaries of communities, including some individuals, and excluding others. Each new medium in effect helps to produce a distinct public. Erin Blake’s work on zograscopes, for example, elaborates the idea that media assist in the construction of the modern, Western public sphere, with its corresponding liberal subject (today known as “the consumer”). Although she draws upon the work of Jürgen Habermas, Blake ignores the often-mentioned circulation of print media as the basis of the public sphere, instead looking to shared social practices to understand how space is visualized. Her public is literally a sphere; in her essay the bourgeois circles of eighteenthcentury London pop into 3-D as they enter the rational and impersonal arena of public space via engravings glimpsed through new optical devices. This new medium, according to Blake, helps the public to map itself. Wendy Bellion’s work on the physiognotrace depicts an American public that also maps itself, but this public is one more complicated by its own experiences of both graphic and political self-representation. By analyzing the American reception of this profile or silhouette-tracing device, Bellion introduces her readers to the cartography of the public sphere, showing the ways in which new media are adapted within the very discursive conditions, the very rules that they help to transform.
Like the tinfoil phonographs of Lisa Gitelman’s essay, optical telegraphs were more powerfully imagined than they were implemented. Very few were ever built or used, yet the idea of them circulated widely within the mentality, the public imagination, of their age. Joseph Lancaster’s classroom telegraphs literally disciplined students, while even broader disciplinary measures may be read in their controlling institutional contexts, as well as glimpsed in the titles of early American newspapers like the American Telegraph [Conn.], Hillsboro[N.H.] Telegraph, and Lincoln [Me.] Telegraph. (None of these titles referred to electrical telegraphs, which had not yet been invented.) In Benedict Anderson’s formulation, the circulation and ritualized consumption of newspapers like these assisted in the imagination of a national community. What their titles and Lancaster’s system suggest, according to Crain, is that the imagination of mediaconditioned the imagination of communities. Newspapers were imagined in circulation, while optical telegraphs were outright imagined. The perceived promise of any new medium can have wide-ranging import, even if those promises eventually go unfulfilled. To many observers, the tinfoil phonographs of 1878 promised a new, more modern and immediate type of text, as recordings might indelibly “capture” speech, without the intercession of literate humans wielding pencils and paper. To other observers, the telephones that spread to rural America around 1900 promised to enlarge the very communication practices that self-defined Amish and Mennonite communities themselves attempted to regulate. The wide popular reception of the first promise, Lisa Gitelman speculates, challenged and helped to transform vernacular experiences of writing and print, while raising questions about the instruments and the subjects of public memory. The Old Order perception of the second promise, Diane Umble shows, helped divide the aggregate Amish and Mennonite population, for this perception coincided with the ongoing regulation of intra- and inter-group communication and excommunication. Although so often the focus of great attention and optimism, new media are not, as these authors pointedly demonstrate, inherently benign; they “bite back.”10 They thrive amid unforeseen consequences, often despite the best, most vigorous intentions of their inventors, their promoters, their initial consumers, or of the customary arbiters of public intelligence.
To scientists, the stereoscope could be used objectively to demonstrate that vision is subjective, that the body can produce its own experiences of depth when presented with the right cues. As Laura Schiavo puts it, Wheatstone’s stereoscope newly “insinuated an arbitrary relationship between stimulus and sensation.” Yet within the context of commercially exploited and popularly apprehended photography, stereoscopes were ultimately recast as mimetic amusements that tendered to consumers an instructive and positivist model of how their eyesactually worked to see the world as it really is. Vernacular discourse, in other words, completely inverted the meaning of what the stereoscope “proved.” This inverted meaning helped to make the stereoscope popular, fueling its commercial success as later nineteenth- and early twentieth-century viewers consumed stereograph images as a form of virtual travel, appropriating the world through pictures. At stake was far more than the prestige of Wheatstone or the anti-intellectualism of the marketplace. The rules by which the West knew the world had again come into play. The popularity of stereoscopes helped redraw the very category of the “real,” the consensual practices of “accurate” representation. Assumptions about what count as “rules,” about what is “real” or “accurate” or “normal,” are no less at issue when new media are less popular than stereoscopes were or less patently involved in describing normal human perception. Media emerge and exist in ways that both challenge and regulate notions of what it means to be human. Gregory Radick’s essay provides a clear-cut case. An amateur ethologist using the new medium of recorded sound set out to learn the “language” of monkeys and stumbled into one of the hottest debates in Victorian evolutionary biology and linguistics: How is language uniquely human? In the course of his research, Richard Garner’s recording phonograph became an instrument of knowledge deployed in various philosophic and scientific controversies—in the tension between amateur and professional science, for example, or in the dispute over whether abstraction or instinct founds thought and language, or in discussions about the fundamental differences between humans and animals. Garner worked on monkeys, but not without meddling with the category of the human in two ways. First, he raised anew the definition of “Man” as “the talking animal”; second, he wielded his phonograph as if it were a necessary—and better—third ear.
Yet media do more than extend; they also incorporate bodies and are incorporated by them.11 Media are designed to fit the human, the way telephone handsets or headsets literally fit from ear to mouth, but also the way telephone circuits, satellites, and antennas fit among their potential consumers, as integral parts of communication/information networks that literally shape what communication entails for individuals in the modern age. And if media fit humans, humans adjust themselves in various waysto fit media, knowingly and not. Hands physically adjust themselves to different keyboards, different keypads, and different pointing devices, while users subtly adjust their sense of who they are. Some of these complexities may be glimpsed in Katherine Stubbs’s essay, which reads the history of electrical telegraphy in the United States against and within the fiction that appeared in telegraph trade journals. Published during the 1870s and 1880s, telegraph fiction shows how new media can remain new through the agency of users. Amid ongoing conflicts between labor and capital arising in part from the feminization of the workforce, telegrapher-authors both used and represented the telegraph as a means to explore identity in its relation to the body. In remaking themselves, by negotiating gender-at-a-distance-and-by-telegraph, for instance, telegraphers kept the character of their medium unsettled. In other words, the “newness” of new media is more than diachronic, more than just a chunk of history, a passing phase; it is relative to the “oldness” of old media in a number of different ways. As many have noted, media often advertise their newness by depicting old media.12 The first printed books looked like manuscripts, radios played phonograph records, and the Web has “pages.” Ellen Gruber Garvey and Paul Young each explore less familiar instances in which the new represents the old in order to understand more fully the purchase that “newness” has on the process of representation. As Garvey’s account of scrapbooks explains, scrapbook-makers took old media—literally the old books and periodicals they had lying around—and made them into new media in the form of scrapbooks. “Newness” in this case resonated as much with personal and domestic experiences as it did with public and collective apprehensions of novelty, posterity, or periodicity. Scrapbook-makers tampered with the meanings of the scraps they collected by collecting them, a practice Garvey refers to as “gleaning” and connects to the composition and use of the Web today. Young, on the other hand, presents a “telegraphic history of early American cinema,” reading filmic representations of telegraphs as only the most obvious link between these two media, which seem, in retrospect, so different.
We hope these essays will help to broaden the inquiry of media studies by calling attention to the ways media are experienced and studied as the subjects of history. No ten essays can do more than open the question, but opening the question is crucial, we think, particularly as today’s new media are peddled and saluted as the ultimate, the end of media history. “Newness” deserves a closer look. To that end, we include a brief section of documents for discussion. These documents are not illustrations of our text as much as they are artifacts that themselves point toward the rich and diverse record available to media historians. We hope that they will suggest specific historical and cultural meanings for media and promote a broader discussion of media history. Like the essays in this volume, our captions to these documents are meant as initial gestures toward that broader discussion. We include them to remind readers that the history of media is an ongoing, highly self-reflexive conversation about what we mean and—literally—how we mean it.

First we had media art. In the early days of electronic and digital culture media art was an important way of considering relationships between society and technology, suggesting new practices and cultural techniques. It served as an outlet for the critique of the dark side of computer culture's roots in the military-industrial complex; and it suggested numerous utopian and beautiful ways of engagement with technology, new types of interactivity, sensuous interfaces, participative media practices, for instance. However, the more critical, egalitarian and participative branches of media art tended to be overshadowed by the advocacy of a high-tech and high-art version of it. This high-media art conceptually merged postmodern media theories with the techno-imaginary from computersciences and new wave cybernetics. Uncritical towards capitalisms embrace of technology as provider of economic growth and a weirdly paradoxical notion of progress, high-media art was successful in institutionalizing itself and finding the support of the elites but drew a lot of criticism from other quarters of society. It stuck to the notion of the artist as a solitary genius who creates works of art which exist in an economy of scarcity and for which intellectual ownership rights are declared.

In the course of the 1990ies media art was superseded by what I call The Next Layer or, for help of better words, Open Source Culture. I am not claiming that the hackers who are the key protagonists of Open Source Culture are the new media artists. Such a claim would be rubbish as their work, their ways of working and how it is referenced is distinct from media art. I simply say that media art has become much less relevant through the emergence of The Next Layer. In the Next Layer many more protagonists come together than in the more narrowly defined field of media art. It is much less elitist and it is not based on exclusivity but on inclusion and collaboration. Instead of relying on ownership of ideas and control of intellectual property Open Source Culture is testing the limits if a new egalitarian and collaborative culture.

In the following paragraphs I would like to map out some of the key components of Open Source Culture. It has been made possible by the rise of Free, Libre and Open Source Software. Yet Open Source Culture is about much more than just writing software. Like any real culture it is based on shared values and a community of people.

Open Source Culture is about creating new things, be they software, artefacts or social platforms. It therefore embraces the values inherent to any craft and it cherishes the understanding and mastery of the materials and the production processes involved. Going beyond craftmanship and being 'open source', it advocates free access to the means of production (instead of just "ownership" of them). Creativity is not just about work but about playfulness, experimentation and the joy of sharing. In Open Source Culture everybody has the chance to create immaterial and material things, express themselves, learn, teach, hear and be heard.

Open Source Culture is not a tired version of enforced collectivism and old fashioned speculations about the 'death of authorship'. It is not a culture where the individual vanishes but where the individual remains visible and is credited as a contributor to a production process which can encompass one, a few or literally thousands of contributors.

Fundamental to Open Source Culture's value system is the belief that knowledge should be in the public domain. What is generally known by humans should be available to all humans so that society as a whole can prosper. For most parts and whereever possible, this culture is based on a gift economy. Each one gets richer by donating their work to a growing pool of publicly available things. This is not a misguided form of altruism but more like a beneficial selfishness. Engaged in a sort of friendly competition everyone is pushing the whole thing forward a bit by trying to do something that is better, faster, more beuatiful or imaginative. Open Source Culture is a culture of conversation and as such based on multiple dialogues on different layers of language, code and artefacts. But the key point is that the organisation of labour is based on the self-motivated activity of many individuals and not on managerial hierarchies and 'shareholder value'.

Open Source Culture got a big push forward with the emergence of Linux and the Internet but we shouldn't forget that it has much deeper roots. History didn't start with Richard Stallmans problems with a printer driver. The historic roots could be seen as going back to the free and independent minded revolutionary artists and artisans in 19th century. More recently, it is based on post-World-War-II grassroots anti-imperialist liberation movements, on bottom-up self-organised culture of the new political movements of the 1960ies and 1970ies such as the African American civil rights movements, feminisim, lesbian, gay, queer and transgender movements, on the first and second wave of hacker culture, punk and the DIY culture, squatter movements, and the left-wing of critical art and media art practices.

In terms of the political economy, Open Source Culture could mark an important point of departure, by liberating the development of new technologies from being dictated by capital. The decision of what should be developed for which social goals is taken by the developers themselves. Technological development is not driven by greed but by deep intrinsic motivations to create things and to be recognized for ones contribution. Despite that, Open Source Culture is not an anti-capitalist ideology per se but has the potential to change capitalism from within and is already doing so.

Open Source Culture needs to be constantly aware of capitalisms propensity to adapt, adopt, co-opt and subjugate progressive movements and ideas to its own goals. The 'digital revolution' was already stolen once by the right-wing libertarians from Wired and their republican allies such as Newt Gingrich and the posse of American cyber-gurus from George Gilder to Nicholas Negroponte. More recently adept Open Source Capitalists have used terms such as Web 2.0 and social software to disguise the fact that what those terms are said to describe has emerged from open source culture and the net culture of the 1990ies and the early 2000s. Once more the creativity of the digital masses is exploited by alliances between new and old tycoons. The Next Layer emerges at a time when capitalism is stronger than ever before and it emerges at the very heart of it. This is the beauty of it. It cannot be described in a language of mainstream and underground. Open Source Culture is the new mainstream which is what capitalist media are doing their best to hide, scared by the spectre of communism as well as commonism. We don't need to ressort to the language of the Cold War and its dichotomies, howver.

The Next Layer contains not only a promise but also a threat. It emerges at a time when the means of suppression and control have been increased by rightwing leaders who try to scare us into believing we were engaged in an endless 'war on terror'. With their tactics they have managed to speed up the creation of a technological infrastructure for a society of control. The general thrust of technological development is coming from inside a paranoiac mindset. 25 years of neo-liberalism in the American lead empire have degraded civil liberties and human values. The education system has been turned into a sausage factory where engineers are turned out who construct their own digital panopticons. Scary new nano- and bio-technologies are created in secret laboratories by Big Science. And the bourgeioise intelligentsia meanwhile has stood still and does not recognize the world any more but still controls theatres, publishing and universities. In this situation it is better if Open Source Culture is not recognized as a political movement. The Next Layer will find ways of growing and expanding stealthily by filling the niches, nooks and crannies of a structurally militant and imperialist repressive regime from which, given time, it will emerge like a clear spring at the bottom of a murky glacier.



Technological determinism is the belief that science and technology are autonomous and the main force for change in society. It is neither new nor particularly original but has become an immensely powerful and largely orthodox view of the nature of social change in highly industrialised societies. In this paper I analyse the presence of technological determinism in general discourses about the relationship between social change and science and technology. I show that techno-determinist assumptions underlie developments in what is called technoscience, a term describing new practices in science and technology with particular relevancy for the related fields of genetic engineering and computer science. Those areas create a specific set of narratives, images and myths, which is called the techno-imaginary. The thesis of my paper is that the discourse on media art uncritically relies on many elements of the techno-imaginary. A specific type of media art, which is identifiable with people, institutions and a period in time, is particularly engaged with the tropes of the techno-imaginary. This strand, which I call high media art, successfully engaged in institution building by using techno-determinist language. It achieved its goals but was short lived, because it was built on false theoretical premises. It made wrong predictions about the future of a 'telematic society' and a 'telematic consciousness'; and it missed the chance to build the foundations of a theory of media art because it was and is contaminated by the false assumptions behind technological determinism.
Science and technology are widely understood to be the major, if not the only forces which cause social change. This opinion is called technological determinism. According to this view science and technology are autonomous, which means that they develop according to their own internal logic only. Once new technologies have been invented and are released into the world they have an irresistible impact on the social world. This implies that history is largely a result of the impact of new technologies. By denying the importance of other social forces such as politics and the economy human agency is effectively cancelled as a factor in the shaping of history. In the field of art a new domain has been developed which is variously called media art, digital art or just new media. This field has deeper historical roots but has gained major significance only over the past 25 years. Within this area, which is very diverse and comprises a variety of practices and approaches, a particular discourse has become dominant. I call it 'high media art'. Its ascendancy started in the 1980s and peaked by the mid 1990s. Its proponents used specific narrative strategies which were highly successful in drawing attention to the field and building institutions devoted exclusively to high media art. That discourse on high media art claimed a radical break with the past and a transgression of all other art forms. It took the material basis of its practice, the use of new media technologies and in particular the computer, as major justification for its claims. It presented itself as an avant-garde, not unlike the classical avant-garde of the 1920s, which employed high-technology to create a new aesthetics. This new aesthetics was tied into postmodern theories as well as the idea of a three-dimensional cyberspace, and it borrowed freely from the myths of computer science. Artists produced works which uncritically repeated the narrative strategies of artifical intelligence and artificial life. The techno-imaginary of the 'closed world' (Edwards 1996), developed at a time when America fought ideoligical battles with its nuclear enemy, the Soviet Union, still provides the principles of our own imaginary futures (Barbrook 2005). The media theory of McLuhan, hardened into an ideology, McLuhanism, provides the intellectual framework for high media art in the mid-1990s combined with the fashionable thesises of postmodernism about the immateriality of the world. The discourse of high media art was successful in institution building but compromised by technological determinism. I will show that technological determinism in high media art isn't just a question of interpretation or opinion but foundational for the field, as a major influence on the creation of works and the theories which came with it. Instead of taking a critical position high media art only illustrates science and technology and glorifies the aesthetics and ideology of technoscience.
The thesis, which I present in this paper, is based on a literature review which includes relevant theoretical areas, histories of media art, catalogues, articles, web sites and discussions on mailing lists. It is also based on my own experience of 20 years of working in the field as an artist, curator, critique and theorist. My own close involvement in the field over a long period of time is my main motivation for this work with which I hope to explore and analyse some major theoretic deficiencies. As a practitioner I have acquired knowledge of the practice, of the actual making and doing, which is rarely reflected in theoretic texts which are only based on the analysis of other texts. I hope to be able to bring the theory and the practice more closely together. Although the focus of this paper is primarily a critique my aim is to open up, through this critique, possibilities for further work. My analysis of the field is influenced methodologically by The Field of Cultural Production by Pierre Bourdieu (1993). He presents his approach as an alternative to two positions which were dominant at the time of writing, structuralism and post structuralism on one hand, and Marxist inspired critical theory on the other. According to Bourdieu structuralism’s and critical theory’s ways of reading works mutually exclude each other (Bourdieu 1993, 177). (Post)Structuralism favours an internal reading of works, critical theory an external reading. Bourdieu describes structuralism as "more powerful" (ibid., 178), yet criticises it for stripping the reading of the work off any "references to the social or economic conditions of its production (ibid., 178)." External analysis, in contrast, "directly links these works to the social characteristics (the social origins) of their authors or of the groups for whom they were really or potentially destined and whose expectations they are intended to meet (ibid., 180)." The weakness of this approach is, according to Bourdieu, that "understanding the work means understanding the world view of the social group that is supposed to have expressed itself through the artist acting as a sort of medium." In other words, the author is seen as a ventriloquist for his own social background and supposed 'class interest'. But this approach fails to provide means of understanding the structure of the work, its subtleties and poetic motions which are, "the prerequisite for the fulfilling of its function? (ibid., 181)" Bourdieu claims he can overcome the deficiencies of both poststructuralism and critical theory by applying the theory of the field, a "relational or structural mode of thought to the social space of the producers (ibid., 181)." Different fields are characterised by positions and position taking, by writings and writers, art works and artists who are involved in a struggle to carve out their own niche within a specific area.

A key concept in Bourdieu’s theory is contained in the term 'symbolic capital'. Paradoxically, in avant-garde movements of literature or art, those who show the least interest in outward signs of success such as awards, titles and money, accumulate the highest amount of 'symbolic capital'. They receive strong support from a closely-knit group of followers, often other artists or professional insiders (curators, critics). This results in the 'non-economy' of autonomous art. The economic and the symbolic hierarchies cannot be directly mapped onto each other. The poorest, most obscure artists are the most famous ones. If they get successful too quickly, they run danger of loosing their reputation as being relevant, cutting-edge, fresh, and innovative. Bourdieu loosely groups artists according to this perception. There are successful artists who cater to the tastes of the dominant social group. They have money, wealth, but no symbolic capital. There is the consecrated avant-garde, an avant-garde which is already partly absorbed by the system, which has its critics, its recognised names. They are in danger of being seen as selling out. New artists will come and attack their perceived dominance. Only this latest group, by being seen as staying outside heteronomic power structures, is attributed the highest symbolic capital. It acts in a field of 'restricted' cultural production which has hardly any audience and very little quantifiably measurable impact, yet this group is seen as the true avant-garde.1 Bourdieu's description of the 'game' of cultural production clearly has some limitations insofar as it may perfectly describe the French literary and artistic avant-garde of the 19th and 20th century but might not be universally applicable. For instance, the notion of popular culture with its own subcultures and avant-garde is not reflected properly in Bourdieu's theory. Bourdieu's approach is useful but cannot be adopted blindly. Therefore I use other theoretical frameworks in addition to Bourdieu, in particular science studies and critical theory.
Technological determinism is hardly ever formulated as a clearly stated theoretical position but has nevertheless become "an immensely powerful and now largely orthodox view of the nature of social change (Williams 1974, 13)." According to this opinion science and technology are autonomous, their development follows an inherent logic and is independent of influences from society. Science and technology are the main forces that shape social change, therefore history is determined by technological development. Paul N. Edwards calls it the "billiard ball theory of scientifically induced change" (Edwards 1996). According to this metaphor technology impacts on society like a billiard ball and whirls everything around. Social change is conceptualized in a very particular way, namely as a causal relationship between technology, as the origin of the force for change, and society, as its target. Society is the passive receiver of an 'impact' and has no agency in the process. The 'impact of science' is presented as something completely unavoidable, like a force of nature. In this model, science and society are completely separated. Scientists, locked away in citadels of knowledge, conduct research entirely uninfluenced by society. Scientific research is a disinterested pursuit of truth which follows its own internal dynamics only. Scientific progress is based on the strict application of the scientific method alone. New technologies are applications of scientific knowledge - applied science - put to work in the world. The effects of technology are seen as the primary mechanism that shapes history. These are the core believes behind what is called the strong version of technological determinism. It is the content of statements such as that the computer created the information society; or that the steam engine brought about industrial society. In social struggles about new technologies often the opponents also adhere to the belief of techno-determinism, when they vent their anger at a particular technology because they think it is intrinsically bad. There are a number of weaker versions of technological determinism. In those versions, technologies are seen as symptoms of society, as effects rather than as causes. The development of technologies is still seen as largely autonomous, but the impact is less deterministic. Technologies are perceived as being 'instruments' only, they are neither intrinsically good nor bad, they are only neutral tools. Any ethical questions would arise depending on the way of use or abuse of those instruments. As we cannot know which use society will make of a particular technology, unintended consequences might occur, and we cannot predict in which way exactly technology will shape society.
Technological determinism is behind assumptions such as that technological progress is the key to greater prosperity, wealth and security. Technology will solve a wide range of human and social problems. For instance, government administrations believe that the implementation of CCTV surveillance systems will help to prevent crime and contribute to the upkeep of public order. In TV advertisements the ability of gadgets such as the mobile phones is praised to win new friends or find a lover. Yoghurts are advertised as containing a 'scientifically proven formula' to make you slimmer, healthier and more attractive. This emphasis on technology as the harbinger of hope to solve all kinds of social problems is reflected in the way governments have created technology impact assessment centres since the 1970s. The direction of this type of research ignores the possibility that the assumptions behind the basic formula, technology as cause and effect, might be wrong. The real nature of the relationship between technology and society poses some of the most difficult and most unresolved historical and philosophical questions.
The concept of determinism in science has different meanings. It does not relate to the question if science determines society but to another complex of questions. Is matter organised in such a way that deterministic processes can be observed? And can science formulate descriptions or models of those processes which form objective laws of nature? In this sense, science must believe in determinism to a degree, otherwise it could not conduct its activity. "Determinism came down from the skies to earth", wrote Gaston Bachelard (Bachelard 1934/1988, 101). As a psychologist, he reflected on how the scientific spirit formed, and came to the conclusion that the observation of planets and stars was essential in the historic shaping of a scientific mindset. Whereas life on earth is messy and unpredictable, the observation of regular bodies moving in predictable ways enables to shape the expectation that objective laws of nature exist which can be understood and formulated with the help of mathematics and geometry (Bachelard 1934/1988). In the long run, this enabled the development of an exact science by Descartes and Newton. Until recently histories of science presented the development of modern science from there on as an unbroken continuity to more clarity, preciseness and abstraction. But it can also be argued that regarding the being or ontology of the world and the epistemology, the theory of knowledge that we have about it, at the beginning of the scientific project some crucial design decisions have been made. The gap between subject and object, which the ancient Greeks had already thought about, started to be conceptualized in a much more polarized way than ever before. Descartes distinguished between res extensa and res cogitans.
Philosophical interest turns to the subject, to consciousness, to the possibility of cognition and human rationality (Weber 2003, 27). Nature is turned into an object of cognition, in other words, science 'invents' nature as its object. It is incredibly successful in doing so and science gains ever more knowledge about it. But at the same time the divide between human cognition and the world gets bigger. The more we know about it, nature gets ever stranger to us. Nature is the non-self, the outside, the 'other'. Nature becomes conceptualized as lifeless, dead and abstract matter (ibid., 31). As science uses ever more abstract tools and methods it becomes 'constructivist'. This particular way of conceptualizing nature in science which arguably started with Renaissance opened the door to all kinds of ways of intrumentalising and operationalising it. In a movement which should become more fully understood only recently, science emancipates itself from nature and starts inventing or constructing it. But this process of the emancipation of science is slow and takes hundreds of years. The philosophical debates surrounding this process culminate in logical positivism. In the 1920s and 1930s members of the Vienna Circle tried to achieve two main things. They wanted to purge theories of knowledge from meta-physics and make philosophy a scientific way of speaking about the world. This in turn should help to guide science to become more rigidly defined and therefore more objective. Those theoretical goals led to an increased focus on formalized theories of language, logic and mathematic. Philosophical questioning of logical operators should help to find the universal logic of the world. Logical positivism had many important results and is a complex philosophical school but appearantly makes one major false inference. The logic of the operation of the human mind is projected on nature (Bachelard 1934/1988, Weber 2003). This false inference, also called the 'cultural fallacy', continues as mainstream model of understanding to-day and is where the scientific meaning and the social meaning of determinism meet. By saying that science is the only source of objective knowledge it becomes transcendent to society. This is not religious transcendentalism but means that scientific forms of knowledge transcend the historicity of creating knowledge and theories about knowledge. What is once objectively true must always - and everywhere - remain so. There is a philosophical tension between the objectivity of scientific discovery and personal and political freedom within human societies. Early 'natural philosophy', as science was called in Renaissance, freed humans from the dogmatic truth of the church. But this freedom would hundreds of years later found to be threatened by science becoming a dogma itself, a repressive ideology. According to critical theory and science studies the invention of a new concept of nature by science opens the door for its instrumentalisation. The scientific project of gaining knowledge about the 'laws of nature' means to put nature at our disposal, to operationalize and functionalize it. And rational mastery of the forces of nature implies social mastery, the dominance of one social group over another one (Marcuse 1964/1994). The absolute character of scientific knowledge weighs down from sky on the life of people on earth.
Logical positivism gained a defining influence on the philosophy of science in Britain and the USA after WWII. Moreover, the positivists deep engagement with logic and formal thinking contributed to the newly emerging disciplines of computer science and cybernetics. At the same time science had to open up to the possibility of increased indeterminacy, after Heisenberg's discovery of an objective indeterminacy on the level of matter. Ideas of a mechanistic universe have been put aside with quantum theory. Since then, the main questions in the epistemology of science concern relationships between determinacy and indeterminacy.
Critical Theory is inspired by the analytical method which Karl Marx developed when writing Das Kapital, but went further than Marx and could even turn against him (cf. Cox et all 2004, 8). Marx has shown that technologies are embodiments of social relationships (Marx 1957). In capitalist societies technologies, far from being neutral, are developed with specific social relationships in mind. Critical theory, inspired by Marx, sees the technology that we have as a specific type of technology developed under a capitalist economy (Marcuse 1964/1994). According to Herbert Marcuse an ideology of dominance was intrinsic to the development of the scientific worldview from the beginning. Each techno-social system introduced over the last 150 years, the railway, electricity, cars and highways, created "ideology embodied in the production process (Marcuse 1964/1994, 114)." It reorganised the strata of society according to the original vision contained in the design. Marcuse believed that political represssion is not so much a function of ideology but a function of an apparatus which uses people without them being able to see behind the machinery and overcome its heteronomic tendency. Heteronomy, as opposed to autonomy, means that people's lives are determined by outside factors beyond their control. In capitalism, technological progress is specifically set against the negotiating powers of workers. New inventions are designed to rationalize production and to increase worker's productivity in order to maximize profit. By investing into better machines, workers are submitted to a dialectical process of deskilling and reskilling. Marx analysed this tendency correctly even though he observed industrial capitalism in its very early phase. Since the days of Marx, the rationalization of labour continued, culminating in the Fordist factory, and ultimately in fully automated factories. Rationalization is not only carried out by investment into better machines but also by scientific management, also known as Taylorism. In the late 20th century with the help of the computer also other areas of human labour, not just physical work, can be replaced by machines. Ever more sophisticated forms of technological and organisational dominance are developed.
A second insight by Marx, which was also made productive by critical theory, concerns the fetishisation of commodities. By basing the exchange value solely on money the human labour that goes into the production of goods is hidden. The labour, equals human life-times, is not visible in the product anymore. Hiding the origins of commodities enables them being fetishized. The world appears as a world of shiny things, of decontextualized consumer products which nowadays appear all dancing and singing in TV adverts. Marx's insights about commodity fetishism and technologies as embodiments of social relationships has been of defining influence on both critical theory and a branch of science studies called the social shaping of technologies (SST). The social shaping of technology is a line of inquiry that asks why and how nature is made operational in specific ways serving particular interests. SST forces us to rethink what we mean when we speak about technology. Technology is never just technical but combines what is possible in terms of the engineering techniques of a time and what is desireable in a certain socio-historic context. Technologies do not just exist as technical artefacts but imply certain forms of social organisation which they help to create and maintain and on which they also depend. Therefore we should better think of technologies as techno-social artefacts. Those artefacts are not merely things - dead objects - but results of and constituitive for social relationships. The development of technology and capitalism in a mutually dependend interplay has gone on over a considerably long period of time. Techno-social artefacts have been created layer upon layer. Because we have become accustomed to live with and inside techno-social systems created by capitalism, we tend to forget that they are man-made and contingent. Because they have shaped our habitat for such a long time, we see them as a sort of second nature; it is SST's task to unentangle the social content of technologies (MacKenzkie and Wajcman 1985). Because in capitalism the work of people is hidden behind fetishised commodities this task has become so hard. The ideology of technological determinism masks the social content of technology and naturalises both technological progress and the capitalist economy.
This might be due to the influence of Marshal McLuhan who is generally credited as being one of the most, if not the most influential thinker on the influence of (new) media on society in the 20th century. The theory about media and social change which he developed, influenced by Harold Innis, is epitomised by the slogan "the medium is the message". According to McLuhan the way we think is determined by the proportionate relationship of the senses - the sense ratio. He believed that all technologies were extensions of us. As tools such as the knife or the axe were extensions of our body and limbs, media technologies were extensions of our senses and central nervous system.
The introduction of a new medium which favours a particular sense was of profound influence on the patterns of perception and the way we thought. Each new medium signifies a break boundary in human history and history can be presented as a sequence of a few large chapters - from oral culture to script, to print, to electronic culture. When we moved from an oral culture to a culture based on script we exchanged an 'ear for an eye'. With writing we left behind magic and the tribal world. But only with the printing press literacy could fully develop. Modern western society is a direct result of the influence of the printing press which favours the visual sense: "Civilisation is based on literacy because literacy is a uniform processing of a culture by a visual sense extended in space and time by the alphabet (McLuhan 1964/1965, 86)." Literacy is made responsible for the homogenisation of western societies; it created the preconditions for getting people used to the clock; and it automatically led to the violent birth of nation states competing for military and industrial hegemony. From the printing press it was a logical step (a logic inherent to the technology itself) to the Fordist factory, the defining technology of modern society. But then in McLuhan’s history of civilisation, at first unnoticed, with the advent of the telegraph and electric light, then more visibly with the invention of wireless telegraphy, radio and television, the 'electric age' began. The sense ratio once more changed. We exchanged an 'eye for an ear' because electronic media foster an oral culture, and accordingly we moved forward into the past of a tribal society living in a global village
Williams criticises that in McLuhan’s theory "all media operations are in effect de-socialised; they are simply physical events in an abstract sensorium (ibid., 127)." The apparent sophistication in McLuhan's approach is that he pays tribute to the specificity of media and their characteristics. But he does so on the basis of excluding all other factors such as social, cultural, political or ethical decisions made by people who by their very nature would be open to scrutiny and questioning. Whereas the initial formulation that the medium is the message is a simple formalism "the subsequent formulation - 'the medium is the massage' - is a direct and functioning ideology (Williams 1974, 127)." Williams, in 1974, said that McLuhan's particular rhetoric was unlikely to last long. But because this particular ideological representation of technology was coming from the most powerful nation state of the world, it would have its successors (ibid. 128). Richard Barbrook (2005) set out to explore that path. In Imaginary Futures (2005) he shows how McLuhan inspired a discourse which has still a lasting influence. According to Barbrook, in 1964 the 'Commission on the year 2000', also known as Bell commission, tried to formulate a plausible alternative to 'cybernetic communism'. America was still reeling from the Sputnik shock, when the Soviet Union was first capable of sending a communication satellite into orbit. As the Cold War logic locked the nuclear enemies into an arms race, any hot war was not winnable. Therefore the only way of winning the war was by showing that America had the better ideology, that it 'owned' the future. The Bell commission took McLuhan's ideas and re-rendered them in a more rationally sounding way. It created an ideology of McLuhanism which was purged from the more eccentric aspects of its originator.
At the time when the Bell Commission formulated its thesis, The US military was pouring huge resources into artificial intelligence (AI) research. J.C.R Licklider initiated a concerted effort of academic research into computer science funded, largely, by the military. One of the many research programmes funded by Licklider led to the invention of the internet. Other research areas included interaction with a computer via a graphical user interface using first a light-pen, then a mouse, video conferencing and early forms of virtual reality. As Barbrook argues, the military origins of the net and many advancements in computer science are well known, but usually brushed aside as insignificant, thereby obscuring the fact that the imaginary future of the 1960s was still the imaginary future of today. McLuhanism, a theory which was fetishised because it had de-linked itself from its origins, promised the glorious future of a post-industrial information society.
Contemporary technosciences have abandoned the 'correspondence theory' of science which demands a truthful representation of nature. Technoscienes instead construct their objects of study, they produce artefacts and hybrids in the laboratory (or on the computer) and then examine them. This method of constructivism is constituitive for the methodology of technoscience as well as for its understanding of nature (Weber 2003, 132). Over the course of the 20th century technoscience develops a radically new understanding of nature, of mind and of what is life. Natural systems of order and architectures which had been seen as unchangeable become historicised and open for modification. The dynamisation and historisation of the concept of nature implies that nature is becoming dynamic and self-organising. As cognition is increasingly recognized as being constructed, nature itself is also understood as an organising and constructing entity (Weber 2003). This shift is not marked by a break with the modernistic past, but by a radicalisation of some of its tendencies (Weber 2003, 136). It keeps some of modernism's epistemological foundations, which were used by science but not made explicit (tinkering, purposeful manipulation), but it takes them to the extreme and makes them more visible. Technoscience continues the logic of modern science by keeping a distanced relationship to nature, which is founded on a deep distrust of the possibility of gaining direct knowledge of nature and world. As Jutta Weber (2003) puts it, science can only explain how things work, not why and what for. Science does not answer ontological questions, because it is based on a deep ontological split in its very foundation (the cognitive subject vs. lifeless matter). This would not be a problem if the experimental and constructivist character of science was generally acknowledged, as an activity of humans under given socio-historic circumstances. Under such premises science cannot be expected to give answers which are eternally and universally true. This should be seen as a liberation of science from political demands, not as a weakening of its epistemological foundations (Latour 1999). But unfortunately science carries the historic baggage of objectivity and therefore technoscience turns into a battle ground over social power. There is a strong tension between the changed ontological foundation of technoscience and its continued naturalising rhetoric about nature. According to the representational strategy of technoscience nature has become a generalised formal system for processing algorithms and information. Nature has turned into a giant universal computer which transforms information, which is immaterial and free of context (Weber 2003, 220).
Instead of making it clear that this marks a fundamental ontological shift, technoscience sticks to naive concepts of realism and hides behind veils of mystification. Even though nature is no longer its object, strictly speaking, just its material, it nevertheless still uses nature as important legitimising and ideological entity. To the outside world technoscience presents itself as the science of old, involved in a disinterested pursuit of truth. Technoscience does only what nature always has done, the apologists of technoscience say. Technoscience, by creating new disciplines such as artificial intelligence (AI) and artificial life (AL) does nothing else but applying the 'principles of life' in artefacts. This is possible because 'in principle' organisms function like that flexible chameleon computer. The organising, saving, modifying and re-disseminating of information are being declared to be life's indispensable characteristics, characteristics which are fortunately shared by computers. In a tricky mimetic movement the specific qualities of the universal calculating machine and its software applications become essential characteristics of life (Weber 2003, 176). So, a reversal of principles happens, nature gets naturalised, reified, nailed down by technoscience. Value free and objectively science has to say what is the nature of nature, the nature of man, the nature of woman, and by doing so, our place in the world gets objectively determined. The narrative strategies of naturalism, biologism and positivism can be seen as 'manuals' how to declare nature to become the only foundation for norms and values (Weber 2003, p.40) - but of course this is nature as analysed, segmented, augmented, sliced and stitched together by reductionist and male dominated science. As many feminist studies of science have shown, biologism has served to legitimise the hierarchical structuring of gender relations (cf. Haraway 1985, 1996, Weber 2003). Naturalising strategies turn social relationships into matters of objective truth.
The re-interpretation of the concepts of nature, mind and life was made possible by the development and convergence of the paradigms of cybernetics, information theory and computer science. Alan Turing formulated the theory of the universal symbolic processing machine - the theoretic principles behind the computer. Of special significance is the separation of hardware and software. One and the same apparatus (mechanical and electronical) can be used to process any kind of algorithm. This introduces the new category of trans-classic machines. Earlier, machines could essentially only perform tasks they were specifically made for. The trans-classic machine can perform any operation that can be formulated as an algorithm. Claude Shannon, aided by Warren Weaver, formulated a mathematical theory of information which separated the content of communication from its carrier medium. Shannon was explicitely only concerned with the optimisation of the transmission of data via electronic networks, independently of the content of the data. Nevertheless Shannon's model was extended into a general model of communication.
Shannon's model contained an element of feedback which allowed for error correction. Aspects of two-way communication involving feedback mechanisms within machines, animals and humans also were of central concern to Norbert Wiener's cybernetic theory. At around the same time biology turned its focus to the molecule, following the reductionist strategy of science, yet also recognising properties of living things as systems understood according to the cybernetic paradigm (Weber 2003). The cross-fertilisation of those theories led to a new understanding of life as patterns of information (code) independent of the carrier medium (matter, hardware, the body). Life was no longer thought to be a property of matter but one of structure, a pattern of information, represented in the genetic code. During the second half of the 20th century technoscience rewrote body as text, used the metaphor of the immune-system and re-invented the self as (genetic) code (Weber 2003, 196-202). A key concept in the construction of this new paradigm are 'cellular automata', an idea of John von Neumann, inspired by an earlier text by Turing. Those are 'finite state machines' based on an on-off logic. Von Neumann took also inspiration from work by Warren McCulloch and Walter Pitts (Weber 2003, 160-196). They tried to develop a mathematical model for nerve functions and interpreted neurons in the brain according to an on-off logic. In the 1940s von Neumann tried to develop a computer model (on paper) which could simulate a biological neural network. Decades later, with progess in computing power and new programming techniques his concept could finally be realized. In the 1980s von Neumann's cellular automata, advances in neuroscience and computing (parallel processing) inspired 'connectionism' - a brain-computer analogy based on an assumed analogy between the network of neurons in the brain and the interaction of cellular automata in parallel processing computers - so called neural networks (Turkle 1995).
Another key development was the attempt to create a computer based artificial intelligence (AI). Since the late 1950s AI tried to construct machines which were intelligent, whereby a limited notion of intelligence was applied which prioritized symbolic operations and logical thought (Edwards 1996). Major funding for this project came from the US military. Following a 'closed worlds' logic of containment during the climactic years of the Cold War, the military tried to eliminate the slow and error prone human factor from decision making in fully automated and closely corresponding weapon systems involving early detection radar systems (SAGE) and nuclear retaliation capabilities by intercontinental ballistic missiles (Edwards 1996). The project of AI had the not so insignificant side effect of channeling massive financial resources into nascent computer sciences and build ever faster supercomputers (ibid.).
The project of AI was of significant influence on areas it came in touch with. Psychology, which until the 1950s had been dominated by behaviourism, now dared to turn its attention to internal states of mind. A new type of psychology was invented, which described inner states in terms of rules and logic - cognitive science. According to Sherry Turkle computer science was its 'sustaining myth' (Turkle 1996, 128). A sustaining myth is not an explicit part of a theory but an unacknowledged assumption which is called upon in representative strategies. To serve the specific needs of AI, the discipline of linguistics was reshaped as computer linguistics (Edwards 1996). As the objectives of scientific disciplines were redefined and new sciences were created the understanding of the human mind was fundamentally reshaped. Thinking became an act of information processing. The act of creating an 'intelligent' computer implied that intelligence was a function of computation. Joseph Weizenbaum, a professor in the Department of Engineering and Computer Science at the MIT, became the first prominent critic of the mechanistic approach to concepts such as mind, intelligence and consciousness. As a rather lone voice in the 1960s within the computer science community his criticism of the trivialisation of life was of no big effect.
AL and the closely related field of emergent artificial intelligence (AI) were developed in the 1980s through combined theoretical and practical efforts in computer science, cybernetics and biology rebranded as life science. The cybernetic paradigm had made possible the parallelisation of nonorganic and organic systems as open and changeable systems. Both, organic and non-organic systems can be conceptualized as consisting of variable components whose properties can be formulated according to communication- and information-theoretical models. "This makes not only possible the technologisation of the living but also the making seem alife of technology [trans.A.M.] (Weber 2003, 139)." When life basically can be described as a pattern of information - the genetic 'code' -, then information can also be seen as alife (Yoxen 1986, Weber 2003). Under this basic premise the new disciplines of emergent AI and artificial life (AL) were developed. Using new programming techniques such as genetic algorithms,8 life-like phenomena were simulated inside computers. Some scientists such as Richard A Langton and Tom Ray stand for a 'strong' approach in AL (Turkle 1995, Reichle 2005). They do not interprete replicating pieces of code as reasonable simulations of life but as living beings in the literal sense. A similar trend is observable in so called bottom-up robotics (Steels 1999, Brooks 2002) whereby robots are programmed to develop forms of emergent behaviour. Emergence is a key concept in AL and the related area of emergent AI. It means that systems are capable of arriving at a higher level of organisation spontaneously. That qualitative leap can not be programmed into systems from the top-down but can only emerge from the interaction of individual pieces of code known as 'agents' in a bottom-up way. Those software agents - strings of code that represent relatively simple actions and behaviours - are called autonomous agents. They have been designed by a programmer initually but equipped with ways of 'learning' their future interactions are not predictable. Emergent behaviour can be simulated in digital systems and also in robot systems conceptualized as embodied AL.
Technoscience is more than the activity of researchers doing their work. It is also projecting an image of itself to key audiences and the public at large. This discourse, inspired by science, but going beyond it, uses narrative strategies aimed at persuading the world that its actions are not only justified but necessary. It uses scientific findings, popular science, visual means (computer graphics, animations) and sensational announcements to shape the image of technoscience. All elements of this discourse together, and the sort of images and intellectual representations it creates, are referred to as the techno-imaginary.
Technoscience claims to be doing nothing but its job, but is actually massively involved in representational politics, not only with its practices and interventions but also its promises. According to Haraway, the promises of technoscience make its main social importance. "It does not matter if they ever get realized, what matters is that those ideas always remain alive in the timezone of unbelievable promises (Weber 2003, 144)." Actually, it is better if those promises never get realized, so that the expectation can be kept alive. The prophecies of technoscience about the future already shape the present. And, as Richard Barbrook points out, current activies in technoscience and related discourses in academic writing and the press are shaped by the technoimaginary of the past (Barbrook 2005). Some of the more extreme threats and promises of technoscience's need to be seen in the light of this strategy of the unfulfilled prophecy. The techno-imaginary projects futures in a grey zone between science and science fiction. Biologists in search of the 'secret of life' promise to slow down the ageing process so that the life-span could extend to hundreds of years and potentially, immortality. The robotics scientist Hans Moravec has predicted that robot intelligence would soon overtake human intelligence and render human life meaningless unless we decide to become robots, or cyborgs, too (Moravec 1998). The same author also wishes to upload himself to a main frame computer and continue a life freed from the fetters of bodily existence, not unsimilar to his collegue Marvin Minsky, who, like other cyber-Platonists, suggests that the body is only a burdon without which we would do better. Tom Ray's Tierra project was already mentioned. Here, small bits of code forming an 'information ecology' competing for resources inside a computer's RAM (rapid access memory), are considered to be new forms of life. Those proposals are easily dismissible as fantasmagories, yet they serve an important function within the discourse of the techno-imaginary by diverting the attention from the mainstream discourse of technoscience, which, on closer looks, tries to appear more rational yet is based on similar fundamental shifts in the understanding of the being of nature and humans.
When I reviewed the literature on media art, it became apparent that there is a problem with finding systems of classification, of categorisation and even a clear definition of the art form. Despite a 25 year history of media art, and some would say it's much longer, this work is only just beginning. "The terminology for technological art forms has always been extremely fluid" says Christiane Paul (2003). According to her, 'digital art' has first been called computer art, then multimedia art and is now subsumed under the umbrella term 'new media art' (Paul 2003, 7). Other words which have been used to refer to the field as a whole or to sub-genres of it are: electronic art, art & technology, video art, software art, net art, generative art, information art, virtual reality art, game art, tele/robotics art, hypermedia, hypertext, interactive installation. Potentially this list could be much longer. The choice of different terms for more or less the same thing often betrays a preference for a certain flavour: someone is speaking historically situated and from a specific theoretic or artistic perspective - Bourdieu's position taking. For instance, while some artists are happy being labelled as net artists, others prefer to talk about telematic art, whereby the latter appears to give the field more gravity.
It is not a diffuse 'essence' of media art which justifies it to speak of it as a separate field but the existence of a system of institutions which are more or less exclusively concerned with it. Institutionally media art is characterised by the existence of two types of institutions. On one hand there are large festivals, such as Ars Electronica, since 1979 held in Linz, Austria, and large brickand-mortar institutions such as the ZKM in Karlsruhe, which attract major funding, organise big exhibitions and produce heavy catalogues. On the other hand there are many small institutions, sometimes called 'self-institutions'11 - so called media labs or hack labs - which have been thriving over the last 10 years, forming an alternative or 'unstable' field (Druckrey 2005) with increasingly world-wide connections and a more decentralised and networked approach. Whereas the large institutions face typical pressures for legitimisation such as demands to be instrumental in regional development, the world-wide network of small institutions often lives on shoe-string budgets mostly provided by state funding agencies. Some activities are not funded at all or are rather selffunded - made possible by the energy and work of participants. According to Bourdieu this area could be called a field of restricted production. Economically it is insignificant but discursively it is important. I am not trying to construct a binary opposition between two types of institutions and acknowledge the existence of many medium sized institutions and a lively transfer between the fields. However, it is important to state that there is an institutionalised field and that it is not homogenous but heterogeneous.
Histories of media art are put into a trajectory of the genealogy of media technologies rather than art history. In The Automation of Sight: From Photography to Computer Vision, Lev Manovich (1996), draws a direct line from the invention of perspective to computer generated images. He also places this trajectory within a history of automation. "By automating perspectival imaging, digital computers completed the process which began in the Renaissance (Manovich 1996, 231)." But, as Manovich points out, the inventor of the algorithm which makes perspectival rendering on computers possible, Lawrence G.Roberts, had a 'more daring goal' in mind than creating a tool for art. The computer should not only be able to render but also to 'understand' 3-D images (through pattern recognition). Thus, the project of 3-D computer generated images was a part of the project of AI in the context of the Cold War. Yet Manovich portrays this in an euphemistic language, presenting computer vision as "the culmination of at least two histories, each a century long" (ibid., 233), the history of mechanical devices designed to aid human perception, and the history of automata. Manovich does mention that the history of automation is situated in the context of rationalisation in the industrial process and that the Czech word Robot means forced labour, yet he does not spell out what this means. Instead he celebrates 3-D imaging as technology's inevitable progress. Siegfried Zielinski comments on this frequently encountered narrative strategy.
Zielinski demands that we should not continue to find the old confirmed in the new (Zielinski 2002, 11). In those readings, history turns into a promise of continuity, a celebration of progression. He thinks that this is boring as well as paralysing for the work of the mediaarchaeologist. He demands instead to find the new in the old, to let ourselves be surprised and not just look for confirmation of what we already know. As a counter-strategy Zielinski proposes the concept of a 'deep time of media' in the form of a un-archaeology (ibid., 13) which opens up spaces for the imagination. Too quickly we tend to orient ourselves toward a new 'master medium' after which all symbolic systems have to be re-arranged, until the next master medium arrives (ibid., 17).
Christiane Paul presents a slightly different trajectory. She claims that "the notion of interactivity and 'virtuality' in art were explored early on by artists such as Marcel Duchamp and László Moholy-Nagy in relation to objects and their optical effects (Paul 2003, 13)." According to Paul, Duchamp's work was "extremely influential in the realm of digital art" because of the "shift from object to concept" (ibid., 13). Paul formulates a genealogy of digital art slightly different from Weibel's or Manovich's, emphasising the influence of Duchamp via OULIPO, a French literary group, to Fluxus and conceptual art. The conceptual 'link' here is that Dadaists, the OULIPO writers and Fluxus artists frequently created art works which were based on the execution of a set of instructions and/or rules, which can be compared to computer algorithms, which are, conceptually speaking, nothing else but sequences of instructions carried out in loops (ibid., 13). This view is supported by Peter Suchin who argues that the art of the 1960s, "institutionalised under the collective heading of 'Conceptual Art' and its legacies [...] is a key determinant of today's new media art practices." (Suchin 2004, 67) Other conceptual links between contemporary media art and art movements in the past focus on the exhibition Cybernetic Serendipity, at the ICA, London 1968, (Paul 2003, 18), as well as on the exhibition 'Software Art', curated by Jack Burnham in 1970. Younger artists who are now using the term 'software art' for their own work are claiming Burnham's show as a conceptual predecessor (Goriunova and Shulgin 2003). However, there is no continuity between the surge in cybernetic art in the late 1960s and the reappearance of the 'cyber' paradigm in the 1980s. There is even less continuity between Burnham's Software Art show which remains an early and isolated example from which there can be traced no continuous line toward software art in 2005. Thus, when such long jumps are being made, it is reasonable to assume that a desire for historic legitimisation is at work. Such moves can also be seen as following the logic that Bourdieu describes in Principles for a Sociology of Cultural Works.
Despite that there are obviously problems with history writing, I want to present a short overview of themes and positions which have been taken in a 'deep' history of media art. The most obvious theme is the notion of techno-utopianism. Hereby I differentiate between a totalitarian technoutopianism and a more participatory or democratic form of utopianism. Futurism, Suprematism and Constructivism formulated a programme of techno-utopianism which demanded that artists should use science and technology to help create the utopian society populated by the new man. Humanity reinvents itself based on the powers of science and technology. Art is carrying the banner of an utopianism which is totalitarian. The poetic writing of Khlebnikov about radio which becomes 'the heart and brain of the future society' (Khlebnikov 1921/2005) is characterised by a one-size-fits-all solution. Similar sentiments and totalitarian leanings are contained in the radio manifesto by Marinetti and Masnata (1933/1992). The high point of modern art was also the high point of modernity. Connecting new communications media with a utopian vision of society was not exclusive to art but prevalent in societies both sides of the Atlantic. Between 1900 and 1939 techno-utopianism was a common strand independently of the political ideology. While the Russians supported communism and Marinetti supported Mussolini, other techno-utopians such as Marconi and Edison built monopolistic business empires. In Not Just Another Wireless Utopia I have compared the different utopian visions competing at around 1900 and how they relate to the utopianism surrounding the internet and mobile telephony (Medosch 2004). Linking media with utopian social ideas is not unique to the 20th century. Richard Barbrook traces the political roots of media totalitarianism in the name of media freedom back to the French Revolution. The Jacobean's idea of media freedom was that the revolutionary elite needs to tightly control all media (in those days the press) because unfortunately the masses are not yet able to act in their own best interest.
In the 1980s a particular type of media art started to gather momentum. The media art scene, for which the Ars Electronica Festival in Austria provided a platform, became increasingly international, with contributions from Japan, Brazil, Australia, Canada, the USA and Europe. Changes in the thematic orientation of the festival allow charting the rise of this new type of media art. In the early 1980s the festival presented itself as an odd mix with some pioneers of cybernetic art such as Herbert W. Franke and Otto Piene attending, but also spectacles aimed at winning over large audiences, musical concerts and even operatic productions in public space involving workers from a local steel factory. As Peter Weibel recalls, in those early years Ars Electronica had been almost everything it possibly could have been, "an Ars Metallica, an Ars Pneumatica, an Ars Pyrotechnica, only not an Ars Electronica (cf. Weibel 1999, 72)." This changed, according to Weibel, when he and Gottfried Hattinger gained more influence on the festival's direction from 1986 onwards. In 1987 Ars Electronica for the first time had a theme, The Free Sound. In 1989 networks became the festivals theme for the first time with In the Network of Systems. In 1992 Peter Weibel took sole responsibility for the festival's artistic direction and presented the scientific disciplines endophysics and nanotechnology as the festival's themes. In the following year the theme was Artificial Life - Genetic Art (cf. Weibel 1999, 72-74). By that time a certain type of media art, which I call high media art, was established as the leading paradigm. In the decade between 1985 and 1995, high media art developed its forms, its milestone works and its narrative strategies, which altogether were successfully deployed in institution building. I call this form high media art for two main reasons, because it resonates with high-tech as well as with high-brow or high-cultural values. For the realisation of those works expensive and complex technology was used - which implies issues of accessibility, inclusiveness and structural dependencies. The works themselves usually presented themselves as clean, large scale productions in a sterile technological atmosphere. The dirt of the streets, so present in Gibson's Neuromancer (Gibson 1984), was rarely to be felt at high media art exhibitions. The digital aesthetics of high media art was compatible with the black cube inside the white cube of the museum: as viewers enter a darkened room with multiple projection screens they are made aware that they are entering the holy inner sanctum of a shrine to the digital; the same aesthetics is also compatible with corporate lobbies or boardrooms, with steel, glass and transparency. As the yet to be built institutions will cost a lot of money, the institutional projects have to be pitched at the highest level in industry and politics, and those circles need to get assured that they are getting value for money - high-tech, high-end art.
Shown at Ars Electronic in 1993, which had the theme Artificial Life - Genetic Art, Sommerer and Mignonneau's Interactive Plant Growing was considered one of the most advaned works of art in this genre. They went on to create Trans Plant (1995-1996) and Life Spacies (1997), works which are variations on the theme of AL. Trans Plant was realized at the ATR - Advanced Telecommunications Research Laboratory - in Kyoto, where the artists hold a research residency, and was commissioned for the permanent collection of NTTICC, Tokyo (Reichle 2005). In this work the artists used a 3-D video-key technology which they had developed themselves and which they patented. As soon as the user enters the exhibition space an image of her within a threedimensional jungle of virtual plants is created. Movements of the user influence the plant growth. The works of Sommerer and Mignonneau are shown around the world and considered to be realising many demands of the paradigm of high media art. They do not create static objects but a framework for a processual exchange between viewer and art work. In most of their works, if there is no viewer, nothing happens at all. The result of the interaction appears to be indeterministic to a certain degree. AL algorithms formulate local rules of interaction so that the exact result cannot be predicted. However, whereas the growth of plants appears to be spontaneous, the extent to which the viewer can actually 'interact' with those virtual worlds is very limited. The framework has been entirely defined by the artists and the viewer/user is left with a few simple forms of physical interaction such as moving hands over a plant or moving the body in front of a screen. The 'message' of the work appears to be very similar to the message of the discipline of AL as a whole: the principles of life can be replicated in digital code. The major difference then is that the artists have developed more beautiful visuals than the scientists. Instead of challenging the naturalistic assumptions behind AL, artists lend their aesthetic skills to the illustration of science and thereby help to advertise the achievements of technoscientific progress.
The claim that 'the world itself' was 'digitally organised' belongs to the class of strong ontological statements. Weibel echoes Flusser's theory of 'digital apparition' (Flusser 1996) where he writes that there was no ontological difference between reality and technological images such as those created by virtual reality techniques. If the latter seem 'less real' then this was only a function of their lower resolution. Turning the attention to digital apparition has the benefit of making us recognise that reality is an apparition too. "What remains is that everything is digital, i.e. that everything has to be looked at as a more or less dense distribution of point elements, of bits (ibid., 244)." Therefore, what we call real, "are those areas, those curvatures and convexities, in which the particles are distributed more densely and in which potentialities realise themselves (ibid., 244)." As Flusser suggests, this parallelism of digital and real is "the digital world picture as it is being suggested to us by science (ibid., 244)." High media art falls in line with an explanation of the ontological status of the world provided by technoscience.
Noticable is the difference between the Ascott of 1984 (Grundmann ed. 1984) and of 1996 who is increasingly working with esoteric concepts. There are many religious motives in high tech spiritualism, not all Christian. A deeper study of religious motives in the discourses of the technoimaginary is not the focus of this paper. However, ideas of a ghost in the machine or the idolatry of machinery are too many to be ignored. High-tech new age spiritualism is pervasive in discourses of technoscience such as AI and AL and is replicated in the works and words of some high media artists such as Roy Ascott.
The discourse on high media art ignores the conditions of its own creations. Nobody seems to be uncomfortable with the degree to which the artists are dependent on the computer industry and technology corporations such as Deutsche Telekom, NTT or Canon. Artists producing work at the highest technical and aesthetic level need to get supported by institutions. They need to accept high levels of separation of labour and bureaucratic management. To a degree, which would need to be established specifically for each work, the works are determined by the black box character of proprietary hard- and software, by the products of commercial software applications and by the skills of the actors involved. In a very direct way the art work is determined by the technologies used. Since in media art works form cannot be clearly separated from function, this is of major significance. It would be naive to assume that the institutionalised context does not have a bearing on the form and content of the art work (Mitchell 2003). The discourses on high media art and postmodernism share a strong focus on information and immateriality. This is something they have in common with technosciene which conducts its own strategy of dematerialisation. High media art provides high class illustration to both post-modern ideas and concepts of computer science, and unconditionally accepts the premises on which those are built. It fails to take a critical position against ideologies of dominance embedded in those theories. As Jutta Weber (2003) argues, although most post-modern theories are civilization critical, they are unable to challenge the ideology of technological determinism inherent to technoscience. As Barbrook (2005) points out, information becomes fetishised in those theories. The two grand narratives of modernism, Adam Smith's liberalism and Marx's analysis of the capitalist political economy agree at least insofar as they see humans at the driving seat of history. Post-modern theories, by dismissing those grand narratives, are actually saying that history was a process without a human subject (Barbrook 2005). High media art takes a similar position by insisting on the ontological status of the work of art as being digital. The seemingly progressive digital aesthetic is socially and politically conservative. The discourse of high media art endorses a version of McLuhanism which has technological determinism at its core.
The narrative strategies of Weibel, Ascott et al succeeded in institution building. But ironically, after high-media art climaxed at around 1995, it soon lost its discoursive relevance. A new paradigm unfolded with the mass popularisation of the internet. The 'really existent' internet showed to be rather different from what the gurus of cybernetic art had imagined it to be. People learned to do their email, download a video or a song, but failed to encounter the ghost in the machine or telematic consciousness. The ordinaryness of life on the net took over. Actually, nobody ever really lived 'on' the net. We learned to step back and understand that our bodies are still real. The ZKM and Ars Electronica remain powerful institutions, but the new aesthetical strategies are now developed elsewhere. Socially and politically aware artists shape the discoursive agenda outside the institutional context provided by high-media art. Weibel, just like the software giant Microsoft, had misinterpreted the relevancy of the internet. Instead of glorification of the products of multinational corporations net artists high-light the participatory culture of the internet. Microsoft quickly developed its own browser software Internet Explorer. Weibel got the guest curator Benjamin Weill to curate the large scale exhibition net_condition in 1998 (Greene 2004). But the discoursive bandwaggon had already left the station. Digital artists joined new alliances with hackers developing free and open source software and a new discourse on art, activism and free or copyleft culture florished. High media art, by winning institutional power, lost its symbolic capital. The discourse of high media art, which had all chances to do so, had not generated foundations on which it was safe to build. As I hope to have shown, the ideology of technological determinism has prevented it from doing so. High media art with its high-tech visions has won a phyrric victory. At the same time the technologisation of society continues and a strong critical art movement dealing with issues surrounding technology and society is as urgently needed as ever.
‘‘In the bubble’’ is a phrase used by air traffic controllers to describe their state of mind, among their glowing screens and flows of information, when they are in the flow and in control. Lucky them. Most of us feel far from in control. We’re filling up the world with amazing devices and systems—on top of the natural and human ones that were already here— only to discover that these complex systems seem to be out of control: too complex to understand, let alone to shape, or redirect. Things may seem out of control—but they are not out of our hands. Many of the troubling situations in our world are the result of design decisions. Too many of them were bad design decisions, it is true—but we are not the victims of blind chance. The parlous condition of the planet, our only home, is a good example. Eighty percent of the environmental impact of the products, services, and infrastructures around us is determined at the design stage. Design decisions shape the processes behind the products we use, the materials and energy required to make them, the ways we operate them on a daily basis, and what happens to them when we no longer need them. We may not have meant to do so, and we may regret the way things have turned out, but we designed our way into the situations that face us today. The premise of this book is simply stated: If we can design our way into difficulty, we can design our way out. ‘‘Everyone designs,’’ wrote scientist Herb Simon, ‘‘who devises courses of action aimed at changing existing situations, into preferred ones.’’ For Victor Papanek, too, ‘‘design is basic to all human activities—the placing and patterning of any act towards a desired goal constitutes a design process.’’ Designing is what human beings do.
Two questions follow this understanding of design. First, where do we want to be? What exactly are the ‘‘preferred situations’’ or ‘‘desired goals’’ that Simon and Papanek talk about? Second, how do we get there? What courses of action will take us from here to there? Although this book addresses those two questions, it is not about the future, and it is not really about the new. I have organized the chapters that follow around ten themes that deal with daily life as it is lived now—not around fantastical science fiction futures. And I will tell you about aspects of daily life in which radical innovation is already emerging: Nothing you read here is a promise or a fantasy that may, one day, come true. One of the things that drove me to write this book was boredom with the schlock of the new. Many of the ‘‘preferred situations’’ that Simon talked about already exist—but in a different and often unexpected context. One of the things you can do next Monday morning, after reading this book, is walk out of your door and take a look around. I am confident you will be surprised by the variety of social innovation taking place in your environment. I have been. That said, addressing the question ‘‘Where do we want to be?’’ brings us up against an innovation dilemma. We’ve built a technology-focused society that is remarkable on means, but hazy about ends. It’s no longer clear to which question all this stuff—tech—is an answer, or what value it adds to our lives. Too many people I meet assume that being innovative means ‘‘adding technology to it.’’ Technology has become a powerful, self-replicating system that is accustomed to respect and receives the lion’s share of research funding. In NASDAQ, tech even has its own stock exchange.
I do not suggest that we have fallen out of love with technology, more that we are regaining appreciation and respect for what people can do that tech can’t. Throughout the modern age we have subordinated the interests of people to those of technology, an approach that has led to the unthinking destruction of traditional cultures and the undermining of forms of life that we judged, once, to be backward. The victims of this approach to modernization have not just been hapless people in rain forests. ‘‘Getting people to adapt’’ to new technology has affected us all. We believed that the assembly line and standardization would make the world a better place, yet along with efficiency came a dehumanization of work. We act no less as slaves to the machine today when we lambaste teachers as ‘‘obstacles to progress’’ when they do not embrace the latest technological fix for education.6 The introduction of a new mass technology—telegraph, railway, electrifi- cation, radio, telephone, television, automobiles, air travel—has always been accompanied by a spectacular package of promises. A certain naı¨vete´ is excusable for the inventors of those early technologies: They had no way of knowing about the unforeseen consequences of their innovations. Today, we don’t have that alibi. We know that new technologies have unexpected consequences.7 The worst kind of tech push combines irresponsibility with wishful thinking. One of the worst current offenders is biotech. When Eugene Thacker (no relation) studied the biotech industry for a book he was writing, he encountered ‘‘blatant disparity between hyper-optimism and an overall lack of concrete results.’’ The future promises of biotech are many and far reaching, but Thacker could not help noticing the comparative absence of any concrete, widespread, sustainable results of the application of biotech in medicine and health care. We are victims, says Thacker, of ‘‘biotech imagineering’’ by vested interests that participate in the assemblage of enticing future visions.
Being skeptical about technology does not mean rejecting it. There’s a lot of technology in this book. For one thing, we don’t have an either/or choice: Terra firma, and terabits, are both here to stay. Broadband, smart materials, wearables, pervasive computing, connected appliances, and other stuff we don’t know about yet will continue to transform the ways we live. The question is, how? Means and ends have lived apart too long in discussions of innovation. Understanding why things change—and reflecting on how they should change—are not separate issues. In the pages that follow, I try to reframe issues of technology and innovation in ways that make it easier for nonspecialists to engage in meaningful dialogue—as things happen. Theodor Zeldin calls this the transition from an age of specifications to one of deliberation.10 We cannot stop tech, and there’s no reason why we should. It’s useful. But we need to change the innovation agenda in such a way that people come before tech. It will be an ongoing struggle, of course. From nineteenth-century mill owners to twentieth-century dot-commers, businesspeople have looked for ways to remove people from production, using technology and automation to do so. A lot of organizations will continue on this path, but they’re behind the times. This book is about a world in which well-being is based on less stuff and more people. It describes an approach to innovation in which people are designed back into situations. In these situations, we will no longer be persuaded that to be better off, we must consume more trashy products and devices. The following pages describe the transition, which is already under way, from innovation driven by science fiction to innovation inspired by social fiction. I’ve collected the best examples I could find of designed services and situations in which people carry out familiar, daily-life activities in new ways: moving around, learning, caring for each other, playing, working. Some of these services involve the use of products, or equipment, to carry them out. This equipment ranges from body implants to wide-bodied jets. But objects, as a rule, play a supporting role. New principles—above all, lightness—inform the ways they are designed, made, used, and looked after. The design focus is overwhelmingly on services and systems, not on things.
One issue we need time to reflect on concerns the sheer number of people we have in the world. The planet’s population has doubled in my generation’s lifetime—something that never happened to a generation before. You and I are the first human beings who have had to adjust to such an explosion of numbers. And yet we persist in the pursuit of ‘‘laborsaving’’ devices and services—using tech as the means. It’s not that we’re dumb. On the contrary, many millions of people have exerted great intelligence and creativity in building the modern world. It’s more that we’re being swept into unknown and dangerous waters by accelerating economic growth. On just one single day of the days I have spent writing this book, as much world trade was carried out as in the whole of 1949; as much scientific research was published as in the whole of 1960; as many telephone calls were made as in all of 1983; as many e-mails were sent as in 1990. Our natural, human, and industrial systems, which evolve slowly, are struggling to adapt. Laws and institutions that we might expect to regulate these flows have not been able to keep up. A good example is what is inaccurately described as mindless sprawl in our physical environment. We deplore the relentless spread of low-density suburbs over millions of acres of formerly virgin land. We worry about its environmental impact, about the obesity in people that it fosters, and about the other social problems that come in its wake. But nobody seems to have designed urban sprawl, it just happens—or so it appears. On closer inspection, however, urban sprawl is not mindless at all. There is nothing inevitable about its development. Sprawl is the result of zoning laws designed by legislators, low-density buildings designed by developers, marketing strategies designed by ad agencies, tax breaks designed by economists, credit lines designed by banks, geomatics designed by retailers, data-mining software designed by hamburger chains, and automobiles designed by car designers. The interactions between all these systems and human behavior are complicated and hard to understand—but the policies themselves are not the result of chance. ‘‘Out of control’’ is an ideology, not a fact.
To do things differently, we need to perceive things differently. In discussing where we want to be, breakthrough ideas often come when people look at the world through a fresh lens. One of the most important design challenges I pose in this book is to make the processes and systems that surround us intelligible and knowable. We need to design macroscopes, as well as microscopes, to help us understand where things come from and why: the life story of a hamburger, or time pressure, or urban sprawl. Equipped with a fresh understanding of why our present situations are as they are, we can better describe where we want to be. With alternative situations evocatively in mind, we can design our way from here to there. Macroscopes can help us understand complex systems, but our own eyes, unaided, are just as important. All over the world, alternative models of organizing daily life are being tried and tested right now. We just need to look for them. When Ezio Manzini ran design workshops in Brazil, China, and India to develop new design ideas for an exhibition about daily life, he encountered dozens of examples of new services for daily life he had never thought of before—and also new attitudes. In many different cultures, he discovered, ‘‘an obsession with things is being replaced by a fascination with events.’’ Both young and old people are designing activities and environments in which energy and material consumption is modest and more people are used, not fewer, in the ways we take care of people, work, study, move around, find food, eat, and share equipment. In a less-stuff-more-people world, we still need systems, platforms, and services that enable people to interact more effectively and enjoyably. These platforms and infrastructures will require some technology and a lot of design. Some services will help us share the load of everyday activities: washing clothes on the roof of apartment blocks, looking after children, communal kitchens and gardens, communal workshops for maintenance activities, tool and equipment sharing, networks and clubs for health care and prevention. The most important potential impact of wireless communications, for example, will be on the resource ecologies of cities. Connecting people, resources, and places to each other in new combinations, on a real-time basis, delivers demand-responsive services that, when combined with location awareness and dynamic resource allocation, have the potential to reduce drastically the amount of hardware—from gadgets to buildings—that we need to function effectively.
There are many things wrong with design in our world, but designers, as a group of people, are not the problem. Thirty years ago, in Design for the Real World, Victor Papanek observed that ‘‘there are professions more harmful than industrial design—but only a few.’’ This kind of blaming and shaming is counterproductive and unjustified. The world contains its share of selfish and incurious designers, of course. But no designer that I ever met set out to wreck the planet, force us to eat fast food, or make life miserable. Our dilemma is that small design actions can have big effects—often unexpectedly—and designers have only recently been told, with the rest of us, how incredibly sensitive we need to be to the possible consequences of any design steps we take. Another reason not to blame designers for our ills is that many of them are working hard, right now, to fix them. They are designing new services and systems that are radically less environmentally damaging, and more socially responsible, than the ones we have now. This book contains many examples of their often-inspiring work. But the challenges and opportunities that face us will not be solved by designers acting on our behalf. On the contrary: As we suffuse the world with complex technical systems—on top of the natural and social systems already here—old-style top-down, outside-in design simply won’t work. The days of the celebrity solo designer are over. Complex systems are shaped by all the people who use them, and in this new era of collaborative innovation, designers are having to evolve from being the individual authors of objects, or buildings, to being the facilitators of change among large groups of people. Sensitivity to context, to relationships, and to consequence.
The chicken breast packets in my supermarket in Amsterdam bear a photograph of the Swedish farmer who rears the birds. He is leaning on the wooden fence of an attractive-looking farm. Behind him are blue sky and green trees. The label recounts a little story about the town where the farmer lives. Before you ask: No, they don’t show a picture of the exchicken itself—but I’m nonetheless intrigued. What’s going on here— why am I being provided with this background information? It’s a packet of chicken, not a package holiday. My questions contain their own answer. The farmer’s locality has become as much a product as the chicken’s leg. The legs of dead chickens look and taste pretty much the same, and it’s a challenge to make each one look attractive and different. Human beings and places, on the other hand, are different from one another. Associate your product with nice people, and a nice place, and it should do well. My chicken-in-a-context is an example of how the focus of both business and social innovation is shifting from locomotion—sourcing things in poor places and shipping them to rich ones—to locality. Authenticity, local context, and local production are increasingly desirable attributes in the things we buy and the services we use. Local sells, and for that reason is a powerful antidote to mobility expansion. But design to enhance locality is easier said than done. Localities contain a lot of nature, for example, and nature is the result of millions of years of iterative, trial-and-error design. Biologists describe as choronomic the influence on a process of its specific context. Choronomy adds value, but often in ways we do not yet understand. Janine Benyus counsels humility in the face of how little we know about even small natural locations.
Social contexts, too, are more complicated the closer you look. The kind of design that focuses on the shape of buildings or that draws thick lines across maps with a felt-tip pen, reconfiguring whole neighborhoods at a stroke, is not well-suited for local situations. The lesson is that design for locality is not about a return to simplicity; it involves dealing with more complexity, not less. Locality matters not just as a place to sell things, but as a medium of innovation. Social contexts, for example, determine the ease with which new ideas, trends, and social behavior spread through populations. ‘‘Once you understand that context matters,’’ writes Malcolm Gladwell, ‘‘you realize that specific and relatively small elements in the environment can serve as Tipping Points.’’ Disregard for context is one of the main reasons, for me, why the new economy failed. Dot-commers promoted ‘‘anytime, anywhere’’ over and above the here and now—and we didn’t buy it. As I explained in chapter , globalization brought with it numerous assertions that economic power is less and less rooted in a place. Distance is dead, geography is obsolete, the pundits declared. They argued that sophisticated distribution and logistics systems, computer-integrated manufacturing and design, and direct marketing have changed what it means to design, produce, distribute, or sell a product or service. Investor pressure to reduce costs, more or less regardless of the consequences, increased pressure on companies to move production around constantly in search of low-cost materials and cheap labor. As the distance between the producers of products or services and their users grew as a result, activities that used to be centralized downtown were steadily dispersed. Two geographers, Stephen Graham and Simon Marvin, described this phenomenon as ‘‘splintering urbanism.’’ Many cities, persuaded that they were now in competition with one another, embraced the concept of marketing. Some started to think of themselves as brands. At first, many were persuaded that snappy communications were the key to success; these places spent lavishly on logos, slogans, and corporate identities. Many of these campaigns were banal— ‘‘Glasgow’s Miles Better,’’ ‘‘EuroLille,’’ and the like—but advertising and design consultants did good business peddling these surface treatments, which persist to this day.
The trouble is that place marketers are not alone in missing this point. Cultural producers, too, are stuck in a point-to-mass mindset. I attended a meeting in Amsterdam on the subject of ‘‘hosting.’’ The invitation posed an interesting question: ‘‘What is the relationship between art biennales and their host cities?’’ Many international art power brokers turned up for this meeting, which was hosted by an organization called Manifesta. At the meeting, the curators and critics and producers seemed to be most interested in ‘‘viewers’’ and ‘‘audiences’’ and ‘‘publics.’’ It dawned on me, as I listened to the art world’s heavy hitters in action, that art has become most attractive to the interests it once ridiculed. The tourism industry loves art because its events and museums are ‘‘attractions.’’ Property developers love art because a bijou gallery lends allure to egregious projects. For city marketers, an art biennale bestows an aura of intelligence on a city. Planners are bewitched by the idea that if they can only lure the ‘‘creative class’’ to their city, their place will become more glamorous. ‘‘Our events are not summer camps,’’ pleaded Franco Bonami, director of the Venice Biennale. Bonami invited more than five hundred artists to that year’s event. But he did not mention one single word about what, if anything, these five hundred people had to say—or why the rest of us should care. After two hours I had to leave. ‘‘Hosting’’ felt like a sales meeting for Saga Holidays. So then I went to Japan where Prada, which at the time was said to be 1.5 billion euros in debt, had lavished 87 million dollars on a new Herzog and de Meuron–designed store in Tokyo. ‘‘Shopping,’’ a public relations person gushed in the press, ‘‘is the fundamental purpose of cities today.’’ In a busy Tokyo street the new store’s Plexiglas exterior, which is like bubble wrap, certainly stood out—and so it should, for that much more. A creative consultant named Christopher Everard told The Economist that ‘‘by using iconic architects, the label is building brand equity.’’ (Everard’s firm is called InterLife Consultancy. I e-mailed him the suggestion that he change its name to ‘‘Get A Life Consultancy’’—but he has not replied.) For me the Prada project smelled like the last days of Rome.
This is not to deny that the economic case for the creative industries is strong. After all, designing spectacles is big business, and tourism is a huge one. One trade fair and exhibition, called Exp, announced itself as ‘‘The Event That Defines the Experience Industry.’’ For bewildered first-time visitors, Exp conveniently divided its global industry into four domains: corporate visitor centers, retail, casinos, and museums. Exp promised to show visitors ‘‘how to gain a greater share of your guest’s discretionary time and disposable income’’; how to ‘‘destroy the myth that great experience need [sic] huge budgets’’; and ‘‘how to surf the generational shift.’’ The website for the exhibition did not mention a session on how to speak English, but so-called experience designers (in Europe, they tend to be called ‘‘interaction designers’’), undeterred, flocked to Exp. In some places, sport is replacing culture as an attractor in urban regeneration. Paris, in its bid for the 2012 Olympics, says the role that investment in sports infrastructures plays in the Games of the twenty-first century will be ‘‘comparable to that played by industrialisation at the end of the 19th century.’’ Claude Be´be´ar, chairman of the Paris Olympic Committee, does not think of sport as kicking a ball around a field. He thinks about twentymillion-dollar sponsorships and about the well-being of those who provide the spectacle. His plans for a sporty Paris, celebrated in a lavish and immense book, feature a boulevard dedicated to sport, bordered with hotels to lodge journalists, an international media center, a superdome, and the Olympic pool. Private road lanes, of the kind Stalin pioneered in Moscow, and a travel time of twelve minutes from bed to track are promised for athletes and officials. If the bed-to-track journey proves too taxing, an electronic games and Internet center will be provided to ‘‘help athletes relax and get in touch with the outside world.
It is not a question of good building, and bad. A beautiful place may never bring about an explosion of life, while a haphazard hall may be a tremendous meeting place. This is the mystery of the theatre, but in the understanding of this mystery lies the one science. It is not a matter of saying analytically, what are the requirements, how best they could be organized—this will usually bring into existence a tame, conventional, often cold hall. The science of theatre building must come from studying what it is that brings about the more vivid relationships between people.15 Many people in theater question whether new buildings are needed at all. Big theaters, in particular, tend to sap energy out of productions and money out of producers. Some producers have taken literally to the streets in so-called promenade and site-specific theater. In Chaucer-like journeys, players and audience move together around cities, through forests, up mountains, or into resonant but abandoned spaces. In the age of the rave, street-level events are everywhere: festivals, concerts, corporate events, church pageants, and fashion shows vie with each other to occupy the streets. In Europe, where theater people are leading the way to a sane policy for space planning, the term ‘‘territorial capital’’ is now being used to describe the ‘‘hard’’ and ‘‘soft’’ assets of a region. Hard assets include natural beauty and features; shopping facilities; cultural attractions; and buildings, museums, monuments, and the like. Soft assets are all about people and culture: skills, traditions, festivals, events and occasions, situations, settings, social ties, civic loyalty, memories, and the capacity to facilitate learning of various kinds. Turning the notion of territorial capital into a policy or a design program is a challenging task. EU countries are committed formally to the worthy ambition to enable a ‘‘European knowledge economy’’ by the year 2010. The problem is that these countries understand what the knowledge economy means in different ways.
The answer lies in webs, chains, and networks of cities and regions. By aggregating their hard and soft assets, collective cities—multicentered cities—can match the array of functions and resources of the metropolitan centers while still (in theory) delivering superior social quality. The ability of small cities to offer a context that supports intimacy and encounter— what the French call la vie associative—is where small-city webs will win out over the big centers. Multicity networks are not a new idea. They date back to the thirteenth century when, in the Hansa League, more than seventy merchant cities collaborated for their common good in order to control exports and imports over a wide swath of Europe. A powerful network of trading partners, with its own accounting system and shared vocabulary, the Hansa League became one of the major economic forces of the Middle Ages. At one stage it controlled much of Scandinavia, the Baltic states, northern Germany, and Poland—and outposts can be found even today as far away as Scotland and the Basque Country.18 Hanze Expo, the League’s modern incarnation, links the Baltic Rim—St. Petersburg, Tallinn, Riga, Rostock—to northern Germany and Holland. One part of this link, Estonia, has pronounced itself to be the Hong Kong of the Baltic.
For networked, multicentered localities to succeed, different kinds of territorial and social capital need to be linked by a combination of physical and informational networks. This integration of hard and soft factors is complex. For one thing, planners and policymakers have been joined by a variety of new players in a game they used to play on their own. Privatized network industries, such as railway companies, airports, electricity suppliers, and telecommunications operators, all want a say in planning discussions. So, too, do citizens. With growing confidence and sophistication, citizen groups are demanding that social agendas—such as social inclusion or environmental sustainability—be factored into planning processes. A nonprofit technology organization called The Open Planning Project (TOPP) argues that information about public places is as important a public good as the physical places themselves. TOPP advocates a free, distributed, and open geographic information infrastructure and is developing new ways to enhance the ability of all citizens to engage in meaningful dialogue about their environment. One of TOPP’s projects, a collaborative weblog called DigitalEarth.org, is conceived as a shared public online space for talking about the environmental information infrastructure. The site includes technology and tools to help citizens deploy geographic data, environmental models, and visualizations.21 The spread of open planning is a profound challenge to planning and design professionals. They are torn between the increasing complexity of the systems they have to deal with and the demand that people be put first.
The Regionmaker as one way to help designers of cities and regions cope with the new demands. ‘‘We keep getting asked to make ‘visions’ for cities and regions,’’ Winy Maas, a principal of the firm, told me, ‘‘but we want to base these on real data, not just our imagination.’’ The design challenge, for Maas, is to represent complex data about regions and cities visually, in order to provide a space in which the different actors now involved can explore options together. The Regionmaker, developed by MVRDV initially for a project called RhineRuhrCity, orchestrates a variety of existing information sources and flows—demographic data or outputs from geographical information systems (GIS) (or geomatics, as they are also called). The Regionmaker supports maps, study charts, and access to databases; imports and exports images and video feeds from helicopters or satellites; connects to the Internet; and uses computer-aided design (CAD) drawings. Maas and his colleagues plan to add to the system information on the movement of people, goods, and information. A housing subroutine will develop scenarios for optimal housing designs. A calculator will optimize natural light in built spaces. A function mixer will propose optimal mixtures of activities according to economic, social, or cultural criteria. The long-term aim is for the system to become a decision support environment in a more proactive and critical sense. ‘‘We could add an Evaluator, or an Evolver that can suggest criticism of the input we make,’’ speculates Maas. Deciding who gets to use these new tools is itself a design action. The principle of open planning is that nonspecialized actors and stakeholders are involved in the creation process, not simply as yes-no responders to precooked proposals. MVRDV’s system has the potential to enable municipalities, citizen groups, and planners to ‘‘compose’’ an optimized mixed neighborhood—but they have to be invited to do so and shown how. All of this takes commitment and time.

The other day I pulled a tattered copy of The Chomsky Reader off a bookshelf of mine. Leafing through some of the Vietnam-era essays collected in that 1987 paperback brought to life a young Tom Engelhardt who, in the mid-to-late 1960s, was undergoing a startling transition: from dreaming of serving his government to opposing it. Noam Chomsky’s writings played a role in that transformation. I stopped at his chilling 1970 essay “After Pinkville,” which I remember reading when it came out. (“Pinkville,” connoting communist influence, was the military slang for the village where the infamous My Lai massacre took place.) It was not the first Chomsky essay I had read. That honor may go to “The Responsibility of Intellectuals,” which he wrote in 1966. (“It is the responsibility of intellectuals to speak the truth and to expose lies. This, at least, may seem enough of a truism to pass without comment. Not so, however. For the modern intellectual, it is not at all obvious.”)

“After Pinkville” still remains vividly in my consciousness from that long-gone moment when a growing sense of horror about a distant American war that came to feel ever closer and more brutal swept me into antiwar activism. Its first sentences still cut to the heart of things: “It is important to understand that the massacre of the rural population of Vietnam and their forced evacuation is not an accidental by-product of the war. Rather it is of the very essence of American strategy.” Before he was done, Chomsky would put the massacre of almost 500 Vietnamese men, women, and children into the grim context of the larger crimes of the time: “It is perhaps remarkable that none of this appears to occasion much concern [in the U.S.]. It is only the acts of a company of half-crazed GIs that are regarded as a scandal, a disgrace to America. It will, indeed, be a still greater national scandal — if we assume that possible — if they alone are subjected to criminal prosecution, but not those who have created and accepted the long-term atrocity to which they contributed one detail — merely a few hundred more murdered Vietnamese.”

So many decades later, something still seems painfully familiar in all of this. Thanks in part to the nature of our media moment, we remain riveted by acts of horror committed against Europeans and Americans. Yet “concern” over what the U.S. has done in our distant war zones — from the killing of civilians at weddings, funerals, and memorial services to the evisceration of a hospital, to kidnappings, torture, and even the killing of prisoners, to drone strikes so “surgical” and “precise” that hundreds below died even though only a relatively few individuals were officially targeted — seems largely missing in action. Unlike the Vietnam era, in the present moment, lacking the powerful antiwar movement of the Vietnam era, “none of this,” to quote Chomsky, “appears to occasion much concern.” Indeed.

There are, however, exceptions to this statement and let me mention one of them. A half-century later, Noam Chomsky is still writing with the same chilling eloquence about the updated war-on-terror version of this American nightmare. His “concern” has not lagged, something that can’t be missed in his new book, Who Rules the World?, which focuses on, among other things, what in the Vietnam-era might have been called “the arrogance of power.” At a moment when the Vietnam bomber of choice, the B-52, is being sent back into action in the war against the Islamic State, he, too, is back in action. And so here is the first part of an overview essay from his new book on American power and the world.

When we ask “Who rules the world?” we commonly adopt the standard convention that the actors in world affairs are states, primarily the great powers, and we consider their decisions and the relations among them. That is not wrong. But we would do well to keep in mind that this level of abstraction can also be highly misleading.

States of course have complex internal structures, and the choices and decisions of the political leadership are heavily influenced by internal concentrations of power, while the general population is often marginalized. That is true even for the more democratic societies, and obviously for others. We cannot gain a realistic understanding of who rules the world while ignoring the “masters of mankind,” as Adam Smith called them: in his day, the merchants and manufacturers of England; in ours, multinational conglomerates, huge financial institutions, retail empires, and the like. Still following Smith, it is also wise to attend to the “vile maxim” to which the “masters of mankind” are dedicated: “All for ourselves and nothing for other people” — a doctrine known otherwise as bitter and incessant class war, often one-sided, much to the detriment of the people of the home country and the world.

In the contemporary global order, the institutions of the masters hold enormous power, not only in the international arena but also within their home states, on which they rely to protect their power and to provide economic support by a wide variety of means. When we consider the role of the masters of mankind, we turn to such state policy priorities of the moment as the Trans-Pacific Partnership, one of the investor-rights agreements mislabeled “free-trade agreements” in propaganda and commentary. They are negotiated in secret, apart from the hundreds of corporate lawyers and lobbyists writing the crucial details. The intention is to have them adopted in good Stalinist style with “fast track” procedures designed to block discussion and allow only the choice of yes or no (hence yes). The designers regularly do quite well, not surprisingly. People are incidental, with the consequences one might anticipate.

The Second Superpower

The neoliberal programs of the past generation have concentrated wealth and power in far fewer hands while undermining functioning democracy, but they have aroused opposition as well, most prominently in Latin America but also in the centers of global power. The European Union (EU), one of the more promising developments of the post-World War II period, has been tottering because of the harsh effect of the policies of austerity during recession, condemned even by the economists of the International Monetary Fund (if not the IMF’s political actors). Democracy has been undermined as decision making shifted to the Brussels bureaucracy, with the northern banks casting their shadow over their proceedings.

Mainstream parties have been rapidly losing members to left and to right. The executive director of the Paris-based research group EuropaNova attributes the general disenchantment to “a mood of angry impotence as the real power to shape events largely shifted from national political leaders [who, in principle at least, are subject to democratic politics] to the market, the institutions of the European Union and corporations,” quite in accord with neoliberal doctrine. Very similar processes are under way in the United States, for somewhat similar reasons, a matter of significance and concern not just for the country but, because of U.S. power, for the world.

The rising opposition to the neoliberal assault highlights another crucial aspect of the standard convention: it sets aside the public, which often fails to accept the approved role of “spectators” (rather than “participants”) assigned to it in liberal democratic theory. Such disobedience has always been of concern to the dominant classes. Just keeping to American history, George Washington regarded the common people who formed the militias that he was to command as “an exceedingly dirty and nasty people [evincing] an unaccountable kind of stupidity in the lower class of these people.”

In Violent Politics, his masterful review of insurgencies from “the American insurgency” to contemporary Afghanistan and Iraq, William Polk concludes that General Washington “was so anxious to sideline [the fighters he despised] that he came close to losing the Revolution.” Indeed, he “might have actually done so” had France not massively intervened and “saved the Revolution,” which until then had been won by guerrillas — whom we would now call “terrorists” — while Washington’s British-style army “was defeated time after time and almost lost the war.”

A common feature of successful insurgencies, Polk records, is that once popular support dissolves after victory, the leadership suppresses the “dirty and nasty people” who actually won the war with guerrilla tactics and terror, for fear that they might challenge class privilege. The elites’ contempt for “the lower class of these people” has taken various forms throughout the years. In recent times one expression of this contempt is the call for passivity and obedience (“moderation in democracy”) by liberal internationalists reacting to the dangerous democratizing effects of the popular movements of the 1960s.

Sometimes states do choose to follow public opinion, eliciting much fury in centers of power. One dramatic case was in 2003, when the Bush administration called on Turkey to join its invasion of Iraq. Ninety-five percent of Turks opposed that course of action and, to the amazement and horror of Washington, the Turkish government adhered to their views. Turkey was bitterly condemned for this departure from responsible behavior. Deputy Secretary of Defense Paul Wolfowitz, designated by the press as the “idealist-in-chief” of the administration, berated the Turkish military for permitting the malfeasance of the government and demanded an apology. Unperturbed by these and innumerable other illustrations of our fabled “yearning for democracy,” respectable commentary continued to laud President George W. Bush for his dedication to “democracy promotion,” or sometimes criticized him for his naïveté in thinking that an outside power could impose its democratic yearnings on others.

The Turkish public was not alone. Global opposition to U.S.-UK aggression was overwhelming. Support for Washington’s war plans scarcely reached 10% almost anywhere, according to international polls. Opposition sparked huge worldwide protests, in the United States as well, probably the first time in history that imperial aggression was strongly protested even before it was officially launched. On the front page of the New York Times, journalist Patrick Tyler reported that “there may still be two superpowers on the planet: the United States and world public opinion.”

Unprecedented protest in the United States was a manifestation of the opposition to aggression that began decades earlier in the condemnation of the U.S. wars in Indochina, reaching a scale that was substantial and influential, even if far too late. By 1967, when the antiwar movement was becoming a significant force, military historian and Vietnam specialist Bernard Fall warned that “Vietnam as a cultural and historic entity… is threatened with extinction… [as] the countryside literally dies under the blows of the largest military machine ever unleashed on an area of this size.”

But the antiwar movement did become a force that could not be ignored. Nor could it be ignored when Ronald Reagan came into office determined to launch an assault on Central America. His administration mimicked closely the steps John F. Kennedy had taken 20 years earlier in launching the war against South Vietnam, but had to back off because of the kind of vigorous public protest that had been lacking in the early 1960s. The assault was awful enough. The victims have yet to recover. But what happened to South Vietnam and later all of Indochina, where “the second superpower” imposed its impediments only much later in the conflict, was incomparably worse.

It is often argued that the enormous public opposition to the invasion of Iraq had no effect. That seems incorrect to me. Again, the invasion was horrifying enough, and its aftermath is utterly grotesque. Nevertheless, it could have been far worse. Vice President Dick Cheney, Secretary of Defense Donald Rumsfeld, and the rest of Bush’s top officials could never even contemplate the sort of measures that President Kennedy and President Lyndon Johnson adopted 40 years earlier largely without protest.

Western Power Under Pressure

There is far more to say, of course, about the factors in determining state policy that are put to the side when we adopt the standard convention that states are the actors in international affairs. But with such nontrivial caveats as these, let us nevertheless adopt the convention, at least as a first approximation to reality. Then the question of who rules the world leads at once to such concerns as China’s rise to power and its challenge to the United States and “world order,” the new cold war simmering in eastern Europe, the Global War on Terror, American hegemony and American decline, and a range of similar considerations.

The challenges faced by Western power at the outset of 2016 are usefully summarized within the conventional framework by Gideon Rachman, chief foreign-affairs columnist for the London Financial Times. He begins by reviewing the Western picture of world order: “Ever since the end of the Cold War, the overwhelming power of the U.S. military has been the central fact of international politics.” This is particularly crucial in three regions: East Asia, where “the U.S. Navy has become used to treating the Pacific as an ‘American lake’”; Europe, where NATO — meaning the United States, which “accounts for a staggering three-quarters of NATO’s military spending” — “guarantees the territorial integrity of its member states”; and the Middle East, where giant U.S. naval and air bases “exist to reassure friends and to intimidate rivals.”

The problem of world order today, Rachman continues, is that “these security orders are now under challenge in all three regions” because of Russian intervention in Ukraine and Syria, and because of China turning its nearby seas from an American lake to “clearly contested water.” The fundamental question of international relations, then, is whether the United States should “accept that other major powers should have some kind of zone of influence in their neighborhoods.” Rachman thinks it should, for reasons of “diffusion of economic power around the world — combined with simple common sense.”

There are, to be sure, ways of looking at the world from different standpoints. But let us keep to these three regions, surely critically important ones.

The Challenges Today: East Asia

Beginning with the “American lake,” some eyebrows might be raised over the report in mid-December 2015 that “an American B-52 bomber on a routine mission over the South China Sea unintentionally flew within two nautical miles of an artificial island built by China, senior defense officials said, exacerbating a hotly divisive issue for Washington and Beijing.” Those familiar with the grim record of the 70 years of the nuclear weapons era will be all too aware that this is the kind of incident that has often come perilously close to igniting terminal nuclear war. One need not be a supporter of China’s provocative and aggressive actions in the South China Sea to notice that the incident did not involve a Chinese nuclear-capable bomber in the Caribbean, or off the coast of California, where China has no pretensions of establishing a “Chinese lake.” Luckily for the world.

Chinese leaders understand very well that their country’s maritime trade routes are ringed with hostile powers from Japan through the Malacca Straits and beyond, backed by overwhelming U.S. military force. Accordingly, China is proceeding to expand westward with extensive investments and careful moves toward integration. In part, these developments are within the framework of the Shanghai Cooperation Organization (SCO), which includes the Central Asian states and Russia, and soon India and Pakistan with Iran as one of the observers — a status that was denied to the United States, which was also called on to close all military bases in the region. China is constructing a modernized version of the old silk roads, with the intent not only of integrating the region under Chinese influence, but also of reaching Europe and the Middle Eastern oil-producing regions. It is pouring huge sums into creating an integrated Asian energy and commercial system, with extensive high-speed rail lines and pipelines.

One element of the program is a highway through some of the world’s tallest mountains to the new Chinese-developed port of Gwadar in Pakistan, which will protect oil shipments from potential U.S. interference. The program may also, China and Pakistan hope, spur industrial development in Pakistan, which the United States has not undertaken despite massive military aid, and might also provide an incentive for Pakistan to clamp down on domestic terrorism, a serious issue for China in western Xinjiang Province. Gwadar will be part of China’s “string of pearls,” bases being constructed in the Indian Ocean for commercial purposes but potentially also for military use, with the expectation that China might someday be able to project power as far as the Persian Gulf for the first time in the modern era.

All of these moves remain immune to Washington’s overwhelming military power, short of annihilation by nuclear war, which would destroy the United States as well.

In 2015, China also established the Asian Infrastructure Investment Bank (AIIB), with itself as the main shareholder. Fifty-six nations participated in the opening in Beijing in June, including U.S. allies Australia, Britain, and others which joined in defiance of Washington’s wishes. The United States and Japan were absent. Some analysts believe that the new bank might turn out to be a competitor to the Bretton Woods institutions (the IMF and the World Bank), in which the United States holds veto power. There are also some expectations that the SCO might eventually become a counterpart to NATO.

The Challenges Today: Eastern Europe

Turning to the second region, Eastern Europe, there is a crisis brewing at the NATO-Russian border. It is no small matter. In his illuminating and judicious scholarly study of the region, Frontline Ukraine: Crisis in the Borderlands, Richard Sakwa writes — all too plausibly — that the “Russo-Georgian war of August 2008 was in effect the first of the ‘wars to stop NATO enlargement’; the Ukraine crisis of 2014 is the second. It is not clear whether humanity would survive a third.”

The West sees NATO enlargement as benign. Not surprisingly, Russia, along with much of the Global South, has a different opinion, as do some prominent Western voices. George Kennan warned early on that NATO enlargement is a “tragic mistake,” and he was joined by senior American statesmen in an open letter to the White House describing it as a “policy error of historic proportions.”

The present crisis has its origins in 1991, with the end of the Cold War and the collapse of the Soviet Union. There were then two contrasting visions of a new security system and political economy in Eurasia. In Sakwa’s words, one vision was of a “‘Wider Europe,’ with the EU at its heart but increasingly coterminous with the Euro-Atlantic security and political community; and on the other side there [was] the idea of ‘Greater Europe,’ a vision of a continental Europe, stretching from Lisbon to Vladivostok, that has multiple centers, including Brussels, Moscow and Ankara, but with a common purpose in overcoming the divisions that have traditionally plagued the continent.”

Soviet leader Mikhail Gorbachev was the major proponent of Greater Europe, a concept that also had European roots in Gaullism and other initiatives. However, as Russia collapsed under the devastating market reforms of the 1990s, the vision faded, only to be renewed as Russia began to recover and seek a place on the world stage under Vladimir Putin who, along with his associate Dmitry Medvedev, has repeatedly “called for the geopolitical unification of all of ‘Greater Europe’ from Lisbon to Vladivostok, to create a genuine ‘strategic partnership.’”

These initiatives were “greeted with polite contempt,” Sakwa writes, regarded as “little more than a cover for the establishment of a ‘Greater Russia’ by stealth” and an effort to “drive a wedge” between North America and Western Europe. Such concerns trace back to earlier Cold War fears that Europe might become a “third force” independent of both the great and minor superpowers and moving toward closer links to the latter (as can be seen in Willy Brandt’s Ostpolitik and other initiatives).

The Western response to Russia’s collapse was triumphalist. It was hailed as signaling “the end of history,” the final victory of Western capitalist democracy, almost as if Russia were being instructed to revert to its pre-World War I status as a virtual economic colony of the West. NATO enlargement began at once, in violation of verbal assurances to Gorbachev that NATO forces would not move “one inch to the east” after he agreed that a unified Germany could become a NATO member — a remarkable concession, in the light of history. That discussion kept to East Germany. The possibility that NATO might expand beyond Germany was not discussed with Gorbachev, even if privately considered.

Soon, NATO did begin to move beyond, right to the borders of Russia. The general mission of NATO was officially changed to a mandate to protect “crucial infrastructure” of the global energy system, sea lanes and pipelines, giving it a global area of operations. Furthermore, under a crucial Western revision of the now widely heralded doctrine of “responsibility to protect,” sharply different from the official U.N. version, NATO may now also serve as an intervention force under U.S. command.

Of particular concern to Russia are plans to expand NATO to Ukraine. These plans were articulated explicitly at the Bucharest NATO summit of April 2008, when Georgia and Ukraine were promised eventual membership in NATO. The wording was unambiguous: “NATO welcomes Ukraine’s and Georgia’s Euro-Atlantic aspirations for membership in NATO. We agreed today that these countries will become members of NATO.” With the “Orange Revolution” victory of pro-Western candidates in Ukraine in 2004, State Department representative Daniel Fried rushed there and “emphasized U.S. support for Ukraine’s NATO and Euro-Atlantic aspirations,” as a WikiLeaks report revealed.

Russia’s concerns are easily understandable. They are outlined by international relations scholar John Mearsheimer in the leading U.S. establishment journal, Foreign Affairs. He writes that “the taproot of the current crisis [over Ukraine] is NATO expansion and Washington’s commitment to move Ukraine out of Moscow’s orbit and integrate it into the West,” which Putin viewed as “a direct threat to Russia’s core interests.”

“Who can blame him?” Mearsheimer asks, pointing out that “Washington may not like Moscow’s position, but it should understand the logic behind it.” That should not be too difficult. After all, as everyone knows, “The United States does not tolerate distant great powers deploying military forces anywhere in the Western hemisphere, much less on its borders.”

In fact, the U.S. stand is far stronger. It does not tolerate what is officially called “successful defiance” of the Monroe Doctrine of 1823, which declared (but could not yet implement) U.S. control of the hemisphere. And a small country that carries out such successful defiance may be subjected to “the terrors of the earth” and a crushing embargo — as happened to Cuba. We need not ask how the United States would have reacted had the countries of Latin America joined the Warsaw Pact, with plans for Mexico and Canada to join as well. The merest hint of the first tentative steps in that direction would have been “terminated with extreme prejudice,” to adopt CIA lingo.

As in the case of China, one does not have to regard Putin’s moves and motives favorably to understand the logic behind them, nor to grasp the importance of understanding that logic instead of issuing imprecations against it. As in the case of China, a great deal is at stake, reaching as far — literally — as questions of survival.

The Challenges Today: The Islamic World

Let us turn to the third region of major concern, the (largely) Islamic world, also the scene of the Global War on Terror (GWOT) that George W. Bush declared in 2001 after the 9/11 terrorist attack. To be more accurate, re-declared. The GWOT was declared by the Reagan administration when it took office, with fevered rhetoric about a “plague spread by depraved opponents of civilization itself” (as Reagan put it) and a “return to barbarism in the modern age” (the words of George Shultz, his secretary of state). The original GWOT has been quietly removed from history. It very quickly turned into a murderous and destructive terrorist war afflicting Central America, southern Africa, and the Middle East, with grim repercussions to the present, even leading to condemnation of the United States by the World Court (which Washington dismissed). In any event, it is not the right story for history, so it is gone.

The success of the Bush-Obama version of GWOT can readily be evaluated on direct inspection. When the war was declared, the terrorist targets were confined to a small corner of tribal Afghanistan. They were protected by Afghans, who mostly disliked or despised them, under the tribal code of hospitality — which baffled Americans when poor peasants refused “to turn over Osama bin Laden for the, to them, astronomical sum of $25 million.”

There are good reasons to believe that a well-constructed police action, or even serious diplomatic negotiations with the Taliban, might have placed those suspected of the 9/11 crimes in American hands for trial and sentencing. But such options were off the table. Instead, the reflexive choice was large-scale violence — not with the goal of overthrowing the Taliban (that came later) but to make clear U.S. contempt for tentative Taliban offers of the possible extradition of bin Laden. How serious these offers were we do not know, since the possibility of exploring them was never entertained. Or perhaps the United States was just intent on “trying to show its muscle, score a victory and scare everyone in the world. They don’t care about the suffering of the Afghans or how many people we will lose.”

That was the judgment of the highly respected anti-Taliban leader Abdul Haq, one of the many oppositionists who condemned the American bombing campaign launched in October 2001 as “a big setback” for their efforts to overthrow the Taliban from within, a goal they considered within their reach. His judgment is confirmed by Richard A. Clarke, who was chairman of the Counterterrorism Security Group at the White House under President George W. Bush when the plans to attack Afghanistan were made. As Clarke describes the meeting, when informed that the attack would violate international law, “the President yelled in the narrow conference room, ‘I don’t care what the international lawyers say, we are going to kick some ass.'” The attack was also bitterly opposed by the major aid organizations working in Afghanistan, who warned that millions were on the verge of starvation and that the consequences might be horrendous.

The consequences for poor Afghanistan years later need hardly be reviewed.

The next target of the sledgehammer was Iraq. The U.S.-UK invasion, utterly without credible pretext, is the major crime of the twenty-first century. The invasion led to the death of hundreds of thousands of people in a country where the civilian society had already been devastated by American and British sanctions that were regarded as “genocidal” by the two distinguished international diplomats who administered them, and resigned in protest for this reason. The invasion also generated millions of refugees, largely destroyed the country, and instigated a sectarian conflict that is now tearing apart Iraq and the entire region. It is an astonishing fact about our intellectual and moral culture that in informed and enlightened circles it can be called, blandly, “the liberation of Iraq.”

Pentagon and British Ministry of Defense polls found that only 3% of Iraqis regarded the U.S. security role in their neighborhood as legitimate, less than 1% believed that “coalition” (U.S.-UK) forces were good for their security, 80% opposed the presence of coalition forces in the country, and a majority supported attacks on coalition troops. Afghanistan has been destroyed beyond the possibility of reliable polling, but there are indications that something similar may be true there as well. Particularly in Iraq the United States suffered a severe defeat, abandoning its official war aims, and leaving the country under the influence of the sole victor, Iran.

The sledgehammer was also wielded elsewhere, notably in Libya, where the three traditional imperial powers (Britain, France, and the United States) procured Security Council resolution 1973 and instantly violated it, becoming the air force of the rebels. The effect was to undercut the possibility of a peaceful, negotiated settlement; sharply increase casualties (by at least a factor of 10, according to political scientist Alan Kuperman); leave Libya in ruins, in the hands of warring militias; and, more recently, to provide the Islamic State with a base that it can use to spread terror beyond. Quite sensible diplomatic proposals by the African Union, accepted in principle by Libya’s Muammar Qaddafi, were ignored by the imperial triumvirate, as Africa specialist Alex de Waal reviews. A huge flow of weapons and jihadis has spread terror and violence from West Africa (now the champion for terrorist murders) to the Levant, while the NATO attack also sent a flood of refugees from Africa to Europe.

Yet another triumph of “humanitarian intervention,” and, as the long and often ghastly record reveals, not an unusual one, going back to its modern origins four centuries ago.

The question of how foreign policy is determined is a crucial one in world affairs. In these comments, I can only provide a few hints as to how I think the subject can be productively explored, keeping to the United States for several reasons. First, the U.S. is unmatched in its global significance and impact. Second, it is an unusually open society, possibly uniquely so, which means we know more about it. Finally, it is plainly the most important case for Americans, who are able to influence policy choices in the U.S. — and indeed for others, insofar as their actions can influence such choices. The general principles, however, extend to the other major powers, and well beyond.

There is a “received standard version,” common to academic scholarship, government pronouncements, and public discourse. It holds that the prime commitment of governments is to ensure security, and that the primary concern of the U.S. and its allies since 1945 was the Russian threat.

There are a number of ways to evaluate the doctrine. One obvious question to ask is: What happened when the Russian threat disappeared in 1989? Answer: everything continued much as before.

The U.S. immediately invaded Panama, killing probably thousands of people and installing a client regime. This was routine practice in U.S.-dominated domains — but in this case not quite as routine. For first time, a major foreign policy act was not justified by an alleged Russian threat.

Instead, a series of fraudulent pretexts for the invasion were concocted that collapse instantly on examination. The media chimed in enthusiastically, lauding the magnificent achievement of defeating Panama, unconcerned that the pretexts were ludicrous, that the act itself was a radical violation of international law, and that it was bitterly condemned elsewhere, most harshly in Latin America. Also ignored was the U.S. veto of a unanimous Security Council resolution condemning crimes by U.S. troops during the invasion, with Britain alone abstaining.

All routine. And all forgotten (which is also routine).

From El Salvador to the Russian Border

The administration of George H.W. Bush issued a new national security policy and defense budget in reaction to the collapse of the global enemy. It was pretty much the same as before, although with new pretexts. It was, it turned out, necessary to maintain a military establishment almost as great as the rest of the world combined and far more advanced in technological sophistication — but not for defense against the now-nonexistent Soviet Union. Rather, the excuse now was the growing “technological sophistication” of Third World powers. Disciplined intellectuals understood that it would have been improper to collapse in ridicule, so they maintained a proper silence.

The U.S., the new programs insisted, must maintain its “defense industrial base.” The phrase is a euphemism, referring to high-tech industry generally, which relies heavily on extensive state intervention for research and development, often under Pentagon cover, in what economists continue to call the U.S. “free-market economy.”

One of the most interesting provisions of the new plans had to do with the Middle East. There, it was declared, Washington must maintain intervention forces targeting a crucial region where the major problems “could not have been laid at the Kremlin’s door.” Contrary to 50 years of deceit, it was quietly conceded that the main concern was not the Russians, but rather what is called “radical nationalism,” meaning independent nationalism not under U.S. control.

All of this has evident bearing on the standard version, but it passed unnoticed — or perhaps, therefore it passed unnoticed.

Other important events took place immediately after the fall of the Berlin Wall, ending the Cold War. One was in El Salvador, the leading recipient of U.S. military aid — apart from Israel-Egypt, a separate category — and with one of the worst human rights records anywhere. That is a familiar and very close correlation.

The Salvadoran high command ordered the Atlacatl Brigade to invade the Jesuit University and murder six leading Latin American intellectuals, all Jesuit priests, including the rector, Fr. Ignacio Ellacuría, and any witnesses, meaning their housekeeper and her daughter. The Brigade had just returned from advanced counterinsurgency training at the U.S. Army John F. Kennedy Special Warfare Center and School in Fort Bragg, North Carolina, and had already left a bloody trail of thousands of the usual victims in the course of the U.S.-run state terror campaign in El Salvador, one part of a broader terror and torture campaign throughout the region. All routine. Ignored and virtually forgotten in the United States and by its allies, again routine. But it tells us a lot about the factors that drive policy, if we care to look at the real world.

Another important event took place in Europe. Soviet president Mikhail Gorbachev agreed to allow the unification of Germany and its membership in NATO, a hostile military alliance. In the light of recent history, this was a most astonishing concession. There was a quid pro quo. President Bush and Secretary of State James Baker agreed that NATO would not expand “one inch to the East,” meaning into East Germany. Instantly, they expanded NATO to East Germany.

Gorbachev was naturally outraged, but when he complained, he was instructed by Washington that this had only been a verbal promise, a gentleman’s agreement, hence without force. If he was naïve enough to accept the word of American leaders, it was his problem.

All of this, too, was routine, as was the silent acceptance and approval of the expansion of NATO in the U.S. and the West generally. President Bill Clinton then expanded NATO further, right up to Russia’s borders. Today, the world faces a serious crisis that is in no small measure a result of these policies.

The Appeal of Plundering the Poor

Another source of evidence is the declassified historical record. It contains revealing accounts of the actual motives of state policy. The story is rich and complex, but a few persistent themes play a dominant role. One was articulated clearly at a western hemispheric conference called by the U.S. in Mexico in February 1945 where Washington imposed “An Economic Charter of the Americas” designed to eliminate economic nationalism “in all its forms.” There was one unspoken condition. Economic nationalism would be fine for the U.S. whose economy relies heavily on massive state intervention.

The elimination of economic nationalism for others stood in sharp conflict with the Latin American stand of that moment, which State Department officials described as “the philosophy of the New Nationalism [that] embraces policies designed to bring about a broader distribution of wealth and to raise the standard of living of the masses.” As U.S. policy analysts added, “Latin Americans are convinced that the first beneficiaries of the development of a country’s resources should be the people of that country.”

That, of course, will not do. Washington understands that the “first beneficiaries” should be U.S. investors, while Latin America fulfills its service function. It should not, as both the Truman and Eisenhower administrations would make clear, undergo “excessive industrial development” that might infringe on U.S. interests. Thus Brazil could produce low-quality steel that U.S. corporations did not want to bother with, but it would be “excessive,” were it to compete with U.S. firms.

Similar concerns resonate throughout the post-World War II period. The global system that was to be dominated by the U.S. was threatened by what internal documents call “radical and nationalistic regimes” that respond to popular pressures for independent development. That was the concern that motivated the overthrow of the parliamentary governments of Iran and Guatemala in 1953 and 1954, as well as numerous others. In the case of Iran, a major concern was the potential impact of Iranian independence on Egypt, then in turmoil over British colonial practice. In Guatemala, apart from the crime of the new democracy in empowering the peasant majority and infringing on possessions of the United Fruit Company — already offensive enough — Washington’s concern was labor unrest and popular mobilization in neighboring U.S.-backed dictatorships.

In both cases the consequences reach to the present. Literally not a day has passed since 1953 when the U.S. has not been torturing the people of Iran. Guatemala remains one of the world’s worst horror chambers. To this day, Mayans are fleeing from the effects of near-genocidal government military campaigns in the highlands backed by President Ronald Reagan and his top officials. As the country director of Oxfam, a Guatemalan doctor, reported recently,

“There is a dramatic deterioration of the political, social, and economic context. Attacks against Human Rights defenders have increased 300% during the last year. There is a clear evidence of a very well organized strategy by the private sector and Army. Both have captured the government in order to keep the status quo and to impose the extraction economic model, pushing away dramatically indigenous peoples from their own land, due to the mining industry, African Palm and sugar cane plantations. In addition the social movement defending their land and rights has been criminalized, many leaders are in jail, and many others have been killed.”

Nothing is known about this in the United States and the very obvious cause of it remains suppressed.

In the 1950s, President Eisenhower and Secretary of State John Foster Dulles explained quite clearly the dilemma that the U.S. faced. They complained that the Communists had an unfair advantage. They were able to “appeal directly to the masses” and “get control of mass movements, something we have no capacity to duplicate. The poor people are the ones they appeal to and they have always wanted to plunder the rich.”

That causes problems. The U.S. somehow finds it difficult to appeal to the poor with its doctrine that the rich should plunder the poor.

The Cuban Example

A clear illustration of the general pattern was Cuba, when it finally gained independence in 1959. Within months, military attacks on the island began. Shortly after, the Eisenhower administration made a secret decision to overthrow the government. John F. Kennedy then became president. He intended to devote more attention to Latin America and so, on taking office, he created a study group to develop policies headed by the historian Arthur Schlesinger, who summarized its conclusions for the incoming president.

As Schlesinger explained, threatening in an independent Cuba was “the Castro idea of taking matters into one’s own hands.” It was an idea that unfortunately appealed to the mass of the population in Latin America where “the distribution of land and other forms of national wealth greatly favors the propertied classes, and the poor and underprivileged, stimulated by the example of the Cuban revolution, are now demanding opportunities for a decent living.” Again, Washington’s usual dilemma.

As the CIA explained, “The extensive influence of ‘Castroism’ is not a function of Cuban power… Castro’s shadow looms large because social and economic conditions throughout Latin America invite opposition to ruling authority and encourage agitation for radical change,” for which his Cuba provides a model. Kennedy feared that Russian aid might make Cuba a “showcase” for development, giving the Soviets the upper hand throughout Latin America.

The State Department Policy Planning Council warned that “the primary danger we face in Castro is… in the impact the very existence of his regime has upon the leftist movement in many Latin American countries… The simple fact is that Castro represents a successful defiance of the U.S., a negation of our whole hemispheric policy of almost a century and a half” — that is, since the Monroe Doctrine of 1823, when the U.S. declared its intention of dominating the hemisphere.

The immediate goal at the time was to conquer Cuba, but that could not be achieved because of the power of the British enemy. Still, that grand strategist John Quincy Adams, the intellectual father of the Monroe Doctrine and Manifest Destiny, informed his colleagues that over time Cuba would fall into our hands by “the laws of political gravitation,” as an apple falls from the tree. In brief, U.S. power would increase and Britain’s would decline.

In 1898, Adams’s prognosis was realized. The U.S. invaded Cuba in the guise of liberating it. In fact, it prevented the island’s liberation from Spain and turned it into a “virtual colony” to quote historians Ernest May and Philip Zelikow. Cuba remained so until January 1959, when it gained independence. Since that time it has been subjected to major U.S. terrorist wars, primarily during the Kennedy years, and economic strangulation. Not because of the Russians.

The pretense all along was that we were defending ourselves from the Russian threat — an absurd explanation that generally went unchallenged. A simple test of the thesis is what happened when any conceivable Russian threat disappeared. U.S. policy toward Cuba became even harsher, spearheaded by liberal Democrats, including Bill Clinton, who outflanked Bush from the right in the 1992 election. On the face of it, these events should have considerable bearing on the validity of the doctrinal framework for discussion of foreign policy and the factors that drive it. Once again, however, the impact was slight.

The Virus of Nationalism

To borrow Henry Kissinger’s terminology, independent nationalism is a “virus” that might “spread contagion.” Kissinger was referring to Salvador Allende’s Chile. The virus was the idea that there might be a parliamentary path towards some kind of socialist democracy. The way to deal with such a threat is to destroy the virus and to inoculate those who might be infected, typically by imposing murderous national security states. That was achieved in the case of Chile, but it is important to recognize that the thinking holds worldwide.

It was, for example, the reasoning behind the decision to oppose Vietnamese nationalism in the early 1950s and support France’s effort to reconquer its former colony. It was feared that independent Vietnamese nationalism might be a virus that would spread contagion to the surrounding regions, including resource-rich Indonesia. That might even have led Japan — called the “superdomino” by Asia scholar John Dower — to become the industrial and commercial center of an independent new order of the kind imperial Japan had so recently fought to establish. That, in turn, would have meant that the U.S. had lost the Pacific war, not an option to be considered in 1950. The remedy was clear — and largely achieved. Vietnam was virtually destroyed and ringed by military dictatorships that kept the “virus” from spreading contagion.

In retrospect, Kennedy-Johnson National Security Adviser McGeorge Bundy reflected that Washington should have ended the Vietnam War in 1965, when the Suharto dictatorship was installed in Indonesia, with enormous massacres that the CIA compared to the crimes of Hitler, Stalin, and Mao. These were, however, greeted with unconstrained euphoria in the U.S. and the West generally because the “staggering bloodbath,” as the press cheerfully described it, ended any threat of contagion and opened Indonesia’s rich resources to western exploitation. After that, the war to destroy Vietnam was superfluous, as Bundy recognized in retrospect.

The same was true in Latin America in the same years: one virus after another was viciously attacked and either destroyed or weakened to the point of bare survival. From the early 1960s, a plague of repression was imposed on the continent that had no precedent in the violent history of the hemisphere, extending to Central America in the 1980s under Ronald Reagan, a matter that there should be no need to review.

Much the same was true in the Middle East. The unique U.S. relations with Israel were established in their current form in 1967, when Israel delivered a smashing blow to Egypt, the center of secular Arab nationalism. By doing so, it protected U.S. ally Saudi Arabia, then engaged in military conflict with Egypt in Yemen. Saudi Arabia, of course, is the most extreme radical fundamentalist Islamic state, and also a missionary state, expending huge sums to establish its Wahhabi-Salafi doctrines beyond its borders. It is worth remembering that the U.S., like England before it, has tended to support radical fundamentalist Islam in opposition to secular nationalism, which has usually been perceived as posing more of a threat of independence and contagion.

The Value of Secrecy

There is much more to say, but the historical record demonstrates very clearly that the standard doctrine has little merit. Security in the normal sense is not a prominent factor in policy formation.

To repeat, in the normal sense. But in evaluating the standard doctrine we have to ask what is actually meant by “security”: security for whom?

One answer is: security for state power. There are many illustrations. Take a current one. In May, the U.S. agreed to support a U.N. Security Council resolution calling on the International Criminal Court to investigate war crimes in Syria, but with a proviso: there could be no inquiry into possible war crimes by Israel. Or by Washington, though it was really unnecessary to add that last condition. The U.S. is uniquely self-immunized from the international legal system. In fact, there is even congressional legislation authorizing the president to use armed force to “rescue” any American brought to the Hague for trial — the “Netherlands Invasion Act,” as it is sometimes called in Europe. That once again illustrates the importance of protecting the security of state power.

But protecting it from whom? There is, in fact, a strong case to be made that a prime concern of government is the security of state power from the population. As those who have spent time rummaging through archives should be aware, government secrecy is rarely motivated by a genuine need for security, but it definitely does serve to keep the population in the dark. And for good reasons, which were lucidly explained by the prominent liberal scholar and government adviser Samuel Huntington, the professor of the science of government at Harvard University. In his words: “The architects of power in the United States must create a force that can be felt but not seen. Power remains strong when it remains in the dark; exposed to the sunlight it begins to evaporate.”

He wrote that in 1981, when the Cold War was again heating up, and he explained further that “you may have to sell [intervention or other military action] in such a way as to create the misimpression that it is the Soviet Union that you are fighting. That is what the United States has been doing ever since the Truman Doctrine.”

These simple truths are rarely acknowledged, but they provide insight into state power and policy, with reverberations to the present moment.

State power has to be protected from its domestic enemy; in sharp contrast, the population is not secure from state power. A striking current illustration is the radical attack on the Constitution by the Obama administration’s massive surveillance program. It is, of course, justified by “national security.” That is routine for virtually all actions of all states and so carries little information.

When the NSA’s surveillance program was exposed by Edward Snowden’s revelations, high officials claimed that it had prevented 54 terrorist acts. On inquiry, that was whittled down to a dozen. A high-level government panel then discovered that there was actually only one case: someone had sent $8,500 to Somalia. That was the total yield of the huge assault on the Constitution and, of course, on others throughout the world.

Britain’s attitude is interesting. In 2007, the British government called on Washington’s colossal spy agency “to analyze and retain any British citizens’ mobile phone and fax numbers, emails, and IP addresses swept up by its dragnet,” the Guardian reported. That is a useful indication of the relative significance, in government eyes, of the privacy of its own citizens and of Washington’s demands.

Another concern is security for private power. One current illustration is the huge trade agreements now being negotiated, the Trans-Pacific and Trans-Atlantic pacts. These are being negotiated in secret — but not completely in secret. They are not secret from the hundreds of corporate lawyers who are drawing up the detailed provisions. It is not hard to guess what the results will be, and the few leaks about them suggest that the expectations are accurate. Like NAFTA and other such pacts, these are not free trade agreements. In fact, they are not even trade agreements, but primarily investor rights agreements.

Again, secrecy is critically important to protect the primary domestic constituency of the governments involved, the corporate sector.

The Final Century of Human Civilization?

There are other examples too numerous to mention, facts that are well-established and would be taught in elementary schools in free societies.

There is, in other words, ample evidence that securing state power from the domestic population and securing concentrated private power are driving forces in policy formation. Of course, it is not quite that simple. There are interesting cases, some quite current, where these commitments conflict, but consider this a good first approximation and radically opposed to the received standard doctrine.

Let us turn to another question: What about the security of the population? It is easy to demonstrate that this is a marginal concern of policy planners. Take two prominent current examples, global warming and nuclear weapons. As any literate person is doubtless aware, these are dire threats to the security of the population. Turning to state policy, we find that it is committed to accelerating each of those threats — in the interests of the primary concerns, protection of state power and of the concentrated private power that largely determines state policy.

Consider global warming. There is now much exuberance in the United States about “100 years of energy independence” as we become “the Saudi Arabia of the next century” — perhaps the final century of human civilization if current policies persist.

That illustrates very clearly the nature of the concern for security, certainly not for the population. It also illustrates the moral calculus of contemporary Anglo-American state capitalism: the fate of our grandchildren counts as nothing when compared with the imperative of higher profits tomorrow.

These conclusions are fortified by a closer look at the propaganda system. There is a huge public relations campaign in the U.S., organized quite openly by Big Energy and the business world, to try to convince the public that global warming is either unreal or not a result of human activity. And it has had some impact. The U.S. ranks lower than other countries in public concern about global warming and the results are stratified: among Republicans, the party more fully dedicated to the interests of wealth and corporate power, it ranks far lower than the global norm.

The current issue of the premier journal of media criticism, the Columbia Journalism Review, has an interesting article on this subject, attributing this outcome to the media doctrine of “fair and balanced.” In other words, if a journal publishes an opinion piece reflecting the conclusions of 97% of scientists, it must also run a counter-piece expressing the viewpoint of the energy corporations.

That indeed is what happens, but there certainly is no “fair and balanced” doctrine. Thus, if a journal runs an opinion piece denouncing Russian President Vladimir Putin for the criminal act of taking over the Crimea, it surely does not have to run a piece pointing out that, while the act is indeed criminal, Russia has a far stronger case today than the U.S. did more than a century ago in taking over southeastern Cuba, including the country’s major port — and rejecting the Cuban demand since independence to have it returned. And the same is true of many other cases. The actual media doctrine is “fair and balanced” when the concerns of concentrated private power are involved, but surely not elsewhere.

On the issue of nuclear weapons, the record is similarly interesting — and frightening. It reveals very clearly that, from the earliest days, the security of the population was a non-issue, and remains so. There is no time here to run through the shocking record, but there is little doubt that it strongly supports the lament of General Lee Butler, the last commander of the Strategic Air Command, which was armed with nuclear weapons. In his words, we have so far survived the nuclear age “by some combination of skill, luck, and divine intervention, and I suspect the latter in greatest proportion.” And we can hardly count on continued divine intervention as policymakers play roulette with the fate of the species in pursuit of the driving factors in policy formation.

As we are all surely aware, we now face the most ominous decisions in human history. There are many problems that must be addressed, but two are overwhelming in their significance: environmental destruction and nuclear war. For the first time in history, we face the possibility of destroying the prospects for decent existence — and not in the distant future. For this reason alone, it is imperative to sweep away the ideological clouds and face honestly and realistically the question of how policy decisions are made, and what we can do to alter them before it is too late.


In October this year a former drone operator started talking about the psychological consequences of his work. He described the situation of being behind the screens in the air-conditioned containers at a US air force base in the Nevada desert. Since then several news stories about cases of PTSD and anxiety among drone operators have emerged, questioning the presumed benefits of using drones in war, which governments and military persuasively have been advocating for. In 2011 Air Force psychologists conducted a mental-health survey of 600 combat drone operators. 42% of the operators reported moderate to high stress levels, and 20% reported emotional exhaustion or burnout. Another study revealed that drone operators suffer from the same levels of depression, anxiety, PTSD, alcohol abuse and suicidal tendencies as traditional combat pilots (Power(2013)). The psychologists ascribe these psychological effects of the work of drone operators to “existential conflict”. The operators don’t train with a unit: they are pushing buttons and guiding a missile at a target that they follow through a live-feed on a screen, transmitted from the camera on the drone. They don’t put their own lives in danger; there is no tangible combat with the enemy, no anchoring in the reality of war - only the surveillance and the killing. At night they go home to sleep in their own bed (Owen(2013)).

How do we perceive the life circumstances of other people, when they are mediated through moving images displayed on a screen? Does the fact that the operator suffers the same psychological consequences as the soldier on the battleground suggest that it essentially doesn’t make a difference whether the killing of the enemy occurs locally or thousands of miles away, transmitted from the drone camera to a high resolution screen? By means of looking at the human consequences of drone warfare, is it possible to investigate our perceptions of others on a more general level, through affective and cognitive processes taking place when interacting with them through a screen?
Media archaeologists talk of the “bad ancestry” of certain media objects, pointing to their roots of origin. Were the technologies originally developed for the entertainment of people or for the surveillance of them? For military purposes or for personal use? Can these borders even be drawn up so distinctly? The story of the 20th century is inseparable from evolving cinema techniques – from telescopic viewfinders atop rifles to sophisticated cybernetic cameras, says Paul Virilio, determining the longstanding intimate relationship between war and cinema (Hussein(2013)).

Through this research I am trying to investigate this relation between the filmic qualities of the drones’ visual field and the cognitive and pre-cognitive reactions to the transmitted material - both from the viewpoint of the people operating the drones, and of the public confronted with the online/news footage of this new warfare at a distance. Drone footage is not cinema in any common sense of the term, but it is nevertheless moving image, captured by a camera and viewed on a screen. Drone footage thus possesses “filmic qualities”, which may prove valuable in examining the complexity of drone warfare. When watching moving images we form relations to what/who is represented on the screen, both through cognitive recognitions as well as affects and bodily reactions. Is it possible to transfer this way of analyzing film to the viewing of video footage of the drone? It becomes evident that it is not so easy to desensitize this looking as, for example, Dave Grossman (a colonel in the military arguing for the correlation between distance and the “ease” of killing) wishes it to be, when we take into consideration the psychological ramifications of the work of the drone operators (Hussein(2013)). In order to understand the array of implications of the use of real-time video transmission in modern warfare it is important to bring back the human reaction when we look at visual representation of others.

Looking at drone attack videos on Youtube I felt shocked, numbed and distanced from what I saw – the soundless birds-eye vision following the infrared dots, escalating in an equally soundless explosion, as if almost shaking the screen. How should I process these visual testimonies of the direction, in which media technology advancements are taking our societies? It made me wonder how we can redefine our relationship to viewing traumatic or violent visual material in the information society. In this sense how can we determine the importance of the psychological processes of viewing and relating in a sensible way to this material?
Screen technology increasingly inhabits our lives and we interact through it. Has this changed our perception of interacting with others? Does this disembodied interaction change our sense of empathy towards the other person? My hope is to point to the importance of creating a discourse around these subject matters, fostered through an awareness of the media form, a focus on the importance of sound structuring the image, the absence of the human body through the mediated interaction, and the possibility of forming an empathetic relationship, which we are able to build by interacting through affective responses with other people, even though this meeting is mediated through a screen. With this research I am also hoping to uncover how my explorations into this subject matter has opened up an approach for me to further understand and conduct artistic research as a method for art-making.

Drones, or UAV’s, are unmanned aircrafts that are remotely controlled by operators, who can surveil the geographical area below the drone, which is being captured by it’s built-in cameras (for technical specifications see Valdes:2012) and transmitted live to the screen of the operator. In some cases the drones also carry missiles that the operators can direct at a target (Valdes(2012)).

It seems that in the current discourse about drones (for both commercial, private and military use) is permeated with equal amounts of fascination and fear, as the public has to deal with the drastically developing technologies and uses of drones in our daily lives and in military operations abroad (Obama has ordered five times as many drone strikes in his first term as Bush did in his 8 years in office (Owen(2013))). Huge amounts of money are put into development and research of drone use, spanning widely from “delivery drones”, affordable “toy drones” and drone swarms to extensive, ambitious inventions for military use. It is specifically in regards to the military and secret surveillance where drones are being developed that many people can’t help but imagine a looming, Orwellian technological dystopia. And in all fairness the drones used in warfare do have quite a sinister appearance; like some breed of artificial creatures that, with their grey, eyeless noses, seem to have evolved in a lightless, gloomy environment. Their appearance and function is simply quite unsettling .
The hope for developing these new technologies is to eventually be able to remove more ground forces and replace them with unmanned technology systems (Nakashima, Whitlock(2011)). This is also the argument used by government officials and the military, the rhetoric being that they do work; they save lives of soldiers, is a good alternative to ground operations as they strike more precisely, and cause next to no civil casualties. Opponents however argue that they violate international law, create more enemies of the occupying power than what they eliminate, cause much more civil casualties than official reports state, cause severe psychological conditions for people living in drone-surveilled areas and, as mentioned in the introduction, cause psychological problems for the people operating the drones from afar (Hussein(2013)). I will not delve too deeply into the political and technical aspects of the drones used in war, as this has been eloquently covered in media and scientific reports, but turn towards the media technology of drones, analyzing the relationship between air warfare and cinema.

Why can it be useful to look at media objects in a historical context? On a very basic level it matters to put the media objects that shape our current reality, the way we experience life, into a time context. It roots us in our perception of our current situation, and the speed with which the technologies that we use develop. It can foster a better understanding of how technologies shape our current situation, or how we shape our current situation by developing certain aspects of technology while abandoning others.
When I went to the Technical Museum in Berlin I encountered a small installation of a taxidermy pigeon hanging in a glass cabinet together with some outdated photographical surveillance equipment. Around it’s neck an analogue still camera was mounted. This was a historical account of how aerial surveillance technology has developed. I wondered why the museum, with its extensive collection of military aircrafts didn’t devote more attention to this timely topic. The history of modern military surveillance begins with photography (Manovich(1995)). In 1882 unmanned photo balloons were used to gain overview of enemy territories, and in 1907 pigeon photography was invented in Germany to be used in the 1st and 2nd world wars, both by Germany and the United States (Deutsches Technikmuseum(2007)).
However Radar was developed in the mid 30’s and soon became the major surveillance technology during WW2, introducing the concept of real-time screen-based surveillance, and thus eliminating the inherent delay that made photo-surveillance inefficient. According to Manovich this development brought along a new type of screen that differentiated itself from photography and film. These two representational forms freeze the photographed in time, whereas the new type of screen (a predecessor for video monitors, tv, computer screen and instrument display – in short, moving images that implements live transmission) relies on continuous updating in real-time. The radar screen changes while tracking the referent, constituting a closer genealogical relationship to audio waves than the photographical image.
Manovich’s article, An Archaeology of a Computer Screen from 1995, may today seem a bit outdated, in regards to its fascination with the Virtual Reality screen. However, it provides some useful tools when looking at the genealogy of the screen, and what Matthew Fuller in a 2013 talk about Kittler has termed “bad ancestry” of certain media objects (Fuller(2013)). Manovich traces this genealogy back to the painting , which is followed by the dynamic screen, introducing film, television, and video, where images can be displayed as moving over time. This brings in a certain “viewing regime”, where the viewer is asked to suspend disbelief and identify with what the image displays – it tries to focus our attention within the framework of the screen. Of course with this “aggressive” screen, the degree to which it is required of the viewer to submit to the illusion of the represented, depends on the context (e.g. whether it is in a dark cinema or in front of a TV where people often interact while watching). This viewing regime has changed with the computer, the interactive screen, where the viewer has control over what he wants to view, navigating between different windows and programs simultaneously. Where the emergence of the cinema screen is well known and indisputably developed for the entertainment of people, Manovich turns our attention to the genealogy of the computer screen. He juxtaposes its history and emergence to that of the cinema screen, claiming, that even though it appeared in the middle of the 20th century it was not introduced into the public realm until much later. The computer was (mostly) developed and used by military, specifically in regards to surveillance. This is the screen of real time. Radar became a superior way of gathering information about enemy locations, and the computer (along with other key principles and technologies) partly developed to provide an effective way to process and display this information. According to Manovich modern human-computer interface owe their existence to military research (For a more detailed description of this early development see Manovich(1995), Pp.5-6).

So what becomes the point of focus here is the media objects themselves and not what passes through them. Media archaeologists are interested in what the origins of media can tell us about our current situation – in Friedrich Kittler’s words the way they “determine our situation”. It is only the media that can see what it becomes, not their inventors, according to Fuller. In that way Kittler is opposing the McLuhan idea that media are extensions of man saying that:” Media are not pseudopods for extending the human body. They follow the logic of escalation that leaves a written history behind it”(Kittler(1985)). With the increasingly greater presence of media objects, and the way these objects seamlessly are able to merge themselves into our lives, it becomes important to “break the illusion of timelessness” as Erkki Huhtamo requests in his article “Screen Tests: Why Do We Need an Archaeology of the Screen?” (Huhtamo (2012)). My aim in terms of looking at drone technology through a media archaeological lens is to attempt to deconstruct this “illusion of timelessness”, to use it as a tool for thinking about how we can foster a better awareness of the complexities of the use of drones in modern warfare, by considering the media technology behind them. The aim is to think of how to change focus from the political aspect of using these technologies and consider the human reactions to the turns and roles certain media forms have taken in our societies. What is important in regards to the media technologies used in drone warfare is to think about where these technologies have emerged from, and how one can, by looking at these origins, investigate our reactions and interactions with these new, sometimes obscure or not-yet-defined, media forms that are so rapidly changing essential factors in our society.
While Manovich juxtaposes the viewing regime of the real-time screen of the computer and the cinema screen, it might be useful to consider the act of looking from another angle when dealing with the real-time screen of the drone. Paul Virilio has made the connection between war and cinema, looking at the relationship between act of viewing (surveilling) and power. He traces the camera’s systematic role in the military context of the 20th century in his work “War and Cinema – the Logistics of Perception”. He points out that “…For men at war, the function of the weapon is the function of the eye” (Virilio(1989), Pp.20). He states that while the image was originally developed as a system of information and amusement, it is most often utilized as systems of observation and control. From the emergence of military photography in the American Civil War, over telescopic viewfinders on rifles, to a strategy of global vision with spy-satellites, drones etc., making “real-time warfare” possible on a distance. One may wonder if Virilio ever imagined that his observations would become as integral a part of modern warfare as they have become within the last 10 years, with the emergence of remote airbases and drone operations.

As the book’s title indicates, he is interested in the relationship between looking and the act of taking aim (killing), stating that:” The act of taking aim is a geometrification of looking, a way of technically aligning ocular perception along an imaginary axis that used to be known in French as "the faith line" … to denote the ideal alignment of a look, which starting from the eye, passed through a peep-hole and the sights and on to the target object" (Virilio(1989)Pp.3). Hereby he reminds us of the shared genealogical origin of the image and the tools of war. With his theories of speed and politics he also introduces the idea of the shift in focus from a politics of space to a politics of time, where whoever is in control of instant information, communication and surveillance is in power. It is the society with access to information technologies and who understands the power of controlling and utilizing image technologies moving in time (real-time in the case of surveillance) who are in power. The drone technology becomes a figurative and literal representation of this power structure, where the image and possessing the best technology for using this image becomes the most efficient and overruling weapon. Of course, much can be said about the power relations and inequalities of this type of warfare as an unequal exchange, pointing more towards policing action than warfare, but that is not the main focus of this research. In order to make a turn towards the main subject of the work, the human consequences of drone warfare, it can be useful to approach the subject from another cinematic reading – that of the viewer.

In “War and Cinema” Virilio is quoting Merleau-Ponty: “The problem of knowing who is the subject of the state and war will be of exactly the same kind as the problem of knowing who is the subject of perception” and goes on to say that “however it is not human observers or military analysts themselves who will have this ubiquitous and surgically precise vision: rather, a ‘sight machine’ aboard an intelligent satellite will automate perception of enemy territory in the finest detail, helping the missile’s ‘expert system’ to reach its decision at the speed of electronic circuitry”. (Virilio Pp. 2(1989)). For Virilio technological advancements will ultimately create, not a simulation as Baudrillard states, but a substitution of reality, a virtual reality that becomes more powerful than the actual human reality. In this new virtual reality the image of real time becomes the most powerful weapon as it, through the acceleration of our media environment, becomes too quick for human perception and thus these images can provide a strategic advantage. In Virilios theories about the advancement of image-based technologies the human has thus become obsolete.

To Virilio the loss of the human in the contemporary world, under the influence of always accelerating technologies, will become the main problem of our society. He mourns the loss of “the phenomenological dimension that privileged lived experience”, explaining that the new forms of technologies create an abstract, virtual realm, which replaces that of the phenomenological, lived experience. But Virilio could not in 1989 predict the situation of a drone war, one that is so close to his technological dystopia in all other senses, where technology creates all-encompassing possibilities for killing and surveilling on a distance. But there are people involved in this act of looking, and the people behind the screens perceive what they transmit – and they’re getting PTSD, burnout and anxiety from this looking.

David Grossman, a former lieutenant, psychologist and scholar in military research, has written about psychological consequences of killing in combat. Grossman writes in his book “On Killing”, “I have not found a single instance of individuals who have refused to kill the enemy under these circumstances, nor have I found a singe instance of psychiatric trauma associated with this type of killing” (Hussein(2013)). Surely the impact of killing in combat on ground is something fundamentally different and much more traumatizing than sitting behind a screen and directing missiles at targets through a live-feed video representation. And surely it is easier to order these soldiers to kill without having to deal with “refusing”, or the messiness of combat and physical damage. But when Brandon Bryant, who was one of the first and most outspoken drone operators to talk about his experiences, tells his version of the work he was doing in those air-conditioned containers, it becomes evident that the work of killing for the drone operator is by no means “easy”, nor is it like shooting in a video game (which some critics have pointed a phenomenological resemblance to). In his powerful interview with the magazine GQ he revealed the haunting realities of a job that are much more horrifying than any video game could be. He describes how he would attack, and then being ordered to linger above the site for hours on end watching the bloody consequences unfold. The funeral for the killed, or the cleaning up of body parts. Or he would be a birds-eye observer of other haunting scenarios in the reality of a war-ridden country, not being able to intervene in the atrocities he saw. Mostly, though, his work would just be an endless loop of watching and waiting for orders, or following possible targets, gaining an intimate and voyeuristic view into the mundane, human aspects of their lives. Another factor that played into his diagnosis of PTSD was the fact that the operators were more or less on their own dealing with their role in the war (unlike soldiers that live together in a camp building a kinship); “I kind of finished the night numb…Then you just go home. No one talked about it. No one talked about how they felt after anything. It was like an unspoken agreement that you wouldn’t talk about your experiences.” (Power(2013)). He tells about an episode where he saw a man, whose leg he had just shot off, bleed to death, through the infrared camera of the drone, in a soundless moment where all time stood still for him, while the man slowly turned the same color as the ground he was laying on. Or another where, in a swift moment before firing a missile, he thought he saw a child running right into his aim, before blowing up the scene in an infrared burst of light (Power(2013)).

Several videos of drone attacks can be found online. Together with the eyewitness accounts from Bryant and other operators, they give an insight into the “viewing regime” of the drones and the, now rather obvious, implications they have on the viewer. In the next paragraphs this relationship between the footage that is transmitted on screen by the drone camera is examined from two angles: That of the drone operator, shot in real time and with the direct interaction with the death of the enemy, and that of the viewer of these recorded testimonies, which can be found on Youtube.

Bryant’s testimony of the psychological consequences of his work suggests that, in order to understand the complexity of drone warfare in its totality, we need to look beyond the technical as well as the political musings on the phenomenon of the drone. In the light of this it becomes clear that Virilio’s premonitions of a humanless war machine, a catalyst for powerful real-time images which are taking over the reality of real human experience, does not adequately describe this very peculiar turn that the contemporary relationship between war and the moving image has taken, with the use of drones in modern society. Bryant reminds us in a way that even though these technological advancements and distancing of the human body in the combat situation were meant to desensitize the act of killing, to make it clean and effective, it is nearly impossible to shut out cognitive and affective reactions to the visual material that is being represented on the screens.
Growing up in a visual media culture, a culture of war images, but also of gaming and movies, one must assume that the people that work as drone operators are accustomed to processing and digesting visual imagery – also of the violent kind. But it is not the presumed resemblance with strategic video games or the like that concern most critics of drones. The operators know very well that this is real life combat, and, even though they are the ones firing, they are still following commands from an extensive chain of lawyers and top-commanders. I would argue that working as a drone operator is working in a field where, when perceiving visual imagery, affective responses are left in an ambiguous state, and where the boundary of the screen, between the observer and the observed, is not as distanced as is suggested by military proponents of drone warfare. It is worth mentioning the fact that the resemblance of this footage to visual imagery encountered in media culture is quite similar, but put in a completely different context. One where the operators can view their destruction in high resolution, where the live-feed shows all that video games don’t: The human, everyday activities of the people living under the cameras. It is this truly weird combination of silently partaking in people’s lives on the other side of the planet, witnessing real-time footage of the atrocities of war while partaking in (and being aware of partaking in) this reality, together with the very asymmetry and distancing of the drone technology, that is essential to the perception of the drone footage.

In order to deal with human perception of visual imagery (especially of the traumatic kind) it can be useful to look at theory exploring affect and emotions when looking at images. Investigating affective responses seems to be a growing tendency within media-, literary- and art theory as a means of understanding our information – and image-based culture. It is the conception that there is more to the human reaction to ‘sensorial inputs’ than what can be read through semiotics, that there are pre-cognitive responses, which are not easily defined or “measurable”. Affect theory, in terms of understanding cultural effects of art/media inputs, is a way to understand how these inputs actually effect us on a biological level, e.g. through rising pulse, neurological patterns and galvanic skin response. These theories go beyond a formalist reading of the sensorial input and try to uncover how the body “bears information” and reacts immediately and pre-cognitively to such input. This information comes before any understanding that we receive from reading the content in a cultural context of signs and pre-conceived knowledge. What is important to theorists such as Brian Massumi is to highlight the fact that there is potential for some kind of “raw” experience in humans, when encountering a sensory input, and when we try to measure this sensory input in a structural way, we hinder this understanding of bodily potential and thus it becomes a cultural function: “Nature and culture are in mutual movement into and through each other” (Massumi(2002), Pp.11).

Affects are pre-personal intensities occurring when the body passes from one experiential state to another, and can thus not be described as either a subjective feeling or an emotion (feelings are personal and biographical, emotions are social and affects are thus pre-personal)(Shouse(2005)). These distinctions are important if one is to use affect theory as an alternative method for understanding human reactions to cultural phenomena, as opposed to formalistic or structural readings etc. Feelings are sensations that have been labeled and checked against previous experiences and are personal because each person has unique sets of experiences to refer to in their previous experiences. Emotions are the projections or displays of feelings – and can thus be either genuine or fake. They can be affected by social expectations or other motivations. Affect is the moment of unformed and unstructured experience and is thus very abstract, as it cannot be fully described in language because it is outside of consciousness – and it is this abstractness that appeals to many contemporary cultural theorists (Massumi(2002), Pp. 15). Shouse writes that “Affect is the body’s way of preparing itself for action in a given circumstance by adding a quantitative dimension of intensity to the quality of an experience” (Shouse(2005)).

The body has it’s own way of understanding inputs that cannot be fully captured in language. According to Shouse affect is what “makes feelings feel and what makes the intensity of a feeling”. Affect always precedes consciousness (and will) and can be described as the multiple stimuli our bodies register and thus transform into a bodily intensity, which, according to Massumi can be described as “pure potential” (Massumi(2002), Pp. 30). Shouse quotes Silvan Tompkins explaining that affect has the power to influence consciousness in the way that it can raise our awareness of our biological state, comparing the state of affect to the pain mechanism, which makes us aware of our bodily presence. I would argue however that it is important to remember that both feelings and emotions (also if they are socially or culturally “learned”) play a big (and important) role in our reactions to visual material, especially when it comes to building empathy, which I will touch upon later.

How can we use our affective responses in terms of understanding sensory input, art, media and other stimuli that we meet in our daily lives? What is interesting about affect is, that since it is unstructured and unformed it can be “transmitted” between separate bodies. This doesn’t imply that one person’s feelings “become” another’s but rather that “your body infolds a context and another body (real or virtual) is expressing intensity in that context, one intensity is infolded into another.”(Shouse(2005)). This creates a form of resonance where bodies respond to each other on a pre-conscious level and the power of the media/art form lies in this ability to create affective resonances independent of content or meaning. It is thus this abstractness, this bodily reaction and transmittable potential that is the power and interesting aspect of affect, which is closely tied to our ability to feel empathy, and which makes it interesting in an art and media context.

A paragraph from the interview with Bryant:” If his mission was to monitor a high-value target, he might linger above a single house for weeks. It was a voyeuristic intimacy. He watched the targets drink tea with friends, play with their children, have sex with their wives on rooftops, writhing under blankets. There were soccer matches, and weddings too. He once watched a man walk out into a field and take a crap, which glowed white in infrared.” (Powers(2013)).

As previously stated, drone footage is not cinema, and it neither has narrative or even documentary value. It’s a hard to define new kind of real-time video-intervention-warfare. But the testimony from Bryant suggests a level of identification with, or relating to, the humanness of the people he is surveilling, as he is following their lives. This identification brings in an affective response that made it very hard to have the desensitized approach to his work his employers probably wished for. In order to understand this bodily reaction looking at the cognitive and affective responses to film-spectatorship can be an interesting pathway into thinking of affect and empathy.

Cognitive film theorists deal with these subject matters, showing how emotional responses, such as empathy, and cognitive processes (inborn mechanisms that reconstructs responses in the viewer, as opposed to cultural knowledge (Grodal (1997) Pp. 20)).) play crucial roles in the reception of film.
Most people can relate to the idea of “sharing” emotions with persons they encounter on a screen, regardless of whether these persons are fictional or real, and we identify with them by following the narrative being built over time and the identification of certain human traits that we mirror ourselves in. The more we gain insight into their life situation, their feelings and struggles, the easier it is for us to identify. Identification with characters has been a subject in film theory since it’s birth. Cognitive film analysts, such as Torben Grodal, however are interested in the physical, bodily reactions to viewing film, taking the spectatorship away from a conception of being passive, to being active. It implies a move away from conventional readings of this character-identification, by looking at bodily reaction patterns as the main source of viewer interaction and identification(Grodal (1997), Pp. 7). For example Grodal notes that in an evolutionary perspective our abilities to empathize and identify with other people, as well as cognitively simulate their situation are linked to our survival as species. We, as humans, try to intuitively understand and simulate the feelings, motives and cognitive focus of others. For Grodal empathy plays a significant role when the viewer cognitively constructs a narrative identifying with the agents of fictions.

Our cognitive “software” is developed through evolution as a tool for navigating through essential biological and human preferences and motivations, which, according to Grodal, are food, security, erotic gratification and social acceptance. Emotions and affects are linked to these preferences, and filmic experiences, which trigger these deeply rooted human conditions, can create empathetic understanding within the viewer. Our remote senses, our eyes and ears, work to create our embodied perception of the world through our cognitive capabilities. The perception via the remote senses relies on our memory of bodily sensory experiences. For example if we have strong subject identification with the protagonist we will try and simulate their perceptions through mirroring of our own cognitive perceptions. These cognitive identifications, which can in some instances be empathy, happen on a very general level and transcend gender, race, and social boundaries, as well as human/non-human beings (however the identification may be stronger if the protagonist resembles the viewer in a specific way) . The cognitive identification may however not necessarily be positive identification or empathy with a protagonist.

The main point in Grodal’s thesis is that the motor system is controlled by basic human preferences such as avoiding danger, finding food or mating, which are transformed into cognitive scripts – plans and goals for identifying, which are related to previous emotional and affective experiences. And empathy will thus very often be the consequence of a long process of following one character, giving the spectator an opportunity to identify with their motives and plans. According to Grodal, cinematic perception is a synaesthetic experience, where the mind and the body are not conceived as separate parts. Cinematic perception requires a bodily participation in the production of the viewer experience – and these mechanisms are constantly at work making sense of what the eye and ear encounters. Grodal argues that cognition, motivation and affect are three aspects of one information-processing system which are only differentiated in the way they appear in consciousness (for instance as intensities or as making sense of a narrative).

Grodal’s theory of film spectatorship is distinctively developed to analyze fiction film, with a relatively large focus on conventional filmic narratives. However the reason I have drawn his work in as an example is the fact that it shows the bodily experience as a parameter for understanding our reactions as perceivers of moving images. If we take into consideration these theories of the processes of cognitive and empathetic identification when watching moving images it may give some useful tools of understanding drone footage and the role of the drone operator/viewer.
Affect occurs when we encounter a sensory stimulus, and in some instances, as Grodal’s theories show, it fosters empathy - specifically when the storyline allow us to follow the life of the character and thus mirror ourselves in their action. In the article “Empathy in The Time of Technology: How Storytelling is Key to Empathy” P.J. Manney discusses the issue of empathy in new technologies, stating that in a world where technology becomes more and more predominant the role of empathy grows larger and ever more important. She also states that, in theory, sensory/media input stimulates mirror neurons, which enable empathy; however, practically empathy is created through storytelling. Mirror neurons reside in the pre-motor area of the brain and they increase an individual’s ability to understand the behavior of others, and without the stimulation of this understanding there is little motivation for the human brain to reach out and “feel for” the other (Manney (2008)).

When reading the testimonies of drone operators such as Bryant, but also Heather Linebaugh in The Guardian, you realize immediately that while drones may be automated killer-machines, the people behind the screens certainly are not. Linebaugh writes in her outcry:” Whenever I read comments by politicians defending the Unmanned Aerial Vehicle Predator and Reaper program – aka drones – I wish I could ask them a few questions. I'd start with: "How many women and children have you seen incinerated by a Hellfire missile?" And: "How many men have you seen crawl across a field, trying to make it to the nearest compound for help while bleeding out from severed legs?" … Few of these politicians who so brazenly proclaim the benefits of drones have a real clue of what actually goes on. I, on the other hand, have seen these awful sights first hand.” (Linebaugh (2013))

It is not my objective to heroize drone operators in any way or talk their case against the military or decision-makers of drone intervention in general. My aim investigating their job is to draw attention to the fact that affective responses and empathy, as I can see it, do occur through these kinds of real-time screen-mediated encounters with others. And looking at drone technology serves for me as example of the human reaction to the mediated other, as the operators are following the day-to-day lives of their targets and also clearly seeing the brutal ramifications of their actions. The “human reality” is in my opinion not rendered away by technology as Virilio proclaimed. It is still very real, present and pressing – we cannot escape from it. What theories of affect and empathy show us is that it is fundamentally hard to remove the body from the sensorial stimuli that we encounter, specifically when it comes to the encounter with other humans. But it is possible to repress these bodily and emotional responses, and as Bryant reminds us, this was encouraged amongst the drone operators, leaving the affective responses in an unaddressed state (they clearly encounter an affective response to what they see and interact with on screen, but there is an attitude of repressing and shutting down emotions as part of the job) that eventually leads to PTSD and anxiety. Linebaugh expresses this with painful clarity:” I know the feeling you experience when you see someone die. Horrifying barely covers it. And when you are exposed to it over and over again it becomes like a small video, embedded in your head, forever on repeat, causing psychological pain and suffering that many people will hopefully never experience. UAV troops are victim to not only the haunting memories of this work that they carry with them, but also the guilt of always being a little unsure of how accurate their confirmations of weapons or identification of hostile individuals were.
Of course, we are trained to not experience these feelings, and we fight it, and become bitter. Some troops seek help in mental health clinics provided by the military, but we are limited on who we can talk to and where, because of the secrecy of our missions. I find it interesting that the suicide statistics in this career field aren't reported, nor are the data on how many troops working in UAV positions are heavily medicated for depression, sleep disorders and anxiety.” (Linebaugh (2013))

Through the camera view of the drone, displayed on the monitor thousands of miles away, the death of the enemy becomes disembodied and distanced, but nevertheless real. There is a sort of quasi-tactility of the human other in the drone view, where the infrared silhouette of the person becomes a symbol for a very fundamentally human quality – heat. The drone operators talk of infrared ghosts haunting their dreams.
My argument is that drone technologies are symptomatic of a very important turning point in terms of how we situate our roles as humans in the context of technological development – on all levels. They show very well the paradox of affective responses and the distance that happens between human bodies through a technological mediation.

Moving towards the role of the public in relation to the drone-attack videos that can be found on Youtube, there is an abundance of critical voices stating that empathy is decreasing with new media technologies, that the citizens of the world are becoming mindless consumers of media information on a variety of platforms, especially blaming social media (Manney (2008)). But in my eyes there is also an abundance of affect in networked media, where interaction and information evokes strong feelings, where activism and outrage against inequality and violence is met with collaboration and mobilization (e.g. witness.org, democracynow.org, change.org etc.). I don’t believe that consumers of media messages today are any less (or more) affected when they encounter, for example, the death or suffering of another person through mediated representation than they were seeing it in the early printing-press days. However I do think that there is little time and often very little context when it comes to digesting these media messages in our current media environment, dominated by feeds, flashes of ‘breaking news’ headlines and exchanges of images and videos. I agree with Manney that there needs to be a context in order to make sense and actively engage with what we encounter in this online media environment.

Watching those drone-attack videos, accompanied by mindless youtube comments and nothing else, I couldn’t help feeling rather distanced. The context was simply too weird. In an environment of networked media where sharing image-based material is incredibly easy, first hand witness testimonies become important ways of creating empathy. And in this case Manney’s cry for non-image based narration fostering empathy may be a valuable point. Her argument is that we might very well risk evolving past our empathetic potentials (potentials that according to Grodal are fundamental to our survival in an evolutionary perspective). However she does not denounce new media technologies’ potential to create more empathy, giving the example of new media literary explorations on blogs and empathetically focused video games. Reading the testimonies of the drone operators left an incredibly powerful impression on me, an impression the drone-footage videos (which nevertheless is the most truly “documenting” evidence of what the drones do) never could provide. They expressed the human ramifications of drone warfare and how it implicates and affects the lives of both the operators and the people living under the drones. To me these testimonies emphasize the importance of creating an awareness of affective response when navigating through a media landscape, so that these affects do not become “trapped” and result in passivity and disconnectedness on the viewer/perceiver’s end. They also show the importance of testimonies, and that the story behind an image is incredibly important for our ability to empathize and thus act towards a particular ethical problem that we encounter .
Because human interaction through mediated extensions in whatever form they may take will not become any less in the future (quite the contrary) there needs to be a discourse around the importance of understanding what happens to our interhuman relations when mediated through a screen, and how we can work to maintain empathy for one another even though we are not face to face. The technological development of the media utilized in drones show that if we blindly put our trust in the presumed effectivity and benefits of new technologies, without considering the implications on ourselves, the technology will truly determine our situation, as Kittler would have said. And this may not be for our benefit at all.

There is great potential for storytelling, activism and empathy through digital/networked media technologies, which has eloquently been shown around the world. I don’t wish to think there is a loss of key human capacities – I wish to think there is a lack of unused potential for emotionally meaningful human interaction through (digital/networked) media technologies. As Clay Shirky calls it, there is a Cognitive Surplus of online citizens coming together for one cause or the other (view Shirky’s TED talk about Cognitive Surplus here). As Shirky states, networked media technologies have the capacity for collaboration, interaction, and creation (whether it’s LOL-cats or online witness/testimony archives). While Shirky is a true internet-optimist, I do think there is a real danger of losing empathy and empathetic skills in an online media environment. This is why it’s of tremendous importance to implement digital/networked (media) technology ethics in education, in the societal discourse, in the work of journalists and in everyday life online. How do we foster and nourish empathy in this new media reality? How do we go about dealing with emotions, affects and feelings when disconnected from face-to-face human interaction? This is what I think Huhtamo is requesting in his “screenology”, however there is more to the equation than merely looking at the technological genealogy of the media objects we use today. It is just as important to realize what technology does to our brains and bodies, and how we can turn technological connectedness into human connectedness – and not the contrary.

Art has a unique ability to foster this immediate intensity, this bodily reaction called affect. Brian Massumi argues, as earlier touched upon, that affect is a kind of unstructured and unformed experience that can be “transmitted” between separate bodies on a pre-conscious level. This perhaps resonates most strongly when we encounter art that deals with difficult or traumatic narratives. One may recognize the feeling of physically reacting to an artwork or an evocatively told story, by disgust, a shiver, choking up unexpectedly etc.. The meeting with the artwork becomes an indefinable and immediate intensity of bodily response. Artists often attempt to understand aspects of the world through different forms of representation, forms that may prove more effective (and affective) than what is used in news media. They often insist on discovering what cannot be conveyed through the conventionally conceived documentary media forms, exactly in order to try and approach what is otherwise hidden, repressed or incomprehensible. It is in these hybrid experimentations across representational forms that artistic research can provide some kind of understanding of the world that hardly can be described in any other way. It provides another view on what we call reality, a view that is not afraid of approaching the unknown, the disorganized or what exists in the grey zones between the presumably “documenting” representation and fiction, in order to question the very possibility of representation, or to reveal some kind of previously hidden or vague nuance.
Taking this approach to artistic research resonates very well with me in the light of affect theory.
Learning about the different aspects of drone war at times placed me in a state of shock. Comparing my reaction to the videos on Youtube, with the one I had reading the testimonies of the operators I became more aware of the impact of the mode of representation, of a context in which to situate the subject matter, of a human face behind the mediated account of drone wars, in order for me to digest the atrocities and human consequences of drone technology. This resonates the importance of the speed with which one digests visual imagery, and the paradox of affective responses and the distance that might occur within the viewer. My work is a reflection upon these reactions. By creating a Zoetrope I wanted to make a visual investigation of the screen in a historical context – as a reaction to the speed of the flow of clips Youtube I forced it into the form and speed of one of the first screens invented. With this screen the viewer can decide the speed of the moving image herself, and reflect upon the meaning of speed when encountering moving images. Learning about Media Archaeology as a theoretical approach made me realize the importance of looking at the development of media objects in order to understand our relationship with them today within a time perspective (to break the illusion of timelessness in Huhtamo’s words). The Zoetrope was also an attempt to visualize the correlation between Virilio’s idea of war and cinema being interrelated, and it’s shape and material is inspired by the burnt, crumbling metal of the war airplanes in the Technical Museum Berlin.

Virilio talks about how the acceleration of events, technological development and speed in our current societies gives way to “…a double movement of implosion and explosion”, so that the “new war machine combines a double disappearance: The disappearance of matter in nuclear disintegration and the disappearance of places in vehicular extermination” (Virilio (1986), Pp. 134). In the animation strip I took the defining moment of destruction, the explosion, making into an eternal loop of imploding and exploding, trying to create a sensorial, interactive experience of this moment as a counter-act to my own perception of encountering the original clip of a drone attack on Youtube.
It became evident as my research progressed, that the media archaeological approach was not sufficient in understanding the implications and aspects of the human interaction through the camera/screen technology of the drone. I wanted to explore how to deal more clearly with the situation of the surveillor (the drone operator) and the live-transmitted footage of the surveilled, and how that relationship may play out. What stuck with me were the profoundly disturbing aspects of the drone operator’s work and the implications this peculiar situation had on them on a personal level. I started researching where they geographically work, where the drones usually patrol, and through the only available tool I had, Google Earth, I tried to imagine what they might see on those screens. I spent hours scrolling through the Nevada Desert and found large residential trailer parks surrounding the air bases, imagining them going there after a day’s work of shooting and surveilling people on the other side of the planet, to eat, sleep, play with their kids. I tried to create a narrative, which, through the beauty and eeriness of the images captured from Google Earth, became quite cinematic. The film thus became another way of attempting to transfer my affective responses to researching this subject matter through my artistic process to whoever may interact with it. I wanted to convey this feeling I had of uncovering, almost counter-surveilling, the lives of the drone operators (however imagined this role may be!), finding pictures of drone test-sites on Google images, discovering how these same pictures had been rendered away subsequently on Google Earth, and looking for hidden air bases in the Saudi desert which you can only find if you know the exact latitude. At the same time I wanted to transfer the eerie feeling I myself had of being surveilled for my activities and search history online.

Another interesting aspect when thinking about the phenomenology of the moving images of drones is the aspect of sound. According to Hussein there is a desynchronization between sound and image in the transmitted image. Whereas the drone operators see all, they lack the sound of the environment they see, whereas where the people living under the drones cannot see the “eye in the sky” but suffer from the perpetual buzzing of the drones (Hussein(2013)). The film is called Machar, which means Mosquito in Urdu – something locals in Pakistan calls the drones. The previously mentioned rapport from Stanford/NYU state that one factor of anxiety and fear for immediate future symptoms among communities living in drone-surveilled areas is the constant buzzing. People are always reminded of this threat of attack, but they never know when it will occur. With the sound track of the film I tried to explore this discrepancy. The sound of an 8mm film projector is accompanying the image when the footage of the geographical areas of the “surveillors” is being displayed. When the footage from the surveilled areas is seen the sound of bugs flying is accompanying the image. Both tracks are composed of 2 versions of the same sound, a real-time and a ultra-slow version in the attempt to point out the relationships between the film image, sound and time. The sound turned out to further emphasize the evocative and cinematic atmosphere of the film, and thus underlining the way sound structures the moving image and how we perceive it.
At the end of the video the satellite camera of Google Earth sweeps over the pixilated topography surrounding the Nevada air base, and as it is colliding with the ground the sound cuts out and the youtube video of a group of black infrared shapes moves across the screen. Suddenly the explosion occurs, to shake the viewer out of the cinematic illusion and point back to the reality of the essential purpose of drones. This shift is an attempt to create an affective reaction in the viewer, and hopefully one of impacting the viewer to take into the consideration the impact of the moving image, how it is structured and how they react to it. In this way I am trying to embody the theoretical research part of this project into an affective response within the viewer.

To me this is as close as I can get to an understanding of what artistic research means to me at the current moment. Through working with this subject matter on a simultaneously theoretical and practical level it has become clear to me that the subject of the technology of drones is just an example, or a catalyst, for me to explore more fundamental aspects of underlying issues and questions of the discourse around me, facing an increasing amount of interhuman communication through new media technologies. Considering more deeply my motivations for and interest in this subject matter it becomes increasingly clear that my attempt to define the new circumstances of interhuman relations through a mediated representation was also a way for me to affirm the role of the human in an increasingly disembodied network society that is forming around me. I want to reassure myself of this bodily connection, and create awareness of the importance to consider the implications that a mindless approach to developing new media technologies can bring along. However I do not wish to fall into the trap of techno-dystopia. I am intrigued by new advances in technology and love the Internet with all its potential – but with precautions. Using Google as a tool to create artwork about surveillance is an attempt for me to use the idea of the Internet as potential for affective resistance, a resistance that is very much alive in the discourse about drones. As much as governments and militaries are trying to obscure the agendas of drone use, the users of the internet are also kicking back. Examples of this are Dronestagram, where a tumblr-user updates his feed with images and facts about each and every US-drone attack, Democracy Now, The Bureau of Investigative Journalism, numerous articles, reports and blogs about locations, numbers and facts regarding drones etc. These for me are prime examples of what Clay Shirky talks about in regards to Cognitive Surplus, which I hope this research will contribute to.

Part of my purpose in presenting this work is to assist people to be aware of certain events that have happened on this planet or are presently happening or are about to happen, events that are radically affecting our consciousness and the way we’re living today. By understanding our present situation, we can open to the possibility of a new consciousness, a new humanity emerging on Earth. In addition, perhaps, my dearest purpose is to inspire you to remember who you really are and give you the courage to bring your gift to this world. For God has given each one of us a unique talent which, when truly lived, changes the physical world into a world of pure light. I’ll also be giving mathematical and scientific evidence to show how we got here, as spiritual beings in a physical world, in order to convince the left-brain analytical part of us that there is only one consciousness and one God, and that we are all part of that Oneness. This is important, for it brings both sides of the brain into balance. This balance opens the pineal gland and allows the prana, the life-force energy, to enter the innermost part of our physical being. Then and only then is the body of light called the Mer-Ka-Ba possible. However, please understand that the evidence I originally learned this information from is in itself not important. The information could in most cases be completely changed to different information without affecting the outcome. In addition, I made many mistakes because I am now human. What is most interesting to me is that every time I made a mistake, it led into a deeper understanding of the Reality and a higher truth. So I say to you, if you find an error, look deeper.
I’ll also be giving my personal experiences, many of which are, I admit, outrageous by the ordinary world’s standards. Perhaps they are not so outrageous by the old world’s standards, but it is you who must decide if they are true or are just stories — or if it even matters. Listen deeply with your heart, for your heart always knows the truth. Then I intend to share with you, as much as I can in the second volume, a specific breathing technique that will help you return to the vastly higher state of consciousness from which we all came. It is the remembrance of the breath connected to the lightbody of the Mer-Ka-Ba. This is one of the primary purposes of this work. At this point a short story of how this book came about is in order. You will read about the angels, so I will not begin there, but rather with the later events. In 1985 the angels asked me to begin teaching the meditation of the Mer-Ka-Ba. I first learne d it in 1971 and had been practicing it ever since, but I did not want to become a teacher. My life was easy and fulfilled. Basically, I was comfortable and didn’t want to work so hard. The angels said that when someone is given spiritual knowledge, they must share it. They said it was a law of creation. Knowing they were right, I opened my first class to the public in the spring of 1985. By 1991 my workshops were filled and overflowing, with hundreds of people on the waiting list. I didn’t know how to reach everyone who wanted this information. In fact, I could not. So in 1992 I made a decision to release a video of one of my workshops and let it go out to the world. Within less than a year it was exploding in sales, but there was one big problem. Most of the people who were watching the videos could not really understand what was presented because it was outside the context and content of their spiritual understanding. I gave a lecture to ninety people in Washington State, all of whom had seen the video tapes but had never been to one of my live workshops. It was there that I realized that only about 15 percent of the people actually knew how to live the meditation by using only the instructions on the video tapes. It was not working. Eighty-five percent were confused and unclear about the instructions. Immediately I took the video tapes off the market. This, however, did not stop the video from continuing to be sold. People wanted the information, so they began to copy the existing tapes and give, sell or lease them to people worldwide. By 1993, it has been estimated that there were approximately 100,000 sets of these tapes in the world. A decision was made. It was determined that the only way we could be responsible with this information was to have a trained person in the room when someone watched the video tapes. Trained means that we had carefully instructed a person to know and live the Mer-Ka-Ba. That person could then orally teach another. This is how the Flower of Life facilitator program was born. There are now over 200 trained facilitators in at least 33 countries. And the system has worked very well. Now things are changing again. People are beginning to understand higher consciousness and its value and concepts. It is now time to release this book to the general public, which is now ready, we feel. A book has the advantage that people can take more time to study the drawings and photos carefully at leisure. And it will also have current updated information such as follows.
Ever since the angels first appeared in 1971,1 have been following their guidance. This is still true today. It was the angels who gave me the meditation of the Mer-Ka-Ba, and it is the meditation that is important here, not the information that is presented. The information is used just to bring us to a point of clarity so we can enter into a particular state of consciousness. Understand that as I received the scientific information in the early years from 1971 to about 1985, I thought it was for my own personal growth. When I would read a scientific paper or magazine, I would discard it, not realizing that in the future I would have to prove what I was saying. Most of the articles have been located, but not all. Yet this information needs to go out. You, the reader, have strongly requested it. Therefore, wherever I can I will document my statements, but some proofs are lost, at least for the moment. Also, part of the information is from nonscientific sources such as angels or interdimensional communications. We understand that “straight science” needs to be separated from a source who is considered psychic. Scientists are concerned about their credibility. As a side note, I would like to comment that this is similar to a male saying to a female that her feelings are not valid and that only logic is true or valid, that logic must be followed. Naturally, she knows another way; it is the way of life itself. It flows. It has no “male logic,” but it is equally true. I believe in both, in balance. If you can conceive of a person using both science and psychic abilities together to explore the Reality, you have come to the right place. Whenever possible I will differentiate between the two types of sources so that you are clear. This means that you must go within yourself to see if this information is true within your world. If something does not feel right, then discard it and go on. If it feels right, then live it and see if it is really true. But it is my understanding that the mind will never really know the Reality until it has joined with the heart. Male and female complete each other. When you read this work you have two choices: You can come from your left brain, your male side, and take notes and carefully see the logic in each step, or you can come from your right brain, your female side, just let go and don’t think — feel, watch it like a movie, expanded, not contracted. Either way will work. It is your choice.
Alittle less than 13,000 years ago, something very dramatic happened in the history of our planet that we’re going to explore in great detail, because what happened in the past is now affecting every aspect of our life today. Everything we experience in our daily living, including the particular technologies we use, the wars that erupt, the foods we eat and even the way we perceive our lives, is the direct result of a certain sequence of events that happened during the end of Atlantean times. The consequences of these ancient events have entirely changed the way we live and interpret reality. Everything is connected! There is only one Reality and one God, but there are many, many ways that the one Reality can be interpreted. In fact, the number of ways to interpret the Reality are just about infinite. There are certain realities that many people have agreed on, and these realities are called levels of consciousness. For reasons we’ll get into, there are specific realities that extremely large numbers of beings are focusing on, which include the one you and I are experiencing right now. At one time we existed on Earth in a very high level of awareness that was far beyond anything we can even imagine right now. We hardly have even the capability to imagine where we once were, because who we were then is so out of context with who we are now. Because of the particular events that happened between 16,000 and 13,000 years ago, humanity fell from that very high place through many dimensions and overtones, ever increasing in density, until we reached this particular place, which we call the third dimension on planet Earth, the modem world. When we fell—and it was like a fall—we were in an uncontrolled spiral of consciousness moving down through the dimensions of consciousness. We were out of control, and it was very much like falling through space. When we arrived here in the third dimension, certain specific changes took place, both physiologically and in the way we functioned in the Reality.
monstrates what is being talked about. I’m in communication with someone right now who is aware of many levels at once. The scientists who are studying her are speechless; they cannot understand how she does what she’s doing. She might be sitting in a room, yet she claims to be watching from outer space. NASA checked her out by asking her to “see” a specific satellite and give specific information that could be known only if someone were actually there. She gave them readings off their instruments, which I’m sure seemed impossible to the scientists. She said she was flying alongside the satellite and simply read them. Her name is Mary Ann Schinfield. She is legally blind, yet she can walk around a room and no one would know that she cannot see. How does she doit? Recently she called me, and while we were talking she asked if I would like to see through her eyes. Of course I said yes. Within a few breaths, my field of vision opened up, and I was looking at or through what looked like a huge television screen chac filled my field of vision. What I saw was astounding. It seemed that I was moving very fast through space without a body. I could see the stars, and at that moment Mary Ann and I, seeing through her eyes, were moving alongside a string of comets. She was very close to one of them. It was one of the most real out-of-body experiences I have ever had. Around the perimeter of this “TV screen” there were about twelve or fourteen smaller TV screens, each one giving extremely fast images. One of them up in the upper right-hand comer was flashing rapidly moving images such as triangles, light bulbs, circles, wavy lines, trees, squares etc. It was this screen that told her what was in the immediate space where her body was located. She could “see” through these seemingly unrelated images. There was another screen in the bottom left-hand corner where she communicated with other extraterrestrial life that was within this solar system. Here is a person who is in a three-dimensional body on Earth, but has full memory and experience of living in other dimensions. This manner of interrupting the Reality is unusual. People do not normally see inner TV screens, but we do exist in many other worlds even though most of us are not aware of it. You presently exist on probably five or more levels. Though there is a break between this dimension and others, when you connect with your higher self you mend that break, after which you start becoming aware of the higher levels and the higher levels start paying more attention to you— communication begins! This connection to the higher self is probably the most important thing that could happen in your life—more important than understanding any of the information I’ll be giving. Connecting with the higher self is more important than learning to activate the Mer-Ka-Ba, because if you connect yourself to your Self, you will get absolutely clear information on how to proceed step by step through any reality and how to lead yourself back home into the full consciousness of God.
There’s one more component to this picture. I’ll be spending perhaps half of our time on left-brain information like geometries and facts and all kinds of information that to many spiritual people would seem totally unimportant. I’m doing this because when we fell, we divided ourselves into two—really three, but primarily into two—main components, which we call male and female. The right brain, which controls the left side of our body, is our feminine component, though it’s truly neither male nor female. This is where our psychic and emotional aspect lives. This component knows that there’s only one God and that oneness is all there is. Though it can’t really explain it, it just knows the truth. So there are not a lot of problems with the female component. The problem is on the left side of the brain—the male component. Because of the nature of how the male brain is oriented—a mirror image of the female—it has its logical component forward (more dominant), while the female has its logical component toward the back (less dominant). The left brain does not experience oneness when it looks out into the Reality; all it sees is division and separation. For that reason, the male aspect of us is having a difficult rime down here on Earth. Even our major sacred books such as the Koran, the Hebrew Bible and the Christian Bible have divided everything into opposites. The left brain experiences that there is God, but then there’s also the devil—perhaps not quite as strong as God, but a huge influence. So even God is seen in terms of duality, as one pole of the opposing forces of dark and light. (This is not true in all sects of these religions. A few of them see that there is only God.) Undl the left brain is able to see the unity running through everything, to know that there is truly one spirit, one force, one consciousness moving through absolutely everything in existence—until it knows that unity beyond any doubt—then the mind is going to stay separated from itself, from its wholeness and from the fullness of its potential. Even if there’s the slightest doubt at all about unity, the left-brain aspect will hold us back, and we can no longer walk on water. Remember, even Thomas walked on water for a short moment when Jesus asked him to, but one little cell in his big toe said, “Wait a minute, I can’t do this,” and Thomas sank into the cold water of polarity reality.
We’re also going to go into the history of the Earth, because it is very important to our present situation. We cannot really understand how we got here if we don’t know the process that led us to this point. So we’ll spend a considerable length of time talking about what happened a long time ago; then slowly we’ll come forward until we get to what’s going on today. It’s all tied together. The same old thing has been going on all along, and it’s still going on—in fact, it has never stopped. Those of you who are predominantly right-brained may feel inclined to skip this left-brained material, yet it is most important for you to hang in there. It is through balance that spiritual health returns. When the left brain sees absolute unity, it begins to relax and the corpus callosum (the band of fibers joining the two hemispheres) opens in a new way, allowing an integration between the two sides. The link between the left and right brain widens, a flow starts, information is passed back and forth, and the opposing sides of the brain begin to integrate and synchronize with each other. If you’re hooked up for biofeedback, you can actually see this happening. This action turns on the pineal gland in a different manner and makes it possible for your meditation to activate the lightbody of the Mer-KaBa. Then the whole process of regeneration and recovery of our previous higher levels of consciousness can proceed. It is a growth process. If you are studying any other spiritual practice, you do not need to stop in order to begin the work with the Mer-KaBa—unless, of course, your teacher does not want to mix traditions. Other meditations that are based on truth can be extremely useful once the Mer-Ka-Ba is spinning, because then noticeable results can evolve very, very quickly. I will repeat myself just so you know for sure: The lightbody of the Mer-Ka-Ba does not contradict or inhibit any other meditation or religion that upholds the belief that there is only one God. So far we’ve talked only about the ABCs of spirituality. These are just the beginning steps. But these first steps are the most important ones I know. Your left brain may love all this information and file it away in neatly labeled pigeonholes; this is fine. Or you can just relax and read this like an adventure story, a mind-stretcher, a fantasy. However you read it, the fact that you are reading this book is what matters, and you will receive whatever you’re meant to receive. In the spirit of oneness, then, let us embark upon this journey of exploration together.
Many ideas we believe today and “facts” we’ve been taught in school are just not true, and people are now beginning to realize this worldwide. Of course, usually these patterns were believed to be true at the time they were taught, but then concepts and ideas changed, and the next generation was taught different truths. For example, the concept of the atom has changed dramatically so many times over the last ninety years that at this point they don’t really adhere to a concept. They use one, but with the understanding that it may be wrong. At one time the atom was thought to be like a watermelon and the electrons were like seeds inside the watermelon. We really know very little about the Reality that exists around us. Quantum physics has now shown us that the person performing the experiment influences the outcome. In other words, consciousness can change the outcome of an experiment, depending on its belief patterns. There are other aspects of ourselves we hold true that may not be true at all. One idea that has been held for a long time is that we’re the only planet in existence with life on it. In our heart of hearts we know this is not true, but this planet will not admit this truth in modern times even though there is powerful evidence of UFO sightings that have been coming from all over the world nonstop for over fifty years. Any subject other than UFOs would have been believed and accepted by the world had this subject not been so threatening. Therefore, we’re going to look at evidence that suggests there is a higher consciousness in the universe, not only in the stars, but perhaps right here on the Earth.
They say it’s very, very old and very small, and that it’s made out of what they called the “heaviest matter in the universe” (which is close, but not actually correct). And they say that it takes “close to fifty years” for this small star to rotate around Sirius. This is detailed stuff. Astronomers were able to validate the existence of Sirius B, a white dwarf, in 1862, and only about fifteen or twenty years ago could they validate the other information. Now, stars are very much like people, as you will begin to see. They’re alive, and they have personalities and many qualities like we have. On a scientific level, they have growth stages. They start out as hydrogen suns, like ours, where two hydrogen atoms come together in a fusion reaction to form helium. This process creates all the life and light that’s on this planet. As a star further matures, another fusion process begins—the helium process—where three helium atoms come together to form carbon. This growth process continues through various stages until it gets all the way up through a particular level of the atomic table, at which point the star has reached the length of its life span. At the end of its life, as far as we know, there are two primary things a star can do. New data on pulsars and magnetars give other options. One, it can explode and become a supernova, a huge hydrogen cloud that becomes the womb for hundreds of new baby stars. Two, it can rapidly expand into what’s called a red giant, a huge explosion that engulfs all its planets—burns them up and destroys the whole system, then stays expanded for a long time. Then slowly it will collapse into a tiny old star called a white dwarf. What the scientists found rotating around Sirius was a white dwarf, which corresponded exactly to what the Dogons say. Then science checked to see how much it weighed, to see if it really was the “heaviest matter in the universe.” The original computations—made about twenty years ago—determined that it weighed about 2000 pounds per cubic inch. That would certainly qualify for heavy matter, but science now knows that this was an extremely conservative estimate. The newest estimate is approximately 1.5 million tons per cubic inch! Black holes aside, that would surely seem to be the heaviest matter in the universe. This means that if you had a cubic inch of this white dwarf, which is now called Sirius B, it would weigh about one and a half million tons, which would go right through anything you set it on. It would head toward the center of the Earth and actually oscillate back and forth across the core for a long time until friction finally stopped it in the very center. In addition, when they checked the rotational pattern of Sirius B around the larger Sirius A, they found it to be 50.1 years. Now, that absolutely could not be a coincidence! It’s just too close, too factual. Yet how did an ancient primitive tribe know such detailed information about a star that could be measured only in this century? But that is only part of their information. They also knew about all the other planets in our solar system, including Neptune, Pluto and Uranus, which we have discovered more recently. They knew exactly what these planets look like when you approach them from space, which we have also only recently learned. They also knew about red and white blood cells, and had all kinds of physiological information about the human body that we’ve recently learned. All this from a “primitive” tribe! , Naturally, a scientific team was sent over to ask the Dogons how they knew all this. Well, that was probably a big mistake for these researchers, because if they accepted that the Dogons really have this information, then ^ by default they must accept how they got it. When they asked, “How did you learn this?” the Dogons replied that the drawings on the walls of their cave showed them.
This is the moment when Sirius and the Sun and the Earth are in a straight line across space. In Egypt, almost all the temples were aligned with this line, including the gaze of the Sphinx. Many of the temples had a tiny hole in the wall somewhere; then there would be another tiny hole through another wall, then through another wall and another, going into some dim inner chamber. In that chamber there would be something like a cube or Golden Mean rectangle of granite sitting in the middle of the room with a little mark on it. At the moment of the heliacal rising of Sirius, a ruby-red light would strike the altar for a few seconds, which would begin their new year and the first day of the ancient Sothic calendar of Egypt.
So the age of the Sphinx has now been put back to at least 10,000 years, maybe 15,000 or a lot more, and it’s already changing the entire worldview of the people on the cutting edge of archaeology. You see, judging by everything we presently think we know, the oldest civilized people in the world were the Sumerians, and they go back to approximately 3800 B.C. Before that, conventional knowledge says there was nothing but hairy barbarians—no civilization at all anywhere on the whole planet. But now we have something man-made and civilized that’s 10,000 to 15,000 years old. That changes everything! In the past, when something new like this is discovered that has a major influence on the viewpoint of the world, it takes about a hundred years for it to get to the people, for the average person to say, “Oh, yes, that is true!” But this time it’ll happen a lot quicker because of television, computers, the Internet and the way things are today. Now scientific circles, for the first time ever, are actually beginning to look at the words of Plato in a new light when he talked about another culture, another continent, from a dim past called Atlantis. The Sphinx is the largest sculpture on the planet. It was not done by hairy barbarians, but by a very sophisticated culture. And it was not done by anybody we now know here on Earth. From a scientific point of view, this is the first solid evidence to be accepted about the true age of civilization. There has been lots of other evidence, but people just kept putting it under the table. This information on the Sphinx has made a crack in our worldview. This took place about 1990, and the crack is now widening. We now have the accepted evidence that there absolutely had to have been someone on Earth who was highly civilized as early as 10,000 years ago. You can see how that’s going to completely change our view of who we think we are.
He was a little guy, and he looked Egyptian. He had dark skin and his hair was kind of long, but pulled back. He had a clean-shaven face except for a thick beard growing from his chin that was perhaps six inches long and tied in five places. He was dressed in simple tan-colored cotton clothing with long sleeves and pants and sat cross-legged facing me. After my shock wore off, I just looked into this person’s eyes. There I saw something I hadn’t seen before except in babies’ eyes. When you look into a little baby’s eyes, you know how easy it is because there’s nothing going on, no judgment, no nothing. You can just fall into their eyes, and they’ll fall into yours. Well, that’s what it was like to look at this man. There were just these big baby eyes in this old body. He didn’t have anything going on. I had an instant connection with this person, and there were no barriers. He touched my heart like no one had ever done before. Then he asked me a question. He said there were three missing atoms in the universe, and did I know where they were? I had no idea what he meant, so I said, “Well, no.” Then he gave me an experience, which I’m not going to describe, that sent me way back in time to the beginning of creation and brought me forward again. It was a very interesting out-of-body experience. When I came back, I understood what he meant about the three missing atoms—at least I thought I did. And I said, “Well, I think what you mean is this,” and proceeded to tell him what I thought. When I finished, he just smiled, bowed and disappeared. A little later my alchemist teacher reappeared. My teacher didn’t know the change had taken place. Everything that happened seemed to be only in my experience. I went away from that totally preoccupied with the experience. At the time, the angels had me working with four other teachers, so I was going from one to the next to the next, and my life was really full. But I couldn’t think about anything except this little man who had appeared to me. I never asked him who he was, and he didn’t return. Time went on, and finally the experience started to fade away. But I always carried the question, who was that guy? Why did he have me go look for those three atoms, and what was this all about? I had a longing to see him again, because he was the purest person I had ever met—ever. Twelve years later I found out who he was. It was Thoth. On November 1, 1984, he reappeared in my life ... and taught me so much. But again, that’s another story for later.
During those times he was called Chiquetet Arlich Vomalites. His name was actually Arlich Vomalites, and Chiquetet was a title that meant “the seeker of wisdom,” because he really wanted to be what wisdom was. After Atlantis sank (we will discuss this subject in great detail soon), Arlich Vomalites and other advanced beings had to wait for about 6000 years before they could begin to reestablish civilization. When Egypt began to come to life, he stepped forward and called himself Thoth, keeping that name all through the time of Egypt. When Egypt died, it was Thoth who started the next major culture, which was Greece. Our history books say that Pythagoras was the father of Greece and that it was from and through the Pythagorean school that Greece unfolded and from Greece that our present civilization emerged. Pythagoras says in his own writings that Thoth took him by the hand, led him under the Great Pyramid and taught him all the geometries and the nature of the Reality. Once Greece was born through Pythagoras, Thoth then stepped into that culture in the same body he had during the time ofAtlantis and called himself Hermes. So it is written, Arlich Vomalites, Thoth and Hermes are the same person. True story? Read The Emerald Tablets, written 2000 years ago by Hermes. Since that time he’s had many other names, but I still call him Thoth. He came back into my life in 1984 and worked with me just about every day until 1991. He’d come in and spend maybe four to eight hours a day teaching me about so many things. This is where the largest body of the information I’ll be sharing with you came from, though it correlates with other information and has been substantiated by many other teachers. The history of the world, especially, came from him. You see, while in Egypt, where he was called the Scribe, he wrote down everything that took place. He was the perfect person for it, right? He was constantly alive, so as a scribe he would just sit there and watch life go by. He was a good impartial witness, as that was a major part of his understanding of wisdom. He seldom talked or acted except when he knew that it was in divine order. Eventually Thoth discovered how to leave Earth. He would go to another planet where there was life and just sit there and watch. He would never interfere, wouldn’t say a single word. He’d be absolutely silent and just watch —just to see how they lived their lives, to get wisdom, to understand—for maybe a hundred years on each planet. Then he would go somewhere else and watch.
After I had been back from Egypt for three or four months, Thoth came in and said, “I want to see the geometries that were given to you by the angels.” The angels had given me the basic information/geometries of how reality is related to spirit, and the angels had taught me the meditation I’m going to give to you. This meditation was one of the first things Thoth wanted from me. That was the exchange: I received all of his memories and he received the meditation. He wanted the meditation because it was a lot easier than the method he was using. His way of staying alive for 52,000 years was very tenuous—it was like hanging on by a thread. It required him to to spend two hours every day in meditation or he would die. He had to spend one hour with his head to the north and his feet to the south, in a very specific meditation; then he had to spend another hour in the reverse position doing a different meditation. Then once every fifty years, in order to keep his body regenerated, he had to go into what’s called the Halls of Amenti and sit for ten years or so before the Flower of Life. (This is a pure flame of consciousness that resides deep in the womb of the Earth and to which humanity’s level of consciousness is completely dependent for its very existence. More later on this subject.) Thoth was very interested in this new meditation because what took him two hours to accomplish takes only six breaths with the Mer-Ka-Ba meditation. It’s quick, efficient and far more accurate; and its potential is much greater, as it leads into a permanent form of awareness. So Thoth began to give me vast amounts of what he knew. When he would appear in my room, we would not speak with words like we’re doing now. We would speak using a combination of telepathy and holographic images. His thoughts to me were holographic, I guess you would say.
Most people by now are aware that something unusual is going on here on Earth. We are in extremely accelerated time, and many events are happening that have never been seen before. There are more people on the planet than have ever been known before, and if we continue at the same rate, in a few more years we will double our population to about eleven or twelve billion people. Regarding our human evolutionary learning curve, the supply of information on the planet is growing far faster than the population. Here’s a fact according to the Encyclopedia Britannica. From the time of our oldest known human civilization, the ancient Sumerians (circa 3800 B.C.), continuing for almost 5800 years until about A.D. 1900, a certain number of bits of information had been collected, a certain number of so-called facts that were added up to determine precisely how many things we knew. Fifty years later, from 1900 to 1950, our knowledge had doubled. That means it took 5800 years to learn a certain amount, then it took fifty years to double it—amazing! But then in the next twenty years, by about 1970, we doubled it again. It took only ten more years, to about 1980, to double that! Now it’s doubling every few years. Knowledge is coming in like an avalanche. The information was coming so fast in the mid-eighties that NASA couldn’t put it into their computers fast enough. I heard that in approximately 1988 they were eight or nine years behind in simply entering the incoming data. At the same time this avalanche of knowledge is building up, the computers themselves, which are boosting the acceleration, are about to make a huge change. Approximately every eighteen months computers are doubling both speed and memory. First we came out with the 286, then the 386; then we had the 486, and now the 586 is out [this was 1993], which makes the 486 obsolete. We didn’t even know how to use the 486 yet, and here’s the 586. And we’ve already got the 686 planned. By the turn of the century or soon after-ward, a home computer will be so powerful and fast that it will surpass all of the present (1993) computers of NASA and the Pentagon combined. A single computer will be so fast and powerful that it can actually watch the whole Earth and give constant weather data for every square inch of the planet. It will do things that now seem absolutely impossible. And we’re beginning to speed up our ability to enter the data: Now huge amounts of information are entered directly from other computers and scanners and direct voice. So with this incredible amount of knowledge entering into human consciousness, it becomes obvious that a major change for humankind is being birthed. For thousands of years spiritual information was kept secret. Priests and priestesses of various religions or cults would give their lives to keep the rest of the world from knowing about one of their secret documents or piece of spiritual knowledge, making sure it remained secret. All the various spiritual groups and religions around the world had their secret information. Then suddenly, in the mid-sixties, the veil of secrecy was lifted. In unison, almost all the spiritual groups of the world opened their archives at the same moment in history. You can browse through books in your neighborhood bookstore and see information that has been sealed and guarded for thousands of years. Why? Why now? Life on this planet is accelerating faster and faster and faster, obviously culminating in something new and different, perhaps just out of the reach of our normal imagination. We are always changing. What does this mean for the world? Why is it happening? Better yet, why is it happening now? Why didn’t it happen a thousand years ago?
Observing these characteristics of a galactic spiral led to another discovery. Other scientists noticed that as our solar system moves through space, it’s not moving in a straight line, but in a helical pattern, a spiral. Well, such a spiral is not possible unless we are gravitationally connected to an other large body, such as another solar system or something larger. For example, many people think the Moon rotates around the Earth, right? It does not. It never has. The Earth and the Moon rotate around each other, and there’s a third component between them approximately one-third of the distance from the Earth to the Moon, which is the pivotal point, an the Earth and Moon rotate around this point in a helical pattern as the also move around the Sun. This happens because the Earth is connecte with a very large body, which is the Moon. Our moon is huge, and it’s caus ing the Earth to move in a particular pattern. And since the entire solar sy tern is spiraling in the same manner through space, then the whole solar sy tern must be gravitationally connected with some other very large body. So astronomers started searching for this body that was pulling on ou solar system. They first narrowed it down to a certain area of the sky tha we were linked with, then they narrowed it down further and further, unti just a few years ago they finally pinned it down to a specific solar system We are linked with the star Sirius—with Sirius A and Sirius B. Our sola system and the Sirius system are intimately connected through gravitation We move through space together, spiraling around a common center. Ou fate and the fate of Sirius are intimately connected. We are one system! Ever since scientists have known about the dark area inside a spiraling galaxy being different, they have discovered that stars don’t just move ou along the curved arm of a spiral. If someone spun a water hose over hi head and you viewed the scene from above, you would see droplets that ap peared to move in spirals. Can you envision that? Each individual drop though, is not moving in a spiral, but is moving radially away in a straighl line from the center; it only appears to be moving in spirals. It’s the same way in a galaxy. Each of these stars is actually moving radially away. At the same time the stars are moving radially away from the center, they are also moving, independent of the system as a whole, from one arm through the dark light into the white light, orbiting the whole galactic system. It probably takes billions of years—I don’t know—for one cycle to complete itself.
On top of that, all kinds of other problems are occurring. Some things are so scary that governments are afraid to tell you anything at all. They won’t tell you about one thing that I simply have to talk about, because it’s so important that somebody has to say something! I know they don’t want me to talk about this, but I don’t think they’ll stop me. 78 We’re finding CFCs in the upper atmosphere. Now, “authorities” in the government have been saying that CFC products like Freon will float up there because they’re lighter than air. But guess what—and you scientist types can check this out: CFCs are not lighter than air, they’re four times heavier than air. They sink, they don’t rise! So how did they get up there? It might have been the 212 aboveground atomic bombs that our governments have blown off in the world. Many people suspect that’s how all those CFCs got up there in the first place, and that it really wasn’t us who caused most of the problem with our air conditioners. It was the atomic governments o/the world. At one point they all went underground with their bombs, and we thought, That’s okay, they’re bombing underground; nothing will happen now. It’s not okay, folks. It’s probably the most dangerous thing that’s going on in the world today, even more than HAARP, and they’re still doing it. I cannot prove what I am about to say, so do not believe it until you can prove it. Adam Trombly, a famous scientist who has accomplished important work in science, has been monitoring the underground atomic bombing around the world. He probably knows more about this than any other person in the world—even the governments recognize this. Trombly explains what happens when these atomic bombs are exploded underground. The energy doesn’t just sit there; it has to go somewhere, so it goes shooting through the Earth, bouncing off its insides, ripping apart the plates and doing incredible damage as it goes bouncing around like a ping-pong ball. This bouncing effect inside the Earth continues for about 30 days after the l explosion. Trombly, much like Jacques Cousteau and others, now has a theory that predicts all kinds of things that will happen—and they’re all happening Mm! Things like the Indian Ocean dropping 23 feet over a very short period of time was predicted by Trombly at least ten years ago—just as Jacques Cousteau had predicted the death of the Mediterranean Sea in ten years. Many brilliant people are speaking out their truth, but few people are listening. If Trombly is correct, we’re only a few more atomic bombs away from the whole planet literally splitting apart in little pieces. The governments around the world have been on red alert since about 1991 over the changes happening to the Earth that were predicted by Trombly. They’re scared to death. Yet I believe China just blew up another one—and the U.S. is talking about blowing one up just because China did!
The governments are really trying to suppress it. Dr. Strecker made a video memorandum of what he believed happened around AIDS. He is a brilliant person. He has worked with retroviruses and is an expert on this subject. He showed the video on television, and the governments threatened him. They allegedly killed his brother and the senator who was sponsoring it. But they didn’t get Strecker—that would have been too obvious, I guess. Dr. Strecker has distributed many of his videos. He got them out to the world, though you don’t hear about it anymore. Dr. Strecker shows on his film how the United Nations was trying to solve an environmental problem. They knew that the biggest environmental problem in the entire world was the human population, and at the rate it was going, the world would double its population by 2010 or 2012. But because of what the Chinese did, allowing only one child per couple, and other strenuous work around the world, they slowed it down. But they believe that it’s still going to happen. It is now estimated that somewhere around 2014 the world population will have doubled. If that happens, computer models have shown that all life on Earth will die or wish they were dead, according to the United Nations, because we can barely keep it together with almost six billion people. Can you imagine what it would be like with II to 12 billion people in the world7 There’s just no way, at least under the present system. So, if you were in the United Nations and knew this potential disaster was going to take place and had to make a decision, what would you do? I’m not judging the people who did this—just put yourself in their position of great power. You see that the Earth is coming to a solid wall, that it’s going to be totally destroyed if something is not changed. So they made a decision—and Dr. Strecker showed the memorandum right on television. The United Nations decided that, rather than hit that wall of 11 billion people, right then and there they were going to create a virus or a disease that would kill specifically three-quarters of the people on Earth. In other words, instead of increasing to 11 billion, they wanted to reduce the current population by three-quarters. He showed the ac-population by three-quarters. He showed the actual U.N. document that planned to eliminate three-quarters of the world’s population. Dr. Strecker showed scientifically exactly how the U.N. did it. They took a virus from a sheep and a virus from a cow and blended them together in a certain way to make the AIDS virus. But before they ever distributed it, they also made a cure for it. The governments have the cure right now, according to Dr. Strecker. The people who were doing this—and history will verify this—were obviously prejudiced, because they singled out two groups: the Blacks and the homosexuals. In Haiti there was an epidemic of hepatitis B moving through the homosexual community, and they all needed to be injected with the hepatitis B vaccine. So U.N. agents took the AIDS virus, put it in the hepatitis B vaccine and injected it into everyone. That’s how the virus started, according to Dr. Strecker. The other evidence that this is true is that throughout the rest of the world, the virus was not given exclusively to homosexuals. In Africa, where at least 75 million people have AIDS, the ratio of male to female infection is almost exactly 50-50, from the beginning until now. Only in Haiti, and eventually in the United States, did it spread almost exclusively through the homosexual population.
According to Dr. Strecker, the World Health Organization, which has been instrumental in creating this disease, has also been concerned about other diseases—and so have doctors pretty much everywhere. For instance, let’s take cancer: Doctors have been concerned that someday cancer will become contagious, not by pollution or foods or things like this, but that it will become airborne or waterborne, like a cold. You’d just walk by somebody with cancer and you’d get it. But the number of different kinds of cancer viruses is so small that the likelihood of that ever happening is pretty slim. It still could happen, but it’s not likely. But for AIDS, there are 9000 to the 4th power or 6,561,000,000,000,000 totally different kinds of AIDS viruses—that’s a huge number. And every time someone gets AIDS, a brand-new virus is created, one that has never been seen before, ever. This means that it’s inevitable, mathematically speaking—it’s just a matter of time— that AIDS will spread rapidly, just like a cold, throughout the world. There is a story going around that the World Health Organization believes that this rapidly spreading form of AIDS may have already begun. Around 1990 or 1991 the WHO checked an African tribe of 1400 members, including everyone from little babies to old people, who obviously had all different kinds of sexual practices (you know, little babies aren’t int o sexual things), and they found that every single member, without exception, had AIDS. That’s when the WHO announced secretly that the virus was probably now airborne or waterborne, and that it might eventually spread like wildfire, like a common cold. There would be a few years’ lag as with any other new disease. If this were to happen, would you know that you are safe? You need to know the truth—you are more than you know!
We’re going to open a new subject: the history of the world and how it relates to the present. Each one of these pieces of the puzzle widens the view. The situation in which we find ourselves in this world didn’t develop at random. Events occurred that we need to remember. Many of us were here in past lives, and we have these memories within us. But that’s beside the point. We need to know exactly what occurred in order to understand how it developed into this situation today. This history, of course, will not be found in history books, because history books of human “civilization” go back only 6000 years, and we need to go back about 450,000 years to begin. This information was first given to me by Thoth around 1985. Thena after Thoth left in 1991,1 became aware of Zecharia Sitchin, read his works, and found out that Sitchin’s and Thoth’s information were almost perfect fits—so perfect it just couldn’t be a coincidence. It was amazing how close they were. Many things that Thoth had mentioned—such as giants in Atlantis, which he didn’t explain further—were explained in Sitchin’s books. And many things that Sitchin appears to have overlooked were deeply explained by Thoth. So the combination of these two sources gives a very interesting viewpoint. You don’t have to accept this viewpoint; you can just listen to it like a legend, think about it and see if it’s workable for you. If something doesn’t feel true to you, then of course don’t accept it. But I believe this is as close as I can get to the truth, and I offer it to you. Remember, I had to translate the geometrical and hieroglyphic images of Thoth into English. Something is bound to get lost, but I do feel it is close enough to trigger your memories. First you must realize something about written history. Somebody has to hold the pen and write it down, so written history is always the viewpoint of the person or people who wrote it. Written history began only in the last 6000 years, but would that history be the same if it had been written by different people? Consider that in most cases it was the winners of the wars who wrote the history books. Whoever won a war said, “This is what happened.” The losers didn’t get to put in their two cents. Look at any of the major wars, especially World War II, which was a very emotional war. If Hitler had won World War II, our history books would be completely different. We’d be examining a totally different set of “facts.” We would be the bad guys, and they would have shown good reason for doing in the Jews etc. But we won, so we wrote it from our perspective.
According to the Sumerian records, approximately 430,000—perhaps as much as 450,000—years ago the Nefilim started having a problem with their planet. It was an atmospheric problem very much like the ozone problem we’re having right now. And their scientists decided on a solution similar to what our scientists have considered. Our scientists have considered putting dust particles into the ozone layer to filter out the Sun’s damaging rays. Nibiru’s orbit takes it so far away from the Sun that they needed to hold in the heat, so they decided to put gold particles into their higher atmosphere, which would reflect the light and temperature back like a mirror. They planned to get large quantities of gold, pulverize it and suspend it in space above their planet. Yes, it is true that they talked about subjects that seem contemporary—ancient humans talking about ETs and sophisticated science. This is not Star Trek or science fiction; it is real. What they said is pretty amazing, and that’s why it’s been so slow coming out into the general public’s knowledge. The Nefilim had the capability of space travel, though they weren’t at that time much further advanced than we are right now, it appears. The Sumerian records show them in their spaceships with flames coming out the back—rocket ships. This is beginning space travel, not sophisticated. In fact, they were so primitive that they had to wait until Nibiru got near enough to Earth before they could even make the trip between the two planets. They couldn’t just take off any old time, but had to wait until they were close. I believe that since the Nefilim weren’t able to leave the solar system, they searched through all the planets that were here and found that Earth had large quantities of gold. So they sent a team here over 400,000 years ago for one purpose only—to mine gold. The Nefilim who came to Earth were headed by twelve members who were like bosses, about 600 workers who were to actually dig the gold, and about 300 who stayed in orbit in their mothership. They first went into the area of present-day Iraq and began to establish themselves and build their cities, but that’s not where they mined the gold [Fig. 3-9]. For the gold, they went to a specific valley in southeast Africa. One of the twelve, whose name was Enlil, was the leader of the miners. They went deep into the Earth and dug large quantities of gold.
Somewhere between 300,000 and 200,000 years ago the Nefilim workers rebelled. The Sumerian records wrote about this rebellion in great detail. The workers rebelled against their bosses; they did not want to keep digging in the mines. You can imagine the workers saying, “We’ve been digging this gold for 150,000 years, and we’re tired of it. We’re not going to do this anymore.” I would probably have lasted about one month. The rebellion presented a problem for the bosses, so the twelve leaders came together to decide what to do. They decided to take a certain life form that already existed on this planet, which was, as I understand it, one of the primates. Then they would take the blood of the primates, mix it with clay, then take the sperm of one of the young male Nefilim and mix these elements together. The tablet actually shows them with what looks like chemical flasks, pouring something from one flask to another to create this new life form. Their plan was to use the DNA of the primates and their own DNA to create a more advanced race than Earth had at that time so that the Nefilim could control this new race for the sole purpose of mining gold. According to the original Sumerian records, we were created to be miners, as slaves to mine gold. That was our only purpose. And when they mined all the gold they needed to save their own planet, their intention was to destroy our race and leave. They weren’t even going to allow us to live. Now, most people hearing that would think, That can’t be us; we’re too noble for something like that. But that is what the oldest written records on Earth state to be the truth. Remember, Sumerian is the oldest known language in the world, older by far than works such as the Holy Bible and the Koran. It now appears that the Holy Bible was birthed out of the ashes of Sumer. What science has discovered is almost as interesting. In the exact place where the Sumerian records say we mined gold, archaeologists have found gold mines. These ancient gold mines are dated back as far as 100,000 years. What is really incredible is that Homo sapiens (that’s us) were mining gold in these mines. Our bones were found there. Those gold mines had been worked at least 100,000 years ago, and they have dated humans in these mines as early as 20,000 years ago. Now, what the heck were we doing mining gold 100,000 years ago? Why did we need gold? It’s a soft metal, not something you could use like certain other metals. It wasn’t found very often in ancient artifacts. So why were we doing this, and where was it going?
To recapitulate and clarify: After the rebellion, when it was decided to create a new race here on Earth, it was the Nefilim who became the mother aspect. The Sumerian record says seven females stepped forward. Then the Nefilim took clay from the earth, blood from the primate and sperm from the young Nefilim male, mixed this together and put it into the wombs of the young female Nefilim who were chosen for this. They gave birth to human babies. So seven of us were birthed at once, not just one Adam and Eve, according to the original stories—and we were sterile. We could not reproduce. The Nefilim continued procreating little humans, making an army of little beings—us—putting them on the island of Gondwanaland. If you want to believe this story, which is part Sumerian record and part Thoth, our race’s mother is Nefilim and our father is Sirian. Now, if it were not for the Sumerian records concerning the Nefilim, this would all seem absolutely outrageous—and it still does. But there’s a tremendous amount of scientific evidence that this is true if you read the archaeological records—not about the Sirian father, but definitely about the Nefilim mother. Science doesn’t understand how we got here. You are aware that there’s a “missing link” between the last primate and us. We seem to come out of nowhere. They do know that we’re somewhere between 150- and 250,000 years old, but they have no idea where we came from or how we developed. We just stepped through some mystical doorway and arrived.
When the time was right, the Naacals from Lemuria created a spiritual representation of a human brain on the surface of their Atlantean island. Their purpose was to birth a new consciousness based on what they had learned during Lemuria. They believed the brain had to come first before the body of the new consciousness ofAtlantis was to emerge. With Thoth’s image of the human brain in mind, you can begin to make sense of their actions. First they made a wall down the middle of the island about 40 feet high and 20 feet wide, which sealed off one side of the island from the other. Literally, you had to go into the water to get to the other side. Then they ran a minor wall across at 90 degrees to the first wall, which divided the island into four parts. Then half of these thousand people, who were of the Naacal Mystery School, went on one side and half stayed on the other, depending on their nature. That could mean that all the women stayed on one side and all the men went to the other side, but as I understand it, where a person went did not depend on the physical body, but his or her dependency on one side of the brain or the other. In this way, approximately half became the male component of the brain and the other half became the female component. They spent thousands of years in this physical state until they believed they were ready for the next step. Three people were selected to represent the corpus callosum, the part of the brain that links the left and right hemispheres together. Thoth’s father, Thome, was one of these. He and two other people were the only ones allowed to go everywhere on the island. Otherwise, the two sides had to remain completely separate from each other.
Suddenly, in a single day, the brain of Atlantis, the Naacal Mystery School, breathed life into the Tree of Life on the surface of Atlantis. This created vortexes of energy rotating out of each of the circles on the Tree of Life. Once the vortexes were established, then the brain ofAtlantis psychically called forth the children of Lemuria. Millions and millions of Lemurians, who by then had settled along the west coast of North and South America and in other places, began to be pulled to Atlantis. A great migration began, and the ordinary people of the sunken Lemuria started moving toward Atlantis. Remember, they were feminine right-brained beings and inner communication was easy. However, the Lemurian body of consciousness had reached only the age of twelve as a planetary consciousness. It was still a child, and some of its centers weren’t functioning yet; they had worked with those energies and had mastered only eight of the ten. So each migrating Lemurian was attracted to one of these eight centers on Atlantis, depending on the nature of the individual. There they settled and began to build cities. That left two vortexes with nobody using them, not a single person. These two vortexes were pulling life toward them, and in life you just can’t have an empty place. Life will find a way to fill it. For instance, if you’re driving along the freeway following another car and you drop too far behind it, somebody will fill in the gap, right? If you leave a place empty, life will step in and fill it. That’s exactly what happened on Atlantis. Though Lemurians settled into only eight of the vortex areas, Mayan records state clearly that there were ten cities in Atlantis when it fell. In fact, you can see those records in the Troano document, which is now located in the British Museum. This document is estimated to be at least 3500 years old, and it describes in detail the sinking ofAtlantis. It’s Mayan, and it contains an authentic account of the cataclysm, according to Le Plongeon, the French historian who translated it.
The pieces of the comet that crashed into the southwestern area of Atlantis happened to be right where the Martians were living, killing a huge portion of their population. The Martians got hurt the worst by consenting to allow the comet to come in. Well, that was too humiliating and painful for them. This was the beginning of a great loss of consciousness for Earth. What was about to take place was the seed for a bitter tree, the same tree we live by today. The Martians said, “It’s all over. We are divorcing you. We’re going to do whatever we want from now on. You can do whatever you want, but we’re going to lead our own lives and try to control our own fate. And we’re not going to listen to you ever again.” You know this whole number. We’ve seen it in divorced families throughout the world. And the children? Look at our world! We are the children! The Martians decided to take over the Earth, of course. Control, the Martian’s primary interface with the Reality, rose to meet their anger. They began to create a building complex like the one they had constructed on Mars a long time earlier, in order to create a synthetic Mer-Ka-Ba once again. The only thing is, around 50,000 Earth years had passed since they had created one, and they didn’t remember exactly how to do it—but they thought they did. So they built the buildings and began the experiment. That experiment is directly tied to a chain of Mer-Ka-Bas that began with the Mars experiments a little less than a million years before. Later, one was done here on Earth in 1913, another one in 1943 (called the Philadelphia Experiment), another one in 1983 (called the Montauk Experiment), and another one that, I believe, they’re attempting to do this year (1993) near Bimini Island. These dates are windows of time that open up and are tied to the harmonics of the situation. The experiments must be timed to these windows in order to succeed. If the Martians had succeeded in setting up a synthetic harmonic Mer-Ka-Ba, they would have had absolute control of the planet, if that was their intention. They would have been able to make anybody on the planet do anything they wanted, though eventually it would have meant their own demise. No higher-order being would place this kind of control on another if they truly understood the Reality.
The Martians’ attempt to control the world took place near one of the Atlantean islands in the area we now call the Bermuda Triangle. There’s an actual building sitting on the ocean floor down there that contains three rotating startetrahedral electromagnetic fields superimposed on each other, creating a huge synthetic Mer-Ka-Ba that stretches out over the ocean and into deep space. This Mer-Ka-Ba is completely out of control. It’s called the Bermuda Triangle because the apex of one of the tetrahedrons—the stationary one—is sticking up out of the water there. The other two fields, are counterrotating—and the faster-rotating field sometimes moves clockwise, which is a very dangerous situation. (When we say clockwise, we mean the source of the field, not the field itself. The field itself would appear to be rotating counterclockwise.) You’ll understand this when you learn more about the Mer-Ka-Ba. When the faster field rotates counterdocfewise (from its source), everything’s okay; but when the faster one moves clockwise (from its source), that’s when time and space distortions happen. Many of the airplanes and ships that have disappeared in the Bermuda Triangle have literally gone into other dimensional levels because of the out-of-control field there. A primary cause of much of the distortion in the world—the distortion between humans such as wars, marital problems, emotional disturbances etc.—is that imbalanced field. That field is not only causing distortions on Earth, it’s causing distortions way, way, way out in remote areas of space because of the way Reality is constructed.
At the time of the synthetic Mer-Ka-Ba failure, there were about 1600 ascended masters on Earth, and they did everything they could to try to heal the situation. They tried to seal the dimensional levels and get as many of these spirits as they could out of people and back into their own worlds. They did everything on every level they could. They eventually got most of the spirits out and healed about 90 to 95 percent or more of the situation, but people still found many of these unusual beings living in their bodies. The situation at that time began to deteriorate extremely rapidly. All the systems on Atlantis—financial, social and all the concepts of how life ought to be—degenerated and collapsed. The continent of Atlantis and all its people became sick. They started getting weird diseases. The entire continent went into a state of survival just trying to live through each day. The situation grew continually worse. For a long period of time it was hell on Earth, horrible. If it had not been slowed down by the ascended masters, it would truly have been the end of this world. The ascended masters (the highest levels of our consciousness at that time) didn’t know what to do to help bring us back into a state of grace. I mean they really didn’t know what to do. They were children compared to the events that had been forced upon them, and they had no idea how to handle it. So they prayed. They called in higher levels of consciousness. They called in everybody who could hear their plea, including the great Galactic Command. They prayed and prayed. So the problem was reviewed on many high levels of life. Similar kinds of events have happened before on other planets; this wasn’t the first time. So before it actually happened, our ascended masters and galactic friends knew that we were going to fall out of grace, out of the high level of awareness we were experiencing at the time. They knew that we were going to fall way down the spectrum of life. Their concern was to figure out some way to get us back up on track after the fall, and they knew it had to be done quickly. They were looking for a solution that would heal the whole Earth, both the dark and the light. They weren’t concerned with a solution where only the Martians would be healed, or only the Lemurians or only part of the Earth. They were looking for a situation that would heal the whole Earth and all of its inhabitants.
These people took their picture to Australia and conducted a study there. They selected a certain number of people from a spectrum of the population, then showed each of them the picture, giving them a certain length of time to look at it. They held the photograph up to someone and said, “How many faces do you see in this photo?” During the time the subjects were given, they would generally come up with six, seven, eight, nine or maybe ten faces. Few people saw more. When they had gotten a few hundred people as their basic sampling and recorded accurately what had been observed, some of the researchers went to England—on the other side of the planet—and showed the picture on a closed-cable BBC television station that broadcasts only to England. They carefully showed where all the faces were, every single face. Then a few minutes later other researchers repeated the original experiment with new subjects in Australia. Suddenly people could easily see most of the faces. From that moment, they knew for certain that there was something about humans that had not been known. Now, the Aborigines in Australia had known about this “unknown” part of us for a long time. They knew that there was an energy field connecting people. Even in our society, we’ve observed that somebody on one side of the planet would invent something very complex at the same moment that someone on the other side of the Earth invented the same thing, with the same principles and ideas. Each inventor would say, “You stole it from me. It was mine. I did it first.” This has happened many, many times, stretching back for a long time. So after this Australian experiment, they began to realize that something very definitely connects us all.
Now that we have the necessary background, we can continue with the drama in Atlantis. The project to rebuild the grid was begun by three men: Thoth, a being named Ra and a being named Araragat. These men flew to a place in what is now Egypt, to the area now called the Giza plateau. At that time it was not a desert, but a tropical rain forest, and it was called the Land of Khem, which means the land of the hairy barbarians. The three men went to that particular place because the axis of the old unity-consciousness grid extended out of the Earth from that point. They were going to rebuild a new grid on the old axis, according to instructions given by higher consciousness. They had to wait until the right moment—until the precession of the equinoxes passed the low ebb in consciousness—before they could act, and this low ebb was still far into their future. After that they would have a little less than half a cycle, about 12,900 years or so, to complete everything by the end of the twentieth century. We couldn’t go any longer than this or we would destroy ourselves and our planet. First they had to complete the grid on the higher dimensions, then they had to physically build the temples in this dimension before the new unity grid would manifest. Once manifested and balanced, they were to help us begin to consciously move into the higher worlds of being and begin anew our path home to God.
So Thoth and friends went to the very spot where the unity-consciousness vortex exited the Earth. This point was about a mile away from where the Great Pyramid sits in the desert today, but then it was out in the middle of nowhere, in the middle of a rain forest. Centered right over the axis of this vortex on the Earth, they created a hole extending approximately one mile into the Earth, lining it with bricks. It took only a few minutes or so, because they were sixthdimensional beings, and whatever they thought always happened. It was that simple. Once the hole aligned with the unity axis was created, they mapped the ten Golden Mean spirals that emerged from the hole and located where they moved above the Earth. They used the hole as the axis, starting far down, and mapped the spirals of energy as they moved up out of the hole and extended into space. One of the spirals exited the Earth not far from the present Great Pyramid. Once they found it, they built a little stone building in front of the hole; that building is the key to the entire Giza complex. Then they built the Great Pyramid. According to Thoth, the Great Pyramid was built by himself, not Cheops. Thoth says that it was completed about 200 years prior to the shifting of the axis. The apex of the Great Pyramid, if the capstone were in place, sat exactly on the curve of the spiral. They lined up the center of the hole with the south face of the stone building and the north face of the Great Pyramid. It has amazed surveyors who have looked at this. Though these structures are a mile away from each other, the south face of the stone building and the north face of the Great Pyramid are in perfect alignment. They do not believe that we could do it any better today even with our modern technology. Later the other two pyramids were also built directly on that spiral. In fact, that’s how the hole was discovered, through aerial photography. They noticed that the three pyramids were laid out on a logarithmic spiral. Then they traced the spiral back to its source and went to that spot, and there was the hole and the stone building. That discovery was made, I believe, in the early 1980s. It was recorded in the McCollum survey that was completed in 1984 by Rocky McCollum. I’ve seen the axis hole and the building with my own eyes. I consider it to be the most important place in all of Egypt and so does the Edgar Cayce A.R.E. There’s also another hole about a city block away from the first spiral, and this spiral starts out a little differently, but then slowly, asymptotically, superimposes itself over the first spiral. To be able to build around this hole in this spiral pattern, the planners had to have a very sophisticated understanding of life. (I’ll explain this understanding later also.) So these two completed spirals defined the axis of what would eventually become the unityconsciousness grid around the Earth.
The location of the sacred sites of the world are no accident. It was a single consciousness that created every single one of them—from Machu Picchu to Stonehenge to Zaghouan—you name it, to anywhere. Almost all of them (with a few exceptions) were created by a single awareness. We’re becoming more aware of this now. Richard Hoagland’s work brings this forth, though he wasn’t the first one. They show how one sacred site is extrapolated from another one, then another and still another. These sites go beyond time, in that they were all built at different times, and they go beyond any particular culture or geographical location. They were obviously done by one consciousness who coordinated the whole enterprise. Eventually researchers will see that this spot in Egypt is the point from which all the other sacred sites were calculated. This Egyptian area is the north pole of the unity-consciousness grid. On the other side of the planet, out in the South Pacific in the Tahitian Islands, is a little island called Moorea, where the south pole of the grid is located. For those of yo u who have been on top of Wayna Picchu for a birds-eye view, Machu Picchu, at about 9000 feet in the Peruvian mountains, seems to be surrounded in a perfect circle by mountains. It’s like a female circle surrounding a phallus rising in the middle. Well, the island of Moorea is similar to this, only it’s shaped like a heart. Each house on Moorea has a heart with the house number on it. The phallic Moorean mountain in the center of the heart is much bigger than Wayna Picchu in Peru, but you will still see the same ring of mountains surrounding this earthen pole. This is the precise south pole of the entire unity-consciousness grid. If you go straight through the Earth at Moorea, you come out in Egypt. It’s off only an ever so tiny bit—there’s a very slight curve, which is natural. The Moorean pole is negative, or female, and the Egyptian pole is positive, or male. All the sacred sites are connected to the Egyptian pole, and they’re all interlinked through the central axis leading to Moorea. It’s a torus, of course.
According to Thoth, the Sphinx goes back at least five and a half million years. I guess eventually that will be brought forth, because he hasn’t been wrong about anything yet. Even John Anthony West secretly suspects that it is a great deal older than 10- to 15,000 years. He wasn’t concerned with making speculations into the millions of years; he just wanted to get it well past the 6000-year mark, because that will crack our previously accepted Earth history. He and his team have now done that, and later, I believe, they’ll try to push the date back further as they introduce more evidence. According to Thoth, approximately one mile under the Sphinx there is a round room with a flat floor and a flat ceiling. Inside this room is the oldest synthetic object in the world—older than any other consciously assembled matter on Earth. According to Thoth, though even he can’t prove it, this object goes back 500 million years when “that which led to human life” began. The object is about two city blocks in size; it’s round like a disk and has a flat bottom and top. It’s unusual in that its skin is only three to five atoms thick. Its top and bottom surfaces have a certain pattern that’s shown in Figure 4-8. The pattern itself is five atoms thick; everywhere else it’s only three atoms thick. And it’s transparent—you can see right through it—almost like it’s not there. This is a ship, but it has no motors or visible form of power. Even though Doreal’s interpretation of The Emerald Tablets states that this ship had atomic motors in it, according to Thoth it does not. Doreal translated The Emerald Tablets in the Yucatan in 1925 and could not understand the description of how the ship was powered. The idea of atomic motors was the farthest-out idea he could think of for a power source. But it is actually propelled by thoughts and feelings, and is designed to connect with and extend your own living Mer-Ka-Ba. This ship is connected directly to the spirit of the Earth, and in The Emerald Tablets it’s called a warship. It was the protector for the Earth.
rain forest and went back to Atlantis to prepare. It sat alone for 200 years, because they knew that at that critical point in the precession of the equinoxes, the poles would shift. They knew that Atlantis would sink, so they waited. One day it finally happened. The catastrophe actually took only one night. Science has proven that when poles shift, it takes about 20 hours. It happens just like that [snaps fingers]. You wake up one normal day, and that evening it’s a totally different world. The whole process is about three and a half days long, but the pole shift happens in about 20 hours. We’re all going to experience this enormous change when we see big chunks of the United States start to drop off into the water—then you’ll know it’s for sure. There are other early signs that will tip you off that the change is about to happen. When enough information has been given, I’ll remind you of what you already hold in your memories. When they saw the very first signs of the shift coming on, Thoth, Ra and Araragat returned to the Sphinx and raised the warship into the sky. All they did was raise the vibration of the molecules only one overtone higher than the Earth exists on. This allowed them and the ship to pass right through the Earth into the sky. Then they moved to Atlantis, lowered the ship to the surface, and picked up the people of the Naacal Mystery School, which included the original immortals from Lemuria as well as those who became enlightened during the time of Atlantis (by that time about another 600 people had ascended). So the original thousand from Lemuria and the 600 from Atlantis had increased the number of ascended masters to about 1600, the only occupants of the ancient airship.
Its connection to the Earth’s magnetic field is not understood by science at this time. If you don’t have a means of protecting your memory, it will be erased, gone. It’ll be like unplugging a computer in the middle of a file. It’s just gone. That’s exactly what happened to the Atlanteans and others who survived the catastrophe but who didn’t have spinning Mer-Ka-Bas. Those very sophisticated people, who were more advanced than you and I, suddenly found themselves in a situation where they didn’t know anything. They had high-tech bodies and high-tech minds, but it was like having a great PC sitting on the table with no software, nothing there. So the population that survived, and there were a few, had to start all over again. They had to begin at square one to figure out how to stay warm, how to make fire and so on. This loss of memory was the result of their forgetting how to breathe, forgetting their Mer-Ka-Bas, forgetting everything—falling down through the dimensions, going into a totally unprotected state and ending up in this very dense world—having to eat food again, doing all kinds of things that hadn’t been part of our experience for a very long time. They were slammed into a very dense aspect of the planet and had to learn to survive all over again. This was all a result of the synthetic Mer-Ka-Ba experiment that had taken place on Atlantis. Without that small group of ascended masters, we would not have survived at all—we definitely would all have left human experience. The whole Earth experiment would have been over forever. But they kept the field alive, just barely, while everything else crashed around them. Besides the ascended masters, there were also two other groups on Earth who had Mer-Ka-Ba fields intact at the time. The Nefilim and the Sirians, our mother and father, kept their fields alive. I don’t know where the Nefilim retreated to within this planet’s dimensional worlds, but the Sirians remained in the Halls of Amend, inside the inner earth. Both of these groups are still here on the planet, hidden within the dimensional worlds.
Egypt became the male component of the grid. That is where the male structures were laid out. There’s hardly any femaleness there compared to female areas of the world. Of course, the polarity to maleness does exist— Isis is that counterpart—but the overall energy flow is male. South America, especially Peru, Central America and also parts of Mexico became the female component of the grid. However, ultimately the entire female aspect of the grid became centered at the complex in Uxmal, in the Yucatan, where many survivors from Atlantis had found refuge. Starting at Uxmal, seven temples are laid out in a spiral, probably a Fibonacci spiral, and they are the seven primary temples of the female component of the grid. These are chakra centers, just like the chakra centers that are laid out down the length of the Nile. These feminine centers begin with Uxmal, then go to Labna, then to Kabah, then over to Chichen Itza, then over to Tulum near the ocean, then way down near Belize to Kohunlich, curving back inland to Palenque. Those seven places created the primary spiral of the feminine aspect of the grid being created for our new Christ consciousness, which we are only now able to access. From Palenque the feminine aspect of the grid splits north and south. Here we see another polarization of the energy. The feminine component of the female spiral of the grid heads south and jumps over to Tikal in Guatemala, and that begins a new octave. When we relate it to music, the seventh site bridges to the eighth note, or the beginning of the next octave of the next spiral. And the spiral keeps going south through the feminine component of the grid. Eventually it moves through places like Machu Picchu and Sacsayhuaman near Cuzco, Peru. One of the main spirals ends in a place called Chavin, in Peru, which was the primary religious center of the Incan empire. From there it goes to Lake Titicaca to a place about a half a mile off the Island of the Sun in Bolivia. Then it makes a 90-degree turn and heads out toward Easter Island and finally to Moorea, where it anchors into the Earth. Heading north from Palenque is the male component of the female aspect of the grid. It goes through the Aztec ruins and up through the American Indian pyramids. (The American Indians made physical pyramids, some remains of which can be seen in and around Albuquerque, New Mexico.) Then the spiral continues to Blue Lake near Taos, New Mexico, which is the counterpart of Lake Titicaca. This is one of the most important areas in the United States, protected for a long time by the Taos Indians. Again, there’s a 90-degree turn at Blue Lake. From there the spiral heads out across the mountains, going through Ute Mountain (on the New Mexico side of the Colorado border) and through many mountains and structures that have been built.
In conjunction with the sacred sites, the creators also used mountains because of their vortex energy. Finally, before the spiral leaves the coast of California, it passes through Lake Tahoe, Donner Lake and Pyramid Lake. From there it goes through underwater mountain complexes until it reaches the Hawaiian Islands, where Haleakala Crater is one of the primary components, then heads south again. It goes through the Hawaiian Island chain that connects for thousands of miles all the way back to Moorea. So it’s a huge open circle that comes around the Earth, starting at Uxmal and connecting at the south pole of the Christ grid. The feminine component of the grid is a massive circle of complexes. Understand that in between each of the major sites mentioned above are literally hundreds of smaller sites—churches and temples of many religions, sacred sites of nature such as mountain peaks and ranges, lakes, canyons and so on. If you could see the greater plan, you would see how they form perfect spirals, first moving clockwise, then moving counterclockwise until they reach their destination, Moorea, in the South Pacific. The pyramids built in the Himalayan mountains were primarily crystalline in nature, meaning they were constructed by using third-dimensional crystals at the corners, aimed to form a pyramid. They built physical pyramids there, too—lots of them. Most of them are not known, though some are. The largest known pyramid in the world so far is in the western mountains of Tibet. It’s a solid-white pyramid that’s in almost perfect condition, with a huge, solid-crystal capstone. At least two teams of scientists have been there, and it has also been photographed from the air. It’s visible only three weeks out of the year when its crystal capstone peers out of the deep snow to view a valley long deserted from human endeavor. I talked with the leader of the team that went into this pyramid. He said it looks like a brand-new pyramid and that there’s nothing written on the walls. It’s white, smooth and hard, like marble. When they entered it, they went down a long tunnel, where they found a large room in the center. There’s no writing anywhere, no designs, no nothing—except that in the middle, high up on a wall, there is one inscription—the Flower of Life! That’s it. If you want to say everything, all you have to do is put that on a wall. That says it all. By the end of this book you’ll understand why.
According to Thoth, there are five different levels of human consciousness possible here on Earth. These are people who have different DNA, completely different bodies and different ways of perceiving the Reality. Each level of consciousness grows from the last one, until finally on the fifth level humanity learns how to translate into a whole new manner of expressing life, leaving Earth forever. The primary visual difference between these types is their height. The first-level people are about 4 to 6 feet tall. The second-level people are about 5 to 7 feet tall, where we are at now. Third-level people are about 10 to 16 feet tall, which we are about to translate to. The fourth-level being is about 30 to 35 feet tall, and the last is about 50 to 60 feet. These last two levels are for the distant future. This may seem strange at first, but do we not begin as a microscopic egg and get larger and larger until we are born? Then we continue to grow taller and taller until we are adults. According to this theory, the human adult is not the end of our growth pattern. We continue through DNA steps until we are 50 to 60 feet tall. Metatron, the Hebrew archangel who is the perfection of what humanity is supposed to become, is 55 feet tall! Remember the giants who lived here on Earth referred to in chapter 6 of Genesis? According to the Sumerian records, they were about 10 to 16 feet tall. When we look at a three-year-old and a ten-year-old, we know that they have different levels of consciousness, and it is primarily by their height that we make this judgment. According to Thoth, each level of consciousness has different DNA; however, the primary difference is the number of chromosomes. Using this theory, we are now on the second level and have 44+2 chromosomes. An example of the first level is certain aboriginal tribes in Australia where they have 42+2 chromosomes. On the third level, which we are about to move to, people have 46+2 chromosomes. The next two levels have 48+2 and 50 + 2, respectively. We will discuss this in depth in the second volume of this book and show the sacred geometry around this understanding, which will make it clear.
After Akhenaten was gone, the 300 immortal Egyptians joined the Tat Brotherhood and waited from roughly 1350 B.C. to about 500 B.C.—about 850 years or so. Then they migrated to a place called Masada, Israel, and formed the Essene Brotherhood. Even today Masada is known as a capital of the Essene Brotherhood. These 300 people became the inner circle, and mostly ordinary people formed an outer circle, which became very large. Mary, the mother of Jesus, was one of the members of the inner circle of the Essene Brotherhood. She was immortal even before Jesus became immortal. Joseph came from the outer circle. This is according to Thoth; it’s not written in the records. It was part of the Egyptian plan that the next step would be to bring in someone who would demonstrate exactly how to become immortal when starting as an ordinary human, put the experience into the akashic records and make it real. Somebody had to do it. According to Thoth, Mary and Joseph came together and mated interdimensionally (which we’ll talk about later) to create the body for Jesus, which would allow his consciousness to come in from a very, very high level. When Jesus first came in, he began life on Earth as human as any of us. He was totally human. And through his own work he transformed himself to the immortal state through resurrection, not through ascension, and put into the akashic records the process of exactly how to do it. This is according to Thoth, and it was planned a long, long time before it ever took place.
We’re going to begin with a realization that the Christian and the Egyptian understandings of Reality are almost identical. The Christian understanding is derived from the Egyptian. Here are the first three sentences of the Christian Bible: “In the beginning God created the heaven and the earth. And the earth was without form and void, and darkness was upon the face of the deep, and the spirit of God moved upon the face of the waters. And God said, ‘Let there be light,’ and there was light.” To begin with, this statement that the Earth was without form until it came out of the Void, out of nothing, is exactly what the Egyptians believed. It’s also what many other religions believe. Both Egyptian and Christian religions believe that all that’s needed to start the process of creation is nothing and spirit, and when those two concepts are brought together, then all things can be created. They believe that creation begins by the movement of spirit. In the second sentence it says, “The earth was without form and void” and that the spirit of God moved upon the face of the waters. Then in the very next sentence, God says, “Let there be light.” The movement happened first, then the light happened immediately after. According to the Egyptian belief, one tiny detail was left out of the current Christian Bibles. It isn’t necessarily wrong in the older Bibles, though. There are 900 versions of the Bible in the world, and in many of the older ones the first sentence says, “In the beginning there were six.” It starts out in other ways too; it’s been changed many times over the years. The ancient Egyptians would say that the way our modern Bibles begin creation is impossible, especially if you think about it from a physics point of view. Imagine a dark, infinite space that goes on forever and ever in all directions. There’s nothing in it—just infinite space with nothing in it. Imagine yourself—not your body, but your consciousness—being in the middle of that. You’re just floating there with nothing. You can’t really fall, because where would you fall to? You wouldn’t know if you’re falling down or going up or off to the side; in fact, there’s no way to experience any motion at all. From a purely physics or mathematical point of view, motion itself, or kinetic energy, is absolutely impossible in a void. You can’t even rotate, because motion cannot become real until there’s at least one other object in the space around you. There has to be something to move relative to. If you don’t have something relative to move to, how would you know you’re moving? I mean, if you went up thirty feet, how would you know that? There’s no change. With no change, there’s no movement. So the ancient Egyptians would say that before God “moved upon the face of the waters,” He/She had to first create something to move relative to.

This paper explores issues related to software that I have developed for personal use in my studio. The software, called Hodos, can generate paintings which bear an uncanny resemblance to work I did before becoming involved with computers.

First I will outline the features of the art concept which occupied the mature phases of my own work long before my involvement with electronics. Then we will focus on the essential features of the "artistic decision" procedures used in expressing the art concept.

Next we will review some of the salient features of the software which embodies these art form ideas. A review of several works generated by the software will show that they reflect essential characteristics of the earlier paintings.

Finally we will see that this view provides important considerations for the future of art:

First, this kind of software is a medium of a different "order" than any historical medium. Because such software embodies the procedures for artistic improvisation it can be used for innovative variation on the artist's theme without the artist being present. Although each work may be "one of a kind" it does belong to a family. We must ask whether, to what extent, and how the artist's hand is present in the work or in a family of works. What can we say of the apparent feeling in the brush strokes?

Second, perhaps a more important consideration, is the 'quantum leap' in procedure or process. This new artistic process, while hardly the same, is remarkably analogous to the biological process of epigenesis. The software, Hodos, may be viewed as a genotype (gene) since it is the code for "how to make the work". The software can make a "family" of works - with each work being unique (one of a kind, yet familial). The potential for crossing families of different artists opens new domains which includes the hybridization of form and, eventually, a genealogy of form.

These considerations open the door to a series of interesting questions on authorship, originality, the role of the individual and the art-making process.

The Dance Between Order and Chaos. In making these works the first phase was to delineate carefully one or several rectangular fields with variable spacing creating visual movement relative to the overall field. These rectangles were carefully filled with a heavy synthetic impasto making a slightly raised rectangular relief on the panel.

Next, in very intense sessions, I would mark the surface with spontaneous gestures, sometimes with brushes , sometimes with crayons or pencils. The gestural marks were imbedded in layers of colored stains that drew out the grain of the wood.

The gestural aspects of the work represented that aspect of human experience that might come from "uncontrol" - such as a feeling or an impulse. The dominant rule was that the marks had to be entirely spontaneous without any conscious editing. For several years I had worked through hundreds of gestural works in an effort to make irrational marks.

Through this process, in a kind of spiritual quest - one has to empty the self of "thinking", be entirely present to the moment, and strive to be one with one's world. To be one with the brush, the crayon, the panel, the universe - in a free flowing gesture was indeed the goal. Being most "free" was also being most "joined". A gesture free of rigid aesthetic conceptions harmonizes easily with natural forces. This can be seen in the splash of paint that yields to unseen forces like gravity. It reveals the forces to which it yields and to which it is joined. Can this be achieved with software?

In the final stages of the work the rectangular areas were painted according to another set of rules - but never absolutely so. Each rectangle was painted with a value and a color that gave it a visual push or pull slightly forward or slightly receding from the predominant picture plane. Simultaneously the placement was intended to lend precarious but effective balance between all the lateral inclinations. The push or pull, advancing or receding, and the lateral movements, were a visual means of keeping a tension (opposition) between the rectangles and yet maintaining an equilibrium in the overall painting.

Meanings and Interpretations. The present works represent apparent polarities of human experience - the rational and the irrational, body and spirit, life and death, heaven and earth. Both in the paintings and in human experience we find that radical polarity generally implies its opposite - dark implies light or conversely light implies dark.

The works, as a visual dialectic between control and uncontrol, embody an idea about how to make art. They also represent the spiritual struggle of life itself. One learns that peace and unity lie in the balance between reason and feeling - between all those interior forces that push and pull us in opposite directions. They seem related to traditional oriental wisdom rooted and  imaged in the yin/yang, woman/man, moon/sun, soft/hard, earth/heaven.
In order to translate an art concept into software one must first describe the specific art making procedures. This is achieved by thinking through each step. For example if the procedure begins with drawing a line then the basis for all the line drawing procedures must be identified. This includes the basis for the starting point, the color decisions, the quality and character of the line, and its changing angles, flow, and length. Once these procedures and relationships are understood they can be translated into software. The software code is a formal system equivalent to the art concept, an isomorphism in a computer language.

A similar isomorphism lies at the heart of D. Hofstadter's book Godel, Escher, Bach: An Eternal Golden Braid. In discussing how one can view a formal system both typographically and arithmetically, he notes that

"typographical rules for manipulating numerals are actually arithmetic rules for operating on numbers.

This simple observation is the heart of Godel's method, and it will have an absolutely shattering effect . It tells us that once we have a Godel-numbering for any formal system, we can straight-away form a set of arithmetical rules which complete the Godel isomorphism. The upshot is that we can transfer the study of any formal system - in fact the study of all formal systems - into number theory".

To reiterate, if we can describe the procedures for expressing our "art concept" then we can code those procedures and work with them arithmetically. Suppose then that we are able to describe an "art concept" rather comprehensively. The description of an art concept is essentially an outline of the decision system that governs the art-making procedures. We outline why, how, and where we prefer to draw lines or paint areas. We outline the basis for how we scale relationships, choose colors and space the pictorial elements within the work. We identify the conditions of acceptable and unacceptable combinations of shape, scale, form and color. When these steps have been done in a thoroughly systematic way, we have described a formal system that can then be coded.

While Hodos embodies the essential art concepts with which I work the process is still rather primitive. There are tested routines planned for the existing program and more to be developed. The process never ends. One reaches a plateau which provides the viewpoint from which one sets up operations to attain the next plateau. Through this dialectic the original art concept undergoes transformation and the software evolves to the next stage.

Let us consider how this works specifically for the most obvious "control" elements, the rectangular shapes. In the wood panels I created in the 1960's the rectangular shapes do not overlap, they are never closer or further apart than certain distances relative to the over-all space. They are painted with opposing hues but with intensities and values (shades) just close enough to neutral to keep the balance precarious. Further analysis would show other subtle relations as part of the overall formal system. All of this can be coded so that once the first move is made the procedure grows a set of rectangular shapes based on the artist's rules.

The automated procedures for identifying areas use both control (by fixing parameters) and uncontrol (by selecting randomly within the parameters). For example, a 'starting point' routine may need several pieces of controller information such as preferred and forbidden starting areas. If one thinks of the pseudo-randomizing routine as 'tossing dice' , then the procedure in plain English might read as follows: "Find a valid starting point by tossing dice weighted for not more than a 25% deviation from the center of a preferred region.

The artist, in the refining process, usually reviews scores of possibilities before making a software decision. By observation the artist learns to adjust the routine and the data to achieve the desired formal effects. After some experience the artist might see how he can modify the algorithm to introduce variable scaling relationships that were not evident in earlier work. This feedback process provides a way for both refinement and growth.

We can do the same with every element that the "art concept" requires for building the work. For example, a scribble routine is coded to rotate randomly within certain parameters. The rotation parameters have been determined by experience with what has worked and what hasn't. Still the variables are changeable and are revised from time to time. Sometimes the slightest change of a routine opens up new vistas. The "uncontrol" elements that course through the program may be better understood if we outline their sources and how they entered the work.

Uncontrol: Sources and Evolution

In the initial phases of my computer work I sought to write a simple program just to sample the computer as a medium, that is, just to get the computing process to generate the work. The simplest way would be to mime automatic art. Step one was to make a scribble loop that would unveil the inner workings of the computer, similar to the way a Paul Klee doodle revealed the inner life of Paul Klee. That venture was not very successful but it yielded insight into computerized automatic processes. Such work is related to a genre of artistic activity which began early in this century.

Automatic art, tachism, and abstract expressionism are terms loosely associated with the idea that one can express something of one's psychic life which lies under, above or beyond conscious life by working in a dream-like or trance-like state. Such drawing, similar to doodling in an absent minded state, was practiced by the surrealists in the 1920's and even earlier by some of the dadaists.

Both spontaneous gestural drawing and the dadaist non-sensical juxtaposition of words employ techniques which step outside of rational procedure. Non-sense sounds, like non-sense drawing were used in the "dada" soirees to shock those who felt complacent with the power of reason. Some felt that, if it was reasonable to kill and be killed in the trenches, then perhaps "non-sense" should reign rather than reason. After all, some concluded, only political "reasoning" made "sense" out of the war.

Out of that artistic milieu came artists who learned to enter the frontiers of their inner world; through dreams and free associations both artists and poets opened a new artistic frontier. My first computer efforts were simple routines to get the computer to mime some of the artistic forays that took place early in this century.

My first exhibition piece, "The Magic Hand of Chance", was exhibited on a 25 inch monitor cabled to a personal computer. The program worked in real time generating a series of six visual improvisations which were displayed in dynamic sequence on the monitor. All words and images were generated in real time and each sequence was always an original improvisation. These works with their non-repetitive improvisations, evoked the kinds of surprises that we get from free association.

The routines included The Sayings of Omphalos, the Greek word omphalos (navel) being the name of the computer I used at that time. The title alludes to the sayings of Mao Ze-Dong. They have the form of Wisdom Literature but are actually playful nonsense pieces. An occasional juxtaposition of terms may shock us, as in free association, altering our viewpoint.

The program also has self titling routines which generate original improvisational titles for each sequence. Some examples: "Instantly Frayed Whimsy"; "Lunkhead Thrills Iceman"; "Your Joyous Alarm"; "Bad Egg Brews.

The most nonsensical of these routines is Jabberwock, which is part of a larger program that was later abandoned. The program titles a subject of discourse and then proceeds to write a paper. Here is some Jabberwock written on September 1, 1988. Jabberwock titled its subject Enkaom Suxe Ilib. The first paragraph reads: "Schusca umopoh. Efet enkaom ku cuile oteoj yucou pnuibi suxe cu suxe. Ku ilib eey okoy oteoj uiyaurd autifib sciefio ilmedth. Qithi soraihio bu ghiahu sorio suxe. Sonua oray ultap enkaom ruweko suxe ku unuedum eg. Suxe ghiahu oden proveo ioyus ulmeuk".

Jabberwock and the Magic Hand of Chance provided the kind of logic from which the painting program grew. Eventually I became interested in "hard copy" and began to write software for building pictorial elements with lines and brush-strokes.

An outside observer, without knowing the techniques employed, might view them as a natural evolution in the hand of the artist. The automated techniques are so transparent that the viewer sees the artist's hand in individual lines and paint strokes. But the artist's algorithms work in lieu of the artist and the plotter's arm works in lieu of the artist's hand. This frees the artist to concentrate creative energy on the next level of the dialectic.

Some of the most recent works are built with simple brush strokes derived from a range of about 8 to 16 control points. Several of these works, titled Woo Way have some resemblance to Chinese traditional calligraphy. Figure 8 shows a classic example of cao shu 'grass writing' by Zhang Xu of the Tang period, The text reads yan cia 'under the cliff'. Such calligraphy , still taught and practiced in today's academies, was practiced by the literati.

The Hodos brush strokes represent an effort to achieve the fresh qualities of spontaneity and sureness of stroke. Woo Way is word-play, an allusion to the Chinese traditional wisdom of Wu Wei which literally means "do nothing" and suggests letting nature take its course. It implies that one should not interfere with the natural course but rather flow with the way. The computer gestures do not show the characters for Wu Wei, but perhaps the process is a form of Wu Wei.
Thus far we have been discussing how an "art concept" can be coded when its formal system can be described. However, software in itself is something like a musical score. While all the musical ideas may be embodied in the score we still can't experience the music unless it is played on an instrument. The computer can be cabled to many different kinds of peripherals such as a printer, a plotter, a synthesizer, and a telephone.

Usually I review how the code is working through crude simulations on a monitor. But the pixels on the monitor have only a token relationship to paintings. While many artists get hard copy by photographing the monitor I prefer to work with a plotter. A plotter is a drawing machine which can execute drawing instructions sent to it by a computer. Such machines are commonly used for architectural and engineering drawing.

Hodos is written to drive a pen plotter which has 14 pen stalls arranged in two banks with 7 pens in each bank. Permanent inks are mixed for refillable pen cartridges and arranged on the plotter in a classical palette going from warm to cool colors. The program follows rules about where to look for the pens but the defaults can be altered by the artist at start up.

Work is executed on rag papers with a moderate tooth. A standard single frame size is 21.5 inches by 32 inches on a 24 by 36 inch paper. Two frames lengthwise easily achieves a six foot work.

The Brushes. I felt somewhat foolish introducing this routine. I had spent long hours developing a routine to make the drawing machine paint with a brush. It seemed clumsy and almost pointless at first. But through trial and error with the brush mounts, the software, the inks, and the paper, a vast untapped potential emerged. Figure 7 shows the first use of the paint-brush routine in a finished work.

With the software driven stroke there is a sureness and directness that is almost exhilarating. The software knows for sure where beginnings and endings are - precisely. It remembers the stroke and can improvise with the same stroke in a scalar fashion, and do so without failure. This latter ability, which I have only begun to explore, promises to be a rich ground for development.

Hodos introduces art-making procedures of a different order than traditional procedures. The closest analogy might be the score for a musical composition. Both the musical score and software art provide instructions for the creation of an artistic form. The traditional musical score, however, however, requires skilled players and provides the rules for playing only the same piece each time. Hodos requires no skilled players and can improvise an original work every time.

A process which can improvise on an artist's own form ideas without the artist being present extends the power of the original art concept. For example, a colleague from a university some thousand miles away suggested sending my software as a "visiting artist" next semester.

What are the implications? Traditional technologies require that the artist must direct even the simplest improvisation on a print or photograph. The change in exposure, color or light must either be made or directed by the artist. In software art some of the artist's sensibilities about how to make improvisations are coded. This code extends the artist's ability to improvise or preview improvisational possibilities without appparent limits. Such power of improvisation provides the artist with an awesome leverage for exploring form-making ideas.

The process for making art with a "personal expert system" has a strong resemblance to the biological process of epigenesis. The epigenesis of organisms is the process whereby a mature life form grows from its seed. In this analogy the software may be viewed as genotype or the seed that contains all the information necessary for growing the mature form. A brief review of the biological terms may help clarify this analogy.

In genetics the term "epigenesis" is used to describe the process whereby the "phenotype" (a physical organism) grows (unfolds) from a genotype (DNA). The analogous procedure for the paintings suggests that: Hodos, the software, may be viewed as "genotype", the painting as "phenotype", and the process as epigenesis. Software art has adequate information about "how to grow the painting" through a series of recursive graphic routines; each unfolded offspring is a variant of its predecessor. The analogy stops here as these "offspring" cannot beget the next generation. However, they do point the way for the artist in designing the next generation of software. (Note: Since the original draft of this paper the genetic algorithm has been used by Karl Simms and other artists to grow generations of form)

In summary, a new order of art making systems has been emerging. In the present stage the feed-back from the program helps the artist develop the next stage of software. The evolving program helps create itself. This is no different from using a compiler to design a better compiler. There are plateaus of sophistication. The evolving software is the new art and it can be viewed as analogous to genetic code, genotype.

The software also has the potential for hybridization. Let us assume that we could code an art idea of Wassily Kandinsky and a musical idea by M. P. Mussorgsky. We might then build a hybrid "Kandinsky-Mussorgsky" code. One could add audio and visual drivers for all or part of the code, as desired. Eventually one could pair this art concept with another. Each frontier opens a new frontier.

Within the next decade or so advances in high-level software will make it possible for more artists to easily build their own personal expert systems. Families of improvisational works will grow by generations. Familial form traits, like gene characteristics, will reside in the high level expert systems that artists use to make their own expert systems. Some inherited familial features will pop up in some works and not in others as artists learn how to get their own form preferences to prevail.

In a very uncanny way software appears to have a life of its own. The artist's role is to humanize it - give it the form and structure to serve the quality of our art and of our life.

What is an algorithm?  An algorithm or algorism may be viewed simply as a detailed recipe for carrying out a task. The term has its origin in mathematics as the step by step procedure for solving a problem. The  commonplace procedures we use for multiplying and dividing numbers are algorithms. With precise details for each step the procedures yield the same result whether executed by a computer or by a human. This is why robots are able to handle many tasks that were once done only by humans.

Many view an "algorithmic procedure" as a strictly mathematical operation. Today we are inclined to view any well defined procedure as an algorithm. A recipe for baking bread is an algorithm. Follow the recipe faithfully and you will duplicate the kind of bread made by the person who wrote the recipe.

Machines can also be programmed to follow recipes. The programmed circuitry in  bread-making machine directs the machine's mechanism on precisely how to mix ingredients, knead the dough,  and bake the bread.  In theory it should succeed every time but there are also factors of  mechanical and human error. The quality and measure of ingredients may contribute to unwanted results.

Within the past quarter century operational instructions have been imbedded in the design of many industrial and household  utilities.  They implement our daily use of  telephones, automobiles, cameras, TV's, and radios. Our hospitals, factories, banks, and shopping centers all depend on the algorithms that control inventories, transactions, communications and security.  They are ubiquitous and our mass culture would collapse without them.

Algorithmic procedures are also imbedded in  the digital tools used in the arts. Use of these tools influences form in  the practice of film, architecture, photography, music, printmaking, and all types of electronic sound and image. The drudgery of executing algorithms that would require immense time, or even be impossible to execute without computing power, has been  handed over to the machine leaving humans more free to focus on the creative part of their work. For the artist this means improving and improvising the art-making  procedure.  For the algorist, work on the algorithm is work on the procedure.

Clearly early civilizations developed procedures for counting and measuring. They also created procedures for weaving, grinding, making fire and cooking. Any of these procedures, when well defined, could be viewed as an algorithm. Indeed  weaving technology played an important role in the history of computers.  If we can spell out the procedure for any given task then, given all the necessary materials and skills, we should be able to carry out the task.

Architectural plans, musical scores and dance notations bear one feature in common - they are all recipes for carrying out a task. From this perspective a broad range of  notational systems can be viewed and studied as algorithmic procedure.  From this perspective algorithmic procedures for generating artistic forms enjoy a rich and varied tradition  even though we have used other terms to describe them.

In Art History.  A history of algorithms in the visual arts would be voluminous touching many phases in every culture at every turn - the Egyptian canons for drawing the human figure,  the infinite geometric play in Islamic art and the role of both linear perspective and proportion in Renaissance art.   In China we would find the Mustard Seed Manual and in Byzantium the conventions for icon painting.  In Europe, by the  Seventeenth Century we would find extremely sophisticated algorithms for plotting the dizzying perspectives  imaging the passage from earth to heaven.

Even so, notational systems for the visual arts played a limited role when compared to notational systems for music.  A gifted composer could compose  a score for a profoundly moving musical passage that could be played hundreds of years later by a skilled virtuoso. Not so for the painter. While Leonardo could easily employ an algorithm for creating the perspective space in the Last Supper, he could not, at that time, compose an algorithm for rendering the face of Judas.

As computers became more accessible to artists in the 1970's and 1980's some artists began to experiment with algorithmic procedure.  The new technology offered them methods of working algorithmically that were unavailable before the advent of computers. By  the 1980's a number of us were working with the pen plotter,  a machine with a "drawing arm".  Seeing other's work at various venues we came to know each other and share ideas.  Algorists like Harold Cohen, Manfred Mohr, Jean Pierre Hebert and this author had achieved mature work but we had no common identity.  Each in their own way had invented algorithmic procedures for generating their art. By doing so each created their own distinctive style.  Clearly style and algorithm were linked in a very important way.

One of the concerns for educators in  the early 1980's was whether we should be teaching programming in our art schools or rather wait for advances in computing power, software programs, and printing technologies.  With the growth of PC computing power, refinement of raster printing technologies, and  professional software for the visual artists more and more artists took up what was generally called "computer art". The unique features and form-generating capabilities for algorithmic procedure in the hands of the artist was easily lost in the widening world of "computer art".  It was in this milieu that a small group of artists, including this author, introduced panels for addressing the role of "algorithms & the artist". Following one such panel at the 1995 SIGGRAPH conference it was Jean Pierre Hebert, Ken Musgrave and myself who agreed to work towards a common identity for those who practiced algorithmic art.

Within a short time we introduced our identity as "algorists" and Jean Pierre Hebert wrote an algorithm defining an "algorist" as applied to artists. Within a decade this usage led us to a better understanding of the role of algorithmic procedure in shaping world culture at the turn of the Century.

For the past 40 years I have worked with pure visual form ranging from controlled constructions with highly studied color behavior to spontaneous brush strokes and inventive non-representational drawing. Such art has been labeled variously as "concrete", "abstract", "non-objective", and "non-representational". In its purest form such art does not re-presentother reality. Rather "it is" the reality. One contemplates a pure form similar to the way one might contemplate a fine vase or a sea shell.  Early 20th Century pioneers of this art include artists like Piet Mondrian, Frantisek Kupka and the brothers Naum Gabo and Antoine Pevsner.

In the last quarter of the 20th Century a radically new form-generating procedure became available. By joining algorithmic procedure and computing power some artists began generating forms with surprising visual qualities.  A vast uncharted frontier of form waited to be conceptualized  and concretized. By the 1980's I was composing detailed procedures for generating forms that were accessible only through extensive computing. On-going work concentrates on developing this program of form generators.  By joining these procedures with fine arts practice I create two dimensional art objects to be contemplated much as we contemplate the  forms of nature .

Form generation as epigenesis. The greater part of this creative work in the past 15 years has been developing art form generators.  These are original detailed procedures, for initiating and improvising form ideas. Such form generators may be likened to biological genotypes since they contain the code for generating forms. The procedure for executing the code, somewhat analogous to biological epigenesis, grows the form. The creation and control of these instructions provides an awesome means for an artist to employ form-growing concepts as an integral part of the creative process. Such routines provide access to countless visual structures that constitute a new frontier of visual forms for the artist.

The Work. Works are executed with a multi-pen plotter coupled to a PC driven by the software. The plotter, choosing from an array of pens loaded with pigmented inks draws each individual line. Most works require thousands of lines with  software controlled pen changes. An optional brush routine allows  occasional substitution of a brush for a pen. Brush strokes are plotted using Chinese brushes adapted to the machine's drawing arm. The Diamond Lake Apocalypse series of illuminated digital scripts is reminiscent of medieval manuscripts. Many of these works are enhanced with a touch of gold or silver leaf applied by hand. However, the design elements illuminated with gold are always code generated and machine plotted.

Greetings once again dear ones, in this time of new beginnings.  We come to bring encouragement and love to the many of you who are experiencing seeming discord both within and without.  Old ideas are quickly becoming obsolete and dissolving, but the new ones are not yet fully able to manifest (personally and universally) leaving you confused.

It may appear as if the world is functioning as usual, and yet you are finding that much of it no longer resonates with you in the ways you have come to know and expect.  This is the confusion and causes you to question yourselves.  Never doubt or  believe that you are simply making  things up, are over tired, or just plain going crazy  as change begin to take place within your awareness. Change  signifies  a  consciousness that is expanding--often away from what is familiar or known.

You are not consciously aware of how much is happening on other levels as you sleep, meditate, or simply go about your day.  Human beings are programmed to analyze and mentally figure everything out, but this approach does not work with what is now taking place on  deeper levels.

The sense of separation with which mankind  has lived lifetime after lifetime,  has manifested a world of much unnecessary suffering for many.   The few who recognized the error of this concept have  always been harshly silenced by the un-awakened majority, and so the myth lived on. Thus sincere seekers  became used to looking outside of themselves to those deemed more knowledgeable and trained in truth, but more often than not were and still are told that man is tarnished and must work his way back into the good graces of a God made in man's image and likeness through suffering and great effort.

False teachings have never affected truth, but through lifetimes did affect your acceptance of truth.   All experiences (many not so pleasant) resulting from this ignorance are important facets of your evolutionary journey and have brought you to where you are now.  There is no need for fear or doubt dear ones, for you are on your way whether you think you are or not.  Leave behind those teachings that tell you that you must believe a certain way, or undergo specific ceremonies in order to be "saved" or spiritual.  It is time to understand that you are already  saved and spiritual and at no time ever, have you not been.

This is the sacred secret long hidden within each where it was never suspected of being--you have always been an expression of Source.  Your Real Self has never nor could it ever, be separate from the One Omnipresent Divine Consciousness for that is all there is.  Where would you come from?

Evolution is simply the gradual process of remembering through lifetimes of stepping and falling, stepping and falling.  Earth is a powerful and difficult school, but you will graduate very prepared as powerful beings of Light.  All is proceeding according to plan and you are now ready to move beyond the stepping and falling part and come into the knowing.

Spiritual readiness is always known and directed by the Higher Self, and you are being guided to and through each necessary step of your journey.  This knowledge will serve to remove from you the burdens of trying to figure everything out mentally and continuously seek.  Many metaphysical books written in the past, laid out definite practices necessary for spiritual growth.  If you are drawn to certain practices, that is fine for many of them were actually based in truth,  but if you are not drawn to them or if they feel "old" to you,  then trust this and never think you have failed in some way because of it.

Most of you have evolved beyond needing a lot of outer tools.  Many (not all) of the older books were written for beginners who needed specific practices in order to awaken to new levels of understanding, there are still many who equate being psychic with being spiritual.

Mankind's shift  into higher levels of awareness is manifesting outwardly as many new and very evolved teachers.  You will recognize them by the energy of their words.  Be open, for they may present  ideas of truth that at first seem strange.  Many very evolved beings of Light are now  incarnating or  choosing to be "walk ins" in order to help the many who are ready  move deeper and higher, beyond what up to now has been known and taught.

There is no need to figure everything out, just know that you are being guided.  The "monkey mind" loves to dictate the spiritual journey with suggestions, ideas, and judgement, but remember the human mind can only access what is already out there in world consciousness unless given more from deeper levels.  You have moved beyond the mental stage--love it, thank it for its suggestions, and  pay it no attention, instead choosing to listen and be guided from within.

Any unpleasant experiences you may be going through right now are simply old energies (concepts, beliefs, experiences) coming  forth to be noticed, interpreted on a higher level, and released.  These experiences are signs that you are ready for the deeper truths that cannot integrate with the old still in place.

Karmic situations needing completion, are coming to a head for many.  Diseases, irrational fears, and all sorts of issues considered to be problems in the third dimension, are presenting themselves to many very evolved souls now spiritually able and ready to release them.  Problems are never a sign of failure for a serious seeker of truth.  When things seem to fall apart, recognize the experience as an indication of your readiness to look deeper and that you are now evolved enough to do the work.  Never see yourselves or others as a failure based on outer appearances, for you may well be witnessing a very evolved soul who has  chosen to work out profound levels of old energy in this lifetime.

There are some tempted to deny "problems" when they appear, believing that this is the more spiritual  way  (problems are not God ordained and therefore do not exist).  This often becomes a trap for the serious student because in reality this is the absolute truth, but the absolute cannot be lived and experienced fully  until one has attained that state of consciousness. Know the truth of the absolute, but live out from your highest attained state of consciousness.

Denial and resistance can solidify problems, for denial and resistance feeds them with energy. Never deny the discords in your life but know that they have no real power other than the power you give them.  We do not say that if you have concern over some health issue you must not visit  a doctor or that this indicates a backward step, for  loving and dedicated help manifests and is available on all levels.

You are moving into a new paradigm of living, emerging into an awareness of yourselves as powerful beings--butterflies emerging from lifetimes spent in heavy cocoons of duality and separation.  The cocoon must be discarded  no matter how familiar or comfortable it may of become  reflecting change with what you eat, the way you play, your work, and all things in life.  You are experiencing a shift into the higher sense of ordinary things which does not mean you must leave behind everything that is important to you, but means you must be willing for some things to assume a new place, evolving into something new or perhaps being left behind.

This  way of being becomes automatic as you begin to live attuned always to the within.   Do you wish to stay in what is known and familiar no matter how uncomfortable, or do you wish to move on to something new?  Be not afraid dear ones, for the spiritually new will always manifest in higher and better forms.  You are not leaving anything real, you coming to understand  and experience these same things on higher levels.

Welcome to spring dear ones, how quickly this year seems to be moving along.  Time as you have always known it is now different because as your energy is lifting and evolving so are all the qualities of the third dimension lifting and evolving.  Time is simply a measure necessary while living in the third dimension.

We wish to discuss the  energy of conflict.  Conflict is the manifestation of duality/separation consciousness .  Everyone seeks the experience of completeness and wholeness as well as to be loved because it is the only reality, the real you, the essence of every living thing.   However, those who do not understand this yearning, interpret it in ways that make sense according to their three dimensional belief system.  "If I act a certain way, dress a certain way. look a certain way, make a lot of money, then I will be loved and loveable. "

Many seek  it through multiple sexual partners, confusing love with sex. When this  brings no real satisfaction, they move on to the next, and the next, always seeking that sense of wholeness and completeness that can be found only within.

There are some who seek  love and acceptance through violence-- "If I eliminate these people who disagree with the truth as I understand it,  then there will be  peace and everything will be perfect.  I will be a hero".

Belief systems based in rigid rules of right and wrong, and implemented by ego driven and un-awakened "leaders" (as well as followers),  have over thousands of years  always resulted in the  pain, suffering, and even death to those who disagreed with them.  What is taking place in the Middle East is not new, it is very, very old and does not belong to one religion alone.

NO belief system on earth is the "one and  only true teaching".  All teachings are simply interpretations of truth.  Truth only flows from within when an individual is spiritually ready to receive it.  Many evolved teachers throughout the ages have received pure Truth,  but have been unable to pass it on to very many because those of less evolved consciousness were unable to fully comprehend what was being taught.  Because of this,  they formed concepts of what was being taught, fitting truth into their personal belief system, and then organizing it.   Truth can never be organized or it is lost among those not ready.

There is an old saying;  "If you can name it, or see it, that is not IT" for Divine Consciousness can never be comprehended by the mind.  This is the purpose of meditation, to move the seeker beyond the plotting, planning, busy, human mind.  The one and only truth is; "I AM", words for which the master Jeshua was put to death by an un-awakened society unable to comprehend the true meaning of these sacred words.

In reality there is no right and wrong, just each soul's struggle to awaken through the experiences of the third dimension.  We do not say there is no need for jails for there are some who need  time away from society to think about their actions and who are unable learn through the  higher ways.  We do say punishment should never devolve into ego driven physical, emotional, or mental cruelty by those who believe that by virtue of their "rightness" the others deserve anything they do to them.

Much of what you see in the Middle East represents the manifestation of very ancient energies now surfacing and being exposed.   Violence never solves anything nor can it ever result in true peace, for violence is a self perpetuating energy, a lesson most governments have yet to learn.

Try not to dwell on the issues of violence you are aware of, for that simply feeds  energy to them.  Your work, because you have chosen to evolve and move into unconditional love,  is to live it.  Live love in every seemingly insignificant  moment of your day.  Love is the activity of a consciousness of oneness and it matters not how small or large the activity--petting a dog, opening a door, giving a smile or encouraging word while centered in the heart.  These seemingly small things are the living of love and as you learn to live this way, you will draw to yourself more and bigger ways of expressing love.  It all starts with one step--the life of spiritual awareness will not drop in your lap without effort and practice.

Society, throughout time has heaped praise and even deification on the "greats" of the human world--artists, actors, musicians, warriors, scientists, writers, religious and world leaders, etc.  This  is fine, but accomplishments  interpreted only from the third dimensional stand point do not add more light to the world and often simply feed egos.

That said, many of the world's "greats" were and are, very evolved souls, some from other higher dimensional  star systems, who have chosen to incarnate on earth in order to help lift the consciousness of mankind at this powerful time through their work.

You are the ones who will change the world dear ones.  Every bit of light you flow adds to the universal world consciousness and at some point the balance is going to tip.  The light of awareness is growing by leaps and bounds now as many awaken to their and everyone else's true nature. What one does to another, one does to self for there is only One.  This realization by a majority is what will end conflict in the world.

It may appear as if the world  is growing worse instead of waking up, but this is because issues that were heretofore unknown by most, are now being exposed.  Things kept hidden  by those standing to profit by or enjoy them, are now surfacing which is why it is so important at this time to use your intuition regarding everything presented to you by the news media.

There are those who do not wish you to know about certain things and so try to negate reports or demonize individuals behind the exposures.   You have evolved well beyond blindly accepting everything you may read or hear at this point, but it is very easy to slip back without question.  This goes for channelings as well, so learn to listen to and trust your intuition for not everything you read is on the highest level.

Exposures are helping to change and clear blind acceptance by those still enmeshed in the third dimensional belief system.  There are many who still believe everything they are told by anyone holding a position of authority be it in the realm of medical, religious, "expert",  or government.  Give gratitude when something  is exposed instead of going into  negative thoughts about the issue itself.

Everything taking place on earth at this time is a part of the ascension process and your part is to live out from your highest attained consciousness of truth.  Be good world citizens--vote and  be informed but do it with the guidance of your intuition, allowing yourself be guided to the candidate that most reflects the highest Light while ignoring  rhetoric so carefully scripted to tell people  what they want to hear.

You are on earth to participate in the ascension process of mankind and Gaia, not to rest back in a meditative state chanting Omn.   In the past, an individual  could only safely pursue the spiritual life through entering a convent or monastery and even now this is appropriate for some but most of you came into this incarnation to be an active part of this process.

Life is not a passive event.  Living spiritually  is the activity of living out from center in every situation regardless of what others may believe.  Living your attained state of consciousness is all that is required. Truth can no longer simply be dinner party conversation or interesting concepts put to the back of the mind while continuing to live life as usual.

Living actively does not mean you must shout truth as you know it from the roof tops, or try to convince everyone in your circle to see the world as you do, it means you begin to LIVE IT and BE IT and that may be a very secret and silent activity.  Your light will draw to you those ready to hear what you have to  say but in the mean time  you sweep the floor, go to work, cook dinner, mow the lawn--continuing to do everyday things but from a higher level of awareness.

If your work is to be in the world as a truth teacher, you will be guided when the time is right. Often you will feel the urge for something new  but the energy is not yet in place for it to manifest, so do not get discouraged.   Most are finding that what they are already doing is expanding  and growing into new and higher ways. This is especially true for some healers of all modalities who are finding their practices unfolding into areas and solutions they never knew existed.

When a soul opens to being taught from within, many higher and better ways can be downloaded by evolved beings from the other side.  This is true for music, medicine, art, anything... When these things come only from the human mind, they are never  new or creative.  There are some great composers who simply listened and wrote.

Try not to pursue some concept of what being "spiritual" means for this simply represents an idea you may be holding about what spirituality is and what it isn't.  Never forget that every action and every breath you take is a Divine activity.  The three dimensional world in its quest for and defining of what determines greatness (in reality the search for completeness and wholeness),  has caused many dear souls to lose heart,  believing they were unworthy and hopeless.

Never forget  that once you commit to the spiritual journey, you are on your way.  You may be guided to a book or a class as new directions unfold for you,  but in general most you are now beyond "needing" them other than for the pleasure of spiritual reading or as reminders.

You are ready to be taught from within, trust this.   It is why you came.

Dear ones,  Greetings to all in  this season of Christian celebration.  Know that the death and resurrection remembered at Easter were meant to provide profound spiritual teachings for all,  Christian as well as non-Christian.

Spring is bringing with it new beginnings, even for those who may be trying very hard to avoid new beginnings.  We speak now of those who believe that their learning curve is finished  and that they are now entitled to sit back and enjoy "old age".  Those in this category will be very surprised when change begins to disrupt this cherished illusion.  Everyone must be prepared for change both within and without regardless of human age and its manifestations .

Refuse to blindly accept into consciousness the beliefs and fears about aging being incessantly thrown at you by corporations who stand to profit from your fears of age and deterioration.    Individual  human age matters not in the spiritual journey for in reality, everyone is as old as God.

We emphasize change simply to remind all now on earth  to not get too comfortable in the illusion, for you all made the choice to be a part of this powerful time of change in order to spiritually evolve and then in turn, assist others.

An energy field filled with Light can lift those receptive and ready  without words ever being spoken and often even without any conscious awareness. This is what happened in the story of the woman touching the hem of Jesus's robe and being healed.  It was not the robe, it was the very illuminated energy field of Jesus.  She was receptive and was thus lifted into his higher frequency consciousness where disease did not exist.

Many of you are beginning to experience others seeking you out--asking what you know, how you may feel about this or that,  or what you believe--your advice.  They do not know  exactly what it is they seek, but they feel your Light.  When this happens, remember always to keep yourselves in compassion and not sympathy, for in sympathy you align with the other's energy.

Every soul seeks Light for it is the focus of the true Self to be remembered, but as with all things, this too is interpreted according to the individual state of consciousness.  Even those seeming to live out from the densest of third dimensional energy are unknowingly seeking the peace and harmony of Light  when their discordant actions may indicate otherwise. A murderer believes he is making things better for himself in some way.

The Arcturian Group wishes to speak of Resurrection--a term  heavily imbued with the Christian teachings of Jesus being crucified and then arising from the dead and leaving the tomb.  The hidden, deeper message is that only through the death of the old, can come new beginnings.  It has been taught and believed by many that the resurrection story is about one person alone--Jesus.   It is time to understand that the birth, death, and resurrection events were all profound teachings given to the world with unconditional love by Jesus, not meant to be pinned on him alone.

Death and resurrections have been experienced by all with varying intensity throughout lifetimes.  You have all had them.  Death and resurrection has been ongoing, serving to lift the soul to new levels of spiritual awareness through physical, emotional, and mental experiences until at last there comes a final and profound death and resurrection in which the soul finally remembers Itself.

The idea of this experience as belonging to one  alone,  has resulted in mankind missing many of the vital and powerful lessons that were given through a great sacrifice by the illumined teacher.

All are in and of One Divine Consciousness.  Jesus, an evolved being and master of Light, incarnated to be the example, showing mankind that the Way is within, but the consciousness of most were not able to accept this and instead chose to make him into a God as many still do.   The same is true of Siddhartha who become the Buddha, and all the great spiritual Lights through out time.

Over and over again, un-awakened individuals have interpreted mystical teachings according to their less evolved state of consciousness.  Then in their enthusiasm, they proceed to organize their interpretations,  and deify the original messenger as being separate and more special than everyone else.  They worship the messenger, and thus miss the message entirely.

It is time to leave behind all the old concepts and beliefs and enter into your own power, dear ones.  It is time to accept and believe that you and all are the expressions of the Divine, even those over-shadowed by ignorance and  dense energy.  It is time to claim your Divine Sonship, your Oneness, and your Divinity.

Easter represents  a rising out from the tomb of false beliefs and concepts and is often  pre-ceded by whatever painful crucifixion experiences may be necessary in order to dissolve those firmly held concepts forming your personal world.

Resurrection is universal--it is the Phoenix rising from the ashes, it is the awakening of Siddhartha Gautama under the Bodhi tree, and is the master Jesus emerging alive with Divine Light from the tomb.

Just as the birth in a manger is symbolic of a new self being born from  humble circumstances, so is  Easter your story presented ever so lovingly by a great master of Light.  It is the story of you, arising out of centuries of living in duality and  separation and finally remembering.

Arise dear ones.  Celebrate, claim SELF, and live, for in Light there is no more crucifixion, only Resurrection.

Dear ones,  we greet you in this time of new beginnings--springtime.   Many of you are exhausted, having been  inundated with the issues of winter weather and sigh with relief as you see more of the warming sun which is bringing with it powerful new energies.  Be prepared to experience some shifting within as well as to observe it in the  general awareness of the world which will be accompanied by some weather related issues.

A new and powerful consciousness is being born and as with all births is accompanied by some pain and difficulty, much of which is dependent upon a willingness or non-willingness to embrace change. Many are discovering themselves no longer able to blindly accept the political nonsense with its accompanying rules and regulations.  The self serving and  adolescent behavior of many so called "leaders" is helping to open the eyes of many.  It is a time of  awakening and the accumulated blinders so many have lived with and taken for granted, are coming off.

We wish to speak of truth, and its energy in the world.  Concepts and beliefs about truth flow from  and manifest through individual  consciousness as well as being held in the impersonal universal consensus consciousness.  Certain ideas  held to be truth within the majority consciousness are more and more being rejected  in personal consciousness.  Every bit of Light you embrace then becomes a part of the general world consciousness and this  diluting and  enlightenment of world consciousness is what allows the uninterested and un-awakened masses to awaken.

Concepts and beliefs about truth  reflect the belief system (state of consciousness) of the individual and easily change.  TRUTH in its highest sense can never be be changed or limited and is infinitely governed and held in place by Divine law.  We speak of course of spiritual truth, the only truth there is, which reflects One omnipresent , omnipotent, omniscient  Consciousness and all that is embodied within it.  This is the absolute and the only reality.

It is time to examine everything you hold to be truth, dear ones for much that passes as truth is simply some concept handed down from generation to generation.  Most third dimensional ideas along with their particular rules and regulations, were learned at the feet of  parents, religious leaders, governments, scientists, and experts, who simply parrot what  they were taught.  Many teachings hold no real truth at all, but simply represent current third dimensional thinking unless the teacher is somewhat spiritually evolved and even then, whatever is presented can rise no higher than the teacher's attained state of consciousness.

This is why you must learn to be taught from within.  You are ready, and many of you are doing it, but question it and are confused because the world does not yet support this way of learning and living.

A good example of an intrenched concept that has passed from generation to generation and held as truth is prejudice--the belief that another is  inferior because of skin color, eye shape,  language,  etc.   Children are taught through the words, actions, and attitudes of adults that  particular traits make some inferior.

Many of these beliefs have been carried forward in cellular memory from  past lives and are now  presenting and will continue to present in various forms until released and cleared.  Some unable to move beyond intensely held beliefs  will make a pre-birth choice for some necessary experience  in order to spiritually grow, and so the pre-birth choice of a former slave owner may be to experience this lifetime as a black person.  Someone who killed and tortured Native Americans in a quest for land in the settling of the US may find himself now living as a Native American, experiencing all the accompanying prejudice.  The same is true for the gay community.

Not all  choices to be born into a group that is prejudiced against are based in Karma.  There are many evolved souls of great light who specifically choose situations in which they will experience prejudice in order to change the energy, to enlighten the world,  and help those of the particular "group" to understand, love, and honor themselves.  You know many of these Lightworkers as the leaders and healers within certain groups.

Karma is  NOT  PUNISHMENT  but is the balancing of energy required when an individual is unable to  move beyond some old outgrown state of consciousness  held fast lifetime after lifetime.  Karma often represents some past life action or interaction ready to be resolved and cleared .  Karmic experiences represent a way of resolution for the un-awakened who are unable or unwilling to clear in higher ways.  Many still need to learn this way,  but karmic experiences are no longer necessary once an individual opens to truth for he is then able to being taught and learn through higher and much easier ways.

If you discover prejudice or even  hatred toward certain groups or individuals surfacing within you, do not react and condemn yourselves, but rejoice for this means you are now evolved enough and ready to honestly examine and clear your belief system of this particular issue.  Ask yourself; "In the light of One, what am I believing?  Yes, I see discord and disharmony, but what is the truth?" This is how you identify any false beliefs you may still be unaware that you  hold about anything,  dear ones.

All serious students of truth are getting scrubbed clean at this powerful time, every seemingly small issue of the false is presenting itself to be examined and let go of--the spiritual  leash is getting very, very short and many of you find that are not having much fun.   Allow the process dear ones, for once cleared of all that is false, you will be able to move, live, and be  in the higher frequencies of unconditional love and Light--Truth.  Life will become what it was meant to be, and you will be free, never to fall back asleep.

Try  not to question and argue the issues you intuitively know need to be let go, but  simply accept that change is the process of evolution.  You cannot carry with you into the higher frequencies of Light the dense and un-illumined energies you have grown used to no matter how fun, comfortable, or lucrative they might be.  As always, there is free will choice  which allows every soul to live the dream as long as desired.

Truth is, always has been, and always will be.  Truth never changes, it is only un-illumined concepts of truth that change  which is what is happening to universal awareness at this time.  Begin to recognize the TRUTH behind "truth" and understand that your embracing and living out from  TRUTH is the only way to enlightenment at which point you can then turn around and assist others to move into their enlightenment--you become Light Workers.

The energy of TRUTH is LOVE and within that Love is oneness, peace, joy, wholeness, and abundance in infinite form and variety.  This is the reality. This is what you seek and this is what you are.
We come with greetings of love and grace to assist all who  seek change and deeper awareness.  We observe that many have already accomplished this; you have done the work and are beginning to see the fruitage of that work.

As Gaia lifts into ever higher dimensional levels, many are beginning to more easily experience and integrate the  deeper truths.  Concepts previously taken for granted as being truth, have become no longer acceptable to ever increasing numbers of people.  Some holding  positions of leadership or fame who comfortably spewed lies or acted in ways to enhance their own power, authority, wealth, and self aggrandizement, are being exposed.  This is happening because so many have begun to listen to and act upon their intuition  instead of simply accepting  "bought and paid for" sources of information.  You are awakening from the dream.

Always listen to your intuition dear ones.  Even if at first you are unable to fully trust, make a mental note of what you receive.   Intuitive guidance and knowledge comes from soul, it is you, and will  guide you through every situation of every day if you acknowledge and allow it.  As you evolve and begin to trust  more deeply in your own Divine nature, intuition will simply become ordinary, no longer needing to be practiced.

The world in general has been trained lifetime after lifetime to look outside of itself for answers, guidance, and happiness which makes perfect sense in a world of duality and separation, for most did not realize there was anywhere else to look.  However, since  duality automatically manifests as  pairs of opposites, the results of looking to the outer are sometimes good, and sometimes not so good.

Most of you are no longer of this energy,  you have "graduated" and come to know that the outer things are interpretations of consciousness.  Mind is the substance of matter.  In third dimensional consciousness, the outer is most often formed through human effort--matter manipulated to make it work.  However, because there is no law supporting  man made forms, there is nothing to hold them in place-- a lesson many are painfully learning at this time.

Those who have always achieved exactly what they desired regardless of any  harm it may of caused other humans or Gaia, are now finding or soon will find that these manipulative ways no longer work, this type of energy is dissolving.  Many who presently sit comfortably on thrones made of ego and self importance will discover that they sit within castles made of sand.

The new and higher resonating energies do not eliminate wealth or power but serve to lift it into  higher forms.  Abundance is a quality of the Divine, ever present already within every individual and held infinitely in place by Divine Law.  The higher consciousness of abundance in all its forms  flows easily and infinitely as oneness,  love, and service--never a tool for personal selfishness. There is nothing evil or wrong about financial abundance.  Like all other qualities of the Divine, through the belief in two powers,  abundance  has been misinterpreted and thus manifests in the world as either too much or too little.  Remember, you are powerful beings, your thoughts and words create, and each time you say; "I don't have.", mind interprets that and forms it in your outer experience.

Arcturian Group wishes to discuss change.  We observe many who retreat into fear and resistance whenever the familiar dissolves or changes.  When  old forms dissolve, know that  the Divine idea behind the form is  still present.  Divine Ideas are changeless and are held forever in place by Divine Law.  Because of this, newer and better forms of every Divine Idea take the place of that which dissolves--if you allow it.

Example: At one point the Divine idea of Omnipresence was interpreted in human consciousness as walking,  then evolved to  riding  horses, then to  horses pulling  sleds or carriages.  Then came the model T and all that has followed it--Omnipresence manifesting as jet planes, elegant automobiles, and fast rails, and it does not end there, there is no end.  Divine ideas never change, what the world lives and sees depends upon  how these ideas are  interpreted according to the consensus consciousness  or individuals.  This is what you are lifting into more Light.

There are those who still base their sense of self  in whatever perceived qualities of value they have--how beautiful they are, how much money they make, or how fashionably they dress . This is seen in the mindless worship and imitation of the rich and famous.  Individuals drawn to imitation are not yet awake to their own Divine Nature, and so have no real sense of self worth.   This state of consciousness  frequently lashes out through violence because the individual feels left out, unworthy, and rejected.  They are frequently are drawn to positions of authority where they are able to act out legally.

Some with a consciousness of unworthiness simply choose to suffer silently, but their energy field often attracts those of a  "bully" state of consciousness because  both carry the same energy of unworthiness--like energy attracts like energy.  Bullying cannot be solved by punishment, but by the attainment of a sense of worth within the individual.  These types of issues are usually carried from past life experiences and need to be cleared at their root, not with medication but with truth.

Many fear the loss of themselves in the process of evolution.  Over lifetimes of infinite experience as human beings, every person develops a personal sense of self and is usually ready to defend it at all costs.  When an individual begins to realize that he must move beyond many of the very qualities he believes to be who he is, he panics.  This can be a terrifying experience especially for strong personalities  and is the point at which some choose to end their spiritual quest.

Your claim to fame, your personality--you, must be surrendered  in order to evolve??  Surely not! What will be  left?  Nothing?   It is at this point that many strong, powerful, and outgoing individuals begin to visualize the  holy cards nuns handed out to school children--men and women with eyes raised and hands folded--booooring.  Nah!

Never fear losing anything real dear ones, for you only ever release and leave behind that false sense of self you have been entertaining about who and what you are.  Nothing real can ever be lost.  You will discover that you are still you, now empowered in higher ways of seeing and being as the real aspects of you become stronger and more enhanced.  You are becoming a new and improved version.

As evolved individuals, you will continue to live and move and  be in the world, but  be much less touched by it.  You will find yourselves able to enter the fray of work, home, or play, and not be tempted to enter into it.  There is no longer anything for lower resonating energies to attach to  for your energy field is now of a higher resonance.

This is Lightwork dear ones.  You came to awaken, evolve, and then help those struggling to do the same.  Forget any dreams you may of had about ascending and then sitting back comfortably as an observer for the rest of your time on earth, or skipping off to a quiet convent or monastery.  No, any Light you attain is not only for you, but for the world in whatever ways or places you may find yourselves.

Many of you are beginning to experience  all sorts of odd situations because you have become carriers of Light--Light workers.  Do not be surprised or put off by the experiences that may start to come your way, for you will be drawn to where you are needed and those who need you (not want) will be drawn to you.  As Lightworkers you will soon find that any old concepts you may still hold of "proper, dignified, or correct" go out the window but at the same time you will discover in this work a love for  mankind that you did not know you were capable of.  Lightwork dear ones, consists of being the Light in whatever form it is needed.  It goes with you wherever you go because it is you.

If you have been in some  difficult situation (home, work, relationship) you will start to notice  that it is unable to affect you as it once did simply because you no longer resonate with that energy or give it power.  This often represents a completion of some sort and is usually part of one's soul contract.  Spiritual resolution of some old karmic issue gives an individual freedom to leave the situation, or even find himself somehow removed from it.  However, there are times in which these situations are a person's chosen Lightwork, that is to simply to remain and be the Light.  Always ask for Guidance and trust your intuition.

Soon you are going to be seeing more and  more change that you will recognize as spiritual.  Up to this point you have all wondered when  you will  see some proof that all the evolutionary ideas are real.  Do not expect changes to appear as explosions dear ones, but seek intuitively behind the headlines, and on the faces and in the words of those around you.   Examine how you feel about what is going on in the world today versus a year or two ago.

Much of the violence in the Middle East  is very old, ancient energy that must be cleared and played out in order for Gaia to clear herself.  Your focus must be on spiritual growth and awareness and then the living of it, for you have nothing to offer the world if you remain in the same old universal belief system.

If you are clearing old energies and resolving old karmic situations, allow the process but do not get so enmeshed in them that you spend the rest of your life laying on a couch declaring that you can do nothing because you are clearing.  If you are guided toward some action,  do it.  If you are guided to be silent and hold the energy of Light for some world situation or person, do it.  It is a new time in which old solutions and rules no longer work so it is fruitless to keep trying to make them work.

In this new time and new energy,  rejoice dear ones for you are very close to seeing a new earth.
Greetings once again from the Arcturian Group. We wish to speak of transformation, a seemingly simple topic, but one that is important to the understanding of truth.  Transformation flows through and as an individual when he awakens out of the dream.   It manifests as the ability to  "see with new eyes and hear with new ears"  while in the midst of duality and separation.  Transformation allows one to recognize whatever false concepts and beliefs he may still hold and replace them with truth.   It brings the ability to say; "I was wrong."

Those unable to allow transformation, delay their ascension into a consciousness of higher dimensional energy.  Fear of change acts to support the continuation of  third dimensional  concepts and beliefs.  Many who resist change are in positions as world "leaders" who acquaint any sort of transformation with loss--loss of their personal power and wealth. Because of their leadership positions, they often attract followers who blindly accept whatever these so called "leaders" believe, giving away personal power in the process.

You are now at the point in your spiritual journey dear ones, where you must acknowledge and reclaim your innate power.  Many of you have done this already, having attained this state of consciousness in other lifetimes, entering into this lifetime with this state of consciousness already in place.

Never fear to say;  "Enough is enough, I no longer choose to live, act, be, or think in old outdated concepts of separation".  Some of you may have already discovered that there willable those who are offended by this.  Friends may drop away, but be not afraid  if this happens dear ones, for energy seeks its own and will soon draw to you those of your resonance.

We speak of transformation now because you are very close to seeing transformation take place.  Much is to be revealed and you must be prepared both within and without for you will be the ones called upon to assist those in fear, unable to let go of their old and comfortable ways of understanding the world.  Mankind is no longer being allowed lazily float from one day to the next believing that the government,  church , parent, friend, or society in general will take care of everything for them.  You are the consciousness and thus substance of the outer world and things are quickly changing.

It is time to claim your identity as Divine Beings who chose to learn  and assist others within third dimensional energy.   Allow the process.  For some, allowing the process may simply mean becoming  be-ers instead of do-ers--allowing themselves to be still and at rest, going within often while old energies surface and clear.  Others may find themselves  being guided to stand up and be counted in some way.

Transformation is a surrender, the ability to open oneself to change.   Surrender is not the giving up of self to some unknown "pie in the sky God" concept.  Spiritual surrender is the  natural result of having clear understanding of what one is surrendering to--Truth.  It begins with a strong inner drive to begin a quest for truth which can take many many paths, but which eventually leads to discovering IT within.  This  process cannot be avoided for it is who you are.

There are many who "suspect" the deeper truths but refuse to embrace them, choosing instead to believe that others more "holy" or intelligent, know more than they do.  No dear ones, all have the Divine Voice within,  it is simply a matter of acknowledging, listening, and trusting.  The time is now.

Valentine's Day!
Yes, we are aware of your customs and we join with you for is it not a day that celebrates love?  Love is  all there is, for ONE can never be divided.  It  is only the belief that One can be divided (separation) that has resulted in  the outward manifestations of war, discord, suffering, lack, limitation etc.

Valentine's Day celebrates the romantic/partnership facet of Divine Love.  There are teachings that  preach the "ungodliness" of romantic love and that in order to be "holy" you  must forgo  romantic connections.   Nonsense, this is a part of the same teachings  saying that you must suffer in order to be "holy".  These are man created beliefs with no spiritual law to support them.

However, romantic love must be understood on deeper levels if it is to "work",  and this is the missing element of so many human relationships.  Attraction between two people does not always mean you are meant to marry or establish an intimate relationship with them.  Attraction often occurs as the result of past life connections and frequently happens with someone you intuitively know would not be a good partner for you.

When two people with an intense past life history  meet (male/male, female/female,or male/female) there can be either instant attraction or instant repulsion. Many gay couples have been together in other lifetimes as heterosexuals.  Connections do not end  with death, for you are consciousness, not just physical bodies.

Marriage or committed relationships often happen as the result of pre-birth choices by both parties for the purpose of resolving some unfinished past life issues which may or may not have began as romantic partnerships.  Often those energies needing to be cleared  have their origin in ancient roles lived as mother/child, father/child, friend/friend, servant/master etc. Present life relationships  can provide an easy path in which to bring closure.  This is often the case in those relationships where one or both wonder; "Why did I marry this person?"

Always look deeper dear ones, there really are no accidents at this point.  Look into what you are learning or meant to learn from every situation. Karmic closure can take place even if only one person desires it.  If  he/she is able to recognize the other as an un-awakened Divine being and also chooses to no longer live within an energy of judgement and hatred (which simply serves to feed the karmic situation) then they have reached that place of unconditional love and there is karmic resolution.   An individual can now leave the situation if they choose, knowing that it is finished.

Sometimes two people are intensely attracted to each other on all levels, but for some reason this never unfolds into a relationship. This happens with some who choose in their pre-birth planning not to be with each other in this lifetime.  They understand that intense past life connections and love would not allow them to seek and spiritually grow into their own awareness and power if they were together--they would  be content to simply be together.

Romantic love must be rooted and grounded  in real love or it is simply animal attraction which sadly many consider to be real love.  This concept of love is promoted heavily in all areas of media-film, books, TV shows, magazines, advertising, etc. and particularly on Valentine's Day.

The wise know that sexual attraction alone is not love but is a facet of it, one of the "added" things.  Sexual attraction by itself is based in the human need to propagate the species which can be accomplished without  love.  Intimacy based only in sexual attraction never carries the intense and deep energetic connection and fulfillment that is possible when both are of a consciousness that sees beyond appearances to the deeper realities shining through the other's eyes.

Love relationships  are never dependent upon how the other looks or dresses and it is fruitless and very human to attempt to mold someone into your concept of the perfect partner.  Healthy relationships have similar energy resonance and those in them have close world views  but do not necessarily think and believe exactly as each other because all are here to learn and grow according to their individual need.

Two well matched individuals  be it friends or partners, have attained similar states of consciousness. Observe those living out from un-awakened states of consciousness who enjoy relationships of fights, cheating, anger, angst, and make-ups-- they are both on the same energetic wave length  so to speak--it is their state of consciousness, their concept of how relationships work.

There may be a time when you are tempted to enter in to the lower awareness level of another because of attraction and desire for this person.   Although certainly allowed by free will, it can never result in the happy outcome desired but instead usually ends in heartbreak after a quick, short, intense time together.  One simply cannot go backward to outgrown states of consciousness no matter how appealing doing this may seem.

Misconceptions abound regarding love and partnering.  Governments, religions, families, and friends continually spew that love is only valid if it meets their often very narrow concepts.  The partner must be of a certain  color, gender, education, financial status, or  other acceptable trait.  Love is the experience of One self manifesting as the many.  Love never has been and never will be dependent on mankind's concepts.

There comes a  point at which an individual  no longer "needs a relationship" in order to feel complete, happy, and whole because they have attained a consciousness of spiritual wholeness and completeness.  They have discovered their twin flame to be within, and being in relationship now becomes a personal choice.

Happy relationships only happen when each understands and accepts their own true worth and power because then there is no longer the need for outside validation.  Unawareness of one's true identity causes the un-awakened to seek their completeness from outside of themselves. "If I am married to this good looking, rich, smart person, then I must be loveable and valuable".
Many still carry old cellular memory from times when being in a partnership may of been the only way to avoid death or starvation and this is often reflected in those who jump from one partnership to another never allowing themselves to be alone.  Recognize and allow these old cellular memories to clear if you suspect them in yourselves.

As you grow stronger within, you will look back at past failed attempts at romance and laugh, saying;  "Thank you dear God  for saving me from this relationship."  even though at the time you may of been heartbroken and hurting.  The human mind is programmed to  world concepts of romance and sex and responds thusly until you teach it differently.

If having a life partner represents completeness for you,  you will  draw this to yourself without thought and struggle because it will be the natural manifestation of your state of consciousness, your realization of spiritual completeness.  Being alone may represent completeness for some.

True, deep, and lasting relationships do include romance, great sex, and all that the rigid, structured,  Puritan state of consciousness deemed unholy,  but these are now recognized as being the added things.

We the Arcturian Group send love, joy, fun, and awareness to you all for Valentine's Day.

We come again dear ones, to speak of love and oneness, for in reality there is nothing else.  Accepting this has proven difficult for many, because outer appearances testify to separateness.  Society promotes and functions from this viewpoint and it is what you have taken for granted through many lifetimes. Yes, indeed the many do appear separate, but it is individuality within ONE omnipresent Consciousness that you see.

As you mature spiritually there comes a point where you no longer judge by appearances because you understand that what you see  is the mind interpretation of ever present spiritual realities.  This is why the world has been called "illusion" by so many enlightened masters.  The world is not illusion, how it is perceived  is the illusion.

Images of separation and duality are expressions of a world consensus consciousness of separation and duality. You are now evolving and moving beyond the beliefs of an un-awakened consciousness--leaving behind that which has governed you for so long.  The message of Oneness is pouring from many spiritually reliable sources because mankind is ready to hear and embrace this,  but for some, even those well along in their journey, the idea of Oneness can be difficult.

Most of you have moved beyond considering spiritual ideas  to be simply dinner party conversation and have also discovered that you no longer care to engage in arguments about religion.  World wide there has been a gradual shifting into a consciousness  that sees and understands that there are many paths up the mountain, but only one mountain.  Differences that once seemed  important simply no longer exist as they once did.

Many find that even as they clear and move beyond old fears and difficulties, they still must live, work, and function in third dimensional situations. This is why you are here dear ones, to remember who you are in spite of appearances to the contrary. While on earth as it is, you will always witness some three dimensional activities.   When you find yourself in these situations, keep your energy field clear and filled with Light, which does not allow you to be pulled into lower resonating energy.  Live each moment of each day, doing what is given you to do and saying what needs to be said from a place of unconditional love and you will not be in or of the lower resonating energy.  However, as we have stated many times, this never means being a "doormat".

Never broadcast spiritual awareness and knowledge to those not of a consciousness to understand it in hopes of changing some unpleasant situation. Your sacred connection to the Divine within  must be kept silent, secret, and protected  like a newborn babe, until  it grows and becomes strong enough to withstand any influence that would take it from you.

Keep the Sacred Self safe and warm, feed it, love it, and watch it grow.  As it becomes stronger it will  begin to manifest outwardly when you least expect it, in ways you never dreamed of.  It may verbalize the perfect solution to resolve some intense  situation even though you had never consciously considered such a solution.  You many find yourself taking some action (be it small or large) that you had not planned, but which turns out to make some positive impact or previously un-considered change.  These are the activities of the deeper, stronger, and more real YOU.

When  experiences of this sort begin to appear, you start to understand how the unconditioned  (by world beliefs)  consciousness works.  As an individual begins to evolve and grow more deeply into a consciousness of truth , his mind becomes increasingly unconditioned and begins to replace the previous human, thinking, plotting, planning, mind steeped in concepts about everything.   A mind no longer conditioned by false beliefs of duality and separation,  interprets the world accordingly.  Everyone has had experiences of simply "knowing" without thought at some time when it was needed. Do not diminish these experiences, but embrace and celebrate them.

This higher state of consciousness can only unfold as you choose and allow it.  It grows stronger through practice and trust.  IT is not some magic button inside a "you" that is separate and apart from the Divine,  but it IS YOU, the totality of YOU.  Be not afraid dear ones, now when you are being asked to leap off the cliff of your familiar, leaving behind those things that have represented security for you  (even though much of that familiar security may have been experienced as discord and pain).

Humans tend to hold tightly to what they are familiar with.  Remember this when you are deciding whether or not to move beyond someone or some situation in your life that you know has completed and can only serve to hold you in old energy. All is proceeding according to plan for everyone who has made the choice to evolve, even though it may not look that way.  We see Light flowing with ever increasing intensity with each person who awakens out of the illusion.

Never forget that you chose specifically to incarnate within this ascension time frame in order complete your own issues through experiences and clearings, and then move on to assist in the awakening of others.  Try not to drag your feet, "hemming and hawing" that things must be this way or that before you can be more "spiritual".  You already are spiritual, it is just a matter of accepting it.

God is all there is, and therefore you must also be this same  "all that is"--there can be nothing outside of Omnipresence.  The sooner you allow yourself to accept this, the sooner you can begin living it.

Each time you sit quietly, resting in the Light of your own Divinity, you are adding awareness to universal consciousness.  Meditation no longer needs to be the heavily structured process most of you were taught.  Meditation teachings that state;   "you must sit this way, keep the spine straight, and feet on the floor", etc. etc. were simply disciplines designed to help beginners quiet themselves and concentrate within. Structure is  necessary for an un-enlightened human mind used to being busy every second, but you are no longer beginners.

The goal of meditation is simply to rest in a quiet awareness of Oneness--that silent place where you can sigh and state to yourself;  "I and the Father are ONE" and then rest for a moment or two in that peace. This holy place eventually becomes easy to access and is part of every waking moment at which time, meditation as you know it is no longer necessary because you are living  it.

Practice being centered as you go about your chores, your employment, walk through a quiet forest, or even down a busy city street.  You can do this in the midst of a difficult day simply by finding a bathroom, closing the door, taking a deep breath, and resting in  "I am".

Meditation need not be as difficult as so many believe it to be.  Difficulties come when it is entered into with expectations based on concepts  heard about or read regarding what must happen, what it must look like, or what one must feel.  Meditation is simply the practice of resting in what you already are, and is not something you must attain.  Let go of teachings that say meditation must be a certain way or it is not valid.  Your decision to go within is your validity.   You ARE that which you are seeking.

Keep  sacred practices and unfoldments secret and silent within, for these constitute the Babe that must be kept safe until strong and grown. Share your spiritual pearls only with those who are connoisseurs of pearls.

All is One.  Remember that, live that, be that, and you will change the  world.

Greetings dear ones.  We come to say that much is happening on your planet that cannot yet be seen with  physical eyes.  However, you are beginning to feel change in the energy of your personal  world as well as on the universal level.  You may begin to  notice  more camaraderie among those around you--a sense of oneness that did not seem to be there before.  You may notice it within some group you are familiar with like a club you belong to or in family gatherings.  You will notice more unity and less sense of separateness.

These changes are the result of the high resonating Light now pouring onto earth and available to all who are receptive.  Every particle of Light feeds the consciousness of an awakening  world.  Many who were previously oblivious or blindly accepting of world conditions,  now find themselves to be more aware and quickly growing weary of the constant bickering and adolescent behavior of so many self serving world "leaders".  Seeing with new eyes, they are beginning to take note of those who serve only to feed their pockets.  The eyes of mankind are opening and personal power is beginning to be reclaimed.

Thus, as  society awakens, it automatically begins to move into its power.  Even a small thing like signing a petition, helps to activate world energy into new directions. It is not about giving energy to some negative appearance, but rather it is about taking whatever human footsteps may assist others to notice, on their level  of awareness.

You are in the midst of awakening to who you are, realizing that you are and never were less than those who hold power of some sort or another.  Appearances will not change until you change them, for the outer manifestations you see simply represent the consensus consciousness of the majority which at this time is still heavily enmeshed in duality and separation.

The whistle blowers of the world are courageous souls, not to be reviled  as many would have you believe. They are helping to awaken those who  blindly accept everything they are told--the sleeping ones who have given away their power to those claiming to own it.  Despite personal danger to themselves they speak words that need to be heard.  Those who stand to lose personal power or wealth from this information would have you believe that these people are criminals.  No, they are serving to assist the people of the world to wake up, open their eyes, and  hopefully take back their power.

Do what you can each day to awaken others but never in a preaching or  proselytizing way.  It must be done with love and  kindness and respect for the other's belief system.  Often it is not words, but simply your energy field filled with Light that helps to awaken another who is  receptive. Speak the deeper truths only when guided to, when a person is ready to hear or is seeking truth, always trusting your intuition.

We wish to speak of trust--a very misunderstood word.  Trust in its truest sense reflects a state of consciousness able to rest in awareness, living the experiences of each moment from center which often appears to the world as a doing nothing.  In reality, doing nothing is to hold blind faith in some concept, person, or thing--which is  often seen as trust.

Blind faith comes from accepting third dimensional concepts of God,  angels, saints, or even the government  without question, simply because it is what you have been told.  Blind faith is ignorance dear ones, and represents the surrendering of personal power to an unknown something deemed to have power--it is not trust. This is not to say that the angels and Beings of Light are not there to assist you, but to assist you, not do the work for you.

Trust is the natural offspring of a state of consciousness imbued with spiritual truth and not dogma or rites and rituals handed down and accepted from generation to generation.  Trust is the living out from and resting in a consciousness  aware and  knowledgeable of what is real and spiritually  true.

Trust simply becomes natural at a certain level of awareness, but is never  attained through forever looking outside of one's self, for no matter how much the teacher or path may be revered by society, at a certain point the spiritual journey must become a  solo one, a shift to one own Center.

This is not to say that you will never be guided to some book, class, or teacher along the way but it is to say that you must always listen to  your intuition, and when something feels finished, don't be afraid to leave it and  move on.  Many paths and teachings are only needed for a short while--steps along the way to be left behind at some point which then allows new and higher phases of the journey to begin.

Trust manifests as the student learns to withdraw energy and power from outside forces and place it where it is,  in Divine Oneness. Trust is aware of what is, instead of 3D concepts of what is. The journey through  third dimensional lifetimes has been a stepping and falling, stepping and falling but at some point growth begins to come by Grace and fear dissolves. The spiritual student finally understands who and what he is, and is able to rest in that awareness.  This is Trust.

All of you are ready for this, dear ones.  All of you are spiritually aware on  levels enabling you to begin living, moving, and having your being in true Trust. It is only the world's continued dependence on concepts and beliefs that keeps it in bondage.  Trust lifts the burdens from your personal back and removes the mental drive to "do, do, do", which is the focus of intellectual thinking as well as can be found in many metaphysical teachings.

The metaphysical  protocols were necessary in the beginning, and are still important for the newly awakening, but you are no longer beginners.  You have completed that phase and are ready to shift from the "do, do, do" mentality  into "Be, Be, Be".  Now it is about growing beyond the metaphysical (taking a bad picture and making it into a good picture) and moving into mysticism  (the conscious realization that in reality nothing needs changing).

Rites and rituals  serve their purpose but for most of you, are finished.  The journey is now about a consciousness shift transferring power from the without to the within-- whether the without be "good" or  "bad".    Every belief in some power outside of yourselves must be let go of, and each letting go helps  release you from any bondage to it.  Know that even   "good" appearances are concepts.

Letting go of something does  NOT means you cannot or will not ever again enjoy the finer things of life, nor does it mean you cannot take part in some religious ceremony or service.  It simply means that you no longer imbue these things with a power they do not have, whether over you or for you.  You begin to see the reality behind all appearances and  understand the truths  being represented through rites and ritual, knowing that in and of themselves they have no power.

Because the outer is an expression of one's state of consciousness, as he shifts from a consciousness of duality and separation into a consciousness of truth and Oneness, he will find that the outer forms begin to  manifest in new and higher ways. Never fear the loss of something, for if you carry the substance of it in  consciousness it will re-appear, often in some new and better form.  For example; An attained consciousness of wholeness and completeness often manifests outwardly in simple things like finding the perfect parking spot but can and does also manifest as the right partner, job, or  new order and peace in your life.

Mind is the interpreter of one's state of consciousness translating it into forms appropriate for each individual. If living alone represents wholeness for someone, their attained consciousness of completeness  will not manifest outwardly  as a partner.  Likewise,  a consciousness imbued with  concepts and beliefs of duality and separation must appear as experiences of both good and bad--duality.

This is the purpose of the physical, emotional, and mental clearings you are experiencing now.  Even those awake and evolved still carry cellular memories from past lifetimes lived fully in a consciousness of duality and separation and these  need to be released and cleared.

You are ready.

Dear ones, We send  wishes for a Happy New Year and add that a year of the new it will be, for you have already cleared much of the energy that held you in bondage to an old state of consciousness and this will allow the new to unfold.

Never believe that your efforts to learn and express love are  wasted or insignificant, for every spark of light and awareness serves to decrease the density of a general world consciousness based in duality and separation.  Each day the Light of the world is becoming brighter and lighter because of the unfolding awareness of so many.  All are in and of the One and therefore the Light you acknowledge within yourself,  you also acknowledge for the world.

Love is the key to everything dear ones--love of self and love of everything in your life.  Love every aspect of your lives--the nice as well as the not so nice, for every experience  can act to bring you into deeper spiritual awareness if you allow yourself to dig and discover the beliefs behind the experience.

It is impossible to truly love anything until you allow self to love self.  A consciousness   selective with regard to love, one that  excludes self, reflects the third dimensional beliefs of duality and separation.  A consciousness that excludes self can never express love in its highest sense because it does not yet contain it.

Concepts and beliefs about love in an un-awakened society are usually based on sexual attraction, power, and the need to feel loved, all of which are relentlessly promoted through the media, "experts", family, friends etc.   Only LOVE, which is the recognition, realization, and action of a state of consciousness reflecting the ONE SELF will shift world consciousness into its next level of evolution.

Many say; "Oh but you don't know the awful things I have done.  I could never love myself because I am not worthy of love."   Others may say; "I was always told and treated as a child that I was unworthy or unlovable, and I just can't let that go."  YES YOU CAN!  It is time to stop continually feeding  the issues, beliefs, concepts, memories and whatever still  holds you in bondage. These things have only the power you feed them and you who read these messages are evolved enough at this point to acknowledge this, allowing any "victim mentality" of the past to cease and dissolve into the nothingness that it is.

This is what clearing old energy is all about, the more intensely you may experience something the more it is spiritually begging to be looked at and cleared.  Clearings are graduations, and not negative experiences.  Lovingly allow them by getting to the root of your belief system, no matter how physically, emotionally, or mentally painful the process may be.  Once something is cleared spiritually it is gone, you have uprooted it and not just cut off a branch that will re-sprout at some later date.

The more an individual focuses on human problems the more these issues become firmly embedded within consciousness.  Be alert to your thoughts as you go about your day. There comes a time in everyone's spiritual journey when it is necessary to surrender everything that no longer serves his evolution.  Much of what you considered to be who and what you are, is now changing.   The new you is moving into a new state of consciousness but it cannot happen while grasping tightly to the old.  This is what is meant in the bible story of not putting new wine into old wineskins.

You are not your past actions whether of lifetimes long ago or this one, even if that lifetime was filled with seeming errors.  Learn to love the mistakes you have made along the way and give gratitude for them because this is how you learn and grow.  It is only the third dimensional consciousness that affixes blame and shame.  These messages are being read by those ready to take the next steps--YOU. Those not ready, do not resonate with these words.

Bless and love everything in your life, always remembering that every soul makes pre-birth choices regarding the necessary evolutionary experiences he needs and is evolved enough handle .  An example is that many who experience harsh and abusive childhoods have specifically chosen that particular situation in order to activate and once and for all resolve issues of low self esteem and self loathing or to resolve some karmic relationship that has been in place for many lifetimes.

Many have not been able to clear the more deeply buried issues until now  when the higher dimensional energies are flowing intensely, giving them the strength and guidance to do the work.  Some issues needing to be cleared will not come to conscious awareness until the individual  is ready and this is determined by their Higher Self.  Clearings of more intense and deeply held energies often clear in layers, so if you have done the work to clear and resolve something and find that lo and behold it is back, just know that you are now ready to clear the next layer and will know when it is finished.

Many who struggle with some physical, emotional, or mental difficulty are only now evolved enough to  move through and resolve issues behind it, usually from lifetimes ago. Love and honor the courage of those who struggle dear ones, for many that seem handicapped in some way are old and wise souls who chose their situation in order to quickly evolve learn something.  Often it is an evolved soul who chooses to be born  handicapped or stillborn in order to assist in the evolution of the parents.

Love does not mean you must have emotional feelings for everyone.  Feelings and emotions are often tied to human concepts of love.  Love is the recognition, acceptance, and honoring of self, others, and everything as spiritual, for there is nothing but God.  Period.   It is only three dimensional beliefs of separation  that cause everything in the world to seem separate.

Look in the mirror each day and say to yourself "I LOVE YOU".  Warts and all, I love you.  Love the warts, for they are you and you are not separate from the ONE.  Mankind is on the brink of  entering into a time of awakening, leaving behind that spiritually immature  consciousness filled with false ideas and untruths. The world has been asking and praying for this, and yet remains reluctant to embrace it, afraid of leaving behind what is familiar no matter how unpleasant.

This, now, is the time for trust, trusting that the dance is finished, following a certain ritual, book, or Guru is finished.   This, now, is the time to accept that You are IT.  That which you have been desiring, searching for, praying for, you already are, but it is up to each individual as to whether or not they accept this.   Free will allows everyone to play in the dream for as long as they wish, but the door is open now for those who choose to enter in.

Let the new year of 2015 be one of celebration in the midst of whatever occurs.  Celebrate each day, giving gratitude for all things.  Celebrate your awakening, and celebrate who you are by no longer giving  power to the illusions of the third dimensional belief system.  Starve these  pictures of their power through your recognition of Divine  Light within each and every individual regardless of what they are reflecting outwardly.  Everyone is  learning, but the lessons of others need not become yours.

2015 is going to be a year of awakening and change for many.   Those holding tightly to old concepts and beliefs will find themselves removed from them one way or another and it is much easier to make the choice yourself rather than  have the Universe do it for you.

LOVE yourselves, dear ones.  LOVE is our message today and always.   LOVE is the key to all things.  LOVE is the answer you seek and is all there is.

LOVE EVERYTHING.

Greetings from the Arcturian Group.  Know that everything is proceeding according to plan, and be not afraid dear ones, for the many changes and upheavals happening on earth at this time are all  proof of the shifting energy of evolution.  Remember as you observe chaos taking place in the world, that what has become old, more often than not, only leaves after a great deal of resistance.

We wish to speak today of change, a topic we have spoken of before.   There are many who as of yet do not associate needed change with personal change.  These dear ones see and work toward change in politics, religion, and government, but do not realize that they themselves are the politics, religion, and government.  Outer change can only manifest from the substance from which it is formed--consciousness.

You as creators, have over time created many different three dimensional worlds, each  manifesting the belief system and consensus consciousness of the times.  When enough people awaken, the enlightened world consciousness will once again shift the dynamic of the whole, for consciousness and its manifestation are one and the same.  Notice that as concepts and beliefs change (states of consciousness) laws and acceptable ways of living also change.

Change is extremely  difficult for those who find  security and comfort in sameness.  It is very easy to rest back in ways that up to now have  worked to bring a  level of harmony and peace regardless of how it was accomplished. These dear ones struggle and  resist, and often without thought automatically bring judgement and criticism to any suggestions of change in their personal world or the world in general.  This is an expression of fear.

Those accustomed to believe without question what authority  figures tell them, respond with; "We have always done it this way and it has worked. The government, priest, expert has said...".   However this mind set is no longer working because the energy manifesting and supporting it is dissolving. These ways  were perfect and necessary for their time, but it is a NEW TIME, and if you look closely, you will discover cracks beginning to appear in the foundation of long held world concepts.

Many commonplace ideas, beliefs, and world solutions  no longer resonate with an awakening mankind and because they no longer hold the belief energy necessary to keep them in place, are crumbling. This appears as chaos and brings about all varieties of  fear for those of a superstitious and  un-awakened states of consciousness.

In your personal lives many of you are discovering that long standing relationships with certain friends or members of your family seem to be weakening--you are no longer  on the same page so to speak.  This is because resonance attracts like resonance.  If you have made a shift in consciousness, you no longer resonate with those who remain in the previous energy.  This can be very confusing and even hurtful for those who do not understand and interpret the situation on a three dimensional level.

It is not that  you are better than them, it is simply a matter of being in a new place energetically--like energy attracts like energy.   Often there comes a point in the spiritual journey at which the student finds they have no friends at all.  This is the point at which you must trust this part of the journey, not trying to fix or make things work again according to popular thinking.   Very quickly the student will find that those of a similar resonance are attracted into their sphere, and become the new friends.  Know that you are being guided every step of the way and cannot be other than on your chosen path.

Fear governs the actions of many large corporations and individuals as they observe their business models no longer resonating with the majority.  This is happening in the medical/pharmaceutical fields as well as with organized religions and world  governments.  Frequently no holds are barred financially or dishonestly in  the effort to bring customers back.

Without evolving change in the consciousness of a business and individuals involved,  heavy promotion tactics simply will not work because as with individuals, the energy no longer resonates--they are not speaking to the new consciousness and  people are not so easily fooled as they once were.  You will begin to observe this frantic struggle to make the old work as it once did,  more and more in all sorts of situations.

Change is often difficult for spiritual leaders and light workers if their focus and teachings have been  locked into specific practices and teachings. They will find that what once worked , no longer works.  This is because any spiritual practice or beliefs  rigidly not allowed to evolve, will not resonate with the evolving consciousness of serious seekers.  As students move on and fewer are attracted, many a sincere spiritual teacher is left to wonder why and this question will become the teachers lesson.

Dogma, rigidity, and righteousness have no place in true spirituality for they represent stagnation--impossible within an omnipresent and  infinitely unfolding Divine Consciousness.  Duality and separation beliefs  cannot be carried into the new energy.

Work, play, beliefs, practices, and every spark of daily living is moving into  new and higher expression.  Your job is stand back and allow, letting  go of any attempts to figure it all out with the mind according outdated and often incorrect world beliefs.  A mind  conditioned through many lifetimes with third dimensional beliefs needs time to reprogram.  This means allowing new awareness's to integrate and become one's state of consciousness while at the same time releasing everything recognized as old and finished.

The minute there is resistance to anything, an energy block is created.  We are not saying you must embrace every new idea of truth that presents itself,  for always there must be discernment ( a facet of personal power).  Be alert to  ideas and beliefs being  hyped as "new" that are simply repackaged "old".    It is imperative that without pre-judgment, you honestly examine in the light of your unfolding evolution, every concept and belief you  hold as true or become newly aware of even if  just to say; "Interesting".

Non resistance is  much easier than fighting every idea that may be different from what you have always believed.  It is ego (the sense of separation from the whole), that does not want to admit it may of been incorrect about something.  Let that false sense of self dissolve into the nothingness that it is dear ones, for it is simply the belief you are a human being who must struggle for every bit of recognition or value.

Never become spiritual door mats, living in fear of expressing personal choices because of rigidly binding man made rules that define spirituality.

It is time for all choosing to proceed on a spiritual evolutionary journey to trust that there is a bigger picture unfolding and that you need not know everything about it.  Be open, allow, and let go without fear for you can only expand  more and more--life is unending.

Trust that if you have chosen to evolve,  you are doing it.  The "train leaves the station" with your intent.  You will be on your chosen path and always guided  even when it seems as if nothing is happening. The do-ing part is complete for many of you.  It is time now for the Be-ing part which may seem right now to be a doing nothing.

Throughout time, you have been programmed to believe that spiritual progress could only happen with spiritual "do-ing--reading, writing, healing, studying, taking classes, having energy sessions. This has always been the way for spiritual growth and learning and you did all these things sincerely and well.

We do not say you will never again be guided to some teacher, book or class.  However, know that it is now a NEW TIME--a time for greater selectivity, trust, and allowing.  A time now to  listen,  rest in, live, and practice the truths you learned allowing yourselves to birth a new state of consciousness.

There is nothing to become, you already are.

Greetings once again dear ones.

We wish to explain that because so much is happening on the deeper levels,  you must be prepared for changes that will soon manifest on earth  in many areas.  Increasingly intense energies of transformation and clearing are bringing about physical,  emotional, mental, and spiritual change not only for individuals, but also for Gaia who is a living soul and not a piece of dirt.  This information is not given to cause fear, but simply to help you to understand that certain events are vital and necessary in order for mankind to evolve.  Know that always, those involved in earth events have  agreed on the deeper levels to be a part of the experience.

Never forget  that you are not simply flesh and blood bodies living on a physical planet.  The  time  is long past due for mankind to understand, accept responsibility, and correct the damage reaped upon Gaia  and all her life forms  in the name of greed.  As more and more people awaken, change is inevitable, but it often takes  a catastrophe or some devastating event for people to even question these things and the question is;   Is money worth the  continual destruction of life, including air and water?

As changes take place on earth,  learn to flow with them, not  resisting or struggling to keep the status quo in the belief that "The good old days" were better.


The United State is  celebrating a holiday of freedom at this time.  Do you truly understand freedom, or do you  continue to believe what you are told about how free you are?  Look around you dear ones, and observe how society has gradually allowed itself to be deprived of multiple freedoms under the guise of keeping you safe.   FEAR  is an effective means of control.  You need  these rules in order to be safe, don't  you...?   It is a NEW TIME.

Question everything you have heretofore accepted without question in the belief that anyone in authority must know what is best for you.  You are at a point in your evolutionary journey at which you must  reclaim the power and trust you ignorantly gave away.  Start by learning to  trust your inner guidance and no longer acting as lemmings blindly following a just as blind leader over the cliff. It is  time for peoples of all countries to stop waving  flags and shouting:  "My country right or wrong, my country".  It is time to move into a consciousness of the  Oneness of all people.   It is a NEW TIME.

As a result of many lifetimes lived in the third dimensional energy of duality and separation, a universal ignorance of oneness  became the consensus consciousness of mankind.   This sense of separation manifests outwardly as the haves and have not's of the world--some with power, intelligence, and strength and some without.  This belief system continues today,  allowing those with the most money or strength to control everyone else.   Up to now most have simply accepted  this.  There are still many people who have need for rules and laws and someone to tell them how to live. but for all awakening souls,  it is a NEW TIME.

Pause and think before voting for the individual with the loudest voice  promising exactly what you want to hear.  Question, ponder, and most importantly, go within to experience the resonance of the state of consciousness of the candidate.  Feel their energy, listen to the spaces between the words, trusting your intuition to guide you. This is how you begin to reclaim your innate power and move past the hype and noise created by those who would be in power.

This will create a true democracy, which was the intention of the founding fathers for the United States.  The United States was founded on deep spiritual principles which over time  have been manipulated for personal agendas and allowed to deteriorate from their original high resonating focus.  When personal religious beliefs and narrow concepts are allowed to determine laws effecting all but  benefiting the few,  true freedom is lost and the ideals of a declaration of independence no longer exist.

Many still hold  ideas of war with pride.  We too honor those who selflessly sacrificed themselves because these brave souls  were living out from their highest sense of right.  Living out from your highest sense of right is all that is required of anyone.  This way of resolving issues  has been  the consciousness of the world in the past and now.    However, it is a NEW TIME.

There is a popular  saying; "Freedom is not free".  Freedom IS free, it is the sense of separation and wars that act to prevent it.  Violence can never and will never bring peace.  They are two very separate energies.

You are ready to experience true freedom dear ones, the freedom that has been ever present as  your innate birthright and can  never be taken from  you for it is who you are.  However, freedom can only manifest from a state of consciousness that knows who and what it is--not who and what it has been told it is.

You are not meant  to live according to  the whims of corporations,  organized religions, and big government.  You ARE the corporations, religions, and government if you choose to be. This does not mean you must stand in the street shouting and protesting, for often this simply gives an issue more power, not less.  It does mean that silently, secretly, and quietly, you  begin to trust your ability to make your own choices regarding what you believe.  It means working for change--questioning, pondering, going within, and then taking whatever actions you may be guided to take which may simply be to send Light.  It means choosing not to limit yourselves to the "acceptable beliefs" of the third dimensional belief system.

Spirituality cannot be separated from daily living for it is one and the same but the world in general does not yet understand this.  God cannot be prayed to for victory of one group over another--there is only ONE.   Bring your highest level of spiritual awareness to every physical  emotional, and mental activity of daily living and you will find more awareness is given for you have opened the door.

Realize that true freedom can only be experienced personally and globally when there evolves a consciousness that all people and all living things are Divine and One within that Universal Consciousness many call God.

Freedom is your soul, your identity, the reality of you as spiritual beings.  You can never be separated from it unless you believe you can be.  It does not look that way because at this time the world is manifesting a gross  ignorance of this.  Freedom is  the essence of your very being and as this awareness becomes a global state of consciousness, it will manifest outwardly as the peace so many seek, while yet believing that war and separation will bring peace.

Celebrate the birth of your country dear ones, but never forget the real focus and intention that brought it into being. The event of independence that you celebrate on the 4th of July is not finished, but only represents the first layer of many evolutionary steps necessary for mankind to evolve into a consciousness of real freedom.

This message is given as always, with love for you, our sisters and brothers.

Dear ones we are well aware of the disappointment many of you feel as to how slowly the new earth seems to be emerging.  This is because you are viewing the process with specific expectations of how things "should" be according to whatever concepts you may be holding.   There is no fault in this,  as it simply reflects how you have learned to live life in third dimensional energy.

You expect evolutionary changes to manifest in certain familiar ways, but remember, the human consciousness is limited to what is already known.  Be open to what is as of yet unknown, for change is, and will manifest in unexpected ways.  Know that powerful new energies are silently pouring in and available to all,  but will not always manifest in the ways you expect. Much of what you perceive as chaos, is actually the  cleansing and clearing of old energy.

It is very important to take time to rest during the day for the process of integrating these powerful  new energies often leaves an individual feeling exhausted.   Even with busy work/family schedules it is important to include  small breaks  which can be as simple as  shutting a bathroom door and resting for a few minutes in the realization of "I AM".   If possible, a short nap works best.

The integration process does not happen overnight, but is exactly that,  a process.  Be patient dear ones, and let go of the temptation to believe  you are doing nothing simply because  you and the world are not as you had anticipated--yet.

Some, when feeling the need for a nap in the middle of the day,  resist, believing that it indicates  laziness or a physical backward step because it is something you have never before needed to do.  It is important not to resist, for the integration process requires a great deal of  energy, leaving less for the active lifestyle you may of become accustomed to. The person who insists on continuing old habits of going, going, going, in spite of a need for more rest, will soon find himself very depleted of energy

Taking time to stop and rest is not a doing nothing as it may appear, but is an action  integral to the releasing/integrating  process.  You have been programmed to believe that busy-ness equals productive and successful living.  Both school and parents taught you that time spent doing nothing was idleness, a bad thing, but as with so many other beliefs this is simply a concept handed down from generation to generation.

In past lifetimes this was true and thus many still carry cellular memory of lives in which those who did not plow and harvest, or work long and tedious hours for someone else, did not eat.  It is not meant that every moment of every day be a struggle.  Know that the energy of joy and fun is closest to Divine energy.

You are finding that much you previously believed to be truth has become gradually fading cellular memory.  You are beginning to understand that you are not simply physical bodies, but are consciousness in varying states of awareness who have chosen for learning purposes, to experience the physical world in a physical body. This is the root of the struggle for those who believe  they were born as either the wrong sex or skin color.

Cellular memory accumulated from lifetime after lifetime lived as one gender will dominate the state of consciousness of an individual and has nothing to do with the body.   Because these dear ones do not understand the bigger picture, they assume a physical mistake of some sort has occurred  and thus seek to fix it by becoming the gender they strongly resonate with.  Gender choice is a pre-birth decision made by each soul in order to integrate whatever male or female energies may be needed to create wholeness and balance.

Try not to judge or compare your journey with that of others, but instead closely examine everything you do not yet understand in your own personal experience or that of the world in order to find the hidden lessons--pearls of wisdom waiting to be discovered.   Everyone's journey is individual,  based on the experiences of past lifetimes.

More and more and moment by moment as you begin to resonate with the new and higher Light frequencies,  you will also begin to experience seeing and hearing higher dimensional energies.  Be not afraid when these things happen, but trust the  process for changes are happening and nothing is going to stay the same.  There is nothing to fear.   Those who continue to hold fast to what is finished, will find themselves grasping air in a dissolving world.

We wish to speak of gratitude which is the counter balance of love.  Gratitude in its purest sense, is the acknowledgement of  "I HAVE", and not simply the act of saying "thank you" for some good received as is commonly believed.   In this deeper spiritual sense, gratitude  acknowledges that every good seeming to come from outside of self is actually the manifestation of ever present wholeness and completeness.  Gratitude flows easily from a consciousness of one's innate completeness and abundance because of oneness with Source.

Unexpressed gratitude acts to  block to the flow so to speak,  for love and gratitude balance each other. Think of a coin--love on one side and gratitude on the other.  Begin to practice  gratitude in every area of your life--for what you have as well as what you believe you don't have materially, emotionally, mentally, and spiritually for the experience of lack serves as an important window into one's belief system.   Always remember that  painful lessons were chosen by you as necessary for attaining the growth and evolution you seek.

You are beginning to understand that you are creators, that your words and thoughts take form and manifest.  The act of expressing gratitude serves to retrain a cellular memory of lack into  "I have.  I am abundant and am infinitely  blessed by virtue of Oneness with Source" .  Living each day from this awareness allows you to experience the energy of love and helps you to more deeply  understand  whatever lessons you may be having.

Because gratitude is a facet of  love it  must be integrated along with all the other facets of love,  eventually  becoming your  attained state of consciousness, fully integrated and manifesting as unconditional LOVE.

Practice with the simple things  for gratitude need not a grand statement, but simply the awareness of "I have because I am".  As you drive to work give gratitude for a paved road and stop signs.   When you shop give gratitude for the ease and availability of what you may need.

Gratitude can be practiced  on all levels.  Materially it can be by giving away those things  you no longer need or use (not junk)  which tells your cells; "I am grateful that I have enough to share".   Emotionally giving a shoulder to cry on is the acknowledgement of  "I give thanks for the lessons that have provided me with broad shoulders".  Mentally,  giving words, information, intelligent guidance to those who seeking on that level says;  "I have been there, I can tell them how it worked for me. I am grateful for an ability to assist others. I have wisdom".

Even a penny donated in the spirit of love, carries powerful  energy for spirit knows nothing of material monetary values, but only recognizes intention.

Give gratitude for the fact that you have evolved to the place where you are able to recognize the Divine nature of everyone you come into contact with.  Express gratitude for someone's smile, a bird song, or a good book and you will soon come to realize that every single moment of an ordinary day holds infinite opportunities for gratitude.

This is the journey, this is what you have asked for, this is why you are here.  Your time on earth is a privilege.  Make the most of it.

Dear ones, rapid change is taking place for many by choice as well as by need.  The path of evolution can  difficult  up to the point at which a spiritual transition takes place and truth becomes one's state of consciousness and  begins to manifest at that level.  Much of what is being projected into world consciousness through the media, churches, government, etc., simply no longer resonates with many of you.   Long standing life styles and traditions often involving friends and/or family are beginning to  feel old and finished.

Some beliefs  are easily left behind and no one even notices, but frequently attempts toward personal change serve to activate a period of intense questioning, judgement, and criticism from family and friends. This may cause the awakening student to doubt or even return to what is familiar, but because he has evolved and is no longer resonating with the past, he will not stay in what is finished.

Confusion and doubt often occurs as an individual begins to receive and contemplate ideas of truth completely foreign to his present belief system.  As a once solid spiritual foundation begins to dissolve it  becomes extremely stressful for those holding structured religious beliefs.

Be not afraid dear ones, for there are no accidents at this point.  Know that you are always being guided, and are never alone. The Higher Self always brings forth what is needed when you are  ready. Learn to trust the process,  knowing that at this point  you are  right where you need to be, doing what you need to do, and doing a fine job of it.

Discouragement with its sense of failure are the manifestations of a consciousness filled with " I should.  I need to.  There is only one right way."--all facets of duality and separation.  It is time to cease the ever running inner dialog  of self criticism and judgement, allowing these beliefs to dissolve into the nothingness they are. There are no "shoulds" in higher dimensional energies, only love for every individual and their choices.

As spiritual students, you are at a point where you cannot evolve further until acknowledging  "self".   Spiritual students the world throughout time and now have struggled to find and understand what  God is.  Many  gods and concepts  are the result, many still being accepted as the only truth.  Most of you have come to know and accept that the Divine is within,  your own true nature and real SELF.  However,  at the same time, many believe their humanity to be something separate in the belief that "self" is simply ego and must be eliminated.  The "self" is SELF, but in the human state is conditioned by concepts and beliefs of separateness.  The "self" cannot be left out.

An absence of self-love has been falsely considered to be humility, but it is not.  It is the manifestation of separation consciousness.  Humility is an awareness of who you are and where any gifts you may have,  flow from.

Many look back on their lives, believing that  past actions or even present ones make them unworthy of love and find it impossible to love the person they believe themselves to be.  This is because they hold false concepts of what love is.  Loving self means acknowledging, accepting, and loving not only the Divine within, but also that part of you that is still learning--the  part that has made many mistakes and often seems to be a failure.  It is a journey of bringing together the inner child and the Divine Self.

There can no longer be a dwelling on "sins" of the past for  this simply serves to cement  energies of the past, all of which are illusory.  Remember, a person can only live out from their highest state of attained consciousness.  The things you may of done long ago, are not what you would do today.

The inner child is a term used for the accumulated energy of an immature and unawakened state of consciousness carried with and added to,  lifetime after lifetime.

Inner child work can be a powerful tool in learning to  love  "self".  Although this work is often considered  a psychological practice, it  actually goes much deeper.  The inner child is that innocent and trusting facet of you that has been rejected, disappointed, and hurt.  It is the unawakened you as a small child, your individuality in it's most innocent state.  As the experiences of the third dimension began in every lifetime,  you only knew and thus accumulated  the cellular memories of a child's defenses.

Most humans still carry some if not all the energy of wounds experienced in the child  consciousness along with the immature ways with which they attempted to address them.  As an adult, these  energies begin to manifest as the immature thoughts and actions that  pop up unexpectedly when you least expect them, usually in times of stress.  Most adults  usually ignore these feelings, not realizing that they are dismissing their inner child  as irrelevant and thus perpetuating the situation. This energy can only be cleared by you.  The child must be acknowledged, accepted, loved, guided, protected, etc. and it is time.

Honor what ever age your inner child may be at this time.  You will know through the types of inner dialogs you experience.  Speak gently to this part of yourself, telling it that from now on you are going to love and  protect it and  most importantly acknowledge it.   The inner child is the you who for no good  reason often feels hurt, angry, sad, or depressed and as an  immature child, afraid.   Be patient, knowing as with all children, the child is doing the best it knows at that moment and needs your support.

Many have  intellectual spiritual knowledge  (crown chakra), but until you embrace the whole of you acknowledging and accepting the child (heart chakra), there will  be struggles.  As you learn to accept,  listen to, support, and talk to this facet of your individuality without dismissing it as nothing,  it matures--first responding as a child, then an adolescent, and finally as an adult.  Finally you are able to, fully integrate this healed aspect and "self" becomes SELF.


Evolution on earth is a gradual  progression  toward remembering and accepting  the true identity of self and others through the experiences of many lifetimes lived in an energy of duality and separation while gradually evolving into a consciousness that learns from within instead of from without.  Know that regardless of appearances, in reality all is perfect, always has been, and always will be.

You have spent lifetimes trudging through a forest of dense and dark energy, trying to do your best every step of the way.  At a certain point you  began to see light filtering through the trees and  moved towards it, but after finally moving out and  into the bright light, you noticed that you had accumulated grass, sticks and all sorts of other debris along the way.  You are now in the process of discovering and removing the accumulated debris, some of which is deeply embedded.  This in turn will allow you for the first time to see yourselves and others clearly.

When truth  is new, there is often doubt and a desire to remain in what is  familiar.  However, once out of the forest and into the light, any glamour the old may still hold for you, soon disappears.    This is where most of you are now--a point at which you must cease looking  with  "forest vision".  You have emerged from  the forest and must leave behind the ways of the forest.  You now have light which will allow you to discover an infinity of ever present beauty and perfection previously unknown.

There are no accidents at this point in your journey.  All experiences are opportunities to examine your belief system without judgement or criticism.  It is time move beyond any self loathing or  other beliefs  that may suggest you are "less than".   You are ready to move beyond intellectual spiritual studies and into the  acceptance and  living of it. This means your spiritual journey must include honoring, appreciating, understanding, accepting, and loving  who you are--warts and all.

LOVE can never be governed by the standards of a society still enmeshed  in concepts of good and evil.  LOVE is  based in and flows out from  SELF.  Your awareness of this truth expands and grows with every unfoldment but  "self" cannot be excluded in the process.

Try to love the journey itself as well as all you meet along the way but most important is the knowing  of "self" to be SELF and that none  are more Divine than you in every moment.

We come in love to assist all in their evolutionary journey of remembering.

The world appears to be filled with intense images of both good and evil at this time--duality expressed at its best and highest.   This is because ever increasing energies of Light are greatly effecting the stability of anything and everything formed of old beliefs and concepts.  Rejoice dear ones,  for it is you doing the heavy lifting that is changing  world consciousness--the outer is always a reflection of the inner.

Slowly but surely, a consciousness of Oneness is emerging.  More and more people are being intuitively guided to  assist man, animals, plants, and the environment of Gaia herself in whatever ways seem to be needed.  An understanding of many within One, is gradually being born which starts on the unconscious level and then moves into conscious awareness.

Higher dimensional energies are serving to open the hearts of all who are ready, shifting them more deeply into the realization that every person, no matter the color of their skin or language they speak, as well as all life forms, are in and of the One substance and seek the same things according to their level of understanding.  These things are peace, harmony, abundance, joy-- all facets of unconditional love, the Divine essence of all that exists, ever present and always calling.

Often it is only an experience of some seeming disaster that brings this awareness to those still holding tightly to a sense of separation, choosing to live in white towers of self-righteousness and power, and who see themselves as separate and better than everyone around them.  When change comes for these people, rejoice not in their seeming "fall from grace" but in the realization that they too are  learning and evolving,  but doing it the "hard way".

Today's message is about love, a topic spoken of and discussed on all levels throughout the world.
A deep and truer understanding of the Divine nature of  Love is vital to spiritual  evolution.  Love is the activity of oneness and is the basis for everything that exists.  It is the glue that holds together all things and is interpreted and experienced according to the state of consciousness of the individual.

Example:   Love, the connecting energy between all things within the One Omnipresent Consciousness, when interpreted through un-evolved states of consciousness  will manifest outwardly as  war, rape, abuse, and all the experiences of duality and separation.  The One has never changed nor could it ever be less that what it is.  It is the state of the individual consciousness as well as consensus  consciousness that interprets and manifests how you see and experience the world.

A higher sense of love is beginning to awaken within the hearts of mankind.   Love, the connecting energy  between all living things is beginning to be experienced and acknowledged profoundly  more than  in  belief systems of the past.

Most of you have lifetimes  in which love was generally only thought of as something between a mother and child, or perhaps between friends or lovers and often with strings attached.  Love was more of a barter and almost never unconditional.  Often  there was little or no love in marriages which were simply contracts assuring convenience and safety for two countries, or families.  For those of poorer means, it often represented another pair of hands to do the work.

Women were more often than not, considered bargaining chips or simply chattels, a means to an end.  Love even in the human sense  rarely entered into the picture.  When romantic love did happen, it was frequently discouraged or even forbidden  if the union did not fulfill the concepts and beliefs of those in power, be it simple families or royalty.

There has been very little  understanding of the real nature of  Love except for the occasional mystic.  Concepts based in duality and separation have ruled the consciousness of mankind lifetime after lifetime, conditioning and interpreting love as well as everything else, in  very limited ways.

Love is beginning  to be recognized as much more than simple emotional  attachment. Over time those of a higher state of consciousness tried to reveal unconditional love through their words and actions,  but for the most part people were unable to embrace that level of consciousness.  Your inner work is birthing a new world consciousness  and mankind is rapidly attaining an ability to learn about and live Love as the universal and unconditional truth of being that it really is.

You have evolved and are now ready to accept and integrate the deeper truth of unconditional love as being all there is.  This must become an attained state of consciousness if you are to move into the higher dimensional energies emerging and available at this time  which means awareness and practice on your part.  You do not have to have fully attained this state of consciousness in order to move forward, but must have the honest intention to do so.  The higher dimensions are dimensions of Love, and cannot be entered still holding fast to a consciousness of separation with all its accompanying  judgement and criticism.


The necessity for unconditional love  occasionally  acts as a block for the spiritually ready individual unable to embrace the idea of having to love everyone.  When this happens it means there is a misunderstanding of what unconditional Love really is.   It is possible to have unconditional love even for those living at very dense levels of awareness  once it is  understood that unconditional love is not an acceptance of their actions nor a lowering  into their level of consciousness.

It is not  attraction to or acceptance of everyone and everything regardless of how much  havoc they may be causing and neither is it an emotional desire to be with the person or place or thing although that can be a part of it.  Unconditional Love  is recognition--the recognition of ever present Divinity within all things even when the object does not care a whit about it.


Unconditional love means every person or situation  you become aware of must be seen with new eyes,  which in turn trains the mind to interpret in new and higher ways.  Begin to visualize a diamond in the heart of every person you become aware of as you go about your day.  Make this diamond a  symbol  representing the person's innate Divinity even if you do not like or respect what they represent.  The news, the world, the things you hear about  or see on TV are all opportunities to practice  blessing the world through recognition.

There must come at some point, a realization of oneness in world consciousness if it is to evolve beyond duality and separation and you are Light workers making it happen.

Try and let go of judgement and criticism of appearances for fear, worry, and concern serves to give them a power they do not have and cements whatever illusory energy they do have.  This can be difficult for those of you used to actively fighting the negative issues of the world in your quest for change.

You are being asked now to take the next step which is a  letting go of resistance, and a moving into unconditional love.  Your mind will accept this after awhile although at first it will rebel, for it has been trained in other ways.


This never means you cannot sign a petition, or take some action you are being guided to take.  It does  mean  that your first action now becomes one of unconditional love which may or may not lead you to a next action.

Attaining this state of consciousness and the next step of your journey and you are ready.

Greetings dear ones.  We observe that all is proceeding according to schedule, and most of you are beginning to recognize and experience the new and powerful energies of transformation.  Everyone's experience may well be different so try not to compare yours with what you may read or hear regarding the spiritual experiences of another.   Every person is unique by virtue of having lived different experiences through many lifetimes, finally arriving at their present state of consciousness.  You are  clearing the residue of hundreds of lifetimes lived in ignorance and no one person is exactly the same as another.

Most of you are now beginning to see and experience the world from a higher dimensional viewpoint which is manifesting as new ways, forms, and ideas of those things you are already familiar with.  The reality (Divine Idea) of everything in the outer world  always remains present and infinite in Divine Consciousness.  Spiritual evolution allows you to interpret and manifest these Divine ideas on a new and higher levels.

An example could be how the three dimensional world views death.   Death means complete obliteration to those believing there is nothing beyond one lifetime, except the slight chance of a heaven or hell based on how "good" a person was.   Thus, death to them is  something to be feared and avoided at all costs.  Those of a more illumined consciousness, who realize there could never be real death, are somewhat free of this intense fear and anxiety.   There is always some sadness with parting, but none of the terror and resistance born of ignorance.   Death is recognized to be a transition, not an ending--a  part of everyone's evolutionary  journey on earth, often welcomed by those tired and ready to go home. The Divine Idea here is Omnipresence.  You, as expressions of the Divine,  always will be...place is of no importance.

Soon you begin to see changes within  governments and law for the consciousness of mankind is rapidly awakening and ready to reclaim personal powers ignorantly surrendered over lifetimes.  More and more people are beginning to realize that they are not powerless  and this change of the inner will in turn manifest as changes in the outer.

The continuous clearing of old energy is  not meant to go on forever, and  many of you are now moving into a time of creating.  Because you are beginning to accept that you are indeed powerful spiritual beings, your thoughts and words  are manifesting  more quickly.  As you now move into a more evolved state of consciousness, most of the old programming from  lifetimes of ignorance that served to block your ability to consciously manifest, has dissolved or is in the process of dissolving.

For thousands of years the mankind has for the most part been unaware of his true nature and thus a world consciousness of individual powerlessness  became the consensus belief.  Those in positions of dominance have always worked to convince the majority they had no power of their own and so must look to others (them) for their good.  The result has been a world of people ignorant of who they really were, believing, obeying, and  following the demands of the rich and powerful without question.  This will end when a majority awakens, serving to lift and open world consciousness to new and higher levels of awareness.

Arise dear ones, and remember who you are.  It is time to claim your Divine sonship,  being ever mindful of how you speak and think, for you are creators.

Note how some individuals who are experts in certain areas of medicine or science through intense study, research, and belief,  often discover these very issues manifesting in their own families or themselves.   Every billboard, ad, or announcement regarding some campaign to fight a disease or condition serves to  materialize  it more firmly in the belief system of all who read it and thus manifest more of it.

When an individual is enmeshed in the human belief system of duality and separation, their lives  reflect some good and some bad.   Painful life lessons are often a part of a soul's pre-birth contract in order to resolve some karmic issue or learn some needed lesson, but three dimensional experiences can also be  impersonal, simply  reflecting a state of consciousness that accepts two powers.

Man was not meant to slip so deeply into three dimensional density, but you are completing the hard work necessary in order to move beyond it and are doing a fine job.  A  job that required lifetimes of often painful experiences necessary for building courage and strength.  Remember this fact whenever you are tempted to believe yourself to be a failure in any way, for in reality there can be no failure, only learning experiences

It is difficult to move beyond  popular thought until truth becomes an attained state of consciousness at which point much of the three dimensional belief system begins to simply lose its relevance and power over you.   You see and hear the same things as everyone else, but you no longer react in the same way.

Those of you reading these messages are quickly moving into a consciousness able to discern  truth where the world sees discord and are beginning to instinctively recognize what is hype and promotion for the financial benefit of a few.  You are learning see the world with new eyes and  live out from  a higher awareness of what is going on around you.  Because of this, you are helping to shift world consciousness to new levels.

Look back a few years to your lives at an earlier time dear ones, and you will quickly realize how far you have come in very a short time.  Just a few years ago, you might of jumped on the band wagon of many popular three dimensional  beliefs,  those same beliefs you clearly see differently today.

You are awakening, and remembering who you are--coming to realize that you no longer  need to seek, beg, or plead from some  "pie in the sky" god for the good you have sought in vain for lifetime after lifetime.

Rejoice, for you have finally found IT where IT has always been-- within-- patiently and  lovingly awaiting your recognition.


I think most of you know from the announcement of this series of seminars and workshops during the summer, they're entitled 'The Value of Psychotic Experience.' And many people who are interested in an entirely new approach to problems of what have hitherto been called mental health are participating in these seminars and workshops, and doing something which is extremely dangerous and in a way revolutionary. For this reason:

We are living in a world where deviant opinions about religion are no longer dangerous, because no one takes religion seriously, and therefore you can be like Bishop Pike and question the doctrine of the Holy Trinity, the reality of the virgin birth, and the physical ressurection of Jesus, and still remain a bishop in good standing. But what you can't get away with today, or at least you have great difficulty in getting away with is psychiatric heresy. Because psychiatry is taken seriously, and indeed, I would like to draw a parallel between today and the Middle Ages in the respect of this whole question.

When we go back to the days of the Spanish Inquisition, we must remember that the professor of theology at the University of Seville has the same kind of social prestige and intellectual standing that today would be enjoyed by the professor of pathology at Stanford Medical School. And you must bear in mind that this theologan, like the professor of pathology today, is a man of good will. Intensely interested in human welfare. He didn't merely opine; that professor of theology KNEW that anybody who had heretical religious views would suffer everlasting agony of the most apalling kind. And some of you should read the imaginative descriptions of the sufferings of Hell, written not only in the Middle Ages, but in quite recent times by men of intense intellectual acumen. And therefore out of real merciful motivation, the Inquisitors thought that it was the best thing they could do to torture heresy out of those who held it. Worse still, heresy was infectious, and would contaminate other people and put them in this immortal danger. And so with the best motivations imaginable, the used the thumbscrew, the rack, the iron maiden, the leaded cat-of-nine-tails, and finally the stake to get these people to come to their senses, because nothing else seemed to be available.

Today, serious heresy, and rather peculiarly in the United States, is a deviant state of consciousness. Not so much deviant opinions as having a kind of experience which is different from 'regular' experience. And as Ronald Lang, who is going to participate in this series, has so well pointed out, we are taught what experiences are permissable in the same way we are taught what gestures, what manners, what behavior is permissable and socially acceptable. And therefore, if a person has so-called 'strange' experiences, and endeavors to communicate these experiences--because naturally one talks about what one feels--and endeavors to communicate these experiences to other people, he is looked at in a very odd way and asked 'are you feeling all right?' Because people feel distinctly uncomfortable when the realize they are in the presence of someone who is experiencing the world in a rather different way from themselves. They call in question as to whether this person is indeed human. They look like a human being, but because the state of experience is so different, you wonder whether they really are. And you get the kind of--the same kind of queasy feeling inside as you would get if, for the sake of example, you were to encounter a very beautiful girl, very formally dressed, and you were introduced, and in order to shake hands, she removed her glove, and you found in your hand the claw of a large bird. That would be spooky, wouldn't it?

Or let's suppose that you were looking at a rose. And you looked down in the middle where the petals are closed, and you suddenly saw them open like lips, and the rose addressed you and said 'good morning.' You would feel something uncanny was going on. And in rather the same way, in an every day kind of circumstance, when you are sitting in a bar drinking, and you find you have a drunk next to you. And he tells you, 'undistinguishable drunken ranting' and you sort of move your stool a little ways away from this man, because he's become in some way what we mean by nonhuman. Now, we understand the drunk; we know what's the matter with him, and it'll wear off. But when quite unaccountably, a person gives representation that he's suddenly got the feeling that he's living in backwards time, or that everybody seems to be separated from him by a huge sheet of glass. Or that he's suddenly seeing everything in unbelievably detailed moving colors. We say, 'well that's not normal. Therefore there must be something wrong with you.' And the fact that we have such an enormous percentage of the population of this country in mental institutions is a thing we may have to look at from a very different point of view, not that there may be a high incidence of mental sickness, but that there may be a high incidence of intolerance of variations of consciousness.

Now in Arabic countries, where the Islamic religion prevails, a person whom we would define as mentally deranged is regarded with a certain respect. The village idiot is looked upon with reverence because it is said his soul is not with his body, it is with Allah. And because his soul is with Allah, you must respect this body and care for it, not as something that is to be sort of swept away and put out of sight, but as something of a reminder that a man can still be living on Earth while his soul is in Heaven. Very diffent point of view. Also in India, there is a certain difference in attitude to people who would be called nuts, because there is a poem--an ancient poem of the Hindus-- which says 'sometimes naked, sometimes mad, now's a scholar, now's a fool, thus they appear on Earth as free men.'

But you see, we in our attitude to this sort of behavior, which is essentially in its first inception harmless, these people are talking what we regard to be nonsense. And to be experienced in nonsense. We feel threatened by that, because we are not secure in ourselves. A very secure person can adapt himself with amazing speed to different kinds of communciation. In foreign countries, for example, where you don't speak the language of the people you are staying with, if you don't feel ashamed of this, you can set up an enormous degree of communication with other people through gesture and even something most surprising, people can communicate with each other by simply talking. You can get a lot across to people by talking intelligent nonsense, by, as it were, imitating a foreign language; speaking like it sounds. You can communicate feeligns, emotions, like and dislike of this, that and the other; very simply. But if you are rigid and are not willing to do this type of playing, then you feel threatened by anybody who communicates with you in a funny way. And so this rigidity sets up a kind of vicious circle. The minute, in other words, someone makes an unusual communciation to you about an unusual state of consciousness, and you back off, the individual wonders 'is there something wrong with me? I don't seem to be understood by anyone.' Or he may wonder 'what's going on? Has everybody else suddenly gone crazy?' And then if he feels that he gets frightened, and to the degree that he gets more frightened, he gets more defensive, and eventually land up with being catatonic, which is a person who simply doesn't move. And so then what we do is we whiffle him off to an institution, where he is captured by the inquisitors. This is a very special priesthood. And they have all the special marks that priesthoods have always had. They have a special vestment. Like the Catholic priest at mass wears a *, the mental doctor, like every physician, wears a long white coat, and may carry something that corresponds, shall we say, so a stole, which is a stethescope around his neck. He will then, under his authority, which is often in total defience of every conceivable civil liberty, will incarcerate this incomprehensible person, and as Lang has pointed out, he undergoes a ritual of dehumanization. And he's put away. And because the hospitals are so crowded with people of this kind, he's going to get very little attention. And it's very difficult to know, when you get attention, how to work with it.

You get into this Kafka-esque situation which you get, say, in the state of California, if you are sent to such an institute as Vacaville prison, which is as you drive on the highway from San Francisco to Sacramento, you will encounter Vacaville about halfway between. You will see a great sign which will say 'California State Medical Facility.' The state of California is famous for circumlocution. When you go underneath a low bridge, instead of saying 'Low Bridge,' it says 'Impaired Vertical Clearance.' Or when you're going to cross a toll bridge, instead of saying, plainly, 'Toll Bridge,' it says 'Entering Vehicular Crossing.' And when it should be saying, plainly, 'Prison,' it says either 'California State Medical Facility,' or 'California State Correctional Facility,' as it does as Soledad. Now Vacaville is a place where people get sent on what they call a one- to ten-year sentence. And there is a supervising psychiatric medical sort of social service staff there, who examine the inmates once in a while because they have such a large number. It's a maximum security prison, much more ringed around with defences than even San Quentin. I went there to lecture to the inmates some time ago. They wanted someone to talk to them about meditation and yoga, and one of the inmates took me aside--a very clean-cut all-American boy. And he had been put in there probably for smoking pot; I'm not absolutely sure in my memory what the offense was. He said 'You know, I am very puzzled about this place. I really want to go straight and get out and get a job and live like an ordinary person.' He said 'I think they don't know how to go about it. I've just been refused release; I went up before the committee; I talked to them. But I don't know what the rules of the game are. And incidentally, the members of the committee don't either.'

So we have these situation, you see, of confusion. So that when a person goes into a mental hospital and feels first of all perhaps that he should try to sort himself out and talk reasonably with the physician. There is introduced into the communications system between them a fundamental element of fear and mistrust. Because I could talk to any individual if I were malicious and interpret every sane remark you make as something deeply sinister; that would simply exhibit my own paranoia. And the psychiatrist can very easily get paranoid, because the system he is asked to represent, officially is paranoid. I talked with a psychiatrist in England just a few weeks ago. One of the most charming women I've come across, an older woman, very intelligent, quite beautiful, very reasonable. And she was discussing with me the problem of the LSD psychosis. I asked her what sort of treatments they were using, and all sorts of questions about that, and she appeared at first to be a little on the defensive about it. We got onto the subject of the experience of what is officially called 'depersonalization,' where you feel that you and your experience--your sensory experience--that is to say all that you do experience: the people, the things, the animals, the buildings around you--that it's all one. I said 'do you call this a hallucination? After all,' I said, 'it fits the facts of science, of biophysics, of ecology, of biology, and much better than our ordinary normal experience fits it.' She said 'that's not my problem.' She said 'that may be true, but I am employed by a society which feels that it ought to maintain a certain average kind of normal experience, and my job is to restore people to what society considers normal consciousness. I have no alternative but to leave it at that.'

So, then. When someone is introduced into this situation, and it's very difficult to get attention, you feel terrified. The mental hospital, often in its very architecture, suggests some of the great visions of madness, of-- You know that feeling of-- The corridors of the mind. If you got lost in a maze and you couldn't get back. You're not quite sure who you are, or whether your father and mother are your real father and mother, or whether in the next ten minutes you're still going to remember how to speak English. You feel very lost. And the mental hospital in its architecture and everything represents that situation. Endless corridors, all the same. Which one are you in? Where are you? Will you ever get out? And it goes on monotonously, day after day after day after day after day. And someone who talks to you occasionally doesn't have a straight look in his eye. He doesn't see you as quite human. He looks at you as if you're weird. What are you to do? The best thing to do is get violent, if you really want to get out. Well then they say that's proof that you're crazy. And then as you get more violent, they put you off by yourself, and the only alternative you have, the only way of expressing yourself is to throw shit at the walls. Then they say, 'well, that's conclusive. The person isn't human.'

Well, the question has been raised a great deal in the last few days on the television, as to whether this is a sick society. And I have listened to a perfectly beautiful pschoanalyst with a thick German accent. Oh, marvelous things! 'Eet ees quite obvious dat society is quite hopeless, you zee.' And I have listened to four red-blooded Americans saying 'most people in this society are good people, and it's a GOOD society, but we have a very sick minority.'

Now, what I want to do in--certainly this first part of the seminar--is to call in question, very fundamentally, all of our basic ideas about what is sickness, what is health, what is sanity, what is insanity. Because I think we have to begin from this position of humility; that we really don't know. It's reported that shortly before he died, Robert Oppenheimer, looking at the picture of technology, especially nuclear technology, said 'I'm afraid it's perfectly obvious that the world is going to hell.' It's going to destroy itself, it's on collision course. The only way in which it might not go to hell is that we do not try to prevent it from doing so. Think that one over. Because it can well be argued that the major troublemakers in the world today are those people with good intentions. Like the professor of theology, University of Seville, professor of psychiatry at wherever you will. The idea that we know who is sick, who is wrong. Now, we are living in a political situation right now where a most fantastic thing is occuring. Everybody knows what they're against; nobody knows what they're for. Because nobody is thinking in terms anymore of what would be a great style of life. The reason we have poverty is that we have no imagination. There's no earthly reason; there's no physical, technical reason for there being any poverty at all anywhere. But you see, there are a great many people accumulating what they think is vast wealth, but it's only money. They don't know how to use it, they don't know how to enjoy it, because they have no imagination.

I'm announcing not the date, but the intention of conducting a seminar for extremely rich people entitled 'Are You Rich and Miserable?' because you very probably are. Some aren't, but most are. Now the thing is that we are living in this situation where everybody knows what they're against, even if they say 'I'm against the war in Vietnam. I am against discrimination against colored people, or against any different race than the discolored race,' and so on. Yeah, so what? But it's not enough to feel like that; that's nothing. You must have some completely concrete vision of what you would like, and therefore I'm making a serious proposition that everybody who goes into college should as an entrance examination have the task of writing an essay on his idea of heaven, in which he is asked to be absolutely specific. He is not allowed, for example, to say 'I would like to have a very beautiful girl to live with.' What do you mean by a beautiful girl? Exactly how, and in what way? Specifically. You know, down to the last wiggle of the hips, and down to every kind of expression of character and socialbility and her interests and all. Be specific! And about everything like that. 'I would like a beautiful house to live in.' Just what exactly do you mean by a beautiful house? Well you've suddenly got to study architecture. You see, and finally, this preliminary essay on 'My Idea of Heaven' turns into his doctoral dissertation. So in a situation where we all know what we're against, and we don't know what we're for, then we know WHO we're against. We're defining all sorts of people as nonhuman. We say they're totally irrational. They're totally stupid. People will say, 'oh, those niggers, they're completely uneducated, they'll never learn a thing, there's nothing you can do about it, they're hopeless, get rid of them.' The Birchers are saying the same sort of thing. Other people, the liberals are saying the same thing about the Birchers. 'They're stupid, get rid of them.' The only result, then, the only thing anybody can think of in this sort of situation is 'get your gun.' And this sets up a vicious circle, because everybody else gets his gun. And the point from which we have to begin, then, is that we don't know who is healthy and who is sick. Who is right and who is wrong. And furthermore, we have to start, I think, from the assumption that because we don't know, there isn't anything we can do about it.

There's a Turkish proverb that I like to quote: 'He who sleeps on the floor cannot fall out of bed.' Therefore, we should make it a beginning--a basic assumption about life that even supposing you could improve society, and you could improve yourself, you were never sure that the direction you moved it in would be an improvement.

A Chinese story, kind of a Taoistic story about a farmer. One day, his horse ran away, and all the neighbors gathered in the evening and said 'that's too bad.' He said 'maybe.' Next day, the horse came back and brought with it seven wild horses. 'Wow!' they said, 'Aren't you lucky!' He said 'maybe.' He next day, his son grappled with one of these wild horses and tried to break it in, and he got thrown and broke his leg. And all the neighbors said 'oh, that's too bad that your son broke his leg.' He said, 'maybe.' The next day, the conscription officers came around, gathering young men for the army, and they rejected his son because he had a broken leg. And the visitors all came around and said 'Isn't that great! Your son got out.' He said, 'maybe.'

You see, you never really know in which direction progress lies. And this is today a fantastic problem for geneticists. They genetecists, you know, because they think they are within some degree of controlling the DNA and RNA code, believe that it is really possible perhaps to breed the kind of human beings that we ought to have. And they say 'hooray!' But they think one moment and they think 'ah-ah-ah-ah-ah, but what kind of human being?' So they're very worried. And just a little while ago, a national committee of graduate students and geneticists had a meeting at the University of California and the asked a group of psychologists, theologans and philosophers to come and reason with them about this and give them some insight. And I was included. That means that they are REALLY desperate. So I said 'I'll tell you what, the only thing you can do is to be quite sure that you keep a vast variety of different kinds of human beings, because you never know what's going to happen next. And therefore we need an enormous, shall I say, varied battery of different kinds of human intelligence and resources and abilities. So that there will always be some kind of person available for any emergency that might turn up. So you see, there's a total fallacy in the idea of preaching to people. This is why I abandoned the ministries, I've often said, not because the church didn't practice what it preached, but because it preached. Because you cannot tell people what sort of pattern of life they ought to have, because if they followed your advice, you might have a breed of monsters. Look at it from the point of view that the human race is a breed of monsters.

I was thinking about it this afternoon, driving down from Monterey to here, and looking at the freeways, and all these little cars going along them, and I was wondering if I considered that the planet was a physical body like my own, whether I might not feel that this was some sort of an invasion of weird bacteria that were eating me up. Whether it may be that the birds and the bees and the flowers--animals in general--were a kind of healthy bacteria. You know, bees and birds sort of wander about, generally mix in with the forest and the fields and carry on a rather disorganized but very interesting pattern of life, whereas human beings cut straight lines across everything. Railways. They cover themselves with junk. A bird may have a little nest, but it doesn't have to surround itself with automobiles and books and buildings and phonograph records and universities and clutter up the whole landscape with a lot of bric- a-brac. Human beings pride themselves on this. 'You see, this is culture!' This is a great achievement. Build a building, you know? It's all you can get money for. You can't get money for professors, but you can get them for new buildings. So we cover the Earth with clutter. And so the Earth might feel as if we might feel if suddenly we got a disease which instead of leaving us soft-skinned, covered us with crystalline scabs, and this would be proliferating all over the place--a pox! Are we a pox on the planet? Don't be too sure that we're not. Consider simply this:

There is a good argument--keep in mind I'm saying these things to provoke you, to make you a little insane by being in doubt of all the assumptions which you think are firmly true. It is quite possible, you see, that the whole enterprise of man to control events on the Earth by his conscious intelligence, by his language, by his mathematics, and by his science is a disaster. We say look at his successes, look how much disease we have cured. Look how much hunger has been abolished. Look how we have raised the standard of living. Yeah. But in how long a time?

Well, even if we say this started with the dawn of known history, it's a tiny little fragment of time, as compared with the time in which the human species has existed. And if it's the Industrial Revolution, it narrows down to the teenieest, weeniest little bit of time. How do we know this is progress? How do we know that this is a success? It may be a disaster of unimaginable proportions. It may be. But the truth is, we don't know. Of course, it could be possible, that every star in the heavens was once a planet, and that planet developed intelligent life, which in due course discovered the secrets of atomic energy, blew itself up into a chain reaction, and as it exploded throughout various masses which began in due course to spin around it, became planets, and after a while developed intelligent life. After millions of years, as the central star started to cool off, they blew themselves up in turn, and that's the way the thing goes on. That's of course the theory of the Hindus. Not literally, but they do have the theory, you see, that life, every manifestation of the universe, begins in a glorious way, and then it deteriorates. But then everything does. Isn't everything always falling apart and getting older and fading out? Why shouldn't various species, why shouldn't various planets, why shouldn't various universes be going through the same course?

You see, that's a totally upside-down view in respect to our common sense. We think everything ought to be growing and improving and getting better and better and better and better and better and better. Look at it the other way around, it might be quite different. Then there's another thought. We know that the truth, the way theing are is an interaction, or better, transaction between the physical world and our sense organs, and that therefore, what we know as existence is a relationship. It is the way certain what we will call for the moment electrical vibrations make impression upon sense organs of a certain structure. Now that's a limited way of talking about it, but it will do for the moment. Therefore, according to the structure of the sense organs, the vibrations will appear of be manifested in different ways. In other words, I can move my finger like this, and if it happens to pluck the string of a violin, it will go 'plunk!' In which case my finger and its motion will be manifested as 'plunk!' But if it should so happen that I should strike the string of a bass fiddle, it will go, 'bunggggg' and so the finger will be 'bunggggg' But if the same motion should strike the skin of a drum, 'thunk,' so the finger will be 'thunk,' now what is that motion truly? It's whatever it interacts with. If it goes across somebody elses skin, it'll be something I can't make a noise about. It'd be a feeling. If it does it in front of an eye, it will be a motion.

So depending on the structure of shall we say for the moment the receptor organs, so will the reality be. Now behind the receptor organs--the senses are not at all simple--behind the senses they are inseperable from an extraordinarily complex neurological structure. And not only that, but a system of cultural standards as to what events are to be noticed and what events are to be ignored. What is important for a certain reason such as survival, and what is unimportant, and therefore we further modify the selectivity of the sense organs and of the nervous system as a whole with a selective system of what is culturally accepted as real or unreal, important or unimportant.

So we end up you see, with the possibility that so complex a selective system may have a great many variations, and that people that we call crazy have a different system of evaluation. They may have a difference of neural structure, as would obviously be the case if there were lesions caused by syphillis, or by brain tumors. But what about something not quite at that level, but at the level of the selectivities they imply which would correspond to what I call social conditioning. Now we know the proverb that genius is to madness 'cross the line. And how do we know whether a certain modification in the structure of the whole sensory system is a sickness or whether it is a growning edge--some kind of improvement in the human being. Well we have certain very, very rough standards which we apply to this, but we can never be quite sure because what we call sanity is mob rule. Sanity is simply the vote or organisms that recognize themselves to be humans and they get together and say 'Well, the way we see it is the way it is.' And you will remember in Kipling's story in the 'Jungle Book' called 'Cause Hunting' how the monkeys, the bandiloot are laughed at because every once in a while they get together in a meeting and shout 'We all say so, so it must be true!'

But herein you see lie the deepest political problems. How is the majority to tolerate, to absorb, to evaluate a minority? It's an academic problem. We have standards as to who are sound scholars, reliable scientists--we give them a PhD. And they all get together and uphold the standards. But then they suddenly realize that they're getting a little narrow and that things aren't going on, and suddenly somebody says one day 'Old so-and-so, who we always thought was quite mad and very, very unorthodox has suddenly come up with an idea that we've all got to think about.' So one would say that every university faculty has to include in its membership at least five percent screwballs. Every culture has to tolerate within its domain a lot of weird people. Now there's no possibility that everybody in the United States is going to be a hippie. But the fact that a large number of young people are hippies should be a matter of congratulations, even if you don't want to live that way yourself. Not to mention the various racial variations that we have among us: negroes, Mexicans, Chinese, Japanese, and so forth. All this is exceedingly important, because as I said to the geneticists, this preserves variety. And a culture which is insecure in itself--I'm getting back to a sort of starting point--cannot tolerate this.

Now in England as I remember it, they were much more secure. When I was a boy, 15 years old, in a very orthodox Church of England school, I announced that I was a Buddhist. Nobody turned a hair. Here, if somebody announces that he's something strange, they have to go before the principal, and there's a big problem, and the FBI is brought in, and this, that, and the other. But they said 'Jolly wot, the man's a buhddist!' And positively encouraged me in my deviant interest, and gave me the first prize in the divinity class. Now exactly the same kind of relaxed attitude is necessary here.

Let's ask a few questions that don't need answers. Is the American family such a drag that a few kids living in free-love communes are a fundamental threat to it and will pervert all our nice boys and girls to live that way? Are American universities so boring that a few students who drop out and form their own univerisities are a threat to the total system and will pervert all the other nice children in there? Are a few kids going around in elegant beards and long hair going to turn all our boys into weirdos?

Say, I had a funny experience. When I was in England I attended services at Westminster Abbey. I took my wife there because I really wanted to her to see this thing, because it's the heart and soul of British establishment. The dean of Westminster is like the Dali Lama almost. They had this very elegant victorian service--beautiful vestments, choir and everything--and as they were coming out in procession, the choir came first, which were little boys with proper haircuts and surplices.?A and red caps on, there were a number of older boys wearing surplices--the special kind of surplice that is worn by its color of a British public school. Y'know, the public schools are not public schools, they're very private schools, very exclusive schools, and the school of Westminster is one of the top, like Eaton or Harrow. Suddenly, these boys in surplices turn up, with these enormous Beatles haircuts whishing all over the place. I couldn't believe my eyes, because I used to be a King's Scholar, and in our day, we were very proper and all wore mortarboards over short hair. And then behind these surpliced boys, there were the commoners of the school, who were not King's Scholars and therefore didn't wear surplices, but wore striped black pants, black coats, wing collars and black ties. And we always used to walk in procession as we came out, like this, but here were these boys with a similar hairdo coming out. .apparent visual joke here that I guess you'd have to be there to get, but very funny, it would seemA My god, what's going on? This is Westminster Abbey! But the dean of Westminster doesn't turn a hair, he takes it all in stride. He's perfectly secure. He knows he is who he is. He knows it's ordained by Jesus Christ and everything else and it's all right, and if you want to come in and do something different, it's all right.

And that is the attitude we have to have in regard to everything deviant, psychotic, and weird. Because we are not sure what's right, who's sane, which end is up. In a relativistic universe, you don't cling to anything, you learn to swim. And you know what swimming is. It's a kind of relaxed attitude to the water, in which you don't keep yourself afloat by holding the water, but by a certain giving to it, and it's just the same with relationships to people all around.

Zen has attracted attention over the years, since 1927, when Dr. Daisetz Suzuki first published his essays in Zen Buddhism, and he had a very odd fascination with Westerners. To begin with, very many intelligent Western people were becoming--had already become, dissatisfied with the standard brands of their own religions, and this dissatisfaction had of course begun to take place quite seriously towards the close of the 19th century, and at that time, we began to be exposed to Oriental philosophy or religion, whatever you want to call it, because the great scholars like Maxmilla, Riese DavidsÙ and so on were translating the texts of Buddhism and Hinduism. And already in 1848, the Jesuit had translated the Tao Te Ching, the Taoist texts from China into French, and translations into English then became available.

What happened was rather curious, because we were receiving Oriental tradition on a far higher level of sophistication than we were receiving the Christian or the Jewish traditions. The average person was exposed to an extremely low level of Christianity, and therefore immediately compared this to the highest level of Hinduism and Buddhism, much to the detriment of the former, because you could no go into your parish church, even if you lived in a very good neighborhood, even in a university neighborhood and find Meister Eckhart for sale on the entrance table. Nor even would you find some Thomas Aquinas. You found wretched little tracts. And so the comparison was overwheming. It wasn't really fair for the Christian tradition, but that's what happened. Then something else happened, which was that in the year 1875, a strange Russian woman by the name of H.P. Blavatsky founded the Theosophical Society, whose doctrines and literature were a fantastic hodgepodge of the Western occult tradition, a great deal of Hindu and Buddhist lore, a smattering of Tibetan Buddhism and Chinese Buddhism, but it all was very romantic, and presuppose that the adepts of Hinduism, Buddhism, Taoism and so forth were very high order initiates. Supermen. The masters. And they had their secret lodges in the vastness of the Himalayas, and even such places as the Andes, and they were rather inaccessable, because they were in possession of the most dangerous secrets of occult power. But they every now and then felt safe to send an emissary out into the world to teach the ancient doctrine of liberation to mankind.

And so the West, through this, got an extremely glamorous impression of what Oriental wisdom might be. And I remember the media in which I found myself involved in England when Dr. Suzuki first came around was essentially theosophical in its oreintation. They expected Dr Suzuki to be a master in that sense, in that theosophical sense, or if not quite that, then at least in touch with those who were. And the whole idea of the Zen master, the way the whole word 'master' got attached to a teacher of Zen carried with it this theosophical flavor, and also a certain flavor which the Theosophical Society picked up from India where the great guru is somebody enormously revered. People would travel for hundreds of miles just to look at him, to have what is called Tao-Shan, or 'view' of someone like Shri Arabindo or Shri Ramana Maharshi or the current Maharshi, or it would be Shri Rama Krishna or Amandani, who's a lady guru, and there's always the feeling that these people have tremendous powers. And so this is what was expected by many people from Zen masters. But the interesting thing about Zen masters is they're not like that. They're very human. And they wouldn't deign to perform a miracle. I got to know about Zen masters through my first wife, because when whe was an adolescent about 14 years old, she went to Japan, and they lived close to the great monastary of Nonzengi where the master in charge was a very brilliant master by the name of Nonshinkan. He was an old man, and he was-- The man who is appointed to be the roshi or the teacher of Nonzengi of Kyoto was always considered to be just about tops of the whole bunch. We've had the present master, Shibayama Roshi visiting the United States recently. And he used to sit around with her and he'd get a catalog of all the famous sumo wrestlers, who were enormously fat. They have to eat, eat, eat, eat, eat, eat rice, because the whole art depends on their weight. But they're very handsome. And he used to thumb them through sitting next to this little girl and pick out husbands for her. And then he would have nose-picking contests with her. Y'know, they weren't exactly real, but they'd make sort of like picking their noses and flicking the snots at each other.

So you mustn't expect the Zen master to be like the Pope. They can come on very dignified when necessary, but there's always something about them which is fundamentally lacking in seriousness. Even though they may be well-endowed with sincerety. They're two quite different qualities. They are extraordinarily interesting people, as are their students, in the context of Japanese culture. Japanese culture is terribly uptight, because the Japanese are very emotional people, underneath. Tremendously passionate. But they have to hold that in, because they live in a crowded country, and space is the most valuable thing in Japan, especially living space, because 80% of the territory is uninhabitable. It's forested mountains, and you can't grow anything there, you can't make much of a city. So they're all crowded into 20% of the country. And so this feeling of being pressed in by other people is-- They try to handle it by exquisite politeness, and by orderly behavior by vary strong convention. But this makes the average Japanese man and woman kind of nervous. When a Japanese giggles, it's a sign not of being amused, but of being embarrassed. And you'll find all sorts of funny attitudes, such as people putting their hands over their mouths when they're eating, or to conceal a giggle.

And they're tremendously hung up on social indebtedness, whether it's a debt to the emperor, or whether it's a debt to your fathers and mothers, or whether it's a debt to someone in the family, or whether it's a debt to friends whom you visited and they entertained you. Well, you always take gifts with you when you go, but then that still embarrasses your friends to whom you take the gifts, because they have to consider the next time they go to visit you, they've got to take gifts of the same value. And you wouldn't believe what goes on.

So actually, what Zen is in Japan is a release from Japanese culture. It is gettign rid of the hang-ups, but doing it in such a way as not to embarrass the rest of society. So the Zen monks come on as if they're pretty stiff; when they walk out in the street, they almost look like soldiers. When they walk, they stride, they don't shuffle, like other Japanese do. They don't giggle, ever. They have no need to. Because the process of their discipline has liberated them from the social conventions. Only they are very tactful and don't rush out like, you know, a bunch of hippies or something and say 'Look, we're liberated!' They pretend they're the very pillars of society.

So they follow a tradition which is very ancient, which is that in every society, there is an inner group who doesn't believe in the fairy stories they've been told. He sees through. To whom everything becomes completely transparent. You see what games people are playing. And you don't despise them for that. You see, they're involved in that because of their whole conditioning. But you see through all those games. The game--the me game--that everybody is playing is of course the survival game. And we think-- We've got our minds rigged about this in such a way that we live in constant dread of sickness or of death or of loss of property or status. Well, so what? Supposing you do. Everybody's going to die someday. It's a little harder to take when you're 20 than when you're 50, but if you are entirely hung up on the idea that YOU are this particular expression of the universe and that only, you haven't been properly educated. If you were awake, you would understand that you were the whole universe, pretending, projecting itself at a point called here and now, in the form of the human organism. And you would understand that very clearly, not just as an idea, but as an actual vivid sensation, just the same way you know you're sitting in this room. And so the object of Zen, as of other ways of liberation--Taoism, Hinduism; you'll find it even in Christianity in the Eastern Orthodox Church; Islam--the object of these ways of liberation is to bring you to a vivid, perfectly clear, I would say even sensuous realization of your true identity as a temporary coming on and going off, coming on and going off, or vibration as waves, of what there is, and always is, of the famous E which equals MC squared. And you are that. You will be that, and always will be that--accept that. This whatever it is-- which, then no which, then which--it doesn't operate in time. Time is a more or less human illusion. We will discover this to be so in our experiments. You will discover that there is only now, and there never was anything but now and never will be anything but now, and now is eternity.

Now Zen is a little bit unlike the rest of Hinduism and Buddhism in that it's summed up in these four principles: It's a special transmission of the Buddhist enlightenment outside the scriptures. It does not depend on words or letters. It points directly to your own mind-heart and attains therefore Buddhahood directly. Buddhahood means the state of being awakened to the real nature of things. But you see, what IS the real nature of things? It obviously cannot be described. Just as if I were to ask what is the true position of the stars in the big dipper. Well, it depends from where you're looking. From one point in space, they would be completely different in position from another. So there is no true position of those stars. So in the same way, you cannot therefore describe their true position or their true nature. And yet on the other hand, when you look at them, and really don't try to figure it out, you see them as they are, and they are as they are from every point of view, wherever you look at them.

So there is no way of describing or putting you finger on what the Buddhists call reality or in Sanscrit, tathata, which means 'suchness' or 'thatness,' or sunyata, which means 'voidness,' in the sense that all conceptions of the world when absolutised are void. It doesn't mean that the world is, in our Western sense, nothing. It means that it's no thing. And a thing--as I think I explained last night--is a unit of thought. A think. So reality isn't a think. We cannot say what it is, but we can experience it. And that is of course the project of Zen.

Now, it does it by direct pointing. And this is what exciting people about Dr Suzuki's work when he first let people know about Zen in the Western world. It seemed to consist of an enormous assemblage of weird anecdotes. That these people instead of explaining had kind of a joke system, or kind of a riddle system. the basic secret of the Buddha system is simply this, and it's explained by a great Chinese Zen master, whose name was Hui-neng, who died in the year 713 AD. And he explained it in his sutra. He said, 'If anybody asks you about secular matters, answer them in terms of metaphysical matters. But if they ask you about things phusical, answer them in terms of things worldly.' So if you ask a Zen master what is the fundamental teaching of the Buddha, he answers immediately, 'Have you had breakfast?' 'Yes.' 'If so, go and wash your bowl.' Or such a thing as 'Since I came to you master, you have never given me any instruction.' 'How can you say that I've never given you any instruction? When you brought me tea, didn't I drink it? When you brought me rice, didn't I eat it? When you saluted me, didn't I return the salutation? How can you say that I haven't instructed you?' And the student said, 'Master, I don't understand.' And he said, 'If you want to understand, see into it directly, but when you begin to think about it, it is altogether missed.'

They have also in Zen monastaries a funny thing. It's a chin rest. If you spend a long time meditating, it's sometimes convenient to have something to rest your chin on, and it's called a Zen- bon. And so once a student asked the teacher, 'Why did Bodidharma--' who is supposed to have brought Zen, you know from India to China '--why did Bodidharma come to China?' And the master said 'Give me that Zen-bon.' And the student passed it to him and the master hit him with it.

A contrary kind of story. The master and one of his students were working, I think pruning trees. And suddenly the student said to the master, 'Will you let me have the knife?' And he handed it to him blade-first. He said 'Please let me have the other end.' And the master said 'What would you do with the other end?'

There was a group walking through the forest, and suddenly the master picked up a branch and handed it to one of his disciples and said 'Tell me, what is it?' Y'know, the master was still holding it. He said 'Tell me, what is it?' The disciple hesitated, and the master hit him with it. He passed it to another desciple. 'What is it?' The disciple said 'Let me have it so I can tell you.' So the master threw the branch at this other disciple, and he caught it and hit the master.

I was once talking with a Zen master, and in an idle sort of way we were discussing these stories, and he said, 'You know, I've often wondered, when water goes down a drain, does it go clockwise or anticlockwise?' 'Well, I said, it might do either.' He said 'NO! It goes this way!' -apparently something visual here,. So then he said 'Which came first, egg or hen?' So I said, -clucks like hen,. He said 'Yes, that's right.'

Now all these Zen jokes are much simpler in their meaning than you would ever imagine. They are so devestatingly simple that you don't see them. Everybody looks for something complicated. When I was once visited by a Chinese Zen man, I had my little daughter with me, and he said to her, 'You know, once upon a time, there was a man who kept a very small goose in a bottle. A gosling. And it began to grow larger and larger until he couldn't get it out of the bottle. Now, he didn't want to break the bottle, and he didn't want to hurt the goose, so what should he do?' And she said immediately, 'Just break the bottle.' He turned to me and he said 'You see, they always get it when they're under seven.'

So there's that side of Zen, and that side of Zen we would call, essentially, in technical language, sanzen. That means, really, to study Zen in the form of an interchange with the teacher. Sanzen in the monastaries these days is very formal. But these are all stories from Tan and Sung dynasty China, where the relationship of student and teacher was more informal than it has now become. The other side of Zen is za-zen, or the practice of meditation. And that involves-- You can actually practice za-zen in four ways, corresponding to what the Buddhists call the four dignitaries of man: walking, standing, sitting, and lying. Only sitting is the one most used. But you should not imagine that Zen mediation requires absolutely that it be done sitting. People get rather hung up on that, and I get annoyed with people who come back from Japan having studied Zen and brag about how long they sat and how much their legs hurt.

But za-zen is very fundamental to Zen, in one form or another. And it is the art of letting your mind become still. That doesn't mean that it becomes blank. That doesn't mean that you have no what we would call sensory input. It mean simply that you learn how to breath properly. That's very important. And that you stop talking to yourself. The interminable chatter inside your skull comes to rest. So what happens is this-- I should add that there are various schools of Zen, with different methods and different approaches, and my approach to it is again somewhat different from other peoples, but buddhas have always have this kind of elasticity. But what normally happens is this:

You have some difficulty in being accepeted by a teacher, because Buddhism is not on a missionary basis. They don't send out ads and invitations saying 'Come to our jolly church,' you know. They wouldn't dream of doing that. Because it's up to you to seek it out. They're never going to shove it down your throat. So it is difficult to get into a Zen school. It isn't really a monastary as we have monastaries, where the monks take life vows of poverty, chastity and obedience. It's more like a theological seminary, and the monk, or seminarist, as he might more accurately be called, stays there for a number of years, until he feels he's got the thing that he went for. The teacher, the master, is usually unmarried, but that doesn't prevent him from having girlfriends. They are not uptight about sex in Zen, as they are in other forms of Buddhism. They're very-- The whole atmosphere of the monastary is very fascinating. Everybody is sort of alive. They don't dither around. They're all working. But they're very open. In some kinds of Buddhism, they have conniptions if you try to photograph something. 'This is too sacred to be photographed,' sort of attitude. In Zen, they say 'Help yourself! Photograph! Anything! Go on, take picture!' So, completely open.

So then, they have these sesshins. You must distinguish between 'session,' English, and 'sesshin,' Japanese. 'Sesshin' means a long, long period of meditation practice, over say, a whole week. But especially early in the morning, and at certain times of day, they all meet and they sit cross-legged on their mats in meditation. In one set, they meditate on what is called a koan, and that means a 'case,' in the sense of a case in law establishing a precedent. And it's one of these stories. When the great master Joshu, who lived in the Tung dynasty, was asked, 'Does a dog have buddha nature?' he replied 'mu,' which means no. Everybody knows that dogs have buddha nature. So why did the great master say 'mu'? That's a koan. Or Hakuin invented a koan as a proverb in Chinese: One hand cannot make a clap. So the koan is 'What is the sound of one hand?' Of course, it's differently said in Japanese than it is in English. But, you see, it sounds like a very, very complicated problem, and so these students take this problem back for meditation, and they-- First of all, the average person would start trying to arrive at an intellectual answer. And if he takes that back to the teacher, the teacher simply rejects it out of hand, time after time after time.

I had a friend who had this koan, and he was an American. And one day he was going to the teacher for sanzen, and he saw a bullfrog. They have many bullfrogs in Japan, about so big, sitting in the garden, and they're very tame. So he swooped up this bullfrog and dropped it in the sleeve of his kimono. And when he got to the master, he produced the bullfrog as the answer to the koan. The master shook his head and said 'Uh-uh, too intellectual.' So people get desperate about these things, and they go to all sorts of lengths to try and answer them, because they don't realize how simple the answer is. That's what's always overlooked. If you were to answer that koan in English, it gives it to you as it's stated. It says 'WHAT is the sound of one hand?' .Watts finds this very funny, but nobody else does, It's very difficult for people to become that simple. And you can become that simple only through meditation where you stop all the words and you see all the things perfectly directly. And so accomplished Zen people are very, very direct. Their life is completely simplified, because they know perfectly well--and if you look, and see youself--that there is only this present moment. No past. No future.

So what's your problem? You know, you could ask this of anyone. Well, you could say 'I've got all sorts of problems and responsibilites' and so on. All right. Don't other people have some share in this? You see, we are always being spiritually conceited in thinking we have to take care of everybody else, and that can sometimes do people a peculiar disservice, because they get into the idea that everybody should take care of them. And so we go around ingratiating ourselves by making all sorts of promises about which we feel enthusiastic at the time, but the enthusiasm wears off and then we don't keep them and then people get annoyed. And we go about telling people how much we like them when we don't. And all sorts of things of that kind by not being direct, you see. This is the whole idea of Zen, is directness. By not being direct, we create a great deal of trouble. However, the primary concern of Zen is not so much with interpersonal relations, as it is with man's relation with nature. In view of life and death, where are you? They have an incscription that hangs up in Zen monastaries, which says 'Birth and death is a serious event. Time waits for no one.' Which is sort of equivalent to the Christian 'Work out your salvation with diligence.' Or with fear and trembling.

So it begins in a clarification of our relationship with existence. With being. And therefore it lies in a more, I would say, primary or kindergarden level than the encounter group, which is concerned with personal relationships. But I don't think you can set up harmonious personal relationships until you've got with yourself. Until you've got with the sky, the trees, and the rocks, and the water, and the fire. Then you're fundamental. You're really alive. From that position, you can relate much better to other people, because you don't come on as a kind of 'poor little me, who's in this universe on probation and doesn't really belong' attitude. And most of us do that, terribly apologetic for our existence. Just because we're aplogetic, some people are insufferably proud, because they feel they have to compensate for this inferior status in the universe by overdoing it with boastfulness and with agression towards others. But if you know that-- Well, when Dogen came back from China--he lived around 1200 AD, and studied Zen there and founded a great monastary--they asked him 'What did you learn in China?' He said, 'I learned that the eyes are horizontal, and the nose is perpendicular.'

Now in all these things, don't search for a deep symbolism. Some decrepit modern Chinese Zen will look for--will give you a symbolic understanding of all these sayings. But they're NOT symbolic; they're absolutely direct. So when somebody says, you see, that the fundamental principle of Buddhism is a cyprus tree in the garden, you are not to understand this this is some pantheistic doctrine in which the cyprus tree is a manifestation of the godhead. Let me illustrate the point further, because I can't illustrate it intellectually. It's a little bit of a complicated story, but I think you can follow it.

There is a sect of Buddhism in Japan called Jodo-shinshu .Sukhavati?,, which means the true teaching about the pure land. And they have a method of meditation in which they call upon the name of a transcendental buddha called Amida. So they say this formula, 'Namu Amida Butsu.' Namu means like 'hail,' only it means, in other cultures and other languages than ours, instead of saying 'hail,' they say 'name,' 'nama.' So 'Namu Amida Bustu' means 'Hail Amitabha buddha,' or 'Amida' is the Japanese. That formula is called 'Nambutsu,' or 'Having the buddha in mind.'

There was a priest of this sect that went to study with a Zen master, and had made good progress, and the master told him to write a poem expressing his understanding. So he wrote the following poem:

When nambutsu is said, There is neither oneself nor Buddha; Na-mu- a-mi-da-bu-tsu-- Only the sound is heard.

And the Zen master scratched his head awhile, because he wasn't quite satisfied with it, so the student submitted another poem which did satisfy the master, and it went like this:

When the nambutsu is said, There is neither oneself nor Buddha; Na-ma-a-mi-da-bu-tsu, Na-ma-a-mi-da-bu-tsu.

The master was satisfied, but in my opinion it had one line too many.

So you see that the Zen practice involves using words to get beyond words, where we might use words simply for their sound. Let's suppose you say the word 'yes.' Yes. Yes. Yes. Yes. Yes. Yes. Yes. You come to think after a while 'Isn't that a funny kind of noise to make?' And we are delivered from the hypnotic effect of words by this particular use of words. We learn they're only words after all, but we hypnotize people by using words. And children, for instance, have no antibodies against words, so they get absolutely frantic, you know. 'Jeannie called me a sissy!' So what? But children get absolutely desperate about it because we put this power of words upon them, these incantations. These are spells, you see. All magicians embroil people in spells and incantations, because they use words to beguil. And so then, we are from infancy told who we are, what is our identity, what our expectations should be, what we ought to get out of life, what class we belong to. And we believe the whole thing. And having believed it, we come to sense it, as we sense the hard wood of the corner of the table, and we think it's real, and it's a bunch of hogwash. It's an amusing game, if you know that that's all it is, and can be played with eloquence. But the more you know it's ONLY an illusion, the better you can play it.

So then. In this practice, it is very important, as I said last night, to bear it in mind that Zen study or Zen meditation--and this includes yoga and other forms of meditation--is not like any other form of exercise, in that it is NOT done for a purpose. You may ask me 'How can I possibly do something that is not being done for a purpose?' because you have a fixed idea, which is part of the hypnosis, that everything you do is done for a purpose. For what purpose do you have belly rumbles?

I remember Soki Antsuzaki, who was a great Zen master, sitting in his gorgeous golden robes, with incense burning in front of him, and his scriptures open on the stand, and holding a sort of sceptor that Zen masters occasionally hold, and reading a passage from the sutra, then by commment saying, 'Fundamental principle of Buddhism is purposelessness. Most important to attain state of no purpose. When you drop fart, you don't say 'At 9:00, I drop fart.' It just happen.' And all this kind crypto-Christain audience, very embarrassed, stuffing handherchiefs into their mouths.

In Chinese, their word for nature is 'tzu-jan,' in Japanese, 'shi-jen,' at that means, 'what is so of itself. We would say 'spontaneity.' A tree has no intention to grow. Water has no intention to flow. The clouds have no intention to blow. And as the poem says,

When the wild geese fly over the lake, The water does not intend to reflect them, And the geese have no mind to cast their image.

Now, that worries us. First of all, we think that spontaneity is mere capricious action. There's nothing very capricious about the way a tree grows. It's a highly intelligent design. So is the bird. So are you. But a lot of people who don't quite understand Zen think that spontaneity is just doing anything, and the more it looks like anything, the more spontaneous it is. In other words, they have a preconception of spontaneity, that a person behaving spontaneously. Or would probably be vulgar, impolite, rude. It doesn't follow; that's merely a preconception of the nature of spontaneity. Spontaneity is the way you grow your hair, it's not the way you think you ought to grow your hair. It's the way it happens. So that's a really high order of intelligence.

What is happening, then, in the discipline of Zen is that we are trying to move into the place where we use that intelligence in everyday life--but you see, you can't get it on purpose. The purpose, the motivation always spoils it. So you would ask then, 'How do I get rid of purpose?' On purpose? That you ask that question simply shows how tied up you are in the thinking process. You cannot force that process to stop. You have to see it as nonsense. Babble. Interminable babble in your head. So one learns to listen to one's thoughts and let the mind think anything it wants to think, but don't take it seriously. And the idea of you doing this is also a babble in the head. And eventually--but without bothering about any eventually, because in this state, there is no future; you're not concerned about the future. Purpose is always concerned with the future.

Now what bugs Western people about this is they would say 'Are you trying to tell us that life has no meaning, no purpose?' Yes. What's so bad about that? What sort of meaning would you like it to have? Propose me a meaning for life. Anything you want. Well, when people try to think of what the meaning of life is, they say 'Well, I think that we're all part of a plan, and that working as if we were characters in a novel or a play, and we are all working towards a great fulfillment. One day, perhaps after we're dead, perhaps in the future life, there'll be a great gazoozie. There'll be a galuptious, glorious goodie at the end of the line, see? And that's what we're all for, see? To get in with that. And it will all be very, very important, because it won't be something trivial. It will be something extremely holy.' Well I say 'What's your idea of something very holy?' Well, nobody really knows. You know, they think about church, and medieval artists who used to represent heaven in the form of everybody sitting in choir stalls. And I must say hell looked much more fun. It was a kind of sado-masochistic orgy. But heaven looked insufferably dull. And when those little children sang hymns about those eternal sabbaths, it was a a very, depressing future, I can assure you.

But you see, when you follow through these ideas, what do you want? What is the goodie? What is progress all about? You realize that you just don't know. So the question is immediately posed for the meditator, but aren't you there already? I mean, isn't THIS what it's about?

I find it a little difficult to say what the subject matter of this seminar is going to be, because it's too fundamental to give it a title. I'm going to talk about what there is. Now, the first thing, though, that we have to do is to get our perspectives with some background about the basic ideas that, as Westerners living today in the United States, influence our everyday common sense, our fundamental notions about what life is about. And there are historical origins for this, which influence us more strongly than most people realize. Ideas of the world which are built into the very nature of the language we use, and of our ideas of logic, and of what makes sense altogether.

And these basic ideas I call myth, not using the word 'myth' to mean simply something untrue, but to use the word 'myth' in a more powerful sense. A myth is an image in terms of which we try to make sense of the world. Now, for example, a myth in a way is a metaphore. If you want to explain electricity to someone who doesn't know anything about electricity, you say, well, you talk about an electric current. Now, the word 'current' is borrowed from rivers. It's borrowed from hydrolics, and so you explain electricity in terms of water. Now, electricity is not water, it behaves actually in a different way, but there are some ways in which the behavior of water is like the behavior of electricty, and so you explain it in terms of water. Or if you're an astronomer, and you want to explain to people what you mean by an expanding universe and curved space, you say, 'well, it's as if you have a black balloon, and there are white dots on the black balloon, and those dots represent galaxies, and as you blow the balloon up, uniformly all of them grow farther and farther apart. But you're using an analogy--the universe is not actually a black balloon with white dots on it.

So in the same way, we use these sort of images to try and make sense of the world, and we at present are living under the influence of two very powerful images, which are, in the present state of scientific knowledge, inadequate, and one of the major problems today are to find an adequate, satisfying image of the world. Well that's what I'm going to talk about. And I'm going to go further than that, not only what image of the world to have, but how we can get our sensations and our feelings in accordance with the most sensible image of the world that we can manage to conceive.

All right, now--the two images which we have been working under for 2000 years and maybe more are what I would call two models of the universe, and the first is called the ceramic model, and the second the fully automatic model. The ceramic model of the universe is based on the book of Genesis, from which Judaism, Islam, and Christianity derive their basic picture of the world. And the image of the world in the book of Genesis is that the world is an artifact. It is made, as a potter takes clay and forms pots out of it, or as a carpenter takes wood and makes tables and chairs out of it. Don't forget Jesus is the son of a carpenter. And also the son of God. So the image of God and of the world is based on the idea of God as a technician, potter, carpenter, architect, who has in mind a plan, and who fashions the universe in accordance with that plan.

So basic to this image of the world is the notion, you see, that the world consists of stuff, basically. Primoridial matter, substance, stuff. As parts are made of clay. Now clay by itself has no intelligence. Clay does not of itself become a pot, although a good potter may think otherwise. Because if you were a really good potter, you don't impose your will on the clay, you ask any given lump of clay what it wants to become, and you help it to do that. And then you become a genious. But the ordinary idea I'm talking about is that simply clay is unintelligent; it's just stuff, and the potter imposes his will on it, and makes it become whatever he wants.

And so in the book of Genesis, the lord God creates Adam out of the dust of the Earth. In other words, he makes a clay figurine, and then he breathes into it, and it becomes alive. And because the clay become informed. By itself it is formless, it has no intelligence, and therefore it requires an external intelligence and an external energy to bring it to life and to bring some sense to it. And so in this way, we inherit a conception of ourselves as being artifacts, as being made, and it is perfectly natural in our culture for a child to ask its mother 'How was I made?' or 'Who made me?' And this is a very, very powerful idea, but for example, it is not shared by the Chinese, or by the Hindus. A Chinese child would not ask its mother 'How was I made?' A Chinese child might ask its mother 'How did I grow?' which is an entirely different procedure form making. You see, when you make something, you put it together, you arrange parts, or you work from the outside in, as a sculpture works on stone, or as a potter works on clay. But when you watch something growing, it works in exactly the opposite direction. It works from the inside to the outside. It expands. It burgeons. It blossoms. And it happens all of itself at once. In other words, the original simple form, say of a living cell in the womb, progressively complicates itself, and that's the growing process, and it's quite different from the making process.

But we have thought, historically, you see, of the world as something made, and the idea of being--trees, for example-- constructions, just as tables and houses are constructions. And so there is for that reason a fundamental difference between the made and the maker. And this image, this ceramic model of the universe, originated in cultures where the form of government was monarchial, and where, therefore, the maker of the universe was conceived also at the same time in the image of the king of the universe. 'King of kings, lords of lords, the only ruler of princes, who thus from thy throne behold all dwellers upon Earth.' I'm quoting the Book of Common Prayer. And so, all those people who are oriented to the universe in that way feel related to basic reality as a subject to a king. And so they are on very, very humble terms in relation to whatever it is that works all this thing. I find it odd, in the United States, that people who are citizens of a republic have a monarchial theory of the universe. That you can talk about the president of the United States as LBJ, or Ike, or Harry, but you can't talk about the lord of the universe in such familiar terms. Because we are carrying over from very ancient near-Eastern cultures, the notion that the lord of the universe must be respected in a certain way. Poeple kneel, people bow, people prostrate themselves, and you know what the reason for that is: that nobody is more frightened of anybody else than a tyrant. He sits with his back to the wall, and his guards on either side of him, and he has you face downwards on the ground because you can't use weapons that way. When you come into his presence, you don't stand up and face him, because you might attack, and he has reason to fear that you might because he's ruling you all. And the man who rules you all is the biggest crook in the bunch. Because he's the one who succeeded in crime. The other people are pushed aside because they--the criminals, the people we lock up in jail--are simply the people who didn't make it.

So naturally, the real boss sits with his back to the wall and his henchmen on either side of him. And so when you design a church, what does it look like? Catholic church, with the alter where it used to be--it's changing now, because the Catholic religion is changing. But the Catholic church has the alter with it's back to the wall at the east end of the church. And the alter is the throne and the priest is the chief vizier of the court, and he is making abeyance to the throne, but there is the throne of God, the alter. And all the people are facing it, and kneeling down. And a great Catholic cathederal is called a basilica, from the Greek 'basilikos,' which means 'king.' So a basilica is the house of a king, and the ritual of the church is based on the court rituals of Byzantium.

A Protestant church is a little different. Basically the same. The furniture of a Protestant church is based on a judicial courthouse. The pulpit, the judge in an American court wears a black robe, he wears exactly the same dress as a Protestant minister. And everybody sits in these boxes, there's a box for the jury, there's a box for the judge, there's a box for this, there's a box for that, and those are the pews in an ordinary colonial- type Protestant church. So both these kinds of churches which have an autocratic view of the nature of the universe decorate themselves, are architecturally constructed in accordance with politcal images of the universe. One is the king, and the other is the judge. Your honor. There's sense in this. When in court, you have to refer to the judge as 'your honor.' It stops the people engaged in litigation from losing their tempers and getting rude. There's a certain sense to that.

But when you want to apply that image to the universe itself, to the very nature of life, it has limitations. For one thing, the idea of a difference between matter and spirit. This idea doesn't work anymore. Long, long ago, physicists stopped asking the question 'What is matter?' They began that way. They wanted to know, what is the fundamental substance of the world? And the more they asked that question, the more they realized the couldn't answer it, because if you're going to say what matter is, you've got to describe it in terms of behavior, that is to say in terms of form, in terms of pattern. You tell what it does, you describe the smallest shapes of it which you can see. Do you see what happens? You look, say, at a piece of stone, and you want to say, 'Well, what is this piece of stone made of?' You take your microscope and you look at it, and instead of just this block of stuff, you see ever so many tinier shapes. Little crystals. So you say, 'Fine, so far so good. Now what are these crystals made of?' And you take a more powerful instrument, and you find that they're made of molocules, and then you take a still more powerful instrument to find out what the molocules are made of, and you begin to describe atoms, electrons, protons, mesons, all sorts of sub-nuclear particles. But you never, never arrive at the basic stuff. Because there isn't any.

What happens is this: 'Stuff' is a word for the world as it looks when our eyes are out of focus. Fuzzy. Stuff--the idea of stuff is that it is undifferentiated, like some kind of goo. And when your eyes are not in sharp focus, everything looks fuzzy. When you get your eyes into focus, you see a form, you see a pattern. But when you want to change the level of magnification, and go in closer and closer and closer, you get fuzzy again before you get clear. So everytime you get fuzzy, you go through thinking there's some kind of stuff there. But when you get clear, you see a shape. So all that we can talk about is patterns. We never, never can talk about the 'stuff' of which these patterns are supposed to be made, because you don't really have to suppose that there is any. It's enough to talk about the world in terms of patterns. It describes anything that can be described, and you don't really have to suppose that there is some stuff that constitutes the essence of the pattern in the same way that clay constitutes the essence of pots. And so for this reason, you don't really have to suppose that the world is some kind of helpless, passive, unintelligent junk which an outside agency has to inform and make into intelligent shapes. So the picture of the world in the most sophisticated physics of today is not formed stuff--potted clay--but pattern. A self-moving, self-designing pattern. A dance. And our common sense as individuals hasn't yet caught up with this.

Well now, in the course of time, in the evolution of Western thought. The ceramic image of the world ran into trouble. And changed into what I call the fully automatic image of the world. In other words, Western science was based on the idea that there are laws of nature, and got that idea from Judaism and Christianity and Islam. That in other words, the potter, the maker of the world in the beginning of things laid down the laws, and the law of God, which is also the law of nature, is called the 'loggos.?,.' And in Christianity, the loggos is the second person of the trinity, incarnate as Jesus Christ, who thereby is the perfect exemplar of the divine law. So we have tended to think of all natural phenomena as responding to laws, as if, in other words, the laws of the world were like the rails on which a streetcar or a tram or a train runs, and these things exist in a certain way, and all events respond to these laws. You know that limerick,

There was a young man who said 'Damn, For it certainly seems that I am A creature that moves In determinate grooves. I'm not even a bus, I'm a tram.'

So here's this idea that there's kind of a plan, and everything responds and obeys that plan. Well, in the 18th century, Western intellectuals began to suspect this idea. And what they suspected was whether there is a lawmaker, whether there is an architect of the universe, and they found out, or they reasoned, that you don't have to suppose that there is. Why? Because the hypothesis of God does not help us to make any predictions. Nor does it-- In other words, let's put it this way: if the business of science is to make predictions about what's going to happen, science is essentially prophecy. What's going to happen? By examining the behavior of the past and describing it carefully, we can make predictions about what's going to happen in the future. That's really the whole of science. And to do this, and to make successful predictions, you do not need God as a hypothesis. Because it makes no difference to anything. If you say 'Everything is controlled by God, everything is governed by God,' that doesn't make any difference to your prediction of what's going to happen. And so what they did was drop that hypothesis. But they kept the hypothesis of law. Because if you can predict, if you can study the past and describe how things have behaved, and you've got some regularities in the behavior of the universe, you call that law. Although it may not be law in the ordinary sense of the word, it's simply regularity.

And so what they did was got rid of the lawmaker and kept the law. And so the conceived the universe in terms of a mechanism. Something, in other words, that is functioning according to regular, clocklike mechanical principles. Newton's whole image of the world is based on billiards. The atoms are billiard balls, and they bang each other around. And so your behavior, every individual around, is defined as a very, very complex arrangement of billiard balls being banged around by everything else. And so behind the fully automatic model of the universe is the notion that reality itself is, to use the favorite term of 19th century scientists, blind energy. In say the metaphysics of Ernst Hegel, and T.H. Huxley, the world is basically nothing but energy--blind, unintelligent force. And likewise and parallel to this, in the philosophy of Freud, the basic psychological energy is libido, which is blind lust. And it is only a fluke, it is only as a result of pure chances that resulting from the exuberance of this energy there are people. With values, with reason, with languages, with cultures, and with love. Just a fluke. Like, you know, 1000 monkeys typing on 1000 typewriters for a million years will eventually type the Encyclopedia Britannica. And of course the moment they stop typing the Encyclopedia Britannica, they will relapse into nonsense.

And so in order that that shall not happen, for you and I are flukes in this cosmos, and we like our way of life--we like being human--if we want to keep it, say these people, we've got to fight nature, because it will turn us back into nonsense the moment we let it. So we've got to impose our will upon this world as if we were something completely alien to it. From outside. And so we get a culture based on the idea of the war between man and nature. And we talk about the conquest of space. The conquest of Everest. And the great symbols of our culture are the rocket and the bulldozer. The rocket--you know, compensation for the sexually inadequate male. So we're going to conquer space. You know we're in space already, way out. If anybody cared to be sensitive and let outside space come to you, you can, if your eyes are clear enough. Aided by telescopes, aided by radio astronomy, aided by all the kinds of sensitive instruments we can devise. We're as far out in space as we're ever going to get. But, y'know, sensitivity isn't the pitch. Especially in the WASP culture of the United States. We define manliness in terms of agression, you see, because we're a little bit frightened as to whether or not we're really men. And so we put on this great show of being a tough guy. It's completely unneccesary. If you have what it takes, you don't need to put on that show. And you don't need to beat nature into submission. Why be hostile to nature? Because after all, you ARE a symptom of nature. You, as a human being, you grow out of this physical universe in exactly the same way an apple grows off an apple tree.

So let's say the tree which grows apples is a tree which apples, using 'apple' as a verb. And a world in which human beings arrive is a world that peoples. And so the existence of people is symptomatic of the kind of universe we live in. Just as spots on somebody's skin is symptomatic of chicken pox. Just as hair on a head is symptomatic of what's going on in the organism. But we have been brought up by reason of our two great myths--the ceramic and the automatic--not to feel that we belong in the world. So our popular speech reflects it. You say 'I came into this world.' You didn't. You came out of it. You say 'Face facts.' We talk about 'encounters' with reality, as if it was a head-on meeting of completely alien agencies. And the average person has the sensation that he is a someone that exists inside a bag of skin. The center of consciousness that looks out at this thing, and what the hell's it going to do to me? You see? 'I recognize you, you kind of look like me, and I've seen myself in a mirror, and you look like you might be people.' So maybe you're intelligent and maybe you can love, too. Perhaps you're all right, some of you are, anyway. You've got the right color of skin, or you have the right religion, or whatever it is, you're OK. But there are all those people over in Asia, and Africa, and they may not really be people. When you want to destroy someone, you always define them as 'unpeople.' Not really human. Monkeys, maybe. Idiots, maybe. Machines, maybe, but not people.

So we have this hostility to the external world because of the superstition, the myth, the absolutely unfounded theory that you, yourself, exist only inside your skin. Now I want to propose another idea altogether. There are two great theories in astronomy going on right now about the origination of the universe. One is called the explosion theory, and the other is called the steady state theory. The steady state people say there never was a time when the world began, it's always expanding, yes, but as a result of free hydrogen in space, the free hydrogen coagulates and makes new galaxies. But the other people say there was a primoridial explosion, an enormous bang billions of years ago which flung all the galazies into space. Well let's take that just for the sake of argument and say that was the way it happened.

It's like you took a bottle of ink and you threw it at a wall. Smash! And all that ink spread. And in the middle, it's dense, isn't it? And as it gets out on the edge, the little droplets get finer and finer and make more complicated patterns, see? So in the same way, there was a big bang at the beginning of things and it spread. And you and I, sitting here in this room, as complicated human beings, are way, way out on the fringe of that bang. We are the complicated little patterns on the end of it. Very interesting. But so we define ourselves as being only that. If you think that you are only inside your skin, you define yourself as one very complicated little curlique, way out on the edge of that explosion. Way out in space, and way out in time. Billions of years ago, you were a big bang, but now you're a complicated human being. And then we cut ourselves off, and don't feel that we're still the big bang. But you are. Depends how you define yourself. You are actually--if this is the way things started, if there was a big bang in the beginning-- you're not something that's a result of the big bang. You're not something that is a sort of puppet on the end of the process. You are still the process. You are the big bang, the original force of the universe, coming on as whoever you are. When I meet you, I see not just what you define yourself as--Mr so-and- so, Ms so-and-so, Mrs so-and-so--I see every one of you as the primordial energy of the universe coming on at me in this particular way. I know I'm that, too. But we've learned to define ourselves as separate from it.

And so what I would call a basic problem we've got to go through first, is to understand that there are no such things as things. That is to say separate things, or separate events. That that is only a way of talking. If you can understand this, you're going to have no further problems. I once asked a group of high school children 'What do you mean by a thing?' First of all, they gave me all sorts of synonyms. They said 'It's an object,' which is simply another word for a thing; it doesn't tell you anything about what you mean by a thing. Finally, a very smart girl from Italy, who was in the group, said a thing is a noun. And she was quite right. A noun isn't a part of nature, it's a part of speech. There are no nouns in the physical world. There are no separate things in the physical world, either. The physical world is wiggly. Clouds, mountains, trees, people, are all wiggly. And only when human beings get to working on things--they build buildings in straight lines, and try to make out that the world isn't really wiggly. But here we are, sitting in this room all built out of straight lines, but each one of us is as wiggly as all get-out.

Now then, when you want to get control of something that wiggles, it's pretty difficult, isn't it? You try and pick up a fish in your hands, and the fish is wiggly and it slips out. What do you do to get hold of the fish? You use a net. And so the net is the basic thing we have for getting hold of the wiggly world. So if you want to get hold of this wiggle, you've got to put a net over it. A net is something regular. And I can number the holes in a net. So many holes up, so many holes across. And if I can number these holes, I can count exactly where each wiggle is, in terms of a hole in that net. And that's the beginning of calculus, the art of measuring the world. But in order to do that, I've got to break up the wiggle into bits. I've got to call this a specific bit, and this the next bit of the wiggle, and this the next bit, and this the next bit of the wiggle. And so these bits are things or events. Bit of wiggles. Which I mark out in order to talk about the wiggle. In order to measure and therfore in order to control it. But in nature, in fact, in the physical world, the wiggle isn't bitted. Like you don't get a cut-up fryer out of an egg. But you have to cut the chicken up in order to eat it. You bite it. But it doesn't come bitten.

So the world doesn't come thinged; it doesn't come evented. You and I are all as much continuous with the physical universe as a wave is continuous with the ocean. The ocean waves, and the universe peoples. And as I wave and say to you 'Yoo-hoo!' the world is waving with me at you and saying 'Hi! I'm here!' But we are consciousness of the way we feel and sense our existence. Being based on a myth that we are made, that we are parts, that we are things, our consciousness has been influenced, so that each one of us does not feel that. We have been hypnotized, literally hypnotized by social convention into feeling and sensing that we exist only inside our skins. That we are not the original bang, just something out on the end of it. And therefore we are scared stiff. My wave is going to disappear, and I'm going to die! And that would be awful. We've got a mythology going now which is, as Father Maskell.?, put it, we are something that happens between the maternity ward and the crematorium. And that's it. And therefore everybody feels unhappy and miserable.

This is what people really believe today. You may go to church, you may say you believe in this, that, and the other, but you don't. Even Jehovah's Witnesses, who are the most fundamental of fundamentalists, they are polite when they come around and knock on the door. But if you REALLY believed in Christianity, you would be screaming in the streets. But nobody does. You would be taking full- page ads in the paper every day. You would be the most terrifying television programs. The churches would be going out of their minds if they really believed what they teach. But they don't. They think they ought to believe what they teach. They believe they should believe, but they don't really believe it, because what we REALLY believe is the fully automatic model. And that is our basic, plausible common sense. You are a fluke. You are a separate event. And you run from the maternity ward to the crematorium, and that's it, baby. That's it.

Now why does anybody think that way? There's no reason to, because it isn't even scientific. It's just a myth. And it's invented by people who want to feel a certain way. They want to play a certain game. The game of god got embarrassing. The idea if God as the potter, as the architect of the universe, is good. It makes you feel that life is, after all, important. There is someone who cares. It has meaning, it has sense, and you are valuable in the eyes of the father. But after a while, it gets embarrassing, and you realize that everything you do is being watched by God. He knows your tiniest innermost feelings and thoughts, and you say after a while, 'Quit bugging me! I don't want you around.' So you become an athiest, just to get rid of him. Then you feel terrible after that, because you got rid of God, but that means you got rid of yourself. You're nothing but a machine. And your idea that you're a machine is just a machine, too. So if you're a smart kid, you commit suicide. Camus said there is only one serious philosophical question, which is whether or not to commit suicide. I think there are four or five serious philosophical questions. The first one is 'Who started it?' The second is 'Are we going to make it?' The third is 'Where are we going to put it?' The fourth is 'Who's going to clean up?' And the fifth, 'Is it serious?'

But still, should you or not commit suicide? This is a good question. Why go on? And you only go on if the game is worth the gamble. Now the universe has been going on for an incredible long time. And so really, a satisfactory theory of the universe has to be one that's worth betting on. That's very, it seems to me, elementary common sense. If you make a theory of the universe which isn't worth betting on, why bother? Just commit suicide. But if you want to go on playing the game, you've got to have an optimal theory for playing the game. Otherwise there's no point in it. But the people who coined the fully automatic theory of the universe were playing a very funny game, for what they wanted to say was this: all you people who believe in religion--old ladies and wishful thinkers-- you've got a big daddy up there, and you want comfort, but life is rough. Life is tough, as success goes to the most hard- headed people. That was a very convenient theory when the European and American worlds were colonizing the natives everywhere else. They said 'We're the end product of evolution, and we're tough. I'm a big strong guy because I face facts, and life is just a bunch of junk, and I'm going to impose my will on it and turn it into something else. I'm real hard.' That's a way of flattering yourself.

And so, it has become academically plausible and fashionable that this is the way the world works. In academic circles, no other theory of the world than the fully automatic model is respectable. Because if you're an academic person, you've got to be an intellectually tough person, you've got to be prickly. There are basically two kinds of philosophy. One's called prickles, the other's called goo. And prickly people are precise, rigorous, logical. They like everything chopped up and clear. Goo people like it vague. For example, in physics, prickly people believe that the ultimate constituents of matter are particles. Goo people believe it's waves. And in philosophy, prickly people are logical positivists, and goo people are idealists. And they're always arguing with each other, but what they don't realize is neither one can take his position without the other person. Because you wouldn't know you advocated prickles unless there was someone advocating goo. You wouldn't know what a prickle was unless you knew what a goo was. Because life isn't either prickles or goo, it's either gooey prickles or prickly goo. They go together like back and front, male and female. And that's the answer to philosophy. You see, I'm a philosopher, and I'm not going to argue very much, because if you don't argue with me, I don't know what I think. So if we argue, I say 'Thank you,' because owing to the courtesy of your taking a different point of view, I understand what I mean. So I can't get rid of you.

But however, you see, this whole idea that the universe is nothing at all but unintelligent force playing around and not even enjoying it is a putdown theory of the world. People who had an advantage to make, a game to play by putting it down, and making out that because they put the world down they were a superior kind of people. So that just won't do. We've had it. Because if you seriously go along with this idea of the world, you're what is technically called alienated. You feel hostile to the world. You feel that the world is a trap. It is a mechanism, it is electronic and neurological mechanisms into which you somehow got caught. And you, poor thing, have to put up with being put into a body that's falling apart, that gets cancer, that gets the great Siberian itch, and is just terrible. And these mechanics--doctors--are trying to help you out, but they really can't succeed in the end, and you're just going to fall apart, and it's a grim business, and it's just too bad. So if you think that's the way things are, you might as well commit suicide right now. Unless you say, 'Well, I'm damned. Because there might really be after all eternal damnation. Or I identify with my children, and I think of them going on without me and nobody to support them. Because if I do go on in this frame of mind and continue to support them, I shall teach them to be like I am, and they'll go on, dragging it out to support their children, and they won't enjoy it. They'll be afraid to commit suicide, and so will their children. They'll all learn the same lessons.'

So you see, all I'm trying to say is that the basic common sense about the nature of the world that is influencing most people in the United States today is simply a myth. If you want to say that the idea of God the father with his white beard on the golden throne is a myth, in a bad sense of the word 'myth,' so is this other one. It is just as phony and has just as little to support it as being the true state of affairs. Why? Let's get this clear. If there is any such thing at all as intelligence and love and beauty, well you've found it in other people. In other words, it exists in us as human beings. And as I said, if it is there, in us, it is symptomatic of the scheme of things. We are as symptomatic of the scheme of things as the apples are symptomatic of the apple tree or the rose of the rose bush. The Earth is not a big rock infested with living organisms any more than your skeleton is bones infested with cells. The Earth is geological, yes, but this geological entity grows people, and our existence on the Earth is a symptom of this other system, and its balances, as much as the solar system in turn is a symptom of our galaxy, and our galaxy in its turn is a symptom of a whole company of other galaxies. Goodness only knows what that's in.

But you see, when, as a scientist, you describe the behavior of a living organism, you try to say what a person does, it's the only way in which you can describe what a person is, describe what they do. Then you find out that in making this description, you cannot confine yourself to what happens inside the skin. In other words, you cannot talk about a person walking unless you start describing the floor, because when I walk, I don't just dangle my legs in empty space. I move in relationship to a room. So in order to describe what I'm doing when I'm walking, I have to describe the room; I have to describe the territory. So in describing my talking at the moment, I can't describe it as just a thing in itself, because I'm talking to you. And so what I'm doing at the moment is not completely described unless your being here is described also. So if that is necessary, in other words, in order to describe MY behavior, I have to describe YOUR behavior and the behavior of the environment, it means that we've really got one system of behavior. Your skin doesn't separate you from the world; it's a bridge through which the external world flows into you, and you flow into it.

Just, for example, as a whirlpool in water, you could say because you have a skin you have a definite shape you have a definite form. All right? Here is a flow of water, and suddenly it does a whirlpool, and it goes on. The whirlpool is a definite form, but no water stays put in it. The whirlpool is something the stream is doing, and exactly the same way, the whole universe is doing each one of us, and I see each one of you today and I recognize you tomorrow, just as I would recognize a whirlpool in a stream. I'd say 'Oh yes, I've seen that whirlpool before, it's just near so-and-so's house on the edge of the river, and it's always there.' So in the same way when I meet you tomorrow, I recognize you, you're the same whirlpool you were yesterday. But you're moving. The whole world is moving through you, all the cosmic rays, all the food you're eating, the stream of steaks and milk and eggs and everything is just flowing right through you. When you're wiggling the same way, the world is wiggling, the stream is wiggling you.

But the problem is, you see, we haven't been taught to feel that way. The myths underlying our culture and underlying our common sense have not taught us to feel identical with the universe, but only parts of it, only in it, only confronting it--aliens. And we are, I think, quite urgently in need of coming to feel that we ARE the eternal universe, each one of us. Otherwise we're going to go out of our heads. We're going to commit suicide, collectively, courtesy of H-bombs. And, all right, supposing we do, well that will be that, then there will be life making experiments on other galaxies. Maybe they'll find a better game.

Well now, in the first session this afternoon, I was discussing two of the great myths or models of the universe, which lie in the intellictual and psychological background of all of us. The myth of the world as a political, monarchial state in which we are all here on sufferance as subject to God. In which we are MADE artifacts, who do not exist in our own right. God alone, in the first myth, exists in his own right, and you exist as a favor, and you ought to be grateful. Like your parents come on and say to you, 'Look at all the things we've done for you, all the money we spent to send you to college, and you turn out to be a beatnik. You're a wretched, ungrateful child.' And you're supposed to say, 'Sorry, I really am.' But you're definitely in the position of being on probation. This arises out of our whole attitude towards children, whereby we don't really acknowledge that they're human. Instead, when a child comes into the world, and as soon as it can communicate in any way, talk language, you should say to a child, 'How do you do? Welcome to the human race. Now my dear, we are playing a very complicated game, and we're going to explain the rules of it to you. And when you have learned these rules and understand what they are, you may be able to invent better ones. But in the meantime, this is the thing we're doing.'

Instead of that, we either treat a child with a kind of with a kind of 'blah-blah-blah' attitude, or 'coochy-coochy-coochie,' y'know? and don't treat the thing as a human being at all--as a kind of doll. Or else as a nusiance. And so all of us, having been treated that way, carry over into adult life the sense of being on probation here. Either the god is somebody who says to us 'coochy- coochy-coochie,' or 'blah-blah-blah.' And that's the feeling we carry over. So that idea of the royal god, the king of kings and the lord of lords which we inherit from the political structures of the Tigres-Euphrates cultures, and from Egypt. The Pharoah, Amenhotep IV is probably, as Freud suggested, the original author of Moses' monotheism, and certainly the Jewish law code comes from Hammarabi in Chaldea. And these men lived in a culture where the pyramid and the ziggurat--the ziggurat is the Chaldean version of the pyramid, indicating somehow a hierarchy of power, from the boss on down. And God, in this first myth that we've been discussing, the ceramic myth is the boss, and the idea of God is that the universe is governed from above.

But do you see, this parallels--goes hand in hand with the idea that you govern your own body. That the ego, which lies somewhere between the ears and behind the eyes in the brain, is the governer of the body. And so we can't understand a system of order, a system of life, in which there isn't a governer. 'O Lord, our governor, how excellent is thy name in all the world.'

But supposing, on the contrary, there could be a system which doesn't have a governor. That's what we are supposed to have in this society. We are supposed to be a democracy and a republic. And we are supposed to govern ourselves. As I said, it's so funny that Americans can be politically republican--I don't mean republican in the party sense--and yet religiously monarchial. It's a real strange contradiction.

So what is this universe? Is it a monarchy? Is it a republic? Is it a mechanism? Or an organism? Becuase you see, if it's a mechanism, either it's a mere mechanism, as in the fully automatic model, or else it's a mechanism under the control of a driver. A mechanic. If it's not that, it's an organism, and an organism is a thing that governs itself. In your body there is no boss. You could argue, for example, that the brain is a gadget evolved by the stomach, in order to serve the stomach for the purposes of getting food. Or you can argue that the stomach is a gadget evolved by the brain to feed it and keep it alive. Whose game is this? Is it the brain's game, or the stomach's game? They're mutual. The brain implies the stomach and the stomach implies the brain, and neither of them is the boss.

You know that story about all the limbs of the body. The hand said 'We do all our work,' the feet said 'We do our work,' the mouth said 'We do all the chewing, and here's this lazy stomach who just gets it all and doesn't do a thing. He didn't do any work, so let's go on strike.' And the hands refused to carry, the feet refused to walk, the teeth refused to chew, and said 'Now we're on strike against the stomach.' But after a while, all of them found themselves getting weaker and weaker and weaker, because they didn't realize that the stomach fed them.

So there is the possibility then that we are not in the kind of system that these two myths delineate. That we are not living in a world where we ourselves, in the deepest sense of self, are outside reality, and somehow in a position that we have to bow down to it and say 'As a great favor, please preserve us in existence.' Nor are we in a system which is merely mechanical, and which we are nothing but flukes, trapped in the electrical wiring of a nervous system which is fundamentally rather inefficiently arranged. What's the alternative? Well, we could put the alternative in another image altogether, and I'll call this not the ceramic image, not the fully automatic image, but the dramatic image. Consider the world as a drama. What's the basis of all drama? The basis of all stories, of all plots, of all happenings--is the game of hide and seek. You get a baby, what's the fundamental first game you play with a baby? You put a book in front of your face, and you peek at the baby. The baby starts giggling. Because the baby is close to the origins of life; it comes from the womb really knowing what it's all about, but it can't put it into words. See, what every child psychologist really wants to know is to get a baby to talk psychological jargon, and explain how it feels. But the baby knows; you do this, this, this and this, and the baby starts laughing, because the baby is a recent incarnation of God. And the baby knows, therefore, that hide and seek is the basic game.

See, when we were children, we were taught '1, 2, 3,' and 'A, B, C,' but we weren't set down on our mothers' knees and taught the game of black and white. That's the thing that was left out of all our educations, the game that I was trying to explain with these wave diagrams. That life is not a conflict between opposites, but a polarity. The difference bewteen a conflict and a polarity is simply--when you think about opposite things, we sometimes use the expression, 'These two things are the poles apart.' You say, for example, about someone with whom you totally disagree, 'I am the poles apart from this person.' But your very saying that gives the show away. Poles. Poles are the opposite ends of one magnet. And if you take a magnet, say you have a magnetized bar, there's a north pole and a south pole. Okay, chop off the south pole, move it away. The piece you've got left creates a new south pole. You never get rid of the south pole. So the point about a magnet is, things may be the poles apart, but they go together. You can't have the one without the other. We are imagining a diagram of the universe in which the idea of polarity is the opposite ends of the diameter, north and south, you see? That's the basic idea of polarity, but what we're trying to imagine is the encounter of forces that come from absolutely opposed realms, that have nothing in common. When we say of two personality types that they're the poles apart. We are trying to think eccentrically, instead of concentrically. And so in this way, we haven't realized that life and death, black and white, good and evil, being and non-being, come from the same center. They imply each other, so that you wouldn't know the one without the other.

Now I'm not saying that that's bad, that's fun. You're playing the game that you don't know that black and white imply each other. Therefore you think that black possibly might win, that the light might go out, that the sound might never be heard again. That there could be the possibility of a universe of pure tragedy, of endless, endless darkness. Wouldn't that be awful? Only you wouldn't know it was awful, if that's what happened. The point that we all forget is that the black and the white go together, and there isn't the one without the other. At the same time, you see, we forget, in the same way as we forget that these two go together.

The other thing we forget, is that self and other go together, in just the same way as the two poles of a magnet. You say 'I, myself; I am me; I am this individual; I am this particular, unique instance.' What is other is everything else. All of you, all of the stars, all of the galaxies, way, way out into infinite space, that's other. But in the same way as black implies white, self implies other. And you don't exist without all that, so that where you get these polarities, you get this sort of difference, that what we call explicitly, or exoterically, they're different. But implicitely, esoterically, they're one. Since you can't have the one without the other, that shows there's a kind of inner conspiracy bewteen all pairs of opposites, which is not in the open, but it's tacit. It's like you say 'Well, there are all sorts of things that we understand among each other tacitly, that we don't want to admit, but we do recognize tacity there's a kind of secret between us boys and girls,' or whatever it may be. And we recognize that. So, tacitly, all of you really inwardly know--although you won't admit it because your culture has trained you in a contrary direction--all of you really inwardly know that you as an individual self are inseparable from everything else that exists, that you are a special case in the universe. But the whole game, especially of Western culture, is to coneal that from ourselves, so that when anybody in our culture slips into the state of consciousness where they suddenly find this to be true, and they come on and say 'I'm God,' we say 'You're insane.'

Now, it's very difficult--you can very easily slip into the state of consciousness where you feel you're God; it can happen to anyone. Just in the same way as you can get the flu, or measles, or something like that, you can slip into this state of consciousness. And when you get it, it depends upon your background and your training as to how you're going to interpret it. If you've got the idea of god that comes from popular Christianity, God as the governor, the political head of the world, and you think you're God, then you say to everybody, 'You should bow down and worship me.' But if you're a member of Hindu culture, and you suddenly tell all your friends 'I'm God,' instead of saying 'You're insane,' they say 'Congratulations! At last, you found out.' Becuase their idea of god is not the autocratic governor. When they make images of Shiva, he has ten arms. How would you use ten arms? It's hard enough to use two. You know, if you play the organ, you've got to use your two feet and your two hands, and you play different rhythms with each member. It's kind of tricky. But actually we're all masters at this, because how do you grow each hair without having to think about it? Each nerve? How do you beat your heart and digest with your stomach at the same time? You don't have to think about it. In your very body, you are omnipotent in the true sense of omnipotence, which is that you are able to be omni-potent; you are able to do all these things without having to think about it.

When I was a child, I used to ask my mother all sorts of ridiculous questions, which of course every child asks, and when she got bored with my questions, she said 'Darling, there are just some things which we are not meant to know.' I said 'Will we ever know?' She said 'Yes, of course, when we die and go to heaven, God will make everything plain.' So I used to imagine on wet afternoons in heaven, we'd all sit around the throne of grace and say to God, 'Well why did you do this, and why did you do that?' and he would explain it to us. 'Heavenly father, why are the leaves green?' and he would say 'Because of the chlorophyll,' and we'd say 'Oh.' But in he Hindu universe, you would say to God, 'How did you make the mountains?' and he would say 'Well, I just did it. Because when you're asking me how did I make the mountains, you're asking me to describe in words how I made the mountains, and there are no words which can do this. Words cannot tell you how I made the mountains any more than I can drink the ocean with a fork. A fork may be useful for sticking into a piece of something and eating it, but it's of no use for imbibing the ocean. It would take millions of years. In other words, it would take millions of years, and you would be bored with my description, long before I got through it, if I put it to you in words, because I didn't create the mountains with words, I just did it. Like you open and close your hand. You know how you do this, but can you describe in words how you do it? Even a very good physiologist can't describe it in words. But you do it. You're conscious, aren't you. Don't you know how you manage to be conscious? Do you know how you beat your heart? Can you say in words, explain correctly how this is done? You do it, but you can't put it into words, because words are too clumsy, yet you manage this expertly for as long as you're able to do it.'

But you see, we are playing a game. The game runs like this: the only thing you really know is what you can put into words. Let's suppose I love some girl, rapturously, and somebody says to me, 'Do you REALLY love her?' Well, how am I going to prove this? They'll say, 'Write poetry. Tell us all how much you love her. Then we'll believe you.' So if I'm an artist, and can put this into words, and can convince everybody I've written the most ecstatic love letter ever written, they say 'All right, ok, we admit it, you really do love her.' But supposing you're not very articulate, are we going to tell you you DON'T love her? Surely not. You don't have to be Heloise and Abyla to be in love. But the whole game that our culture is playing is that nothing really happens unless it's in the newspaper. So when we're at a party, and it's a great party, somebody says 'Too bad we didn't bring a camera. Too bad there wasn't a tape recorder. And so our children begin to feel that they don't exist authentically unless they get their names in the papers, and the fastest way to get your name in the paper is to commit a crime. Then you'll be photographed, and you'll appear in court, and everybody will notice you. And you're THERE. So you're not there unless you're recorded. It really happened if it was recorded. In other words, if you shout, and it doesn't come back and echo, it didn't happen. Well that's a real hangup. It's true, the fun with echos; we all like singing in the bathtub, because there's more resonance there. And when we play a musical instrument, like a violin or a cello, it has a sounding box, because that gives resonance to the sound. And in the same way, the cortex of the human brain enables us when we're happy to know that we're happy, and that gives a certain resonance to it. If you're happy, and you don't know you're happy, there's nobody home.

But this is the whole problem for us. Several thousand years ago, human beings devolved the system of self-consciousness, and they knew, they knew.

And this is the human problem: we know that we know. And so, there came a point in our evolution where we didn't guide life by distrusting our instincts. Suppose that you could live absolutely spontaneously. You don't make any plans, you just live like you feel like it. And you say 'What a gas that is, I don't have to make any plans, anything. I don't worry; I just do what comes naturally.'

The way the animals live, everybody envies them, because look, a cat, when it walks--did you ever see a cat making an aesthetic mistake. Did you ever see a badly formed cloud? Were the stars ever misarranged? When you watch the foam breaking on the seashore, did it ever make a bad pattern? Never. And yet we think in what we do, we make mistakes. And we're worried about that. So there came this point in human evolution when we lost our innocence. When we lost this thing that the cats and the flowers have, and had to think about it, and had to purposely arrange and discipline and push our lives around in accordance with foresight and words and systems of symbols, accountancy, calculation and so on, and then we worry. Once you start thinking about things, you worry as to if you thought enough. Did you really take all the details into consideration? Was every fact properly reviewed? And by jove, the more you think about it, the more you realize you really couldn't take everything into consideration, becauase all the variables in every decision are incalculable, so you get anxiety. And this, though, also, is the price you pay for knowing that you know. For being able to think about thinking, being able to feel about feeling. And so you're in this funny position.

Now then, do you see that this is simultaneously an advantage and a terrible disadvantage? What has happened here is that by having a certain kind of consciousness, a certain kind of reflexive consciousness--being aware of being aware. Being able to represent what goes on fundamentally in terms of a system of symbols, such as words, such as numbers. You put, as it were, two lives together at once, one representing the other. The symbols representing the reality, the money representing the wealth, and if you don't realize that the symbol is really secondary, it doesn't have the same value. People go to the supermarket, and they get a whole cartload of goodies and they drive it through, then the clerk fixes up the counter and this long tape comes out, and he'll say '$30, please,' and everybody feels depressed, because they give away $30 worth of paper, but they've got a cartload of goodies. They don't think about that, they think they've just lost $30. But you've got the real wealth in the cart, all you've parted with is the paper. Because the paper in our system becomes more valuable than the wealth. It represents power, potentiality, whereas the wealth, you think oh well, that's just necessary; you've got to eat. That's to be really mixed up.

So then. If you awaken from this illusion, and you understand that black implies white, self implies other, life implies death--or shall I say, death implies life--you can conceive yourself. Not conceive, but FEEL yourself, not as a stranger in the world, not as someone here on sufferance, on probation, not as something that has arrived here by fluke, but you can begin to feel your own existence as absolutely fundamental. What you are basically, deep, deep down, far, far in, is simply the fabric and structure of existence itself. So, say in Hindu mythology, they say that the world is the drama of God. God is not something in Hindu mythology with a white beard that sits on a throne, that has royal perogatives. God in Indian mythology is the self, 'Satchitananda.' Which means 'sat,' that which is, 'chit,' that which is consciousness; that which is 'ananda' is bliss. In other words, what exists, reality itself is gorgeous, it is the fullness of total joy. Wowee! And all those stars, if you look out in the sky, is a firework display like you see on the fourth of July, which is a great occasion for celebration; the universe is a celebration, it is a fireworks show to celebrate that existence is. Wowee.

And then they say, 'But, however, there's no point in just sustaining bliss.' Let's suppose you were able, every night, to dream any dream you wanted to dream, and that you could for example have the power to dream in one night 75 years worth of time. Or any length of time you wanted to have. And you would, naturally, as you began on this adventure of dreams, fulfill all your wishes. You would have every kind of pleasure you could conceive. And after several nights of 75 years of total pleasure each, you would say 'Well, that was pretty great. But now let's have a surprise. Let's have a dream which isn't under control, where something is going to happen to me that I don't know what it's going to be.' And you would dig that, and come out of it and say 'That was a close shave, now wasn't it?' Then you would get more and more adventurous, and you would make further and further gambles as to what you would dream, and finally you would dream where you are now. You would dream the dream of the life that you are actually living today. That would be within the infinite multiplicity of the choices you would have. Of playing that you weren't God. Because the whole nature of the godhead, according to this idea, is to play that he's not. The first thing that he says to himself is 'Man, get lost,' because he gives himself away. The nature of love is self-abandonment, not clinging to oneself. Throwing yourself out, for instance as in basketball; you're always getting rid of the ball. You say to the other fellow 'Have a ball.' See? And that keeps things moving. That's the nature of life.

So in this idea, then, everybody is fundamentally the ultimate reality. Not God in a politically kingly sense, but God in the sense of being the self, the deep-down basic whatever there is. And you're all that, only you're pretending you're not. And it's perfectly OK to pretend you're not, to be perfectly convinced, because this is the whole notion of drama. When you come into the theater, there is an arch, and a stage, and down there is the audience. Everybody assumes their seats in the theater, gone to see a comedy, a tragedy, a thriller, whatever it is, and they all know as they come in and pay their admissions, that what is going to happen on the stage is not for real. But the actors have a conspiracy against this, because they're going to try and persuade the audience that what is happening on the stage IS for real. They want to get everybody sitting on the edge of their chairs, they want you terrified, or crying, or laughing. Absolutely captivated by the drama. And if a skillful human actor can take in an audience and make people cry, think what the cosmic actor can do. Why he can take himself in completely. He can play so much for real that he thinks he really is. Like you sitting in this room, you think you're really here. Well, you've persuaded yourself that way. You've acted it so damn well that you KNOW that this is the real world. But you're playing it. As well, the audience and the actor as one. Because behind the stage is the green room, offscene, where the actors take off their masks. Do you know that the word 'person' means 'mask'? The 'persona' which is the mask worn by actors in Greco-Roman drama, because it has a megaphone-type mouth which throws the sound out in an open-air theater. So the 'per'--through--'sona'--what the sound comes through--that's the mask. How to be a real person. How to be a genuine fake. So the 'dramatis persona' at the beginning of a play is the list of masks that the actors will wear. And so in the course of forgetting that this world is a drama, the word for the role, the word for the mask has come to mean who you are genuinely. The person. The proper person. Incidentally, the word 'parson' is derived from the word 'person.' The 'person' of the village. The 'person' around town, the parson.

So anyway, then, this is a drama, and what I want you to is-- I'm not trying to sell you on this idea in the sense of converting you to it; I want you to play with it. I want you to think of its possibilities. I'm not trying to prove it, I'm just putting it forward as a possibility of life to think about. So then, this means that you're not victims of a scheme of things, of a mechanical world, or of an autocratic god. The life you're living is what YOU have put yourself into. Only you don't admit it, because you want to play the game that it's happened to you. In other words, I got mixed up in this world; I had a father who got hot pants over a girl, and she was my mother, and because he was just a horny old man, and as a result of that, I got born, and I blame him for it and say 'Well that's your fault; you've got to look after me,' and he says 'I don't see why I should look after you; you're just a result.' But let's suppose we admit that I really wanted to get born, and that I WAS the ugly gleam in my father's eye when he approached my mother. That was me. I was desire. And I deliberately got involved in this thing. Look at it that way instead. And that really, even if I got myself into an awful mess, and I got born with syphilis, and the great Siberian itch, and tuberculosis in a Nazi concentration camp, nevertheless this was a game, which was a very far out play. It was a kind of cosmic masochism. But I did it.

Isn't that an optimal game rule for life? Because if you play life on the supposition that you're a helpless little puppet that got involved. Or you played on the supposition that it's a frightful, serious risk, and that we really ought to do something about it, and so on, it's a drag. There's no point in going on living unless we make the assumption that the situation of life is optimal. That really and truly we're all in a state of total bliss and delight, but we're going to pretend we aren't just for kicks. In other words, you play non-bliss in order to be able to experience bliss. And you can go as far out in non-bliss as you want to go. And when you wake up, it'll be great. You know, you can slam yourself on the head with a hammer because it's so nice when you stop. And it makes you realize how great things are when you forget that's the way it is. And that's just like black and white: you don't know black unless you know white; you don't know white unless you know black. This is simply fundamental.

So then, here's the drama. My metaphysics, let me be perfectly frank with you, are that there the central self, you can call it God, you can call it anything you like, and it's all of us. It's playing all the parts of all being whatsoever everywhere and anywhere. And it's playing the game of hide and seek with itself. It gets lost, it gets involved in the farthest-out adventures, but in the end it always wakes up and comes back to itself. And when you're ready to wake up, you're going to wake up, and if you're not ready you're going to stay pretending that you're just a 'poor little me.' And since you're all here and engaged in this sort of enquiry and listening to this sort of lecture, I assume you're all in the process of waking up. Or else you're pleasing yourselves with some kind of flirtation with waking up which you're not serious about. But I assume that you are maybe not serious, but sincere, that you are ready to wake up.

So then, when you're in the way of waking up, and finding out who you are, you meet a character called a guru, as the Hindus say 'the teacher,' 'the awakener.' And what is the function of a guru? He's the man that looks you in the eye and says 'Oh come off it. I know who you are.' You come to the guru and say 'Sir, I have a problem. I'm unhappy, and I want to get one up on the universe. I want to become enlightened. I want spiritual wisdom.' The guru looks at you adn says 'Who are you?' You know Sri-Ramana-Maharshi, that great Hindu sage of modern times? People used to come to him and say 'Master, who was I in my last incarnation?' As if that mattered. And he would say 'Who is asking the question?' And he'd look at you and say, go right down to it, 'You're looking at me, you're looking out, and you're unaware of what's behind your eyes. Go back in and find out who you are, where the question comes from, why you ask.' And if you've looked at a photograph of that man--I have a gorgeous photograph of him; I look by it every time I go out the front door. And I look at those eyes, and the humor in them; the lilting laugh that says 'Oh come off it. Shiva, I recognize you. When you come to my door and say `I'm so-and-so,' I say `Ha-ha, what a funny way God has come on today.''

So eventually--there are all sorts of tricks of course that gurus play. They say 'Well, we're going to put you through the mill.' And the reason they do that is simply that you won't wake up until you feel you've paid a price for it. In other words, the sense of guilt that one has. Or the sense of anxiety. It's simply the way one experiences keeping the game of disguise going on. Do you see that? Supposing you say 'I feel guilty.' Christianity makes you feel guilty for existing. That somehow the very fact that you exist is an affront. You are a fallen human being. I remember as a child when we went to the serves of the church on Good Friday. They gave us each a colored postcard with Jesus crucified on it, and it said underneath 'This I have done for thee. What doest thou for me?' You felt awful. YOU had nailed that man to the cross. Because you eat steak, you have crucified Christ. Mythra. It's the same mystery. And what are you going to do about that? 'This I have done for thee, what doest thou for me?' You feel awful that you exist at all. But that sense of guilt is the veil across the sanctuary. 'Don't you DARE come in!' In all mysteries, when you are going to be initiated, there's somebody saying 'Ah-ah-ah, don't you come in. You've got to fulfill this requirement and that requirement, THEN we'll let you in.' And so you go through the mill. Why? Because you're saying to yourself 'I won't wake up until I deserve it. I won't wake up until I've made it difficult for me to wake up. So I invent for myself an eleborate sytem of delaying my waking up. I put myself through this test and that test, and when I convince myself it's sufficiently arduous, THEN I at last admit to myself who I really am, and draw aside the veil and realize that after all, when all is said and done, I am that I am, which is the name of god.'

In last night's session, I was discussing an alternative myth to the Ceramic and Fully Automatic models of the universe, I'll call the Dramatic Myth. The idea that life as we experience it is a big act, and that behind this big act is the player, and the player, or the self, as it's called in Hindu philosophy, the _atman_, is you. Only you are playing hide and seek, since that is the essential game that is going on. The game of games. The basis of all games, hide and seek. And since you're playing hide & seek, you are deliberately, although you can't admit this--or won't admit it--you are deliberately forgetting who you really are, or what you really are. And the knowledge that your essential self is the foundation of the universe, the 'ground of being' as Tillich calls it, is something you have that the Germans call a _hintengedanka_[?] A _hintengedanka_ is a thought way, way, way in the back of your mind. Something that you know deep down but can't admit.

So, in a way, then, in order to bring this to the front, in order to know that is the case, you have to be kidded out of your game. And so what I want to discuss this morning is how this happens. Although before doing so, I must go a little bit further into the whole nature of this problem.

You see, the problem is this. We identify in our exerience a differentiation between what we do and what happens to us. We have a certain number of actions that we define as voluntary, and we feel in control of those. And then over against that, there is all those things that are involuntary. But the dividing line between these two is very inarbitrary. Because for example, when you move your hand, you feel that you decide whether to open it or to close it. But then ask yourself how do you decide? When you decide to open your hand, do you first decide to decide? You don't, do you? You just decide, and how do you do that? And if you don't know how to do it, is it voluntary or involuntary? Let's consider breathing. You can feel that you breath deliberately; you don't control your breath. But when you don't think about it, it goes on. Is it voluntary or involuntary?

So, we come to have a very arbitrary definition of self. That much of my activity which I feel I do. And that then doesn't include breathing most of the time; it doesn't include the heartbeats; it doesn't include the activity of the glands; it doesn't include digestion; it doesn't include how you shape your bones; circulate your blood. Do you or do you not do these things? Now if you get with yourself and you find out you are all of yourself, a very strange thing happens. You find out that your body knows that you are one with the universe. In other words, the so-called involuntary circulation of your blood is one continuous process with the stars shining. If you find out it's YOU who circulates your blood, you will at the same moment find out that you are shining the sun. Because your physical organism is one continous process with everything else that's going on. Just as the waves are continuous with the ocean. Your body is continuous with the total energy system of the cosmos, and it's all you. Only you're playing the game that you're only this bit of it. But as I tried to explain, there are in physical reality no such thing as separate events.

So then. Remember also when I tried to work towards a definition of omnipotence. Omnipotence is not knowing how everything is done; it's just doing it. You don't have to translate it into language. Supposing that when you got up in the morning, you had to switch your brain on. And you had to think and do as a deliberate process waking up all the circuits that you need for active life during hte day. Why, you'd never get done! Because you have to do all those things at once. That's why the Buddhists and Hindus represent their gods as many-armed. How could you use so many arms at once? How could a centipede control a hundred legs at once? Because it doesn't think about it. In the same way, you are unconsciously performing all the various activities of your organism. Only unconsciously isn't a good word, because it sounds sort of dead. Superconsciously would be better. Give it a plus rather than a minus.

Because what consciousness is is a rather specialized form of awareness. When you look around the room, you are conscious of as much as you can notice, and you see an enormous number of things which you do not notice. For example, I look at a girl here and somebody asks me later 'What was she wearing?' I may not know, although I've seen, because I didn't attend. But I was aware. You see? And perhaps if I could under hypnosis be asked this question, where I would get my conscious attention out of the way by being in the hypnotic state, I could recall what dress she was wearing.

So then, just in the same way as you don't know--you don't focus your attention--on how you make your thyroid gland function, so in the same way, you don't have any attention focused on how you shine the sun. So then, let me connect this with the problem of birth and death, which puzzles people enormously of course. Because, in order to understand what the self is, you have to remember that it doesn't need to remember anything,just as you don't need to know how you work your thyroid gland.

So then, when you die, you're not going to have to put up with everlasting non-existance, because that's not an experience. A lot of people are afraid that when they die, they're going to be locked up in a dark room forever, and sort of undergo that. But one of the interesting things in the world is--this is a yoga, this is a realization--try and imagine what it will be like to go to sleep and never wake up. Think about that. Children think about it. It's one of the great wonders of life. What will it be like to go to sleep and never wake up? And if you think long enough about that, something will happen to you. You will find out, among other things, it will pose the next question to you. What was it like to wake up after having never gone to sleep? That was when you were born. You see, you can't have an experience of nothing; nature abhorres a vacuum. So after you're dead, the only thing that can happen is the same experience, or the same sort of experience as when you were born. In other words, we all know very well that after other people die, other people are born. And they're all you, only you can only experience it one at a time. Everybody is I, you all know you're you, and wheresoever all being exist throughout all galaxies, it doesn't make any difference. You are all of them. And when they come into being, that's you coming into being.

You know that very well, only you don't have to remember the past in the same way you don't have to think about how you work your thyroid gland, or whatever else it is in your organism. You don't have to know how to shine the sun. You just do it, like you breath. Doesn't it really astonish you that you are this fantastically complex thing, and that you're doing all this and you never had any education in how to do it? Never learned, but you're this miracle? The point of it is, from a strictly physical, scientific standpoint, this organism is a continuous energy with everything else that's going on. And if I am my foot, I am the sun. Only we've got this little partial view. We've got the idea that 'No, I'm something IN this body.' The ego. That's a joke. The ego is nothing other than the focus of conscious attention. It's like the radar on a ship. The radar on a ship is a troubleshooter. Is there anything in the way? And conscious attention is a designed function of the brain to scan the environment, like a radar does, and note for any troublemaking changes. But if you identify yourself with your troubleshooter, then naturally you define yourself as being in a perpetual state of anxiety. And the moment we cease to identify with the ego and become aware that we are the whole organism, we realize first thing how harmonious it all is. Because your organism is a miracle of harmony. All these things functioning together. Even those creatures that are fighting each other in the blood stream and eating each other up. If they weren't doing that, you wouldn't be healthy.

So what is discord at one level of your being is harmony at another level. And you begin to realize that, and you begin to be aware too, that the discords of your life and the discords of people's lives, which are a discord at one level, at a higher level of the universe are healthy and harmonious. And you suddenly realize that everything you are and do is at that level as magnificent and as free of any blemish as the patterns in waves. The markings in marble. The way a cat moves. And that this world is really OK. Can't be anything else, because otherwise it couldn't exist. And I don't mean this in a kind of Pollyanna Christian Science sense. I don't know what it is or why it is about Christian Science, but it's prissy. It's got kind of a funny feeling to it; came from New England.

But the reality underneath physical existence, or which really is physical existence--because in my philosophy there is no difference between the physical and the spiritual. These are absolutely out-of-date catagories. It's all process; it isn't 'stuff' on the one hand and 'form' on the other. It's just pattern-- life is pattern. It is a dance of energy. And so I will never invoke spooky knowledge. That is, that I've had a private revelation or that I have sensory vibrations going on a plane which you don't have. Everything is standing right out in the open, it's just a question of how you look at it. So you do discover when you realize this, the most extraordinary thing that I never cease to be flabbergasted at whenever it happens to me. Some people will use a symbolism of the relationship of God to the universe, wherein God is a brilliant light, only somehow veiled, hiding underneath all these forms as you look around you. So far so good. But the truth is funnier than that. It is that you are looking right at the brilliant light now that the experience you are having that you call ordinary everyday consciousness--pretending you're not it--that experience is exactly the same thing as 'it.' There's no difference at all. And when you find that out, you laugh yourself silly. That's the great discovery.

In other words, when you really start to see things, and you look at an old paper cup, and you go into the nature of what it is to see what vision is, or what smell is, or what touch is, you realize that that vision of the paper cup is the brilliant light of the cosmos. Nothing could be brighter. Ten thousand suns couldn't be brighter. Only they're hidden in the sense that all the points of the infinite light are so tiny when you see them in the cup they don't blow your eyes out. See, the source of all light is in the eye. If there were no eyes in this world, the sun would not be light. So if I hit as hard as I can on a drum which has no skin, it makes no noise. So if a sun shines on a world with no eyes, it's like a hand beating on a skinless drum. No light. YOU evoke light out of the universe, in the same way you, by nature of having a soft skin, evoke hardness out of wood. Wood is only hard in relation to a soft skin. It's your eardrum that evokes noise out of the air. You, by being this organism, call into being this whole universe of light and color and hardness and heaviness and everything.

But in the mythology that we sold ourselves on at the end of the 19th century, when people discovered how big the universe was, and that we live on a little planet in a solar system on the edge of the galaxy, which is a minor galaxy, everybody thought, 'Uuuuugh, we're really unimportant after all. God isn't there and doesn't love us, and nature doesn't give a damn.' And we put ourselves down. But actually, it's this funny little microbe, tiny thing, crawling on this little planet that's way out somewhere, who has the ingenuity, by nature of this magnificent organic structure, to evoke the whole universe out of what otherwise would be mere quanta. There's jazz going on. But you see, this ingenious little organism is not merely some stranger in this. This little organism, on this little planet, is what the whole show is growing there, and so realizing it's own presence. Does it through you, and you're it.

When you put a chicken's beak on a chalk line, it gets stuck; it's hypnotized. So in the same way, when you learn to pay attention, and as children you know how all the teachers were in class: 'Pay attention!!' And all the kids stare at the teacher. And we've got to pay attention. That's putting your nose on the chalk line. And you got stuck with the idea of attention, and you thought attention was Me, the ego, attention. So if you start attending to attention, you realize what the hoax is. That's why in Aldous Huxley's book 'Island,' the Roger had trained the myna birds on the island to say 'Attention! Here and now, boys!' See? Realize who you are. Come to, wake up!

Well, here's the problem: if this is the state of affairs which is so, and if the conscious state you're in this moment is the same thing as what we might call the Divine State. If you do anything to make it different, it shows that you don't understand that it's so. So the moment you start practicing yoga, or praying or meditating, or indulging in some sort of spiritual cultivation, you are getting in your own way.

Now this is the Buddhist trick: the buddha said 'We suffer because we desire. If you can give up desire, you won't suffer.' But he didn't say that as the last word; he said that as the opening step of a dialogue. Because if you say that to someone, they're going to come back after a while and say 'Yes, but now I'm desiring not to desire.' And so the buddha will answer, 'Well at last you're beginning to understand the point.' Because you can't give up desire. Why would you try to do that? It's already desire. So in the same way you say 'You ought to be unselfish' or to give up you ego. Let go, relax. Why do you want to do that? Just because it's another way of beating the game, isn't it? The moment you hypothesize that you are different from the universe, you want to get one up on it. But if you try to get one up on the universe, and you're in competition with it, that means you don't understand you ARE it. You think there's a real difference between 'self' and 'other.' But 'self,' what you call yourself, and what you call 'other' are mutually necessary to each other like back and front. They're really one. But just as a magnet polarizes itself at north and south, but it's all one magnet, so experience polarizes itself as self and other, but it's all one. If you try to make the south pole defeat the north pole, or get the mastery of it, you show you don't know what's going on.

So there are two ways of playing the game. The first way, which is the usual way, is that a guru or teacher who wants to get this across to somebody because he knows it himself, and when you know it you'd like others to see it, too. So what he does is, he gets you into being ridiculous harder and more assiduously than usual. In other words, if you are in a contest with the universe, he's going to stir up that contest until it becomes ridiculous. And so he sets you such tasks as saying-- Now of course, in order to be a true person, you must give up yourself, be unselfish. So the lord steps down out of heaven and says 'The first and great commandment is `Thou shalt love the lord thy god.' You must love me.' Well that's a double-bind. You can't love on purpose. You can't be sincere purposely. It's like trying not to think of a green elephant while taking medicine.

But if a person really tries to do it--and this is the way Christianity is rigged--you should be very sorry for your sins. And though everybody knows they're not, but they think they ought to be, they go around trying to be penetant. Or trying to be humble. And they know the more assiduously they practice it, the phonier and phonier the whole thing gets. So in Zen Buddhism, exactly the same thing happens. The Zen master challenges you to be spontaneous. 'Show me the real you.' One way they do this getting you to shout. Shout the word 'moo.' And he says 'I want to hear YOU in that shout. I want to hear your whole being in it.' And you yell your lungs out and he says 'Pfft. That's no good. That's just a fake shout. Now I want to hear absolutely the whole of your being, right from the heart of the universe, come through in this shout.' And these guys scream themselves hoarse. Nothing happens. Until one day they get so desperate they give up trying and they manage to get that shout through, when they weren't trying to be genuine. Because there was nothing else to do, you just had to yell.

And so in this way--it's called the technique of reductio ad absurdum. If you think you have a problem, and you're an ego and you're in difficulty, the answer the Zen master makes to you is 'Show me your ego. I want to see this thing that has a problem.' When Bodidharma, the legendary founder of Zen, came to China, a disciple came to him and said 'I have no peace of mind. Please pacify my mind.' And Bodhidharma said 'Bring out your mind here before me and I'll pacify it.' 'Well,' he said, 'when I look for it, I can't find it.' So Bodhidharma said 'There, it's pacified.' See? Becuase when you look for your own mind, that is to say, your own particularized center of being which is separate from everything else, you won't be able to find it. But the only way you'll know it isn't there is if you look for it hard enough, to find out that it isn't there. And so everybody says 'All right, know yourself, look within, find out who you are.' Because the harder you look, you won't be able to find it, and then you'll realize it isn't there at all. There isn't a separate you. You're mind is what there is. Everything. But the only way to find that out is to persist in the state of delusion as hard as possible. That's one way. I haven't said the only way, but it is one way.

So almost all spiritual disciplines, meditations, prayers, etc, etc, are ways of persisting in folly. Doing resolutely and consistently what you're doing already. So if a person believes that the Earth is flat, you can't talk him out of that. He knows it's flat. Look out the window and see; it's obvious, it looks flat. So the only way to convince him it isn't is to say 'Well let's go and find the edge.' And in order to find the edge, you've got to be very careful not to walk in circles, you'll never find it that way. So we've got to go consistently in a straight line due west along the same line of latitude, and eventually when we get back to where we started from, you've convinced the guy that the world is round. That's the only way that will teach him. Because people can't be talked out of illusions.

There is another possibility, however. But this is more difficult to describe. Let's say we take as the basic supposition- -which is the thing that one sees in the experience of satori or awakening, or whatever you want to call it--that this now moment in which I'm talking and you're listening, is eternity. That although we have somehow conned ourselves into the notion that this moment is ordinary, and that we may not feel very well, we're sort of vaguely frustrated and worried and so on, and that it ought to be changed. This is it. So you don't need to do anything at all. But the difficulty about explaining that is that you mustn't try and not do anything, because that's doing something. It's just the way it is. In other words, what's required is a sort of act of super relaxation; it's not ordinary relaxation. It's not just letting go, as when you lie down on the floor and imagine that you're heavy so you get into a state of muscular relaxation. It's not like that. It's being with yourself as you are without altering anything. And how to explain that? Because there's nothing to explain. It is the way it is now. See? And if you understand that, it will automatically wake you up.

So that's why Zen teachers use shock treatment, to sometimes hit them or shout at them or create a sudden surprise. Because is is that jolt that suddenly brings you here. See, there's no road to here, because you're already there. If you ask me 'How am I going to get here?' It will be like the famous story of the American tourist in England. The tourist asked some yokel the way to Upper Tuttenham, a little village. And the yokel scratched his head and he said 'Well, sir, I don't know where it is, but if I were you, I wouldn't start from here.'

So you see, when you ask 'How to I obtain the knowledge of God, how do I obtain the knowledge of liberation?' all I can say is it's the wrong question. Why do you want to obtain it? Because the very fact that you're wanting to obtain it is the only thing that prevents you from getting there. You already have it. But of course, it's up to you. It's your privilege to pretend that you don't. That's your game; that's your life game; that's what makes you think your an ego. And when you want to wake up, you will, just like that. If you're not awake, it shows you don't want to. You're still playing the hide part of the game. You're still, as it were, the self pretending it's not the self. And that's what you want to do. So you see, in that way, too, you're already there.

So when you understand this, a funny thing happens, and some people misinterpret it. You'll discover as this happens that the distinction between voluntary and involuntary behavior disappears. You will realize that what you describe as things under your own will feel exactly the same as things going on outside you. You watch other people moving, and you know you're doing that, just like you're breathing or circulating your blood. And if you don't understand what's going on, you're liable to get crazy at this point, and to feel that you are god in the Jehovah sense. To say that you actually have power over other people, so that you can alter what you're doing. And that you're omnipotent in a very crude, literal kind of bible sense. You see? A lot of people feel that and they go crazy. They put them away. They think they're Jesus Christ and that everybody ought to fall down and worship them. That's only they got their wires crossed. This experience happened to them, but they don't know how to interpret it. So be careful of that. Jung calls it inflation. People who get the Holy Man syndrome, that I suddenly discover that I am the lord and that I am above good and evil and so on, and therefore I start giving myself airs and graces. But the point is, everybody else is, too. If you discover that you are that, then you ought to know that everybody else is.

For example, let's see in other ways how you might realize this. Most people think when they open their eyes and look around, that what they're seeing is outside. It seems, doesn't it, that you are behind your eyes, and that behind the eyes there is a blank you can't see at all. You turn around and there's something else in front of you. But behind the eyes there seems to be something that has no color. It isn't dark, is isn't light. It is there from a tactile standpoint; you can feel it with your fingers, but you can't get inside it. But what is that behind your eyes? Well actually, when you look out there and see all these people and things sitting around, that's how it feels inside your head. The color of this room is back here in the nervous system, where the optical nerves are at the back of the head. It's in there. It's what you're experiencing. What you see out here is a neurological experience. Now if that hits you, and you feel sensuously that that's so, you may feel therefore that the external world is all inside my skull. You've got to correct that, with the thought that your skull is also in the external world. So you suddenly begin to feel 'Wow, what kind of situation is this? It's inside me, and I'm inside it, and it's inside me, and I'm inside it.' But that's the way it is.

This is the what you could call transaction, rather than interaction between the individual and the world. Just like, for example, in buying and selling. There cannot be an act of buying unless there is simultaneously an act of selling, and vice versa. So the relationship between the environment and the organism is transactional. The environment grows the organism, and in turn the organism creates the environment. The organism turns the sun into light, but it requires there be an environment containing a sun for there to be an organism at all. And the answer to it simply is they're all one process. It isn't that organisms by chance came into the world. This world is the sort of environment which grows organisms. It was that way from the beginning. The organisms may in time have arrived in the scene or out of the scene later than the beginning of the scene, but from the moment it went BANG! in the beginning, if that's the way it started, organisms like us are sitting here. We're involved in it.

Look here, we take the propogation of an electric current. I can have an electric current running through a wire that goes all the way around the Earth. And here we have a power source, and here we have a switch. A positive pole, a negative pole. Now, before that switch closes, the current doesn't exactly behave like water in a pipe. There isn't current here, waiting, to jump the gap as soon as the switch is closed. The current doesn't even start until the switch is closed. It never starts unless the point of arrival is there. Now, it'll take an interval for that current to get going in its circuit if it's going all the way around the Earth. It's a long run. But the finishing point has to be closed before it will even start from the beginning. In a similar way, even though in the development of any physical system there may by billions of years between the creation of the most primitive form of energy and then the arrival of intelligent life, that billions of years is just the same things as the trip of that current around the wire. Takes a bit of time. But it's already implied. It takes time for an acorn to turn into an oak, but the oak is already implied in the acorn. And so in any lump of rock floating about in space, there is implicit human intelligence. Sometime, somehow, somewhere. They all go together.

So don't differentiate yourself and stand off and say 'I am a living organism in a world made of a lot of dead junk, rocks and stuff.' It all goes together. Those rocks are just as much you as your fingernails. You need rocks. What are you going to stand on?

What I think an awakening really involves is a re-examination of our common sense. We've got all sorts of ideas built into us which seem unquestioned, obvious. And our speech reflects them; its commonest phrases. 'Face the facts.' As if they were outside you. As if life were something they simply encountered as a foreigner. 'Face the facts.' Our common sense has been rigged, you see? So that we feel strangers and aliens in this world, and this is terribly plausible, simply because this is what we are used to. That's the only reason. But when you really start questioning this, say 'Is that the way I have to assume life is? I know everybody does, but does that make it true?' It doesn't necessarily. It ain't necessarily so. So then as you question this basic assumption that underlies our culture, you find you get a new kind of common sense. It becomes absolutely obvious to you that you are continuous with the universe.

For example, people used to believe that planets were supported in the sky by being imbedded in crystal spheres, and everybody knew that. Why, you could see the crystal spheres there because you could look right through them. It was obviously made of crystal, and something had to keep them up there. And then when the astronomers suggested that there weren't any crystal spheres, people got terrified, because then they thought the stars would fall down. Nowadays, it doesn't bother anybody. They thought, too, when they found out the Earth was spherical, people who lived in the antiguities would fall off, and that was scary. But then somebody sailed around the world, and we all got used to it, we travel around in jet planes and everything. We have no problem feeling that the Earth is globular. None whatever. We got used to it.

So in the same way Einstein's relativity theories--the curvature of the propogation of light, the idea that time gets older as light moves away from a source, in other words, people looking at the world now on Mars, they would be seeing the state of the world a little earlier than we are now experiencing it. That began to bother people when Einstein started talking about that. But now we're all used to it, and relativity and things like that are a matter of common sense today. Well, in a few years, it will be a matter of commons sense to many people that they're one with the universe. It'll be so simple. And then maybe if that happens, we shall be in a position to handle our technology with more sense. With love instead of with hate for our environment.

Holding me firmly with her left arm, Miss Evelyn used the rod most
unmercifully. At first, the pain was excruciating, and I roared out as
loud as I could, but gradually the pain ceased to be so acute, and was
succeeded by the most delicious tickling sensation. My struggles at
first had been so violent as to greatly disorder Miss Evelyn's
petticoats, and to raise them up so as to expose to my delighted eyes
her beautifully formed silk-clad legs up to the knees, and even an inch
or two of naked thigh above.

This, together with the intense tickling irritation communicated to my
bottom, as well as to the friction of my cock against the person of
Miss Evelyn in my struggles, rendered me almost delirious, and I tossed
and pushed myself about on her knees in a state of perfect frenzy as
the blows continued to be showered down upon my poor bottom. At last
the rod was worn to a stump, and I was pushed off her knees. As I rose
before her, with my cheeks streaming with tears, my shirt was jutting
out considerably in front in an unmistakeable and most prominent
manner, and my prick was at the same time throbbing beneath it with
convulsive jerks, which I could by no means restrain.

Miss Evelyn glared at the projection in marked astonishment, and her
open eyes were fixed upon it as I stood rubbing my bottom and crying,
without attempting to move or button up my trousers. She continued for
a minute or two to stare at the object of attraction, flushing scarlet
up to the forehead, and the she suddenly seemed to recollect herself,
drew a heavy breath, and rapidly left room. She did not return until
after my sisters came back from the garden, and seemed still confused,
and avoided fixing her eye upon me.

In two days afterwards, all disagreeable marks of this very severe
whipping had disappeared. On the following day we were invited to pass
the afternoon at the grange, a beautiful place about two miles from us.
The afternoon was fine and warm; we walked there, and arrived about
four o'clock. Mr. and Mrs. Robinson were in the drawing-room, but at
once desired us to go in the garden and amuse ourselves with their
three daughters, whom we would find there. We went at once, and found
them amusing themselves on a swing. Sophia, the eldest, about nineteen,
was swinging a sister about two years younger, a very fine, fully
developed young woman. Indeed, all three sisters were finer women and
more beautiful than the average of young ladies.

Another sister, Agnes, was not seated, but standing on the board
between the ropes. Sophia was making both mount as high as possible.
They were laughing loudly, when we found them, at the exposure each
made--one in advancing, the other retiring. Agnes's light dress of
muslin and single petticoat, as she retired and the wind came up from
behind, was bulged out in front, and exposed her limbs up to her belly,
so that one could see that her mount was already well furnished. The
other, in advancing, threw her legs up, and exposed all the underside
of her thighs and a part of her bottom, and you could just discern that
there was dark hair between the lower thighs and bottom.

As they considered me but a child, I was no check to their mirth and
sport. On the contrary, the gave me a long rope to pull down the swing
when at its highest, and I sat down on the grass in front for greater
convenience. The fine limbs and hairy quims exposed freely before me
from moment to moment excited my passions. None of them wore more than
one petticoat, and they had no drawers, so that when they mounted to
the highest point from me, I had the fullest possible view of all. My
cock soon rose to a painful extent, which I really believe was noticed
and enjoyed by them, I observed, too, that I was an object of attention
to Miss Evelyn, who shortly seated herself in the swing, and allowed me
to swing her with the end of the rope. I even fancied that she threw up
her legs more than was at all necessary; at all events, she naturally,
with the strong feelings I had towards her, excited me more than all
the rest.

We were as merry as could be, and we passed a delightful evening until
eight o'clock, when it began to rain. As it continued, and became very
heavy, Mr. Robinson ordered out the closed carriage to take us home. It
was a brougham, only seated for two. Mary took Eliza on her knee, Miss
Evelyn took me upon hers. I know not how it happened, but her lovely
arm soom passed round my body as if to hold me on her knee, and her
hand fell, apparently by accident, exactly on my cock--the touch was
electric. In an instant, my member stood stiff and strong beneath her
hand. Still Miss Evelyn, who must have felt the movement going on
beneath her fingers, did not remove her hand, but rather seemed to
press more upon it. In my boyish ignorance, I imagined she was not
aware of what was happening. The motion and jolting of the carriage
over rough road caused her hand to rub up and down upon my erected and
throbbing member. I was almost beside myself, and to conceal my
condition I feigned sleep. I let my head fall on Miss Evelyn's shoulder
and neck--she allowed this.

Whether she thought I had really fallen asleep I know not, but I was
quite sensible that her fingers pressed my swollen and throbbing cock,
and I fancied she was measuring its size.

The tight grasp she managed to gain, and the continued jolting of the
carriage, brought me up at last to such a pitch state that a greater
jolt than usual, repeated two or three times in succession, each
followed by a firmer pressure of her charming fingers, caused me such
an excess of excitement that I actually swooned away with the most
delicious sensation I had ever experienced in my life. I was some time
before I knew where I was, or what I was about, and was only made
conscious of our arrival at home by Miss Evelyn shaking me to rouse me
up. I stumbled up, but though partially stupefied, I fancied Miss
Evelyn's eyes shone with a brilliancy I had never before observed, and
that there was a bright hectic flush on her cheek. She refused to go
into the parlour, but hurried to bed on pretence of a headache.

When I retired to bed, and took off my shirt, I found it all sticky and
wet in front.

It was thus I paid down my first tribute to Venus. I thought long over
this evident approach to familiarity on the part of Miss Evelyn, and
went to sleep with a lively hope of a more private interview with her,
when I trusted that her evident passion would initiate me in the
pleasures to be derived from her beauteous body.

But again fate intervened, and another, not less beautiful, more
experienced, and more inclined for the sport, was to be my charming
mistress in love's revels.

Two days after this, Mr. Benson was unexpectedly called away on
pressing affairs, which he feared might detain him three weeks. He left
Mrs. B. with us. As he had to be driven about nine miles to the town
where the coach passed, mamma took the opportunity of going to the town
with him. Mrs. B. complained of not being equal to the fatigue, and
mamma told Miss Evelyn she would like her company, and as the two girls
wanted new shoes, they could go also; I was to remain at home, and
mamma desired me to be quiet and attentive to Mrs. Benson, who,
observing no one, said to me, with a peculiar look:

"I shall want you to hold my skeins, Charlie, so don't go out of the
way, but be ready for me as soon as they are gone."

She then went up to her bedroom, where Mr. B. immediately joined her,
no doubt to re-enact the scene I had already witnessed from the closet
on a previous day. They were fully half an hour occupied together. At
length, all was ready, and off they went, leaving me to a fate I had
little dreamt of.

Mrs. B. proposed we should go up to the drawing-room, which looked out
to the garden, and was nowhere overlooked. I followed her, and could
not help admiring her fine figure as she preceded me in going upstairs.
Although pale in complexion, she was well made, and very elegant in her
carriage, and sat down on a low easy chair, throwing herself completely
back, and crossing one leg over the other, apparently without being
aware that she carried her petticoats up with the action, and exhibited
the beautiful underleg up to the garter.

I had never forgotten the day, when secreted in the closet, I had seen
them completely exposed, and how charming they were. Her present
negligent attitude, although far from the same exposure I speak of, was
still, with the former recollection running in my head, enough to set
my whole blood on fire. I have before remarked what a power beautiful
and well-stockinged legs, and ankles and small feet, had upon my
nervous system, and so it was now. As I gazed upon her handsome legs,
ankles, and feet, I felt my prick swell and throb in a manner that
could not fail to be perceptible to Mrs. B, especially as her head lay
on a level with that part of my person as I stood before her.

Although she continued knitting, I could see that her eyes were
directed to that part of my person, and fixed upon the increasing
distention of my trousers. In a few minutes she gave me a skein of
worsted to hold, and desired me to kneel in front of her, so as to
bring my hands down to the level of the low chair on which she was
seated.

I knelt close to the footstool on which her foot rested; it was raised
up, and a very slight movement brought it against my person, at first
rather below where my throbbing prick was distending my trousers. As
she commenced to wind her ball, she gradually pushed her foot further
forward, until the toe actually touched the knob of my cock, and
occasionally moved it right and left, exciting me beyond measure.

Oh! it was extatic--my prick, swollen to its utmost size, seemed to
fill her exquisite vagina, which although capable of easily
accommodating the larger prick of Mr. B., appeared to be sufficiently
contracted to embrace tightly with its smooth and slippery folds my
stiff throbbing prick. So we continued, I shoving myself into her, and
she upheaving her beautiful bottom to meet me. My hands removed
everywhere, and my mouth sucked her lips and tongue, or wandered over
her pulpy breasts sucking their tiny nipples. It was a long bout
indeed, prolonged by Mrs. Benson's instructions, and she enjoyed it
thoroughly, encouraged me by every endearing epithet, and by the most
voluptuous manoeuvres. I was quite beside myself. The consciousness
that I was thrusting my most private part into that part of a lady's
person which is regarded with such sacred delicacy caused me to
experience the most enraptured pleasure. Maddened by the intensity of
my feeling I at length quickened my pace. My charming companion did the
same, and we together yielded down a most copious and delicious
discharge.

Although I retained sufficient rigidity to keep him in his place, Mrs.
B. would not allow any further connection with her, and she made me
withdraw, and bade me go to sleep like a good boy, and she would give
me a further lesson in the morning.

Finding that she was determined on this point, and that she disposed
herself to slumber, I felt I was obliged to follow her example, and at
last fell fast asleep. It might be about five in the morning, quite
light at that time of year, when I awoke, and instead of finding
myself, as usual, in my own little bed--I found my arms round the
person of a charming woman, whose large plump smooth bottom lay in my
lap, pressing against my belly and thigh. I found my prick already in a
rampant state, and it at once began throbbing and forcing its way
between the delicious cheeks of her immense bottom, seeking the
delightful sheath it had so enjoyed the previous part of the night.
Whether Mrs. B, was asleep or not, I do not know, but am inclined to
think she really was so, from the muttered mistake she made in waking.
She was probably dreaming, for she mechanically raised her thighs. I
pressed my prick stoutly forward against her luxurious body, knowing
that the entrance to the temple of pleasure which had so entranced me
the night before lay in that direction. I found more difficulties than
I expected, but at length began to penetrate, although the orifice
appeared much tighter than on the previous evening. Excited by the
difficulties of entrance, I clasped the lady firmly round the waist and
pushed forcibly and steadily forward. I felt the folds give way to the
iron stiffness of my prick, and one-half of it was fairly embedded in
my extremely tight sheath. I put down my hand to press my prick a
little downwards to facilitate the further entrance; you may imagine my
astonishment when on so doing I found myself in the lady's bottom-hole,
instead of her cunt. This at once explained the difficulty of entrance.
I was about to withdraw and place it in the proper orifice when a
convulsive pressure of the sphincter caused me such exquisite
satisfaction by the pressure of the folds on the more sensitive upper
half of my prick, which was so delicious, and so much tighter, and more
exciting than my previous experience of the cunt that I could not
resist the temptation of carrying the experiment to the end. Therefore,
thrusting my two fingers into her cunt, I pressed my belly forwards
with all my might, and sheathed my prick in her bottom-hole to its full
extent. Mrs. B at this awoke, and exclaimed, "Good Heavens! Fred, you
hurt me cruelly. I wish you would be content with my cunt, I shall be
unable to walk tomorrow. You know it always has that effect. It is
downright cruel of you--but since you are in, stay quiet a little, and
then continue to frig me with your fingers, as you know that eventually
gives me great pleasure."

She calls me Fred, what can she mean? I was, however, too agreeably
situated to speculate on anything, but as I was now buried within her
bottom-hole, I lay quiet for a few minutes as she had requested; and as
her complaints subsided, and I felt a slight reciprocating movement, I,
too, moved within her, working at the same time my two fingers in her
cunt. By this time she was wide awake, and became conscious of who was
her bed-fellow.

We clasped each other in a most enrapturing embrace, and then my lovely
and engaging companion allowed me to turn her in every direction so as
to see, admire, and devour every charm of her exquisitely formed body.
Oh! she was indeed beautiful--shoulders broad, bosom, or rather upper
neck, flat, not showing any projection of the collar bone; bubbies
firm, well separated and round, with most exquisite rosy nipples not
much developed; a perfect waist, small naturally, with charming
swelling hips, and an immense bottom--it was almost out of proportion,
large, but oh, how beautiful. Then her belly, undulating so enticingly,
and swelling out, the lowest part into a very fine and prominent mons
Veneris, covered with a thick crop of silky and curly light hair; then
the entrance to the grotto of Venus had such delicious pouting lips,
rosy, but with hair still thick on each side, which is often not the
case even with women who have a sufficient tuft above, how beautiful
where it exists as it did in this charming and perfect woman,
continuing in beautiful little curls not only down to but around her
lovely pinky and puckered little bottom-hole, the delights of which I
had already, in this infancy of my love education, tasted and enjoyed.
Her two alabaster thighs, worthily supporting by their large
well-rounded fleshy forms, the exquisite perfections of the upper body,
I have already described. How beautiful, elegant, and elongated her
legs were, rising from well-turned ankles and most tiny beautiful feet.
Her skin was white as milk, and dazzlingly fair and smooth. To my young
eyes she was a perfect goddess of beauty. Even now, in advanced life, I
can remember nothing that, as a whole surpassed her, although I have
met many with points unsurpassingly beautiful--some carry it in the
bosom, some in the general carriage, some in the mount of Venus and
bottom together, and some in legs and thighs; but this devine creature,
without having the appearance of it when dressed, was, when stripped,
perfect in all her parts as well as beautiful in face--caressing and
voluptuous by nature, and lending herself, with the most enchanting
graces to instruct me in all the mysteries of love, and let me say, of
_lust_ also.

We caressed each other with such mutual satisfaction that nature soon
drove us to a closer and more active union of the bodies. Fondly
embracing one another, we approached the bed, and being equally excited
threw ourselves upon it, and, in the exquisite contact of our naked
flesh, enjoyed a long, long bout of love, in which my most charming
companion exhibited all the resources of amorous enjoyment. Never shall
I forget the luxury of that embrace. She checked my natural tendency to
rush at once to a completion. I think we must have enjoyed the raptures
of that embrace fully half an hour before bringing on the grand finale,
in which my active companion showed the extraordinary suppleness of her
delicious body by throwing her legs over my back, pushing my bottom
forward with her heels, and raising and sinking her bottom in unison
with each thrust of my terribly stiff prick, which seemed to swell and
become thicker and harder than ever. In retiring from each thrust, her
cunt seemed to close upon my prick with the force of a pair of pincers.
We both came to the extatic moment at the same time, and both actually
screamed with delight; my ardent mistress in her fury of excitement
actually bit my shoulder and drew blood; but I felt it not--I was in
the seventh heaven of delight, and lay for long almost insensible on
her beauteous body, clasped in her loving arms. On coming to our senses:

"Oh, my beloved boy," she said, "never, never, have I experienced such
pleasure. You are a perfect angel. I only fear I shall come to love you
too much."

We turned on our sides without dislodging the dear instrument of our
enjoyment, and my lovely friend prattled on and delighted me with her
toying, embracing, and gaiety. My prick had once more swelled up, and I
wished to quietly enjoy a fuck in the luxurious position in which we
lay; but my lovely friend said--

"That must not be, my dear Charles, I must consider your health. You
have already done more than your age warrants, and you must rise and go
to your bed to recover, by a sound sleep, your strength."

"But feel how strong I am," and I gave a forcible thrust into her
glowing and well-moistened sheath. But, though she certainly was
greatly excited, she suddenly turned round and unseated me, and drew
away from me, refusing to take it again. As she was quite naked, the
movements of her beauteous form were most graceful and enchanting, and
one leg being thrown backwards left her lovely cunt full in view, and
actually gaping open before me. Seized with the strongest desire to
suck and kiss it, as I had done the night before, I begged that at
least she would grant me that last favour, as it could not in any way
do me harm. To this she readily consented, and lay down on her back,
opening her glorious thighs, and with a pillow under her bottom so as
to raise up her cunt into a better position for me to gamahuche her, as
she called it. Before letting me begin, she said--

"My dear Charles, do you see that little projection at the upper part
of my quim, that is my clitoris, and is the site of the most exquisite
sensation; you see it is rather hard, even now, but you will find as
you titillate it with your tongue or suck it, that it will become
harder and more projecting, so apply your lips there."

I did as my lovely mistress desired, and soon found it stiffen and stand
up nearly an inch into my mouth.

The convulsive twitches of her buttocks, the pressure forward of her
hand on my head, all proved the exquisite felicity my lovely friend was
enjoying. I slipped my hand under my chin--the position was awkward,
but I managed to thrust my thumb into her cunt. My forefinger was
somewhat in the way--but finding it exactly opposite the rosy hole of
her bottom, and all being very moist there, I pushed it forward and it
easily entered. I could not move my hand very actively, but I continued
to gently draw my finger and thumb a little back together, and then
thrust forward again. It seemed to add immensely to the pleasure I was
giving her; her whole body quivered with excessive excitement. My head
was pressed so firmly against her cunt that I had difficulty in
breathing, but I managed to keep up the action of tongue and fingers
until I brought on the exquisite crisis--her buttocks rose, her hand
pressed hard on my head and her two powerful and fleshy thighs closed
on my cheeks on each side and fixed me as if in a vice, while she
poured down into my mouth and all over my chin, neck, and hand a
perfect rush of sperm, and then lay in convulsive movements of
enjoyment, hardly knowing what she was doing. As she held me so fast in
every way, I continued to lick up the delicious discharge, and
continued at the same time to pass my tongue over her clitoris. This,
by producing a new excitement, brought her senses round. So relaxing
her hold of me with her thighs she said--

"Oh, my darling Charles, come up to my arms that I may kiss you for the
exquisite delight you have given me." I did so, but took care, in
drawing myself up, to engroove my stiff-standing prick in the
well-moistened open cunt that lay raised on a pillow so conveniently in
the way.

"Oh, you sad traitor," cried my sweet companion. "No, I cannot, I must
not allow it," but I held her tight round the waist, and her position
was too favourable for me to be easily unhorsed.

"Ah! you must not, my dear boy. If you will not consider yourself,
consider me. I shall be quite exhausted." I shut her mouth with my
kisses and tongue, and soon the active movements I was making within
her charming vagina exercised their usual influence on her lubricity,
so as to make her as eager for the fray as myself.

"Stop, my dear Charles, and you shall have it in a new position, which
will give you as much more pleasure as it will me."

"You are not going to cheat me, are you?"

"Oh, no! my darling, I am now as much on fire as you are--withdraw."

I obeyed, half in fear. My fair mistress turned herself round, and
getting on her hands and knees, presented to my ardent gaze her
magnificent bottom. I thought she meant me to once more put it into the
rosy little orifice, and said so.

"Oh! no," she replied, "not there"; but putting her hand under her
belly, and projecting it backwards between her thighs, she said--

"Give it me and I will guide it into the proper place."

Before doing so I stooped forward and pushing my face between the
glorious cheeks of her bottom, sought and found the lovely little
orifice, kissed it, and thrust my tongue in.

"Oh! don't Charles, dear, you tickle me so," then flinching, and
squeezing her buttocks together, I had nothing for it but to put my
prick in her hand. She immediately guided it to and engulphed it in her
burning cunt up to the very hair. I found I apparently got in fully an
inch further this way--the position also gave my beautiful instructress
more power of pressure on my prick--then her glorious buttocks, heaving
under my movements, and exposed in all their immensity, was most
exciting and beautiful. I seized her below the waist with a hand upon
each hip, pressing her magnificent backside against me each time that I
thrust forward. Oh! it was indeed glorious to see. I was beside myself,
and furious with the excitement the view of all these charms produced
upon me. My charming mistress seemed equally to enjoy it, as was
evinced by the splendid movements of her body; till at last overcome by
the grand finale, she sank forward on her belly, and I followed on her
back, without losing the position of my throbbing prick within her. We
both lay for some time incapable of movement, but the internal
squeezing and convulsive pressure of her cunt on my softened, but still
enlarged prick, were exquisite beyond imagining. At last she begged me
to relieve her. Getting out of bed, she sighed deeply, kissed me
tenderly, and said, "My dear Charles, we must not be so extravagant in
future, it will destroy us both--come, let me see you to your bed." The
sight of my lovely mistress standing naked in all the glory of her
beauty and perfection of form began to have its usual effect upon my
prick, which showed symptoms of raising his head again; she gave it a
pat, stooped down, and for a moment plunged its head into her beautiful
mouth, then seizing my night-shirt, she threw it over my head and
conducted me to my own bed, put me in, tucked me up, and tenderly
kissing me, left the room, first unlocking my door and then locking the
door of communication between the two rooms. Thus passed the first
glorious night of my initiation into all the rites of Venus, and at the
hands of a lovely, fresh and beautiful woman, who had only been married
long enough to make her a perfect adept in the art. Never, oh never!
have I passed such a night. Many and many a fine woman, perfect too in
the art of fucking, have I enjoyed, but the novelty and the charm, the
variety and the superiority of the teacher, all combined to make this
night the _ne plus ultra_ of erotic pleasure.

"There, darling, that will do for the moment; I want to have some talk
with you. First, let me thank you for your very discreet behaviour this
day, it quite justifies the confidence I had in you. Your story of the
dream was capital, and just suited the purpose. I hope, my dear
Charlie, that under my auspices you will become a model lover--your
aptitude has already proved in several ways. First and best, with all
the appearance of a boy, you are quite a man, and even superior to
many. You have already shown great discretion and ready wit, and there
is no reason to fear that you will become a general favourite with our
sex, who soon find out who is discreet and who is otherwise--discretion
is the trump card of success with us. Alas! few of your sex understand
this. Let me impress one lesson on you, my dear Charles. You and I
cannot continue long on our present footing. My husband will return and
carry me away, and although circumstances will throw us at intervals
into each other's arms--for you may be sure you will be always welcome
to me--yet my very absence will force you to seek other outlets to the
passions I have awakened and taught their power. I have one piece of
advice to give you as to your conduct to newer lovers--for have them
you must, my dear Charles, however much you may fancy yourself now
attached to me; with these, let them all for some time imagine that
each possesses you for the first time. First of all, it doubles their
satisfaction, and so increases your pleasure. Your early discretion
causes me to think that you will see all the advantages of this
conduct. I may add that if they suppose you have had previous
instruction, they, if they are women, will never rest until they have
drawn from you the secret of your first instructress. You might, of
course, tell some tale of a 'cock and a bull,' but in searching for the
truth and cross-questioning you when you are least aware of it, they
will lead you into contradictions, and the truth will at last be
ferreted out. Now this would be unjust to me, who have risked a good
deal to give you the delightful instructions of last night, and, as I
hope, of many more. So you see, my dear Charles, in all early cases you
must enact the part of an ignoramus _seeking_ for instruction, with
vague ideas of how to set about it. I hope, while I am near you," she
added, "no such occasion will arise, but I feel certain, with your
passions and your power, dear, darling fellow--push away--I!--I!--I
feel for cer--certain they will ar--arise."

Thus ended the very wise and excellent advice this charming woman was
giving me. Do not imagine that I did not pay great attention, and,
indeed, her very reasonable maxims became the guide of my after-life,
and I owe to them a success with women rarely otherwise obtained. Her
sensible remark had been drawn out to such a length, that my prick had
so far rebelled that he had throbbed inside of her delicious cunt so
forcibly as to produce a happy movement of her body that interrupted
and cut short her words.

"Charlie, my darling, pass your middle finger down and rub it on my
clitoris, and then suck the nipple of my bubby next you, and work away
with your glorious prick."

I did as desired. She seconded me with an art quite peculiar to
herself, and at last we both died away in that love's death which is so
overpowering and so delicious. The glorious position we were in
rendered it almost impossible to lose ground, spend as often as you
please; but if my prick had been one that would have shrunk to nothing,
the wonderful power of retaining it within her possessed by my
delicious mistress would have prevented the possibility of exit.

In after-nights I have often fallen sound asleep with it entirely
engulphed within her, and awoke hours afterwards to find her
extraordinary power of retention had held him firm, notwithstanding his
having shrunk up to a mere piece of doughy flesh. In this instance,
after recovering our senses, I still retained my place, and we
recommenced our conversation; my lovely instructress giving me many and
most useful hints for my after-life. I have often since dwelt on the
wisdom of all she so charmingly taught me, and wondered how so young a
woman could have so thorough a knowledge of her sex and the world. I
suppose love is a great master and inspired her on this occasion. I may
here remark that for forty years afterwards this charming woman and I
remained the fastest of friends after being the most ardent of lovers.
She was the depository of all my erotic extravagancies, and never
showed any jealousy, but really enjoyed the recital of my wildest love
combats with others.

Alas! death at last took her from me, and I lost the mainstay of my
existence. Forgive this digression, but I am writing long after these
events, and sorrows will have their vent. Woe is me!

To return to present joys. We continued talking and toying, until I was
again anxious to commence love's combat. My prudent mistress wished me
to finish for the time, and to sleep and refresh ourselves for renewed
efforts; but youth and strength nerved me for the fight, and being
securely fixed, I held her as in a vice, with my thighs around only one
of hers that could have allowed her to escape. Passing my finger down
on her stiffened clitoris I so excited her that she had no wish but to
bring matters to a crisis.

"Stop, my dear," she said, "and we will renew our pleasure in another
attitude."

So withdrawing her leg off my loins, she turned on her side, so as to
present her glorious buttocks before me, and pressed them into my belly
and against my thighs, which seemed to introduce my prick even further
than he was within before. Besides, in all these positions, where a
woman presents her splendid backside to you, it is always more
exciting, and has a greater hold of you than any other way. We did most
thoroughly enjoy this splendid fuck, and without withdrawing, both fell
into the sweetest imaginable slumber. This was one of those occasions
in which, having fallen asleep engulphed, I awoke some five hours
later, to find my prick still lightly held within the velvety folds of
one of the most delicious cunts ever created for the felicity of man,
or, I may say, woman either. You may easily imagine how soon my prick
swelled to his wonted size on finding himself still in such charming
quarters. I let him lay quite still, barring the involuntary throbs he
could not avoid making, and bending my body away from my lovely
mistress, I admired her breadth of shoulders, the beauty of her upper
arm, the exquisite _chute_ of her loins, the swell of her hips, and the
glorious projection and rotundity of her immense buttocks. I slowly and
gently pushed in and out of her juicy sheath, until, awakened by the
exquisite sensations of my slow movements, all her lubricity was
excited, and we ended one of our most delicious encounters, finishing,
as usual, with a death-like exhaustion. She declared I had done enough
for one night, and jumping out of bed, compelled me to betake myself to
my own room, where, I must confess, I very shortly slept as sound as
could be, without at the same time oversleeping myself.

Thus passed several successive nights, until the fall of the moon, when
one day Mrs. B. complained of headache and feeling unwell. I was very
much alarmed, but she took occasion to tell me it was quite natural,
and she would explain to me how it was so at night. I was obliged to be
content with this. At night, she came and sat on my bed, and told me
all the mysteries of the case. How women, not with child, had these
bleedings monthly, which, so far from being hurtful, were a relief to
the system, and that they happened at the full or the new moon,
generally at the former. Further, that all connection with men must
cease at such a time. I was in despair, for my prick was stiff enough
to burst. However, my kind and darling mistress, to relieve me from the
pain of distention, took my prick in her mouth, and performed a new
manoeuvre. Wetting her middle finger with her saliva, she thrust it up
my bottom-hole, and worked in unison with the suction of the knob, and
the frigging of the root of my prick with the other hand. I had a most
exquisite and copious discharge, the pleasure being greatly enhanced by
the action of the finger up my fundament. My charming mistress
swallowed all I could give her, and did not cease sucking until the
last drop had exuded from my throbbing prick.

I was obliged to be satisfied with this, and my mistress informed me I
could have no more enjoyment for four or five days; which, to my
impatience, was like condemning me to as many ages of hope deferred. I
observed, while she was kissing me, that her breath had a peculiar
odour, and I asked her what she had been eating.

"Why do you ask, my dear boy?"

"Because of the difference of your breath, generally so sweet and
fragrant."

She smiled and said it was all from the same cause she had just been
explaining to me, and was very generally so with women at that period.
I mention this because it was the means of my discovering that Miss
Evelyn was exactly in the same state. She had continued her endearing
caresses without proceeding much further than I have already described,
except more frequently kissing me. She now always did so on first
entering the school-room, and also when we were dismissed. I suppose to
prevent an observation or inference, she had adopted the same habit
with my sisters. On this day, having drawn me with her arm round my
waist close to her, when she kissed me I felt the very same odour of
breath that I had observed in Mrs. Benson. She too was languid that day
and complained of headache. I also observed a dark line under her eyes,
and on afterwards observing Mrs. B., saw precisely the same--so I
became convinced they were unwell from the same cause. Mrs. B. had told
me that most women were so at the full of the moon--which was then the
case.

The next day my mother proposed to drive to town, and probably knowing
the state of the case, asked Mrs. B. and Miss Evelyn to accompany her,
as she thought the airing would be beneficial. They at once
accepted--my younger sister cried out, "Oh, mamma, let me go with you
also." Mary interposed, and thought she had the best right--but Lizzie
said she had spoken first. I managed to give Mary a wink and a shake of
the head, which she instantly comprehended, so gracefully giving way,
although with apparent reluctance, it was arranged that Eliza should
accompany the ladies. I now felt my opportunity was at hand to initiate
my darling sister into the delightful mysteries that I had just been
myself instructed in.

At eleven o'clock the carriage drove up, and we stood looking after
them until they were lost to sight. Then returning into the parlour,
Mary threw her arms round my neck, and kissing me, said--

"Oh! I am glad, Charlie, you winked to me, for now you know we can do
as we like, and you can tell me all about this secret, and you must
kiss my little Fanny as you did before, it was so nice. I have thought
of nothing else, but how to have it done again."

"Well, my darling, I shall do all that, and more, but we cannot do so
here. I tell you what we will do--we will pretend to go for a long walk
in the country, but instead of that, we will pass through the shrubbery
into the orchard and hazelwood, and so gain the little remote summer
house, of which I have secured the key; there we shall be safe from all
observation."

This little summer house was at some distance from the house, and in a
lonely corner of the orchard, raised on an artificial mount, so that
its windows should command a lovely view beyond the walls of the
grounds. It was about ten feet square--was beautifully sheltered, and
the ladies in summer took their work there, and occupied it for hours
every fine day; so it was furnished with tables and chairs, and on one
side a long couch without a back. It had already entered into my idea
that this was the spot I should contrive to get to with Mary--little
thinking how chance would throw so glorious an opportunity in my way so
soon. It was always kept locked to prevent it being used by the
servants, gardeners, or others. I knew where the key was kept, and
secured it when the ladies were dressing for their drive--so after
staying sufficiently long to prevent any suspicion, and saying then we
were going for a long walk in the country, so as to prevent them
seeking for us at the summer house if any visitors should chance to
call, we sallied out, but re-entered the grounds where we could not be
observed, and speedily gained the spot we had in view--entered and
locked the door. Then I drew down the blinds, threw off my coat and
waistcoat, and told Mary to take off her shawl and bonnet, and outer
gown.

"But why all this, Charlie, dear?"

"First, my darling--all those are in the way of kissing and toying with
your charming little Fanny, and next, I don't want anything to appear
tumbled when we go back."

This was enough, and she did everything as I desired, indeed, more, for
she took off her petticoat and little corset, saying she would be
cooler thus. So, following her example, I took off my trousers, saying
she would be better able to see and play with my doodle. When these
preliminaries were accomplished, I drew her on my knees--first pulling
up her shift and my own shirt, so that our naked flesh should be in
contact. Seeing that her chemise fell off from her bosom, I first felt
her little bubbies, which were beginning to develope themselves, and
had the tiniest little pink nipples that even my lips could hardly get
hold of. She had pulled up my shirt to look again at the great change
that had occured to my prick--of course, our preliminaries had already
excited it to a stiff-standing position.

"Oh, Charlie, what a size it is to be sure; and how nice to pull this
skin over its head; look how it runs back again. Oh! how funny!"

It was time to stop this, or she would have soon made me discharge.

"Well, then, what is this great secret, and what has it to do with your
doodle and my Fanny?"

"I will tell you, but you must never say a word to a soul--not even to
Eliza, she is too young yet."

"Well, go on."

"I was one day seeking something in the closet in Mrs. Benson's room,
when I heard them coming, and had only the time to slip into the
closet. They entered, locked the door, and Mr. B. laid her on the bed,
and lifted up all her petticoats so that I saw her Fanny quite
surrounded with hairs, as yours will be by and by. Mr. B, stooped down,
and applied his tongue as I did to you the other morning."

"Oh, yes; and it was so nice, Charlie!"

That is exactly what Mrs. B. said when he had done. Then he pulled out
his doodle, such a size, much bigger than mine, and whipped it into her
Fanny. I was quite frightened, and thought he must have killed her. But
no, it went in quite easy; and she hugged and kissed him while he
pushed it up and down for some time, till they both stopped all at
once. He then drew it out, hanging down all wet, and asked if it had
not given her great pleasure. 'Delightful,' she said. I have now got
used to it, but you know you hurt me, and made me so sore the first
time you did it.' After this they left the room, and I got away without
being discovered. But I found out what our two things were made for, we
will do as they did, so lie down on the couch whilst I kneel at the
end, and begin in the way I kissed it the other morning."

"Oh, Charlie, if it is all like that, I shall be so pleased with it."

Down she squatted, drawing up her chemise. My hand wandered all over
her charming belly and mount. Then kneeling down, and putting her legs
over my shoulders, and my hands under her thighs and bottoms, I applied
my tongue at once to her little clitoris, which I found was already
stiff, and showing its head at the upper part of her pinky slit. The
action of my agile tongue produced an instantaneous effect--her loins
and thighs heaved up her bottom to press her little pouting cunt
against my face. Mechanically she put her hand on my head, and muttered
terms of endearment--

"Oh, darling Charlie, how delicious! Oh! do go on! it is so nice, &c."

I wanted no stimulant, but licked away until, with shortened breath;
and greater heavings of her body, she began to stammer--

"Oh! oh! I feel so queer--ah, stop; I am going to faint--I, I, I,
can't--can't bear it any longer--oh!--oh!" Her limbs relaxed, and she
died away in her first discharge, which was very glutinous and nice,
but only scanty in quantity. I let her quiet until she came to; then
looking in her face, and smiling, I asked her how she liked it.

"Oh! I was in heaven, dear Charlie, but I thought it was killing me
--it was almost too much to bear--nothing could be more delicious."

"Oh, yes!" I replied, "there is something more delicious still, but, I
must kiss you in this way again before we try the other; the more moist
the inside is the easier I shall get in."

"But, Charlie, you don't mean to say you will ever get in your doodle,
now that it has grown so big."

"Well, we will try, and if it hurts you too much we can stop."

So I began again to gamahuche her; this time it took a longer effort to
produce the ultimate result; but apparently with still greater effect,
and a more copious discharge. Her little cunt being now relaxed, and
well moistened with her own discharge and my saliva, and well inclined
to receive my prick, I spat upon it and lubricated it from head to
root. Then rising from my knees, I stretched myself over Mary's belly,
and gently directing my prick, and rubbing it up and down first between
the lips, and exciting her clitoris by the same action, I gently and
gradually inserted its head between the lips of her charming little
cunt. There was less difficulty than might have been expected, the
gamahuching and double spending had relaxed the muscles, and her
passions being excited also acted on her organs of generation; at all
events, I got in the head, and about two inches of its length without
her murmuring anything beyond--

"How big it feels--it seems to stretch me so."

All this was exciting me dreadfully, and it was only by the greatest
effort that I did not thrust rudely forward. I now felt I was pushing
against some obstacle, I thrust hard and hurt her. She cried out,
begged me to stop. I was so near the finale that I felt I must go on.
So, plunging forward, I rushed, at the impediment, and made her cry out
most lustily. Probably another push would have decided my position, but
nature could hold out no longer, and I yielded down my erotic tribute
to her virginal charms, without having actually deflowered her. So far,
perhaps, it was fortunate, because I poured into her a torrent of sperm
which was not only balm to her partially wounded hymen, but so relaxed
and lubricated the interior of her cunt as greatly to facilitate my
after-efforts.

I lay quiet still for some time, and the gradual swelling out and
throbbing of my prick reawakened her young passions. She said--

"Charlie, my dear, you said that it would prove delicious in the end,
and I can feel it is becoming so. I have no more pain, and you shall go
on just as you like."

As my prick stiffened at her endearing words and involuntary pressures,
and as I had it completely under control, since I had taken the edge
off its immediate appetite by the last discharge, I held it literally
well in hand; and as I had lost no ground by withdrawing, I started
with the advantage of possession. First I slipped my hand down between
our two bellies and began frigging her clitoris, which immediately
excited her passions to the highest pitch.

"Oh! Charlie, dear, now push it all in--I do so long for it--and I
don't care how it hurts me."

I had been giving short thrusts more to stimulate her passions than to
alleviate my own; and as she was totally unaware of what was going to
happen, she widened her thighs and heaved up her bottom, expanding her
vagina in the act. I gathered my strength together, and as my cock was
standing as stiff as iron, I suddenly drove it forward, and felt that I
broke through something, and gained two inches more insertion at least.
The effect on my poor sister was most painful, she shrieked out
lustily; strove hard to unsheath me, wriggled her body in all
directions to effect this; but I was too securely engulphed for that,
and all her struggles only enabled me the more easily to sheathe him up
to the very hairs. So excited was I by her tears and screams, that I
was no sooner there than a torrent of sperm burst from me, and I lay
like a corpse on her body, but perfectly maintaining the ground I
possessed. This death-like quiet lasted some minutes, and, to a certain
extent, assuaged the violence of the pain I put poor Mary to.
Doubtless, also, the balmy nature of the ample quantity of sperm I had
shot up to her womb helped to soothe her suffering. At all events, when
we were both able again to converse, she unbraided me with the agony I
had caused her, and wished me to get off her at once; but retaining the
advantageous possession of her very tight and delicious sheath, I told
her all was now over, and we might look forward to nothing but
enrapturing pleasure.

Some minutes had elapsed in these remonstrances on one side; and
coaxings on the other, when I suddenly felt her charming little cunt
actually throb upon and give an involuntary squeeze to my prick, which
was still throbbing her. He was far too ready to stand at any time,
still more when engulphed in the exquisite young cunt he had just
initiated into love's mysteries--_bref_--he stood stiff as ever, and
Mary, at first with a shudder of fright, then with all the energy of
awakened passion, began to move her body under me. I held off from any
interference, feeling certain that if the desire came naturally to her
it would doubly enhance my own pleasure. My foresight did not fail me.
Mary's passions became fully aroused, and when so, the trifling
soreness passed out of mind, and we actually had a most delicious fuck,
in which my prick appeared as if in a vice, and Mary wriggled her
backside almost as well as the more artistic movements of Mrs. Benson.
All things must come to an end, but this did so amid screams of delight
on both sides. This single bout began and finished the education of my
darling sister. She hugged and fondled me afterwards, declaring I was
quite right in telling her pleasure followed pain; for nothing could
exceed the enrapturing nature of the sensation my prick had produced.
She thought now that it was not a bit too big, but just made to give
the utmost satisfaction. We remained locked in each other's arms, my
prick still engulphed in its tight and exciting sheath. We fondled and
prattled, until it became again in a state of violent erection, equally
stimulating her tight little cunt, so that we were forced to recommence
our love encounter. I found that my dear little sister possessed
naturally the power of throbbing on or nipping a prick, which the
French call casse-noisette. It is a great gift and adds immensely to
the man's pleasure, and I should think to the woman's too. In my
sister's case it began from the very first complete insertion of my
prick and the years that I afterwards continued to fuck her added
nothing to this delicious accomplishment, except in the variety of
positions in which it could be exercised.

The dear girl was in extasies at the pleasure she had received, and at
the pain which seemed to be past. Oh! she was so sweetly caressing that
I could not withdraw from her, and we fondled and toyed until again my
cock rose to his first vigour, and she nothing loath, began her new and
naturally taught gift of bottom upheavings and cunt pressures until
again we sank exhausted in the death-like ending of love's battles. On
recovering our senses, I was obliged to withdraw and relieve my sister
of the dead weight of my body on her person.

It has always struck me as extraordinary how the most delicate women
will support a heavy man on their persons, not only without flinching,
but even with ease and pleasure--but so it is. On rising and
withdrawing, we were both alarmed to see that my prick was all bloody,
and that blood and semen were oozing from her cunt. We had no idea this
would be the case, and at first I was as frightened as she was. A
moment's reflection showed me that it was only the natural result of
forcing my way in, and that the pleasure since enjoyed proved it to be
of no consequence. I soon convinced and calmed my sister on the
point--fortunately the sofa covering was red, and applying my
handkerchief, I wiped up all the semen mixture, and, in fact, no marks
remained; the same handkerchief wiped all results from Mary's dear
little cunt, and as her shift had been kept well up, fortunately no
stains appeared upon that.

We now ate some luncheon and drank some wine that we had prudently
brought with us. We then began playing and romping together--she
wanting always to get hold of my prick, and I to pull her about in
every way. It was gloriously warm weather, so I proposed we should off
with every thing. In a trice we were as naked as we were born, and flew
into each other's arms in a frenzy of delight, then we had a mutual
thorough inspection. My darling sister gave every promise of becoming a
magnificent woman--her shoulders were already wide--her arms well
shaped, although still thin--her waist small--the swell of the hips
already well developed--as to her bottom, it stuck out well and hard
behind, quite charming to see, and giving promise of very ample
dimensions hereafter.

I made her kneel on the low couch, with her head well up and her thighs
open; kneeling behind, I gamahuched her until she spent; then rising,
shoved my prick into her cunt, in her then position, and had a
downright good poke, which she, too, found was a way that gave her
extra excitement. We passed thus some hours in mutual delights. I
taught her the side fuck which had so charmed me with my delightful
instructress, and I found dear Mary even an apter scholar than myself
had proved. The afternoon advancing, we dressed, and eradicating all
signs of what we had been doing, returned to the house, mutually
promising to keep thoroughly secret all that had passed and agreeing
that no sign of unusual familiarity should escape us. I strongly
advised Mary to get some warm water and bathe her cunt well, for, as
may be supposed, I had taken the opportunity of teaching her the true
erotic language as applied to the organs of generation of both sexes,
and the name of the connection itself, "fucking."

Thus delightfully ended the first lesson in love taught to my sister,
and such was my first triumph over a maidenhead, double enhanced by the
idea of the close ties of parentage between us. In after-life, I have
always found the nearer we are related, the more this idea of incest
stimulates our passions and stiffens our pricks, so that if even we be
in the wane of life, fresh vigour is imparted by reason of the very
fact of our evasion of conventional laws.

We had both returned to the drawing-room for more than an hour before
the arrival of the ladies. Dear Mary complained of feeling sore and
stiff in every limb. I had advised her to lie down on the sofa and try
to sleep. I did the same, and happily we both dozed off, and never
awoke until the loud rat-tat of arrival at the house door roused us up.
I told Mary to hide all appearance of pain, and only to say, as an
excuse for going early to bed, that we had gone further afield than we
at first intended, and that she was very tired. We were both sent early
to bed, for I was still treated as quite a boy, and I was sound asleep
when my charming Mrs. B. woke me up by her warm caresses. I could well
have spared them that night, but when did one of my years not respond
to the endearments of the woman he loved, and who yielded all to him.
She sucked me dry as usual, and I slept soundly till morning.

The next three days passed without anything to record. Mary did not
allow her real soreness to appear, but heroically went through her
sufferings, for she told me afterwards she felt very severe pains all
over, doubtless her whole nervous system had been overexcited, and this
was the natural reaction; it was so far fortunate that not a shadow of
a chance of our having fresh connection occurred, so she had time to
perfectly recover from the ill effects of her first initiation into the
erotic raptures. I continued to have the relief each night of the
charming mouth of my loved and beautiful instructress. At last, the
abominable _menses_, as she called them, were past and gone. For a full
twenty-four hours after, she would not allow me to reassume all the
privileges she had previously granted, and admit me to share her bed.
She told me this was necessary to prevent any recurrence, and also that
in some cases a virulent white discharge occasionally followed for some
hours, sufficiently acrid to affect my local health, and "that," she
added, "was now too precious in her estimation to risk it in any way."
I thought it hard at the time, but it was only another proof of the
thoughtful wisdom of this estimable woman. At last, I was again in full
possession of her charming person. Oh! how we did revel in all the
luxuries and lubricity; almost every night my enchanting friend found
some new position to vary and enhance our erotic raptures. One new dose
was laying me down flat on my back, then straddling over me, she sank
on her knees, and with body erect, lifted up or rather bent back my
stiff-standing prick, until he was fairly below her open cunt, then
guiding it exactly to the proper entrance, she sank her body slowly
down upon it until fully engulphed, hair crushed hair, then as slowly
raising again, she drew off until all but the nut was uncovered, to
again sink down. In this position we could both see the whole process.
At length, becoming too excited, she sank on my bosom, then one arm and
hand pressed her splendid buttocks down on my throbbing prick after
every elevation of her magnificent backside while my other hand,
doubling round behind her, introduced the middle finger up her charming
bottom-hole, and worked in and out in unison with both our heaving
movements, until stopped by the grand crisis, when death-like langour
overcame us both almost at the same moment. I must not forget to
mention that from time to time I paid a visit to the small and rosy
orifice that lay so near to the more legitimate altar of Venus. It was
a variety of enjoyment that my lovely mistress acknowledged to me she
at times felt much inclined to enjoy, but only after having the front
path of pleasure well fucked and lubricated with sperm, which alone
caused the other mucous membrane to feel inclined that way.
