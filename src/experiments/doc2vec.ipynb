{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from IPython.display import clear_output\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "from IPython.core.display import display, HTML\n",
    "import locale\n",
    "locale.setlocale(locale.LC_ALL, 'de_DE.utf-8')\n",
    "\n",
    "# used for loading or saving\n",
    "model_file = '/home/ramin/projects/ECO/src/python/modelbuilder/parsed_v3_valid.doc2vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4266193, 'sentences')\n"
     ]
    }
   ],
   "source": [
    "# 2 Build sentence list (each sentence needs at least 1 tag)\n",
    "filename = '/home/marcel/drive/data/eco/NAIL_DATAFIELD_txt/parsed_v3/parsed_v3_valid.txt'\n",
    "\n",
    "sentences = []\n",
    "from random import shuffle\n",
    "\n",
    "for uid, line in enumerate(open(filename)):\n",
    "    ls = gensim.models.doc2vec.LabeledSentence(words=line.split(), tags=['SENT_%s' % uid])\n",
    "    sentences.append(ls)\n",
    "print(len(sentences),'sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "# 3 TRAINING OR LOADING the doc2vec model and save it\n",
    "# ALTERNATIVE: LOAD THE MODEL IN THE NEXT CELL\n",
    "\n",
    "# tutorial https://rare-technologies.com/doc2vec-tutorial/\n",
    "# proposes shuffling or learning reate adjustment. we gonna do both\n",
    "# in total 20 epochs\n",
    "# took ca. 6.30 hours\n",
    "\n",
    "# FOR SAFETY REASON, BUILD ONLY WHEN FLAG IS SET\n",
    "\n",
    "train_model = False\n",
    "do_tests = True\n",
    "\n",
    "if train_model:\n",
    "    model = gensim.models.Doc2Vec(alpha=0.025, min_alpha=0.025)  # use fixed learning rate\n",
    "    print('building vocab') \n",
    "    model.build_vocab(sentences)\n",
    "\n",
    "    base_alpha = model.alpha\n",
    "    base_min_alpha = model.min_alpha\n",
    "\n",
    "    for mepoch in range(2):\n",
    "        model.alpha = base_alpha \n",
    "        model.min_alpha = base_min_alpha\n",
    "        for epoch in range(10):\n",
    "            print('epoch',mepoch * 10 + epoch)\n",
    "            model.train(sentences)\n",
    "            model.alpha -= 0.002  # decrease the learning rate\n",
    "            model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "        shuffle(sentences)\n",
    "\n",
    "    # saving the model    \n",
    "    model.save(model_file)\n",
    "    print 'model trained and saved'\n",
    "else:\n",
    "    model = gensim.models.Doc2Vec.load(model_file)\n",
    "    print 'model loaded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 6 Tiny helper functions\n",
    "\n",
    "def print_word_list(wl):\n",
    "    str =  ' '.join(wl)\n",
    "    pattern = re.compile('\\s.\\s')\n",
    "    shift = 0\n",
    "    for ma in pattern.finditer(str):\n",
    "        str = str[:ma.start(0)-shift]+ma.group(0)[1:]+ str[ma.end(0)-shift:]\n",
    "        shift +=1\n",
    "    if str[-2] == ' ':\n",
    "        str = str[:-2] + str[-1:]\n",
    "    return str\n",
    "\n",
    "def get_print(sentence_or_similar):\n",
    "    if type(sentence_or_similar) is gensim.models.doc2vec.LabeledSentence:\n",
    "        word_list = sentence_or_similar[0]\n",
    "    elif type(sentence_or_similar) is int: # just an index\n",
    "        word_list = sentences[sentence_or_similar][0]\n",
    "    else: # TaggedDocument class\n",
    "        word_list = sentences[int(sentence_or_similar[0][5:])][0]\n",
    "    return print_word_list(word_list)\n",
    "\n",
    "    \n",
    "def get_index_tag(sentence):\n",
    "    return sentence[1][0]\n",
    "\n",
    "def get_index(sentence_or_similar):\n",
    "    if type(sentence_or_similar) is gensim.models.doc2vec.LabeledSentence:\n",
    "        return int(get_index_tag(sentence_or_similar)[5:])\n",
    "    else:\n",
    "        return int(sentence_or_similar[0][5:])\n",
    "    \n",
    "def equal_word_lists(index1, index2):\n",
    "    wl1 = sentences[index1][0]\n",
    "    wl2 = sentences[index2][0]\n",
    "    if len(wl1) != len(wl2):\n",
    "        return False\n",
    "    else:\n",
    "        for i in range(len(wl1)):\n",
    "            if wl1[i] != wl2[i]:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def get_lab_sent_by_similar(similar):\n",
    "    print get_index(similar)\n",
    "    return sentences[get_index(similar)]\n",
    "\n",
    "def get_similarity_by_index(index1, index2):\n",
    "    return model.docvecs.similarity(index1,index2)\n",
    "\n",
    "# HTML Helper\n",
    "def pack_into_elem(tag, clazz, content):\n",
    "    return '<' + tag + ' class=\"' + clazz + '\"> ' + content+ ' </' + tag +'>'\n",
    "\n",
    "pre = '''<style>\n",
    "          .act {font-weight: bold}\n",
    "          .i {color: grey}\n",
    "          .sim {color: orange}\n",
    "          .n {color: blue}\n",
    "          .p {color: red}\n",
    "          .r {color: green}          \n",
    "     </style>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Haven, CT: Yale University Press.\n",
      "similar sentence 30\n",
      "\n",
      "SIMILAR SENTENCES\n",
      "\n",
      "New Haven, CT: Yale University Press. ('SENT_2560974', 0.9662174582481384)\n",
      "New Haven, CT: Yale University Press. ('SENT_2310654', 0.9655746221542358)\n",
      "New Haven, CT: Yale University Press. ('SENT_2724632', 0.9649016857147217)\n",
      "New Haven, CT: Yale University Press. ('SENT_2859928', 0.9643991589546204)\n",
      "New Haven, CT: Yale University Press. ('SENT_1940547', 0.964377760887146)\n",
      "New Haven, CT: Yale University Press. ('SENT_2130561', 0.9637207984924316)\n",
      "New Haven, CT: Yale University Press. ('SENT_2957770', 0.9633453488349915)\n",
      "New Haven, CT: Yale University Press. ('SENT_313266', 0.963297426700592)\n",
      "New Brunswick, NJ: Rutgers University Press. ('SENT_4208346', 0.9628815650939941)\n",
      "New Haven, CT: Yale University Press. ('SENT_2871939', 0.9628716707229614)\n",
      "New Haven, CT: Yale University Press. ('SENT_2131374', 0.9623834490776062)\n",
      "New Haven, CT: Yale University Press. ('SENT_44934', 0.9622904062271118)\n",
      "New Haven, CT: Yale University Press. ('SENT_1723810', 0.9617049694061279)\n",
      "New York: Oxford University Press, 1996. ('SENT_1416359', 0.9614766836166382)\n",
      "New Haven, CT: Yale University Press. ('SENT_2364576', 0.9614515900611877)\n",
      "New Haven, CT: Yale University Press. ('SENT_1668672', 0.9607366919517517)\n",
      "New Brunswick, NJ: Rutgers University Press. ('SENT_2364188', 0.9606520533561707)\n",
      "New York: Oxford University Press, 1990. ('SENT_2216191', 0.9606196284294128)\n",
      "New Haven, CT: Yale University Press. ('SENT_3414595', 0.9606038331985474)\n",
      "New Haven, CT: Yale University Press. ('SENT_2690977', 0.9603100419044495)\n",
      "New York: Oxford University Press, 1993â€“. ('SENT_2739536', 0.960299015045166)\n",
      "New Haven, CT: Yale University Press. ('SENT_1941287', 0.9601824879646301)\n",
      "New Haven, CT: Yale University Press. ('SENT_2599089', 0.9599466919898987)\n",
      "New Brunswick, NJ: Rutgers University Press. ('SENT_3588027', 0.9597839117050171)\n",
      "New Haven, CT: Yale University Press. ('SENT_1941260', 0.9594319462776184)\n",
      "New Haven, CT: Yale University Press. ('SENT_3275265', 0.9593220353126526)\n",
      "Proceedings of the National Academy of Sciences 98:7879â€“7883. ('SENT_2606244', 0.9592281579971313)\n",
      "Cambridge, MA: Harvard University Press, 1982. ('SENT_1533744', 0.9590833187103271)\n",
      "New Haven, CT: Yale University Press. ('SENT_2885081', 0.9590560793876648)\n",
      "New Haven, CT: Yale University Press. ('SENT_2690459', 0.9590076208114624)\n"
     ]
    }
   ],
   "source": [
    "# 5 Test: printing sentence 9 and getting the most similar ones.\n",
    "if do_tests:\n",
    "    test_sentence_index = 2639533\n",
    "    print get_print(test_sentence_index)\n",
    "    sims = model.docvecs.most_similar('SENT_'+str(test_sentence_index),topn = 30)\n",
    "    print 'similar sentence',len(sims)\n",
    "    print '\\nSIMILAR SENTENCES\\n'\n",
    "    for sim in sims:\n",
    "        print get_print(sim),sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here isa classic example of scientific discovery being concerned with the union of opposites: Kepler â€™s laws and Galileo â€™s laws being both reconciled and superseded bya principle which was both superior to both and yet comprehended both.\n",
      "They also demonstrate both the importance and the failure of law to address this dilemma.\n",
      "It seemed to many both too little and too late.\n",
      "Solution This sketch demonstrates both lcd.ScrollDisplayLeft and lcd.ScrollDisplayRight.\n",
      "Solution This sketch demonstrates both lcd.ScrollDisplayLeft and lcd.ScrollDisplayRight.\n",
      "Still, both of you know what you are talking about.\n",
      "Mr. Johnson sued both Mr. Womack and Hooters.\n",
      "He sums up muchof whatis containedin both of the scatologicalimages thathavebeen examinedhere.\n",
      "They routinely interact with both the courts and corrections.\n",
      "It was both bathetically funny and extremely poignant.\n",
      "Tolerance of Laws That Are Both Underinclusive and Overinclusive Many laws are both underinclusive and overinclusive.\n"
     ]
    }
   ],
   "source": [
    "# 7 Test: iterate over similar sentences\n",
    "# needs the sentences loaded (cell 2)\n",
    "if do_tests:\n",
    "    index = 1983\n",
    "    # len(sentences)\n",
    "    # print sentences[index]\n",
    "    sentence = get_print(index)\n",
    "    print sentence\n",
    "    selected_indices = [index]\n",
    "\n",
    "    for sentence in range(10):\n",
    "        sims = model.docvecs.most_similar('SENT_'+str(index))\n",
    "        while True:\n",
    "            selected = random.choice(sims)\n",
    "            check_index = int(selected[0][5:])\n",
    "            if check_index not in selected_indices:\n",
    "                break\n",
    "        index = check_index\n",
    "        selected_indices.append(index)\n",
    "        print get_print(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 8 Story Treenode class\n",
    "\n",
    "PARENT = -1\n",
    "QUIT = -2\n",
    "NEXT = -3\n",
    "\n",
    "num_similars = 10\n",
    "num_random = 10\n",
    "\n",
    "class LabSentTreeNode:\n",
    "    \n",
    "    def __init__(self, labeledSentence, parent = None):\n",
    "        self.sentence = labeledSentence\n",
    "        self.sentence_index = get_index(self.sentence)\n",
    "        self.similars = self.get_similars()  \n",
    "        self.randoms = self.get_randoms()\n",
    "        self.children = {} # index: SentenceTreeNode\n",
    "        self.selected_child = '' # None\n",
    "        self.parent = parent\n",
    "             \n",
    "    def get_similars(self):\n",
    "        return model.docvecs.most_similar(get_index_tag(self.sentence),topn = num_similars)\n",
    "    \n",
    "    def get_randoms(self):\n",
    "        randoms = []\n",
    "        for index in range(num_random):\n",
    "            rnd_sen = sentences[random.randint(0,len(sentences))]\n",
    "            randoms.append(rnd_sen)     \n",
    "        return randoms\n",
    "                \n",
    "    def print_options(self):\n",
    "        for index, sentence in enumerate(self.similars):\n",
    "            add = '(*)' if get_similar_index(sentence) in self.children else ''\n",
    "            print index, add, get_print(sentence), \"%.3f\" % sentence[1]\n",
    "        if self.parent:\n",
    "            print 'p: ', get_print(self.parent.sentence)\n",
    "        if self.sentence_index < len(sentences) - 2:\n",
    "            print 'n: ', get_print(sentences[self.sentence_index + 1])           \n",
    "        for index,sentence in enumerate(self.randoms):\n",
    "            print 'r'+str(index) +\": \",  get_print(sentence)\n",
    "            \n",
    "    def get_options_html(self):\n",
    "        html = ''\n",
    "        for index, sentence in enumerate(self.similars):\n",
    "            content = pack_into_elem('span','',get_print(sentence))\n",
    "            index = pack_into_elem('span','',str(index)+': ')\n",
    "            sentence_index = get_index(sentence)\n",
    "            index += 'â—ï¸' if sentence_index in added_sentences else ''\n",
    "            index_distance =  locale.format('%d', abs(self.sentence_index - sentence_index), 1)\n",
    "            similar = pack_into_elem('span','sim',(\"%.3f\" % sentence[1]) + ' / ' + str(index_distance))\n",
    "            html += pack_into_elem('div', '', index + content + similar)\n",
    "        if self.parent:\n",
    "            content = pack_into_elem('span','',get_print(self.parent.sentence))\n",
    "            index = pack_into_elem('span','','P: ')\n",
    "            sentence_index = self.parent.sentence_index\n",
    "            index_distance =  locale.format('%d', abs(self.sentence_index - sentence_index), 1)\n",
    "            similarity = get_similarity_by_index(self.sentence_index, self.parent.sentence_index)\n",
    "            similar = pack_into_elem('span','sim',(\"%.3f\" % similarity) + ' / ' + str(index_distance))\n",
    "            html += pack_into_elem('div', 'p', index + content + similar)\n",
    "        if self.sentence_index < len(sentences) - 2:\n",
    "            content = pack_into_elem('span','n',get_print(sentences[self.sentence_index + 1]))\n",
    "            index = pack_into_elem('span','','N: ')\n",
    "            sentence_index = self.sentence_index + 1\n",
    "            index += 'â—ï¸' if sentence_index in added_sentences else ''\n",
    "            index_distance =  locale.format('%d', abs(self.sentence_index - sentence_index), 1)\n",
    "            similarity = get_similarity_by_index(self.sentence_index, self.sentence_index + 1)\n",
    "            similar = pack_into_elem('span','sim',(\"%.3f\" % similarity) + ' / ' + str(index_distance))\n",
    "            html += pack_into_elem('div', 'n', index + content + similar)\n",
    "        for index,sentence in enumerate(self.randoms):\n",
    "            content = pack_into_elem('span','',get_print(sentence))\n",
    "            index = pack_into_elem('span','','R'+str(index)+': ')\n",
    "            sentence_index = get_index(sentence)\n",
    "            index += 'â—ï¸' if sentence_index in added_sentences else ''\n",
    "            index_distance =  locale.format('%d', abs(self.sentence_index - sentence_index), 1)\n",
    "            index_distance =  locale.format('%d', abs(self.sentence_index - sentence_index), 1)\n",
    "            similarity = get_similarity_by_index(self.sentence_index, get_index(sentence))\n",
    "            similar = pack_into_elem('span','sim',(\"%.3f\" % similarity) + ' / ' + str(index_distance))\n",
    "            html += pack_into_elem('div', 'r', index + content + similar)\n",
    "        html += pack_into_elem('div', '', 'Q: Quit ðŸ’£')\n",
    "        return html\n",
    "    \n",
    "    def get_sentence_html(self):\n",
    "        return pack_into_elem('p', 'act', get_print(self.sentence))\n",
    "    \n",
    "    def select_child(self):\n",
    "        u_input = raw_input('Next child: ')\n",
    "        if u_input == 'p':\n",
    "            selected_index = PARENT\n",
    "        elif u_input == 'q':\n",
    "            return None\n",
    "        elif u_input == 'n':\n",
    "            selected_index = NEXT\n",
    "        elif u_input.startswith('r'):\n",
    "            selected_index = 100 + int(u_input[1:])\n",
    "        else:\n",
    "            try:\n",
    "                selected_index = int(u_input)\n",
    "            except ValueError:\n",
    "                return self\n",
    "        if selected_index >= 0 and selected_index < len(self.similars):\n",
    "            lab_sent = get_lab_sent_by_similar(self.similars[selected_index])\n",
    "            child =  LabSentTreeNode(lab_sent, self)\n",
    "            self.children[u_input] = child\n",
    "            self.selected_child = u_input\n",
    "            return child\n",
    "        elif selected_index >= 100 and selected_index < len(self.randoms) + 100:\n",
    "            #print 'random sen'\n",
    "            child =  LabSentTreeNode(self.randoms[selected_index - 100], self)\n",
    "            self.children[u_input] = child\n",
    "            self.selected_child = u_input   \n",
    "            return child\n",
    "        elif selected_index == PARENT and self.parent:\n",
    "            return self.parent\n",
    "        elif selected_index == NEXT:\n",
    "            child =  LabSentTreeNode(sentences[self.sentence_index + 1], self)\n",
    "            self.children[u_input] = child\n",
    "            self.selected_child = u_input   \n",
    "            return child\n",
    "        # a weird number\n",
    "        return self\n",
    "        \n",
    "    def toJSON(self):\n",
    "        children_toJSON = {}\n",
    "        for child_index in self.children:\n",
    "            children_toJSON[child_index] = self.children[child_index].toJSON()\n",
    "            \n",
    "        return {'sentence':get_print(self.sentence),\n",
    "                'index':get_index(self.sentence),\n",
    "               'children':children_toJSON,\n",
    "                'selected_child':self.selected_child\n",
    "               }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 8 Story creator/log helper functions\n",
    "\n",
    "def get_story(root_node):\n",
    "    act_sentence = root_node\n",
    "    story = []\n",
    "    while act_sentence:\n",
    "        story.append(get_print(act_sentence.sentence))\n",
    "        if act_sentence.selected_child  != '':\n",
    "            act_sentence = act_sentence.children[act_sentence.selected_child]\n",
    "        else:\n",
    "            break\n",
    "    return story\n",
    "\n",
    "def log_json(root_node):\n",
    "    with open('log.json','w') as output:\n",
    "        output.write(json.dumps(root_node.toJSON(),indent=2))\n",
    "    \n",
    "def log_story(root_node):\n",
    "    story = get_story(root_node)\n",
    "    with open('story.txt','w') as output:\n",
    "        for l in story:\n",
    "            output.write(l+'\\n')   \n",
    "    \n",
    "def print_story(root_node):\n",
    "    story = get_story(root_node)\n",
    "    for l in story:\n",
    "        print(l)   \n",
    "\n",
    "def dump_story(root_node):\n",
    "    with open('story.dump','w') as dump_file:\n",
    "        pickle.dump(root_node,dump_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“– â­ ðŸ“–\n",
      "Islamic thought reciprocated by dividing the worla into dar-a/-Islam, the region of submission to the will of God.\n",
      "To suppose God, it will be said, is to deny him.\n",
      "He said what Scaring wanted him to say.\n",
      "I do n't know how to say it.\n",
      "If they say that they believe somethingâ€”they do not.\n",
      "After all, you say toe-may-toe, andI say toe-ma-toe.\n",
      "At the same time, it is to say much more.\n",
      "I don â€™t think there is much more to say.\n",
      "I do n't know how to say it.\n",
      "ButI believe that the reality- the truth thatI recognize in suffering asI do n't in comfort and happiness- that the reality of pain is not pain.\n",
      "That is why we speak of truth, not reality.\n",
      "But the honest truth is that we really do not know.\n",
      "Therefore, if you tell people the truth, they are not going to accept what you do.\n",
      "There isa lot of truth to that.\n",
      "Th e doctor 's wife told me the truth.\n",
      "Thou shalt deny God and love truth; thereforeI am an atheist.\n",
      "Th e doctor 's wife told me the truth.\n",
      "Takea look at the following truth table.\n",
      "Next comes the truth procedure of love itself.\n",
      "The World as Will and Representation, Â§27, p. 146-47.\n",
      "Deleuze and Guattari, A Thousand Plateaus, p. 12.\n",
      "This is immediately evident when we consider that silence is never without the recollection of what preceded it, that mourning itself is an act of remembrance, and that the dynamic stasis that conceived silence presupposed the act of remembrance between what is now and what was then.\n",
      "The concept itself is better understood by what it is not, rather than what it is.\n",
      "ðŸ‘‹ðŸ½\n"
     ]
    }
   ],
   "source": [
    "# 9 Story creator\n",
    "\n",
    "load_from_log = True\n",
    "\n",
    "added_sentences = set()  \n",
    "\n",
    "if load_from_log:\n",
    "    with open('story.dump','r') as in_file:    \n",
    "        root_node = pickle.load(in_file)\n",
    "        actual_node = root_node\n",
    "        while actual_node.selected_child != '':\n",
    "            added_sentences.add(actual_node.sentence_index)\n",
    "            actual_node = actual_node.children[actual_node.selected_child]\n",
    "            \n",
    "else:  \n",
    "    sentence = sentences[random.randint(0,len(sentences))]\n",
    "    root_node = LabSentTreeNode(sentence)\n",
    "    actual_node = root_node\n",
    "\n",
    "while actual_node:\n",
    "    clear_output()\n",
    "    log_json(root_node)\n",
    "    log_story(root_node)\n",
    "    dump_story(root_node)\n",
    "    added_sentences.add(actual_node.sentence_index)\n",
    "    display(HTML(pre + actual_node.get_sentence_html() + actual_node.get_options_html()))\n",
    "    time.sleep(0.4)\n",
    "    actual_node = actual_node.select_child()\n",
    "clear_output()\n",
    "print 'ðŸ“– â­ ðŸ“–'\n",
    "print_story(root_node)\n",
    "print 'ðŸ‘‹ðŸ½'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRYING TO TRAIN MORE\n",
    "# RUNS BUT DOESNT EXTEND THE MODEL FILE\n",
    "print len(model.docvecs)\n",
    "line = 'Therefore, if you tell me the truth, they are not going to reject what you say.'\n",
    "ls = gensim.models.doc2vec.LabeledSentence(words=line.split(), tags=['SENT_%s' % len(sentences)])\n",
    "sentences.append(ls)\n",
    "model.train([ls])\n",
    "print len(model.docvecs)\n",
    "print sentences[len(sentences)-1]\n",
    "get_similarity_by_index(len(sentences)-1,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "next to similarity(d1,d2) there is also n_similarity(ds1, ds2), which takes lists.\n",
    "```\n",
    "s_index = 400\n",
    "s = sentences[s_index]\n",
    "res = model.docvecs.most_similar(get_index(s))\n",
    "# print res\n",
    "for s in res:\n",
    "    print get_print(s)\n",
    "    print s[1]\n",
    "    index = get_index(s)\n",
    "    print get_similarity_by_index(s_index,index)\n",
    "    print model.docvecs.n_similarity([s_index],[index])\n",
    "    print '######'\n",
    "    \n",
    "```\n",
    "resulting in:\n",
    "```\n",
    "0.570710778236\n",
    "0.570710771218\n",
    "0.570710771218\n",
    "######\n",
    "I have only been twice; so many things bewildered me.\n",
    "0.563167154789\n",
    "0.563167148085\n",
    "0.563167148085\n",
    "######\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<script>\n",
       "var voices = synth.getVoices();\n",
       "for(var i = 0 ;i < voices.length; i++){\n",
       "console.log(voices[i].name);\n",
       "}\n",
       "\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "html = '''\n",
    "<script>\n",
    "var voices = synth.getVoices();\n",
    "for(var i = 0 ;i < voices.length; i++){\n",
    "console.log(voices[i].name);\n",
    "}\n",
    "\n",
    "</script>\n",
    "'''\n",
    "display(HTML(html))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<script>\n",
       "var synth = window.speechSynthesis;\n",
       "var   voices = synth.getVoices();\n",
       "\n",
       "var text= \"Islamic thought reciprocated by dividing the worla into dar-a/-Islam, the region of submission to the will of God. To suppose God, it will be said, is to deny him. He said what Scaring wanted him to say. I do n't know how to say it. If they say that they believe somethingâ€”they do not. After all, you say toe-may-toe, andI say toe-ma-toe. At the same time, it is to say much more. I don â€™t think there is much more to say. I do n't know how to say it. ButI believe that the reality- the truth thatI recognize in suffering asI do n't in comfort and happiness- that the reality of pain is not pain. That is why we speak of truth, not reality. But the honest truth is that we really do not know. Therefore, if you tell people the truth, they are not going to accept what you do. There isa lot of truth to that. Th e doctor 's wife told me the truth. Thou shalt deny God and love truth; thereforeI am an atheist. Th e doctor 's wife told me the truth. Takea look at the following truth table. Next comes the truth procedure of love itself. The World as Will and Representation, Â§27, p. 146-47. Deleuze and Guattari, A Thousand Plateaus, p. 12. This is immediately evident when we consider that silence is never without the recollection of what preceded it, that mourning itself is an act of remembrance, and that the dynamic stasis that conceived silence presupposed the act of remembrance between what is now and what was then. The concept itself is better understood by what it is not, rather than what it is.\";\n",
       "  var utterThis = new SpeechSynthesisUtterance(text);\n",
       "  var selectedOption = \"Agnes\"\n",
       "  for(i = 0; i < voices.length ; i++) {\n",
       "    if(voices[i].name === selectedOption) {\n",
       "      utterThis.voice = voices[i];\n",
       "    }\n",
       "  }\n",
       "  utterThis.pitch = 1.5;\n",
       "  utterThis.rate = 1.0;\n",
       "  synth.speak(utterThis);\n",
       "\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pitch = 1.5\n",
    "rate = 1.0\n",
    "\n",
    "text = get_story(root_node)\n",
    "var = 'var text= \"'+' '.join(text) + '\";'\n",
    "\n",
    "\n",
    "html = '''\n",
    "<script>\n",
    "var synth = window.speechSynthesis;\n",
    "var   voices = synth.getVoices();\n",
    "\n",
    "'''+ var +'''\n",
    "  var utterThis = new SpeechSynthesisUtterance(text);\n",
    "  var selectedOption = \"Agnes\"\n",
    "  for(i = 0; i < voices.length ; i++) {\n",
    "    if(voices[i].name === selectedOption) {\n",
    "      utterThis.voice = voices[i];\n",
    "    }\n",
    "  }\n",
    "  utterThis.pitch = '''+str(pitch)+''';\n",
    "  utterThis.rate = '''+str(rate)+''';\n",
    "  synth.speak(utterThis);\n",
    "\n",
    "</script>\n",
    "'''\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
