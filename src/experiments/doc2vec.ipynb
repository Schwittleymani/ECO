{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from IPython.display import clear_output\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "from IPython.core.display import display, HTML\n",
    "import locale, os, socket\n",
    "from numpy import int64\n",
    "locale.setlocale(locale.LC_ALL, 'de_DE.utf-8')\n",
    "\n",
    "if not os.path.exists('logs'):\n",
    "    os.makedirs('logs')\n",
    "\n",
    "host = socket.gethostname() \n",
    "if host == 'lyrik':\n",
    "    model_file = '/home/ramin/projects/ECO/src/python/modelbuilder/parsed_v3_valid.doc2vec'\n",
    "    print \"U ARE ON LYRIK\"\n",
    "    # ... move data and model into some convinient folder. so that model/parsed_v3_valid is there and\n",
    "    # NAIL_DATAFIELD_txt/parsed_v3/parsed_v3_valid.txt is there\n",
    "else:\n",
    "    # local\n",
    "    model_file = '../../models/parsed_v3_valid.doc2vec'    \n",
    "\n",
    "if not os.path.isfile(model_file):\n",
    "    print \"MODEL FILE IS NOT THERE. GO AND FIND IT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4266193 sentences\n"
     ]
    }
   ],
   "source": [
    "# 2 Build sentence list (each sentence needs at least 1 tag)\n",
    "\n",
    "if host == 'lyrik':\n",
    "    filename = '/home/marcel/drive/data/eco/NAIL_DATAFIELD_txt/parsed_v3/parsed_v3_valid.txt'\n",
    "else:\n",
    "    # local\n",
    "    filename = '../../data/parsed_v3/parsed_v3_valid.txt' # parsed_v3_all.txt\n",
    "\n",
    "if not os.path.isfile(model_file):\n",
    "    print \"TEXTFILE FILE IS NOT THERE\"    \n",
    "\n",
    "sentences = []\n",
    "from random import shuffle\n",
    "\n",
    "for uid, line in enumerate(open(filename)):\n",
    "    ls = gensim.models.doc2vec.LabeledSentence(words=line.split(), tags=['SENT_%s' % uid])\n",
    "    sentences.append(ls)\n",
    "print len(sentences),'sentences'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded. 4266193 vectors\n"
     ]
    }
   ],
   "source": [
    "# 3 TRAINING OR LOADING the doc2vec model and save it\n",
    "# ALTERNATIVE: LOAD THE MODEL IN THE NEXT CELL\n",
    "\n",
    "# tutorial https://rare-technologies.com/doc2vec-tutorial/\n",
    "# proposes shuffling or learning reate adjustment. we gonna do both\n",
    "# in total 20 epochs\n",
    "# took ca. 6.30 hours\n",
    "\n",
    "# FOR SAFETY REASON, BUILD ONLY WHEN FLAG IS SET\n",
    "\n",
    "train_model = False\n",
    "\n",
    "if train_model:\n",
    "    model = gensim.models.Doc2Vec(alpha=0.025, min_alpha=0.025)  # use fixed learning rate\n",
    "    print('building vocab') \n",
    "    model.build_vocab(sentences)\n",
    "\n",
    "    base_alpha = model.alpha\n",
    "    base_min_alpha = model.min_alpha\n",
    "\n",
    "    for mepoch in range(2):\n",
    "        model.alpha = base_alpha \n",
    "        model.min_alpha = base_min_alpha\n",
    "        for epoch in range(10):\n",
    "            print('epoch',mepoch * 10 + epoch)\n",
    "            model.train(sentences)\n",
    "            model.alpha -= 0.002  # decrease the learning rate\n",
    "            model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "        shuffle(sentences)\n",
    "\n",
    "    # saving the model    \n",
    "    model.save(model_file)\n",
    "    print 'model trained and saved'\n",
    "else:\n",
    "    model = gensim.models.Doc2Vec.load(model_file)\n",
    "    print 'model loaded.',len(model.docvecs), 'vectors'\n",
    "    if len(sentences) != len(model.docvecs):\n",
    "        print 'something is fishy, unequal length: ',len(sentences),'sentences and',len(model.docvecs), 'vectors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4 Tiny helper functions\n",
    "\n",
    "def print_word_list(wl):\n",
    "    str =  ' '.join(wl)\n",
    "    pattern = re.compile('\\s\\P\\s')\n",
    "    shift = 0\n",
    "    for ma in pattern.finditer(str):\n",
    "        str = str[:ma.start(0)-shift]+ma.group(0)[1:]+ str[ma.end(0)-shift:]\n",
    "        shift +=1\n",
    "    if str[-2] == ' ':\n",
    "        str = str[:-2] + str[-1:]\n",
    "    return str\n",
    "\n",
    "def get_print(sentence_or_similar):\n",
    "    if type(sentence_or_similar) is gensim.models.doc2vec.LabeledSentence:\n",
    "        word_list = sentence_or_similar.words\n",
    "    elif type(sentence_or_similar) is int64 or type(sentence_or_similar) is int: # just an index\n",
    "        word_list = sentences[sentence_or_similar].words\n",
    "    else: # TaggedDocument class\n",
    "        word_list = sentences[int(sentence_or_similar[0][5:])][0]\n",
    "    return print_word_list(word_list)\n",
    "    \n",
    "def get_index_tag(sentence):\n",
    "    return sentence.tags[0]\n",
    "\n",
    "def get_index(sentence_or_similar):\n",
    "    if type(sentence_or_similar) is gensim.models.doc2vec.LabeledSentence:\n",
    "        return int64(get_index_tag(sentence_or_similar)[5:])\n",
    "    else:\n",
    "        return int64(sentence_or_similar[0][5:])\n",
    "    \n",
    "def equal_word_lists(index1, index2):\n",
    "    wl1 = sentences[index1].words\n",
    "    wl2 = sentences[index2].words\n",
    "    if len(wl1) != len(wl2):\n",
    "        return False\n",
    "    else:\n",
    "        for i in range(len(wl1)):\n",
    "            if wl1[i] != wl2[i]:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def get_lab_sent_by_similar(similar):\n",
    "    print get_index(similar)\n",
    "    return sentences[get_index(similar)]\n",
    "\n",
    "def get_similarity_by_index(index1, index2):\n",
    "    return model.docvecs.similarity(index1,index2)\n",
    "\n",
    "# HTML Helper\n",
    "def pack_into_elem(tag, clazz, content):\n",
    "    return '<' + tag + ' class=\"' + clazz + '\"> ' + content+ ' </' + tag +'>'\n",
    "\n",
    "pre = '''<style>\n",
    "          .act {font-weight: bold}\n",
    "          .i {color: grey}\n",
    "          .sim {color: orange}\n",
    "          .n {color: blue}\n",
    "          .p {color: red}\n",
    "          .r {color: green}          \n",
    "     </style>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Haven , CT : Yale University Press.\n",
      "similar sentence 30\n",
      "\n",
      "SIMILAR SENTENCES\n",
      "\n",
      "New Haven , CT : Yale University Press. ('SENT_2560974', 0.9662175178527832)\n",
      "New Haven , CT : Yale University Press. ('SENT_2310654', 0.9655746221542358)\n",
      "New Haven , CT : Yale University Press. ('SENT_2724632', 0.9649016857147217)\n",
      "New Haven , CT : Yale University Press. ('SENT_2859928', 0.9643990993499756)\n",
      "New Haven , CT : Yale University Press. ('SENT_1940547', 0.9643778204917908)\n",
      "New Haven , CT : Yale University Press. ('SENT_2130561', 0.9637209177017212)\n",
      "New Haven , CT : Yale University Press. ('SENT_2957770', 0.9633453488349915)\n",
      "New Haven , CT : Yale University Press. ('SENT_313266', 0.963297426700592)\n",
      "New Brunswick , NJ : Rutgers University Press. ('SENT_4208346', 0.9628816843032837)\n",
      "New Haven , CT : Yale University Press. ('SENT_2871939', 0.9628716707229614)\n",
      "New Haven , CT : Yale University Press. ('SENT_2131374', 0.962383508682251)\n",
      "New Haven , CT : Yale University Press. ('SENT_44934', 0.9622904062271118)\n",
      "New Haven , CT : Yale University Press. ('SENT_1723810', 0.9617050886154175)\n",
      "New York : Oxford University Press , 1996. ('SENT_1416359', 0.9614765644073486)\n",
      "New Haven , CT : Yale University Press. ('SENT_2364576', 0.961451530456543)\n",
      "New Haven , CT : Yale University Press. ('SENT_1668672', 0.9607367515563965)\n",
      "New Brunswick , NJ : Rutgers University Press. ('SENT_2364188', 0.9606519937515259)\n",
      "New York : Oxford University Press , 1990. ('SENT_2216191', 0.9606196880340576)\n",
      "New Haven , CT : Yale University Press. ('SENT_3414595', 0.9606039524078369)\n",
      "New Haven , CT : Yale University Press. ('SENT_2690977', 0.9603100419044495)\n",
      "New York : Oxford University Press , 1993â€“. ('SENT_2739536', 0.960299015045166)\n",
      "New Haven , CT : Yale University Press. ('SENT_1941287', 0.9601824879646301)\n",
      "New Haven , CT : Yale University Press. ('SENT_2599089', 0.9599466919898987)\n",
      "New Brunswick , NJ : Rutgers University Press. ('SENT_3588027', 0.9597839117050171)\n",
      "New Haven , CT : Yale University Press. ('SENT_1941260', 0.9594320058822632)\n",
      "New Haven , CT : Yale University Press. ('SENT_3275265', 0.9593220353126526)\n",
      "Proceedings of the National Academy of Sciences 98:7879â€“7883. ('SENT_2606244', 0.9592280983924866)\n",
      "Cambridge , MA : Harvard University Press , 1982. ('SENT_1533744', 0.9590833187103271)\n",
      "New Haven , CT : Yale University Press. ('SENT_2885081', 0.9590561389923096)\n",
      "New Haven , CT : Yale University Press. ('SENT_2690459', 0.9590075612068176)\n",
      "Here is a classic example of scientific discovery being concerned with the union of opposites : Kepler â€™ s laws and Galileo â€™ s laws being both reconciled and superseded by a principle which was both superior to both and yet comprehended both.\n",
      "By inserting the inclusivist conjunction â€˜ both . ..\n",
      "One answer to both these questions is â€˜ no â€™.\n",
      "The answer to both questions is simply no.\n",
      "Providing answers to either of these questions , let alone both , is no easy task.\n",
      "Not surprisingly both Bruno and Castellano ended up murdered.\n",
      "In 1939 Meyerhold and his wife , Zinaida Raikh , were arrested and later both murdered.\n",
      "In 1939 he and his wife were arrested and later murdered.\n",
      "Princeton , N.J. : Princeton University Press , 1989.\n",
      "Princeton , N.J. : Princeton University Press , 1991.\n",
      "Cambridge , MA : Harvard University Press , 1971.\n"
     ]
    }
   ],
   "source": [
    "# 5 Test: printing sentence 9 and getting the most similar ones.\n",
    "\n",
    "do_tests = True\n",
    "\n",
    "if do_tests:\n",
    "    test_sentence_index = 2639533\n",
    "    print get_print(test_sentence_index)\n",
    "    sims = model.docvecs.most_similar('SENT_'+str(test_sentence_index),topn = 30)\n",
    "    print 'similar sentence',len(sims)\n",
    "    print '\\nSIMILAR SENTENCES\\n'\n",
    "    for sim in sims:\n",
    "        print get_print(sim),sim\n",
    "        \n",
    "# 6 Test: iterate over similar sentences\n",
    "# needs the sentences loaded (cell 2)\n",
    "if do_tests:\n",
    "    index = 1983\n",
    "    # len(sentences)\n",
    "    # print sentences[index]\n",
    "    sentence = get_print(index)\n",
    "    print sentence\n",
    "    selected_indices = [index]\n",
    "\n",
    "    for sentence in range(10):\n",
    "        sims = model.docvecs.most_similar('SENT_'+str(index))\n",
    "        while True:\n",
    "            selected = random.choice(sims)\n",
    "            check_index = int(selected[0][5:])\n",
    "            if check_index not in selected_indices:\n",
    "                break\n",
    "        index = check_index\n",
    "        selected_indices.append(index)\n",
    "        print get_print(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 7 Story Treenode class\n",
    "\n",
    "PARENT = -1\n",
    "QUIT = -2\n",
    "NEXT = -3\n",
    "\n",
    "num_similars = 20\n",
    "num_random = 20\n",
    "\n",
    "class LabSentTreeNode:\n",
    "    \n",
    "    def __init__(self, labeledSentence, parent = None):\n",
    "        self.sentence = labeledSentence\n",
    "        self.sentence_index = get_index(self.sentence)\n",
    "        self.similars = self.get_similars()  \n",
    "        self.randoms = self.get_randoms()\n",
    "        self.children = {} # index: SentenceTreeNode\n",
    "        self.selected_child = '' # None\n",
    "        self.parent = parent\n",
    "             \n",
    "    def get_similars(self):\n",
    "        return model.docvecs.most_similar(get_index_tag(self.sentence),topn = num_similars)\n",
    "    \n",
    "    def get_randoms(self):\n",
    "        randoms = []\n",
    "        for index in range(num_random):\n",
    "            rnd_sen = sentences[random.randint(0,len(sentences))]\n",
    "            randoms.append(rnd_sen)     \n",
    "        return randoms\n",
    "                \n",
    "    def print_options(self):\n",
    "        for index, sentence in enumerate(self.similars):\n",
    "            add = '(*)' if get_similar_index(sentence) in self.children else ''\n",
    "            print index, add, get_print(sentence), \"%.3f\" % sentence[1]\n",
    "        if self.parent:\n",
    "            print 'p: ', get_print(self.parent.sentence)\n",
    "        if self.sentence_index < len(sentences) - 2:\n",
    "            print 'n: ', get_print(sentences[self.sentence_index + 1])           \n",
    "        for index,sentence in enumerate(self.randoms):\n",
    "            print 'r'+str(index) +\": \",  get_print(sentence)\n",
    "            \n",
    "    def get_options_html(self):\n",
    "        html = ''\n",
    "        for index, sentence in enumerate(self.similars):\n",
    "            content = pack_into_elem('span','',get_print(sentence))\n",
    "            index = pack_into_elem('span','',str(index)+': ')\n",
    "            sentence_index = get_index(sentence)\n",
    "            index += 'â—ï¸' if sentence_index in added_sentences else ''\n",
    "            index_distance =  locale.format('%d', abs(self.sentence_index - sentence_index), 1)\n",
    "            similar = pack_into_elem('span','sim',(\"%.3f\" % sentence[1]) + ' / ' + str(index_distance) + ' / ' + str(self.sentence_index))\n",
    "            html += pack_into_elem('div', '', index + content + similar)\n",
    "        if self.parent:\n",
    "            content = pack_into_elem('span','',get_print(self.parent.sentence))\n",
    "            index = pack_into_elem('span','','P: ')\n",
    "            sentence_index = self.parent.sentence_index\n",
    "            index_distance =  locale.format('%d', abs(self.sentence_index - sentence_index), 1)\n",
    "            similarity = get_similarity_by_index(self.sentence_index, self.parent.sentence_index)\n",
    "            similar = pack_into_elem('span','sim',(\"%.3f\" % similarity) + ' / ' + str(index_distance) + ' / ' + str(self.sentence_index))\n",
    "            html += pack_into_elem('div', 'p', index + content + similar)\n",
    "        if self.sentence_index < len(sentences) - 2:\n",
    "            content = pack_into_elem('span','n',get_print(sentences[self.sentence_index + 1]))\n",
    "            index = pack_into_elem('span','','N: ')\n",
    "            sentence_index = self.sentence_index + 1\n",
    "            index += 'â—ï¸' if sentence_index in added_sentences else ''\n",
    "            index_distance =  locale.format('%d', abs(self.sentence_index - sentence_index), 1)\n",
    "            similarity = get_similarity_by_index(self.sentence_index, self.sentence_index + 1)\n",
    "            similar = pack_into_elem('span','sim',(\"%.3f\" % similarity) + ' / ' + str(index_distance) + ' / ' + str(self.sentence_index))\n",
    "            html += pack_into_elem('div', 'n', index + content + similar)\n",
    "        for index,sentence in enumerate(self.randoms):\n",
    "            content = pack_into_elem('span','',get_print(sentence))\n",
    "            index = pack_into_elem('span','','R'+str(index)+': ')\n",
    "            sentence_index = get_index(sentence)\n",
    "            index += 'â—ï¸' if sentence_index in added_sentences else ''\n",
    "            index_distance =  locale.format('%d', abs(self.sentence_index - sentence_index), 1)\n",
    "            similarity = get_similarity_by_index(self.sentence_index, get_index(sentence))\n",
    "            similar = pack_into_elem('span','sim',(\"%.3f\" % similarity) + ' / ' + str(index_distance) + ' / ' + str(self.sentence_index))\n",
    "            html += pack_into_elem('div', 'r', index + content + similar)\n",
    "        html += pack_into_elem('div', '', 'Q: Quit ðŸ’£')\n",
    "        return html\n",
    "    \n",
    "    def get_sentence_html(self):\n",
    "        return pack_into_elem('p', 'act', get_print(self.sentence))\n",
    "    \n",
    "    def select_child(self):\n",
    "        u_input = raw_input('Next child: ')\n",
    "        if u_input == 'p':\n",
    "            selected_index = PARENT\n",
    "        elif u_input == 'q':\n",
    "            return None\n",
    "        elif u_input == 'n':\n",
    "            selected_index = NEXT\n",
    "        elif u_input.startswith('r'):\n",
    "            selected_index = 100 + int(u_input[1:])\n",
    "        else:\n",
    "            try:\n",
    "                selected_index = int(u_input)\n",
    "            except ValueError:\n",
    "                return self\n",
    "        if selected_index >= 0 and selected_index < len(self.similars):\n",
    "            lab_sent = get_lab_sent_by_similar(self.similars[selected_index])\n",
    "            child =  LabSentTreeNode(lab_sent, self)\n",
    "            self.children[u_input] = child\n",
    "            self.selected_child = u_input\n",
    "            return child\n",
    "        elif selected_index >= 100 and selected_index < len(self.randoms) + 100:\n",
    "            #print 'random sen'\n",
    "            child =  LabSentTreeNode(self.randoms[selected_index - 100], self)\n",
    "            self.children[u_input] = child\n",
    "            self.selected_child = u_input   \n",
    "            return child\n",
    "        elif selected_index == PARENT and self.parent:\n",
    "            return self.parent\n",
    "        elif selected_index == NEXT:\n",
    "            child =  LabSentTreeNode(sentences[self.sentence_index + 1], self)\n",
    "            self.children[u_input] = child\n",
    "            self.selected_child = u_input   \n",
    "            return child\n",
    "        # a weird number\n",
    "        return self\n",
    "        \n",
    "    def toJSON(self):\n",
    "        children_toJSON = {}\n",
    "        for child_index in self.children:\n",
    "            children_toJSON[child_index] = self.children[child_index].toJSON()\n",
    "            \n",
    "        return {'sentence':get_print(self.sentence),\n",
    "                'index':get_index(self.sentence),\n",
    "               'children':children_toJSON,\n",
    "                'selected_child':self.selected_child\n",
    "               }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 8 Story creator/log helper functions\n",
    "\n",
    "def get_story(root_node):\n",
    "    act_sentence = root_node\n",
    "    story = []\n",
    "    while act_sentence:\n",
    "        story.append(get_print(act_sentence.sentence))\n",
    "        if act_sentence.selected_child  != '':\n",
    "            act_sentence = act_sentence.children[act_sentence.selected_child]\n",
    "        else:\n",
    "            break\n",
    "    return story\n",
    "\n",
    "def log_json(root_node):\n",
    "    with open('logs/log-'+str(number_logs)+'.json','w') as output:\n",
    "        output.write(json.dumps(root_node.toJSON(),indent=2))\n",
    "    \n",
    "def log_story(root_node):\n",
    "    story = get_story(root_node)\n",
    "    with open('logs/story-'+str(number_logs)+'.txt','w') as output:\n",
    "        for l in story:\n",
    "            output.write(l+'\\n')   \n",
    "    \n",
    "def print_story(root_node):\n",
    "    story = get_story(root_node)\n",
    "    for l in story:\n",
    "        print(l)   \n",
    "\n",
    "def dump_story(root_node):\n",
    "    with open('logs/story-'+str(number_logs)+'.dump','w') as dump_file:\n",
    "        pickle.dump(root_node,dump_file)\n",
    "        \n",
    "# one list comprehension!!!\n",
    "def concordance(search_word,sen_range = 0):\n",
    "    sentence_indices = []\n",
    "    for index,sen in enumerate(sentences):\n",
    "        wl = sen[0]\n",
    "        for word in wl:\n",
    "            if word == search_word:\n",
    "                sentence_indices.append(index)\n",
    "#                 print 'o',get_print(index)\n",
    "                for r in range(sen_range):\n",
    "                    # prevent missing element\n",
    "                    sentence_indices.append(index-r)\n",
    "                    sentence_indices.append(index+r)\n",
    "    return sentence_indices\n",
    "\n",
    "def concordance_result(sentences,sen_range = 0):\n",
    "    if len(sentences) < 10:\n",
    "        for sentence in sentences:\n",
    "            print sentence, get_print(sentence)\n",
    "    else:\n",
    "        print len(sentences),'sentences. 5 random ones:'\n",
    "        for i in range(5):\n",
    "            print sentence, get_print(random.choice(sentences))\n",
    "\n",
    "def ask_word_input():\n",
    "    indices = []\n",
    "    while len(indices) == 0:\n",
    "        first_input = raw_input('First input: ')\n",
    "        indices = concordance(first_input)\n",
    "    print len(indices), 'Sentences with',first_input\n",
    "    sentence = sentences[random.choice(indices)]\n",
    "    return sentence\n",
    "\n",
    "# con = concordance('cyber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Race , school integration , and friendship segregation in america.\n",
      "Lesbian , gay and bisexual youth in school.\n",
      "Anthony J. Nocella II is visiting professor in the school of education at Hamline University.\n",
      "This may involve integration of large amounts of information.\n",
      "Neal Curtis is Senior Lecturer in the School of Arts , Communication and Culture at Nottingham Trent University.\n",
      "The imagery celebrates Nature â€™ s power and majesty and invites us to take an unsentimental view of human extinction by focusing on forces beyond our control.\n"
     ]
    }
   ],
   "source": [
    "# An Alternative method of creating stories.\n",
    "# Start with a Start term and end End term.\n",
    "# up to 'inbetweens' sentences inbetween will be added\n",
    "\n",
    "import numpy as np\n",
    "from gensim import matutils\n",
    "\n",
    "inbetweens = 20\n",
    "\n",
    "sentence1 = ask_word_input()\n",
    "print get_print(sentence1)\n",
    "sentence2 = ask_word_input()\n",
    "print get_print(sentence2)\n",
    "\n",
    "vec1 = model.docvecs[get_index_tag(sentence1)]\n",
    "vec2 = model.docvecs[get_index_tag(sentence2)]\n",
    "\n",
    "\n",
    "ps = ([np.linspace(vec1[index],vec2[index],inbetweens) for index,v in enumerate(vec1)])\n",
    "ar = np.ndarray(shape=(inbetweens,100), dtype=float)\n",
    "for ind,v in enumerate(ps):\n",
    "    for i in range(inbetweens):\n",
    "        ar[i][ind] = v[i]\n",
    "\n",
    "story = []\n",
    "topn = 3\n",
    "for index, v in enumerate(ar):\n",
    "    print (inbetweens - index - 1),\n",
    "    dists = np.dot(model.docvecs.doctag_syn0norm, v)\n",
    "    best = matutils.argsort(dists, topn, reverse=True)\n",
    "    ind = 0\n",
    "#     best.forEach(lambda s : print(s))\n",
    "#     print get_print(best[ind])\n",
    "    while best[ind] in story:\n",
    "        ind +=1\n",
    "        if ind == topn:\n",
    "            break\n",
    "\n",
    "    if ind < topn:\n",
    "        # Hard sentence equality comparison\n",
    "        contains = [sen for sen in story if equal_word_lists(best[ind],sen)]\n",
    "        if not contains:\n",
    "            story.append(best[ind])\n",
    "            print '*',\n",
    "    else:\n",
    "        'skipping'\n",
    "\n",
    "clear_output()        \n",
    "for ind in story:\n",
    "    print get_print(ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“– â­ ðŸ“–\n",
      "But as with the procedural powers already outlined , the mutual legal assistance components of the convention aren â€™ t limited to cybercrime investigations.\n",
      "Nonetheless , the direct physical act subjects the killer to legal action.\n",
      "Again , analysis revealed the presence of heptachlor in the tissues of dead birds.\n",
      "So it is with the music of birds.\n",
      "With dignity , with equality , it is not the same.\n",
      "But it is not the same with poetry.\n",
      "And the same is with the threat of nuclear war.\n",
      "Or the same in a case with nuclear reactors.\n",
      "Perhaps it was the same in this case.\n",
      "The same in the case of aesthetic experience.\n",
      "In fact , the opposite is the case.\n",
      "ðŸ‘‹ðŸ½\n"
     ]
    }
   ],
   "source": [
    "#### 9 Story creator\n",
    "\n",
    "load_from_log = True\n",
    "\n",
    "added_sentences = set()  \n",
    "\n",
    "number_logs = max(0,len(filter(lambda file_ : file_.endswith('dump'),\n",
    "                          os.listdir(\"logs\"))) - 1)\n",
    "\n",
    "if load_from_log:\n",
    "    with open('logs/story-'+str(number_logs)+'.dump','r') as in_file:   \n",
    "        root_node = pickle.load(in_file)\n",
    "        actual_node = root_node\n",
    "        while actual_node.selected_child != '':\n",
    "            added_sentences.add(actual_node.sentence_index)\n",
    "            actual_node = actual_node.children[actual_node.selected_child]            \n",
    "else:  \n",
    "    sentence = ask_word_input()\n",
    "    root_node = LabSentTreeNode(sentence)\n",
    "    actual_node = root_node\n",
    "  \n",
    "    print get_print(sentence)\n",
    "while actual_node:\n",
    "    clear_output()\n",
    "    log_json(root_node)\n",
    "    log_story(root_node)\n",
    "    dump_story(root_node)\n",
    "    added_sentences.add(actual_node.sentence_index)\n",
    "    display(HTML(pre + actual_node.get_sentence_html() + actual_node.get_options_html()))\n",
    "    time.sleep(0.4)\n",
    "    actual_node = actual_node.select_child()\n",
    "clear_output()\n",
    "print 'ðŸ“– â­ ðŸ“–'\n",
    "print_story(root_node)\n",
    "print 'ðŸ‘‹ðŸ½'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRYING TO TRAIN MORE\n",
    "# RUNS BUT DOESNT EXTEND THE MODEL FILE\n",
    "# print len(model.docvecs)\n",
    "# line = 'Therefore, if you tell me the truth, they are not going to reject what you say.'\n",
    "# ls = gensim.models.doc2vec.LabeledSentence(words=line.split(), tags=['SENT_%s' % len(sentences)])\n",
    "# sentences.append(ls)\n",
    "# model.train([ls])\n",
    "# print len(model.docvecs)\n",
    "# print sentences[len(sentences)-1]\n",
    "# get_similarity_by_index(len(sentences)-1,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The next part is for speech Synthesis in the browser get your story spoken\n",
    "\n",
    "1. get the english voices out of the browser\n",
    "2. let it speak. tweak some parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1 get the voices out of the browser. js > python :)\n",
    "display(HTML('''\n",
    "<script>\n",
    "voices = window.speechSynthesis.getVoices();\n",
    "eng_voices = Array()\n",
    "for(var i=0; i < voices.length; i++){\n",
    "    if(voices[i].lang == \"en-US\") {\n",
    "        eng_voices.push(voices[i].name);\n",
    "    }\n",
    "}\n",
    "console.log(eng_voices);\n",
    "IPython.notebook.kernel.execute('voices='+eng_voices);'''))\n",
    "print voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pitch = 1.0\n",
    "rate = 1.0\n",
    "voice = 'Junior'\n",
    "\n",
    "text = get_story(root_node)\n",
    "var = 'var text= \"'+' '.join(text) + '\";'\n",
    "\n",
    "html = '''\n",
    "<script>\n",
    "var synth = window.speechSynthesis;\n",
    "var   voices = synth.getVoices();\n",
    "\n",
    "'''+ var +'''\n",
    "  var utterThis = new SpeechSynthesisUtterance(text);\n",
    "  var selectedOption = \"'''+voice+'''\"\n",
    "  for(i = 0; i < voices.length ; i++) {\n",
    "    if(voices[i].name === selectedOption) {\n",
    "      utterThis.voice = voices[i];\n",
    "    }\n",
    "  }\n",
    "  utterThis.pitch = '''+str(pitch)+''';\n",
    "  utterThis.rate = '''+str(rate)+''';\n",
    "  synth.speak(utterThis);\n",
    "\n",
    "</script>\n",
    "'''\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for all sentences with a specific word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_word = 'motherfucker'\n",
    "sentence_indices = []\n",
    "le = len(sentences)\n",
    "\n",
    "for index,sen in enumerate(sentences):\n",
    "    wl = sen[0]\n",
    "    for word in wl:\n",
    "        if word == search_word:\n",
    "            sentence_indices.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(sentence_indices) \n",
    "print(sentences[sentence_indices[2]])\n",
    "print get_print(sentences[sentence_indices[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model accuracy function outputs bullsh..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# not sure what crap output that is...\n",
    "model.accuracy('questions-words.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
